{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Transfer Learning ResNet"
      ],
      "metadata": {
        "id": "ofshPDhiaWjo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yWeNrJWE842"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#parameters\n",
        "batch_size = 128\n",
        "num_classes = 10  #CIFAR-10 has 10 classes\n",
        "epochs = 5\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "MyQpcGida8AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Transformation Pipeline for CIFAR-10\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),     #Resnet expect 224*224 Image\n",
        "    transforms.ToTensor(),      #Convert PIL Image to input tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])      #Normalization for pre-trained models\n",
        "])"
      ],
      "metadata": {
        "id": "Pt1CxJ6xbznB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading CIFAR-10 dataset\n",
        "train_dataset = datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "test_dataset = datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "#DataLoader for Batch Processing\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1pClvXQd_h5",
        "outputId": "606f33cd-b861-452c-fcd9-7360ae37e554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:08<00:00, 20.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load a pretrained ResNet18 model\n",
        "model = models.resnet18(pretrained=True)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "CEibNNsW09cH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c411841-091f-4dae-84b6-a3183d3c12f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 103MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oV9mcZbEm6y6",
        "outputId": "b4a636f1-95fe-4f2a-8ca5-550a14210516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object Module.parameters at 0x7aafd18b5460>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Freeze the early layers (feature extractor)\n",
        "for param in model.parameters():\n",
        "  print(param)\n",
        "  param.requires_grad = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAKsP__xDKuz",
        "outputId": "e82f4243-7684-4465-b232-8ad4668c0579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[[[-1.0419e-02, -6.1356e-03, -1.8098e-03,  ...,  5.6615e-02,\n",
            "            1.7083e-02, -1.2694e-02],\n",
            "          [ 1.1083e-02,  9.5276e-03, -1.0993e-01,  ..., -2.7124e-01,\n",
            "           -1.2907e-01,  3.7424e-03],\n",
            "          [-6.9434e-03,  5.9089e-02,  2.9548e-01,  ...,  5.1972e-01,\n",
            "            2.5632e-01,  6.3573e-02],\n",
            "          ...,\n",
            "          [-2.7535e-02,  1.6045e-02,  7.2595e-02,  ..., -3.3285e-01,\n",
            "           -4.2058e-01, -2.5781e-01],\n",
            "          [ 3.0613e-02,  4.0960e-02,  6.2850e-02,  ...,  4.1384e-01,\n",
            "            3.9359e-01,  1.6606e-01],\n",
            "          [-1.3736e-02, -3.6746e-03, -2.4084e-02,  ..., -1.5070e-01,\n",
            "           -8.2230e-02, -5.7828e-03]],\n",
            "\n",
            "         [[-1.1397e-02, -2.6619e-02, -3.4641e-02,  ...,  3.2521e-02,\n",
            "            6.6221e-04, -2.5743e-02],\n",
            "          [ 4.5687e-02,  3.3603e-02, -1.0453e-01,  ..., -3.1253e-01,\n",
            "           -1.6051e-01, -1.2826e-03],\n",
            "          [-8.3730e-04,  9.8420e-02,  4.0210e-01,  ...,  7.0789e-01,\n",
            "            3.6887e-01,  1.2455e-01],\n",
            "          ...,\n",
            "          [-5.5926e-02, -5.2239e-03,  2.7081e-02,  ..., -4.6178e-01,\n",
            "           -5.7080e-01, -3.6552e-01],\n",
            "          [ 3.2860e-02,  5.5574e-02,  9.9670e-02,  ...,  5.4636e-01,\n",
            "            4.8276e-01,  1.9867e-01],\n",
            "          [ 5.3051e-03,  6.6938e-03, -1.7254e-02,  ..., -1.4822e-01,\n",
            "           -7.7248e-02,  7.2183e-04]],\n",
            "\n",
            "         [[-2.0315e-03, -9.1617e-03,  2.1209e-02,  ...,  8.9177e-02,\n",
            "            3.3655e-02, -2.0102e-02],\n",
            "          [ 1.5398e-02, -1.8648e-02, -1.2591e-01,  ..., -2.5342e-01,\n",
            "           -1.2980e-01, -2.7975e-02],\n",
            "          [ 9.8454e-03,  4.9047e-02,  2.1699e-01,  ...,  3.4872e-01,\n",
            "            1.0433e-01,  1.8413e-02],\n",
            "          ...,\n",
            "          [-2.8356e-02,  1.8404e-02,  9.8647e-02,  ..., -1.1740e-01,\n",
            "           -2.5760e-01, -1.5451e-01],\n",
            "          [ 2.0766e-02, -2.6286e-03, -3.7825e-02,  ...,  2.4141e-01,\n",
            "            2.4345e-01,  1.1796e-01],\n",
            "          [ 7.4684e-04,  7.7677e-04, -1.0050e-02,  ..., -1.4865e-01,\n",
            "           -1.1754e-01, -3.8350e-02]]],\n",
            "\n",
            "\n",
            "        [[[-4.4154e-03, -4.0645e-03,  3.1589e-03,  ..., -3.7026e-02,\n",
            "           -2.5158e-02, -4.7945e-02],\n",
            "          [ 5.1310e-02,  5.3402e-02,  8.0436e-02,  ...,  1.4480e-01,\n",
            "            1.4287e-01,  1.2312e-01],\n",
            "          [-7.3337e-03,  2.1755e-03,  3.7580e-02,  ...,  6.1517e-02,\n",
            "            8.0324e-02,  1.1715e-01],\n",
            "          ...,\n",
            "          [-2.6754e-02, -1.2297e-01, -1.3653e-01,  ..., -1.4068e-01,\n",
            "           -1.1155e-01, -4.9556e-02],\n",
            "          [ 2.3524e-02, -1.7288e-02, -1.1122e-02,  ..., -1.8826e-02,\n",
            "           -2.3320e-02, -2.9474e-02],\n",
            "          [ 2.8689e-02,  2.1659e-02,  4.7888e-02,  ...,  2.5498e-02,\n",
            "            3.5346e-02,  1.1280e-02]],\n",
            "\n",
            "         [[ 4.6919e-04,  1.2153e-02,  4.2035e-02,  ...,  4.6403e-02,\n",
            "            4.0423e-02, -1.4439e-02],\n",
            "          [ 4.3463e-02,  6.8779e-02,  1.3268e-01,  ...,  2.8606e-01,\n",
            "            2.6905e-01,  2.0935e-01],\n",
            "          [-5.7621e-02, -2.2642e-02,  3.0547e-02,  ...,  1.3763e-01,\n",
            "            1.6538e-01,  1.7946e-01],\n",
            "          ...,\n",
            "          [-1.0816e-01, -2.5227e-01, -2.9742e-01,  ..., -2.8503e-01,\n",
            "           -2.1493e-01, -1.0320e-01],\n",
            "          [ 4.0709e-02, -3.2771e-02, -6.3450e-02,  ..., -9.2360e-02,\n",
            "           -6.9876e-02, -4.9841e-02],\n",
            "          [ 8.2942e-02,  8.7580e-02,  1.0111e-01,  ...,  5.2714e-02,\n",
            "            6.0968e-02,  4.1198e-02]],\n",
            "\n",
            "         [[-1.6391e-02, -1.3870e-02,  5.2810e-03,  ...,  4.3698e-02,\n",
            "            2.2707e-02, -4.5983e-02],\n",
            "          [ 3.3202e-02,  4.2014e-02,  9.3500e-02,  ...,  2.6162e-01,\n",
            "            2.2970e-01,  1.6694e-01],\n",
            "          [-4.5987e-02, -1.6365e-02,  2.6811e-02,  ...,  1.4951e-01,\n",
            "            1.3216e-01,  1.3579e-01],\n",
            "          ...,\n",
            "          [-7.2129e-02, -1.8902e-01, -2.3389e-01,  ..., -1.9038e-01,\n",
            "           -1.5609e-01, -7.5974e-02],\n",
            "          [ 5.1161e-02, -2.5815e-02, -6.9357e-02,  ..., -5.8999e-02,\n",
            "           -6.1550e-02, -4.4555e-02],\n",
            "          [ 1.1174e-01,  7.8979e-02,  6.5849e-02,  ...,  3.1617e-02,\n",
            "            2.5221e-02,  7.4257e-03]]],\n",
            "\n",
            "\n",
            "        [[[-7.0826e-08, -6.4306e-08, -7.3806e-08,  ..., -9.8000e-08,\n",
            "           -1.0905e-07, -8.3421e-08],\n",
            "          [-6.1125e-09,  2.0613e-09, -8.0922e-09,  ..., -4.9840e-08,\n",
            "           -4.3836e-08, -3.0538e-09],\n",
            "          [ 7.1953e-08,  7.5616e-08,  5.9282e-08,  ..., -9.7509e-09,\n",
            "           -1.0951e-09,  4.2442e-08],\n",
            "          ...,\n",
            "          [ 9.5889e-08,  1.0039e-07,  7.9817e-08,  ..., -1.7491e-08,\n",
            "           -4.7666e-08, -1.3265e-08],\n",
            "          [ 1.2904e-07,  1.4762e-07,  1.7477e-07,  ...,  1.3233e-07,\n",
            "            1.0628e-07,  9.3316e-08],\n",
            "          [ 1.2558e-07,  1.3644e-07,  1.8431e-07,  ...,  2.1399e-07,\n",
            "            1.7710e-07,  1.7166e-07]],\n",
            "\n",
            "         [[-1.2690e-07, -9.6139e-08, -1.0372e-07,  ..., -1.1808e-07,\n",
            "           -1.3309e-07, -1.0820e-07],\n",
            "          [-5.7412e-08, -2.5055e-08, -3.0115e-08,  ..., -7.2922e-08,\n",
            "           -6.7022e-08, -2.2574e-08],\n",
            "          [ 2.1813e-08,  4.8608e-08,  3.1222e-08,  ..., -1.8694e-08,\n",
            "           -7.9591e-09,  3.9750e-08],\n",
            "          ...,\n",
            "          [ 5.6013e-08,  7.5526e-08,  4.4496e-08,  ..., -4.4128e-08,\n",
            "           -5.9930e-08, -1.8247e-08],\n",
            "          [ 7.7614e-08,  9.8348e-08,  1.0455e-07,  ...,  6.3272e-08,\n",
            "            4.1781e-08,  4.5901e-08],\n",
            "          [ 5.9834e-08,  7.1006e-08,  9.0437e-08,  ...,  1.1654e-07,\n",
            "            8.7550e-08,  9.8837e-08]],\n",
            "\n",
            "         [[-4.3810e-08,  1.3270e-08,  7.8275e-09,  ..., -5.8804e-09,\n",
            "           -2.6217e-08, -1.5649e-08],\n",
            "          [ 4.1700e-08,  1.0778e-07,  1.0946e-07,  ...,  7.6403e-08,\n",
            "            7.1450e-08,  9.7615e-08],\n",
            "          [ 1.0436e-07,  1.6586e-07,  1.5933e-07,  ...,  1.3517e-07,\n",
            "            1.3487e-07,  1.6449e-07],\n",
            "          ...,\n",
            "          [ 9.8763e-08,  1.5072e-07,  1.2547e-07,  ...,  6.8316e-08,\n",
            "            6.8382e-08,  1.1367e-07],\n",
            "          [ 9.1435e-08,  1.3576e-07,  1.3793e-07,  ...,  1.1678e-07,\n",
            "            1.1723e-07,  1.4394e-07],\n",
            "          [ 6.2183e-08,  8.8184e-08,  1.0456e-07,  ...,  1.3941e-07,\n",
            "            1.3333e-07,  1.5844e-07]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-6.1896e-02, -3.0206e-02,  1.9225e-02,  ...,  4.3665e-02,\n",
            "           -2.2114e-02, -4.2214e-02],\n",
            "          [-3.8061e-02,  6.0774e-03,  4.5797e-02,  ...,  9.6029e-02,\n",
            "            5.9254e-02,  2.9958e-02],\n",
            "          [-2.9672e-02,  2.7766e-03,  2.0457e-02,  ...,  5.9828e-02,\n",
            "            4.1422e-02,  2.3134e-02],\n",
            "          ...,\n",
            "          [ 1.1916e-02,  4.5701e-02,  4.4892e-02,  ...,  4.7419e-02,\n",
            "            2.2274e-02, -5.4993e-03],\n",
            "          [-3.2468e-02, -1.2210e-02,  2.2023e-02,  ...,  5.8061e-02,\n",
            "           -7.5033e-03, -5.9736e-02],\n",
            "          [-4.3314e-02, -2.8162e-02, -5.9126e-03,  ...,  8.8460e-02,\n",
            "            8.4406e-03, -5.0019e-02]],\n",
            "\n",
            "         [[-6.1292e-02, -1.4004e-02,  1.7229e-02,  ...,  1.8349e-02,\n",
            "           -3.2708e-02, -4.1060e-02],\n",
            "          [-3.1506e-02,  2.4460e-02,  4.5516e-02,  ...,  6.6806e-02,\n",
            "            4.6687e-02,  3.3248e-02],\n",
            "          [-3.2216e-02,  2.0718e-02,  2.3343e-02,  ...,  3.5265e-02,\n",
            "            3.6478e-02,  3.1291e-02],\n",
            "          ...,\n",
            "          [ 1.7739e-02,  6.1040e-02,  4.8247e-02,  ...,  3.7785e-02,\n",
            "            2.8894e-02,  1.3984e-02],\n",
            "          [-1.0890e-02,  2.2079e-02,  4.2737e-02,  ...,  6.0247e-02,\n",
            "            1.6197e-02, -1.2493e-02],\n",
            "          [-2.2284e-02,  1.3220e-02,  3.0897e-02,  ...,  1.0403e-01,\n",
            "            4.0119e-02, -5.3310e-03]],\n",
            "\n",
            "         [[-8.5322e-02, -4.2603e-02,  6.8145e-03,  ...,  3.0751e-02,\n",
            "           -3.4818e-02, -4.9945e-02],\n",
            "          [-2.9215e-02,  1.8165e-02,  5.1092e-02,  ...,  9.0200e-02,\n",
            "            5.3438e-02,  4.0169e-02],\n",
            "          [-3.9932e-02, -1.1100e-03,  9.6176e-03,  ...,  2.4114e-02,\n",
            "            2.6298e-02,  2.5489e-02],\n",
            "          ...,\n",
            "          [-3.1890e-03,  3.0454e-02,  1.6316e-02,  ...,  5.5054e-03,\n",
            "           -6.2689e-03, -8.4638e-03],\n",
            "          [-2.2995e-02, -2.8211e-03,  2.3203e-02,  ...,  3.5888e-02,\n",
            "           -1.4296e-02, -3.2419e-02],\n",
            "          [-9.8894e-03,  7.0542e-03,  1.0659e-02,  ...,  7.0495e-02,\n",
            "            1.2996e-02, -8.3417e-03]]],\n",
            "\n",
            "\n",
            "        [[[-7.8699e-03,  1.9911e-02,  3.4208e-02,  ...,  2.8694e-02,\n",
            "            1.2820e-02,  1.8142e-02],\n",
            "          [ 8.7942e-03, -3.2875e-02, -3.5713e-02,  ...,  7.2533e-02,\n",
            "            4.5889e-02,  5.2383e-02],\n",
            "          [-3.6122e-02, -1.1878e-01, -1.3767e-01,  ...,  3.3811e-02,\n",
            "            3.7806e-02,  2.6944e-02],\n",
            "          ...,\n",
            "          [ 1.7322e-02,  3.9589e-03, -8.2269e-03,  ...,  2.7543e-03,\n",
            "            1.8313e-02,  1.6057e-02],\n",
            "          [-9.5007e-04,  1.6428e-02,  1.7156e-02,  ...,  3.3672e-03,\n",
            "            2.2857e-02,  6.5783e-04],\n",
            "          [ 6.1727e-03,  2.7145e-02,  1.4340e-02,  ...,  7.5867e-03,\n",
            "            1.8770e-02,  1.5624e-02]],\n",
            "\n",
            "         [[-1.3423e-02, -5.0696e-04,  8.0959e-03,  ..., -6.0963e-03,\n",
            "            9.2341e-03,  1.5751e-02],\n",
            "          [-1.8343e-02, -6.7982e-02, -7.0685e-02,  ...,  2.9855e-02,\n",
            "            2.6264e-02,  2.3773e-02],\n",
            "          [-5.4359e-02, -1.4663e-01, -1.6211e-01,  ...,  1.1781e-02,\n",
            "            3.2477e-02,  1.1980e-02],\n",
            "          ...,\n",
            "          [ 8.3686e-04, -1.7564e-02, -1.9535e-02,  ..., -4.1382e-03,\n",
            "            2.4658e-02,  1.2893e-02],\n",
            "          [-6.3183e-04,  1.1788e-02,  2.4810e-02,  ...,  6.1105e-03,\n",
            "            3.9210e-02,  9.6696e-03],\n",
            "          [-7.1831e-03,  6.6918e-03,  5.2723e-03,  ..., -7.6077e-03,\n",
            "            2.7253e-02,  1.7735e-02]],\n",
            "\n",
            "         [[-2.3753e-04, -4.9343e-03,  2.2991e-03,  ..., -4.7958e-02,\n",
            "           -2.6154e-02, -2.3525e-02],\n",
            "          [-3.3053e-04, -5.1502e-02, -5.9977e-02,  ..., -1.7369e-02,\n",
            "           -2.3337e-02, -3.7312e-02],\n",
            "          [-2.2674e-02, -9.9412e-02, -1.1176e-01,  ..., -1.1725e-02,\n",
            "           -8.3744e-03, -4.0615e-02],\n",
            "          ...,\n",
            "          [ 1.1437e-02, -8.0313e-03, -1.4955e-03,  ..., -3.4133e-02,\n",
            "           -8.7267e-03, -2.3526e-02],\n",
            "          [ 2.9522e-03,  6.7770e-04,  1.9933e-02,  ..., -2.2002e-02,\n",
            "            1.4814e-02, -1.4487e-02],\n",
            "          [-1.9085e-02, -2.9430e-02, -2.3284e-02,  ..., -4.8587e-02,\n",
            "           -1.3049e-02, -2.4368e-02]]],\n",
            "\n",
            "\n",
            "        [[[-3.6296e-02,  7.1996e-03,  1.9100e-02,  ...,  1.9602e-02,\n",
            "            1.4870e-02, -1.7298e-02],\n",
            "          [-1.1061e-02,  8.5665e-02,  1.2667e-01,  ...,  1.3744e-02,\n",
            "           -5.5036e-05, -3.0162e-02],\n",
            "          [ 1.1322e-01,  1.8634e-01,  5.0658e-02,  ..., -1.7333e-01,\n",
            "           -7.2041e-02, -6.2474e-02],\n",
            "          ...,\n",
            "          [-5.3062e-02, -2.5781e-01, -2.6747e-01,  ...,  2.6781e-01,\n",
            "            1.4344e-01,  5.5145e-02],\n",
            "          [-2.1009e-02, -2.9969e-02,  1.0245e-01,  ...,  2.0843e-01,\n",
            "           -4.1518e-03, -3.8118e-02],\n",
            "          [-2.2155e-02,  1.2380e-02,  8.4302e-02,  ..., -4.4992e-02,\n",
            "           -1.4687e-01, -9.0890e-02]],\n",
            "\n",
            "         [[-5.3969e-03,  3.2799e-02,  1.5486e-02,  ..., -7.7451e-03,\n",
            "            3.0229e-03,  1.1216e-03],\n",
            "          [ 6.1723e-02,  1.4899e-01,  1.4645e-01,  ..., -2.8897e-02,\n",
            "           -2.0227e-02, -9.1878e-03],\n",
            "          [ 1.6146e-01,  2.0886e-01, -2.5589e-02,  ..., -2.7278e-01,\n",
            "           -1.0735e-01, -6.2971e-02],\n",
            "          ...,\n",
            "          [-1.3723e-01, -4.0863e-01, -3.8551e-01,  ...,  4.0846e-01,\n",
            "            2.6202e-01,  1.3491e-01],\n",
            "          [-5.9388e-02, -6.1187e-02,  1.4197e-01,  ...,  3.5780e-01,\n",
            "            9.0893e-02, -1.7392e-03],\n",
            "          [ 7.8613e-03,  5.8403e-02,  1.5339e-01,  ...,  4.7045e-02,\n",
            "           -1.0095e-01, -9.7920e-02]],\n",
            "\n",
            "         [[-5.6799e-03,  1.3425e-02, -2.6461e-02,  ...,  4.4881e-03,\n",
            "            2.0666e-03,  1.3902e-02],\n",
            "          [ 6.5943e-03,  4.5181e-02,  6.0260e-02,  ...,  1.4368e-02,\n",
            "           -5.0725e-03,  4.0505e-03],\n",
            "          [ 5.5257e-02,  1.2397e-01,  4.3193e-02,  ..., -1.4486e-01,\n",
            "           -7.4489e-02, -5.7533e-02],\n",
            "          ...,\n",
            "          [-3.1513e-02, -1.6334e-01, -1.5795e-01,  ...,  2.2904e-01,\n",
            "            1.2017e-01,  7.1998e-02],\n",
            "          [-1.0456e-02, -1.1248e-03,  8.4582e-02,  ...,  1.5748e-01,\n",
            "            2.2142e-02, -1.0083e-02],\n",
            "          [-4.8639e-03, -5.0065e-03,  3.6341e-02,  ..., -2.4361e-02,\n",
            "           -7.1195e-02, -6.6788e-02]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 2.3487e-01,  2.6626e-01, -5.1096e-08,  5.1870e-01,  3.4404e-09,\n",
            "         2.2239e-01,  4.2289e-01,  1.3153e-07,  2.5093e-01,  1.5152e-06,\n",
            "         3.1687e-01,  2.5049e-01,  3.7893e-01,  1.0862e-05,  2.7526e-01,\n",
            "         2.3674e-01,  2.4202e-01,  3.9531e-01,  4.6935e-01,  2.9090e-01,\n",
            "         2.7268e-01,  2.7803e-01,  2.9069e-01,  2.0693e-01,  2.5899e-01,\n",
            "         2.7871e-01,  2.9115e-01,  3.1601e-01,  3.8889e-01,  3.0411e-01,\n",
            "         2.6776e-01,  2.1093e-01,  2.8708e-01,  3.3243e-01,  4.2673e-01,\n",
            "         3.7326e-01,  7.4804e-08,  1.9068e-01,  1.4740e-08,  2.2303e-01,\n",
            "         1.7908e-01,  2.4860e-01,  2.7400e-01,  2.5923e-01,  2.9420e-01,\n",
            "         2.9924e-01,  2.2369e-01,  2.6280e-01,  2.2001e-08,  2.6610e-01,\n",
            "         2.2089e-01,  2.8429e-01,  3.3072e-01,  2.2681e-01,  3.6538e-01,\n",
            "         2.1230e-01,  2.3965e-01,  2.4950e-01,  5.2583e-01,  2.4825e-01,\n",
            "         2.9565e-01,  2.5878e-01,  4.8326e-01,  2.6670e-01],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 2.3072e-01,  2.5382e-01, -1.0543e-06, -6.6439e-01, -1.6571e-08,\n",
            "         1.6152e-01,  4.5450e-01, -4.3020e-07,  3.0051e-01, -8.0052e-06,\n",
            "         3.4942e-01,  3.1148e-01, -2.4953e-01, -3.4749e-05,  1.0773e-01,\n",
            "         2.1897e-01,  3.8141e-01, -5.2988e-01, -6.2864e-01,  5.7140e-01,\n",
            "         2.9985e-01,  5.8430e-01,  4.8202e-01,  3.2853e-01,  1.9672e-01,\n",
            "         1.9496e-01,  1.5215e-01,  8.5522e-02,  5.1314e-01,  1.5237e-02,\n",
            "         1.6644e-01,  3.3239e-01,  2.4921e-01,  4.4337e-01, -2.8017e-01,\n",
            "        -2.0385e-02, -2.4507e-07,  3.2134e-01, -4.9152e-08,  2.3777e-01,\n",
            "         2.3291e-01,  3.1527e-01,  4.2776e-01,  2.9313e-01,  2.6379e-01,\n",
            "         6.7598e-01,  4.2910e-01,  3.4566e-01, -8.6909e-08,  2.4729e-01,\n",
            "         3.0316e-01,  6.1577e-01,  3.9835e-01,  3.3207e-01, -4.1219e-01,\n",
            "         3.7807e-01,  1.7895e-01,  2.5748e-01, -4.4908e-01,  2.1306e-01,\n",
            "         5.6934e-01,  5.7274e-01, -4.0238e-01,  2.3406e-01],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 5.7593e-02, -9.5114e-02, -2.0272e-02],\n",
            "          [-7.4556e-02, -7.9931e-01, -2.1284e-01],\n",
            "          [ 6.5571e-02, -9.6534e-02, -1.2111e-02]],\n",
            "\n",
            "         [[-6.9944e-03,  1.4266e-02,  5.5824e-04],\n",
            "          [ 4.1238e-02, -1.6125e-01, -2.3208e-02],\n",
            "          [ 3.2887e-03,  7.1779e-03,  7.1686e-02]],\n",
            "\n",
            "         [[-2.3627e-09, -3.9270e-08, -3.2971e-08],\n",
            "          [ 2.1737e-08,  8.3299e-09,  1.2543e-08],\n",
            "          [ 1.1382e-08,  8.8096e-09,  1.5506e-08]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.6921e-02,  1.8294e-02, -2.9358e-02],\n",
            "          [-9.8615e-02, -4.3645e-02, -5.2717e-02],\n",
            "          [-7.9635e-02,  2.9396e-02,  4.1479e-03]],\n",
            "\n",
            "         [[ 1.6948e-02,  1.3978e-02,  9.6727e-03],\n",
            "          [ 1.4297e-02, -6.6985e-04, -2.2077e-02],\n",
            "          [ 1.2398e-02,  3.5454e-02, -2.2320e-02]],\n",
            "\n",
            "         [[-2.2600e-02, -2.5331e-02, -2.3548e-02],\n",
            "          [ 6.0860e-02, -9.6779e-02,  2.4057e-02],\n",
            "          [-1.2750e-02,  9.2237e-02,  4.0152e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 2.2160e-02,  4.2177e-02, -1.6428e-02],\n",
            "          [-2.9667e-02,  5.6865e-02,  2.5486e-02],\n",
            "          [ 4.3847e-03,  5.1188e-02,  1.0436e-02]],\n",
            "\n",
            "         [[ 2.5342e-02,  5.4374e-02,  5.3888e-02],\n",
            "          [-2.8334e-02, -2.0139e-01, -5.6358e-02],\n",
            "          [ 5.6774e-02,  7.4188e-02,  2.1585e-02]],\n",
            "\n",
            "         [[-3.1458e-08,  3.5335e-08,  5.3791e-08],\n",
            "          [-2.6896e-08,  5.1530e-08,  5.4480e-08],\n",
            "          [-3.8487e-08, -1.1234e-08, -7.5787e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.2754e-01,  4.3552e-02, -6.5607e-02],\n",
            "          [-6.0462e-02,  1.5989e-01, -7.7070e-03],\n",
            "          [-9.4202e-02,  5.0750e-02, -7.8154e-02]],\n",
            "\n",
            "         [[-3.3309e-02,  1.6631e-03, -8.8497e-03],\n",
            "          [ 1.5553e-02, -5.8277e-02, -2.7437e-02],\n",
            "          [ 1.3126e-02, -3.0268e-02, -2.1661e-03]],\n",
            "\n",
            "         [[-4.2313e-03,  3.4517e-02,  3.8193e-03],\n",
            "          [ 5.4317e-02, -1.2457e-02,  3.2900e-02],\n",
            "          [ 2.2000e-04,  1.6040e-02,  1.2764e-01]]],\n",
            "\n",
            "\n",
            "        [[[-3.5247e-02,  8.0748e-03,  2.0353e-02],\n",
            "          [ 1.7344e-02, -2.4320e-02, -1.5511e-04],\n",
            "          [-2.7634e-04,  2.8024e-02, -2.3777e-03]],\n",
            "\n",
            "         [[-2.3741e-02, -3.2057e-03, -5.7059e-03],\n",
            "          [-1.1582e-02,  1.7200e-03,  2.1067e-02],\n",
            "          [ 4.3606e-03, -4.6459e-02, -7.2954e-02]],\n",
            "\n",
            "         [[ 3.1002e-08,  5.3568e-08,  3.1873e-08],\n",
            "          [-1.6063e-08, -1.8072e-08, -1.9508e-09],\n",
            "          [-5.8339e-08, -4.5366e-08, -1.2395e-08]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.9689e-03, -2.6809e-02, -4.3760e-02],\n",
            "          [ 2.4518e-02, -2.8396e-02, -3.5896e-02],\n",
            "          [-1.7883e-04, -2.4661e-02, -2.0085e-02]],\n",
            "\n",
            "         [[ 2.1551e-02,  2.2789e-03, -2.5823e-02],\n",
            "          [ 2.3272e-02, -7.9333e-03, -2.0814e-03],\n",
            "          [-5.7062e-03, -2.6934e-02, -1.4421e-02]],\n",
            "\n",
            "         [[-1.9674e-02,  2.7914e-02, -2.0025e-02],\n",
            "          [ 6.3222e-02, -3.9077e-02, -3.3220e-03],\n",
            "          [-2.7434e-02,  1.1390e-02, -3.1608e-03]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 4.3440e-03, -7.6970e-03, -6.4950e-02],\n",
            "          [ 1.3846e-02, -2.2803e-02, -4.6478e-02],\n",
            "          [ 2.7776e-02,  1.6080e-02, -1.3363e-02]],\n",
            "\n",
            "         [[ 4.7379e-02, -2.4982e-02, -2.7605e-02],\n",
            "          [ 7.0091e-02,  4.2084e-03, -1.0805e-01],\n",
            "          [ 1.7526e-02,  4.5647e-02,  7.8810e-03]],\n",
            "\n",
            "         [[ 2.6680e-09,  2.7671e-08,  2.4702e-08],\n",
            "          [ 6.3905e-09,  4.1020e-08,  3.3631e-08],\n",
            "          [ 5.8335e-09,  1.3334e-08,  9.6604e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.5900e-03,  4.7084e-02, -8.6949e-03],\n",
            "          [-6.3011e-03,  5.9585e-02,  5.8667e-03],\n",
            "          [-2.0255e-02,  4.3285e-02,  4.5094e-03]],\n",
            "\n",
            "         [[ 1.1253e-03, -5.7461e-03, -6.8411e-03],\n",
            "          [ 6.0616e-03,  7.3295e-03, -1.1784e-02],\n",
            "          [-1.1455e-03,  5.1868e-03, -1.9867e-02]],\n",
            "\n",
            "         [[ 1.7529e-02,  4.4606e-02, -2.6595e-02],\n",
            "          [ 2.2102e-02,  4.5857e-02,  2.3347e-02],\n",
            "          [ 1.8052e-02,  5.9689e-02,  1.7129e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.9112e-02,  3.4242e-03, -1.7523e-02],\n",
            "          [-2.3682e-02,  2.2716e-02, -3.8301e-02],\n",
            "          [-1.0308e-02, -4.3802e-03, -2.3582e-02]],\n",
            "\n",
            "         [[-4.9607e-02, -3.2724e-03, -1.5345e-02],\n",
            "          [-1.3524e-02,  5.4842e-02,  1.1187e-02],\n",
            "          [-2.3549e-02, -2.8495e-02, -6.6371e-02]],\n",
            "\n",
            "         [[-4.9804e-08, -2.8211e-08, -2.0583e-08],\n",
            "          [-5.2389e-08, -2.8522e-08, -3.5099e-08],\n",
            "          [-3.2171e-08, -3.4110e-08, -4.3153e-08]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.4487e-03,  2.6532e-02, -1.1202e-02],\n",
            "          [ 7.0925e-03,  3.7903e-02, -3.2481e-02],\n",
            "          [ 4.1381e-02,  3.2329e-02,  2.8309e-03]],\n",
            "\n",
            "         [[-6.5955e-03,  1.6476e-02,  2.1810e-02],\n",
            "          [-1.2293e-02,  2.2310e-02,  1.2645e-02],\n",
            "          [-8.9897e-03,  1.1948e-03, -5.2390e-03]],\n",
            "\n",
            "         [[-2.5295e-03,  7.2689e-02, -7.8046e-03],\n",
            "          [-4.2221e-02,  7.9756e-02, -2.7738e-02],\n",
            "          [ 4.6716e-03, -5.6596e-02, -8.2261e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 5.2235e-02,  3.5231e-03, -3.3131e-02],\n",
            "          [ 3.1048e-02,  1.6193e-02,  1.7283e-02],\n",
            "          [ 1.4446e-02,  2.4302e-02, -1.9689e-03]],\n",
            "\n",
            "         [[-2.4717e-02,  8.3009e-03, -6.1336e-02],\n",
            "          [-1.6134e-02,  5.5323e-02, -6.5029e-02],\n",
            "          [-2.4715e-02,  1.0030e-03,  3.2437e-02]],\n",
            "\n",
            "         [[ 1.8496e-08,  5.2798e-09,  4.1820e-08],\n",
            "          [ 3.7489e-08,  2.5450e-08,  3.0419e-08],\n",
            "          [ 1.1246e-08, -5.6956e-09, -2.0008e-08]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.1194e-03, -4.1052e-02, -1.0002e-02],\n",
            "          [ 2.5924e-02, -6.3819e-02,  1.3366e-02],\n",
            "          [ 2.9751e-02, -7.9476e-03,  1.4007e-02]],\n",
            "\n",
            "         [[-2.5166e-03,  2.2051e-02, -1.9967e-02],\n",
            "          [-5.9436e-02,  4.3872e-02,  2.6832e-02],\n",
            "          [-1.7509e-02,  2.4625e-02,  2.4822e-02]],\n",
            "\n",
            "         [[ 3.5832e-02, -7.0357e-02,  3.9452e-03],\n",
            "          [-2.9835e-02,  9.2727e-02,  1.9336e-02],\n",
            "          [-2.9145e-02, -9.7087e-03, -7.3388e-02]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.3090, 0.2147, 0.2366, 0.4259, 0.5137, 0.2181, 0.2204, 0.2300, 0.2640,\n",
            "        0.2695, 0.2138, 0.4602, 0.2661, 0.2319, 0.3900, 0.2389, 0.2660, 0.3634,\n",
            "        0.3474, 0.2477, 0.3285, 0.5349, 0.6440, 0.2275, 0.4482, 0.3078, 0.2604,\n",
            "        0.4651, 0.2179, 0.2858, 0.3426, 0.4420, 0.4450, 0.4500, 0.5516, 0.5092,\n",
            "        0.2564, 0.2634, 0.5664, 0.6410, 0.2228, 0.1986, 0.2460, 0.2242, 0.2143,\n",
            "        0.1982, 0.6368, 0.3106, 0.5049, 0.2403, 0.3065, 0.3760, 0.3794, 0.4281,\n",
            "        0.2991, 0.3326, 0.2596, 0.3345, 0.2006, 0.4351, 0.1683, 0.5149, 0.2629,\n",
            "        0.3254], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.1657,  0.2420,  0.1780, -0.0431, -0.2053,  0.1598,  0.2929,  0.0912,\n",
            "         0.1116,  0.0884,  0.1104, -0.2035,  0.1539,  0.0857, -0.1094,  0.0654,\n",
            "         0.0766, -0.2067, -0.0212,  0.1396,  0.0401, -0.2827, -0.3257, -0.0035,\n",
            "        -0.4373, -0.1248,  0.1282, -0.0874,  0.1199, -0.0829, -0.5315, -0.0780,\n",
            "        -0.3876, -0.0547, -0.1816, -0.1888,  0.1320,  0.0031, -0.2697, -0.2984,\n",
            "         0.1394,  0.2597,  0.1372,  0.0053,  0.0132,  0.3295, -0.2715, -0.0187,\n",
            "        -0.2467,  0.1579,  0.0165, -0.0890, -0.1903, -0.0787,  0.1700, -0.4832,\n",
            "         0.0619, -0.0677,  0.3125, -0.5064,  0.3138, -0.2617, -0.1545,  0.0063],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 2.5947e-02, -1.0458e-01, -4.7712e-03],\n",
            "          [-8.6223e-02, -3.3021e-01, -1.0275e-01],\n",
            "          [-5.7426e-02, -1.9074e-01, -5.4646e-02]],\n",
            "\n",
            "         [[-1.6951e-02,  2.1384e-02, -2.1074e-03],\n",
            "          [-3.2983e-03,  4.5014e-02, -1.1510e-02],\n",
            "          [-5.9602e-02,  6.4942e-03,  2.9080e-03]],\n",
            "\n",
            "         [[-4.4903e-03,  1.9637e-02,  1.3167e-02],\n",
            "          [ 1.3050e-02, -7.7471e-03,  1.1931e-02],\n",
            "          [ 1.3454e-02,  1.1103e-02,  5.5145e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.2706e-03, -7.7438e-03,  2.0753e-02],\n",
            "          [-4.0024e-02, -4.0383e-02, -3.4821e-02],\n",
            "          [-2.0251e-02, -9.5164e-03,  1.3954e-02]],\n",
            "\n",
            "         [[-2.3430e-03,  3.2303e-02, -4.3342e-03],\n",
            "          [ 8.6194e-03,  1.0553e-02,  1.8074e-03],\n",
            "          [-1.2760e-02, -1.0232e-02,  4.5711e-03]],\n",
            "\n",
            "         [[ 1.5302e-02,  2.1361e-02, -7.0908e-03],\n",
            "          [-1.4221e-02,  4.5979e-02,  2.1369e-02],\n",
            "          [ 3.1312e-02,  6.6428e-02,  2.1465e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 5.3422e-02,  4.0515e-02,  9.6680e-03],\n",
            "          [ 3.2884e-02, -2.3474e-02,  3.4642e-02],\n",
            "          [-1.2861e-02,  5.0066e-02,  5.4579e-02]],\n",
            "\n",
            "         [[ 2.8764e-02,  4.3431e-02,  2.8258e-02],\n",
            "          [ 2.8734e-02, -3.5459e-02, -5.2788e-02],\n",
            "          [-5.5119e-02, -7.1813e-02, -8.2970e-02]],\n",
            "\n",
            "         [[ 9.5293e-02,  1.2549e-01, -6.4001e-02],\n",
            "          [-4.1166e-02, -9.0480e-04,  5.1387e-02],\n",
            "          [-1.1311e-01, -7.9823e-02,  1.4373e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-7.6924e-03,  2.0647e-02,  1.9521e-02],\n",
            "          [-6.7352e-03,  1.2601e-04,  4.8309e-03],\n",
            "          [-6.2405e-03, -9.2119e-03, -2.5806e-04]],\n",
            "\n",
            "         [[-2.6153e-02, -2.4641e-02,  4.0970e-02],\n",
            "          [-1.9164e-02, -1.0160e-02,  3.3163e-02],\n",
            "          [ 5.4200e-03,  9.0485e-04,  6.7799e-04]],\n",
            "\n",
            "         [[ 7.7762e-03,  2.6447e-02,  6.3650e-02],\n",
            "          [-3.0608e-02,  2.4959e-02,  1.2951e-02],\n",
            "          [-2.0938e-02, -7.7342e-03, -3.8790e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.0893e-02, -1.4409e-02,  1.5730e-02],\n",
            "          [ 1.6655e-02,  4.4535e-02,  6.3212e-02],\n",
            "          [ 3.4121e-02,  7.3135e-02,  5.9203e-02]],\n",
            "\n",
            "         [[ 2.3195e-03,  7.7598e-03,  2.0308e-02],\n",
            "          [ 2.0457e-02,  4.0029e-02,  3.4744e-02],\n",
            "          [-4.7356e-02, -3.7286e-02,  1.4542e-02]],\n",
            "\n",
            "         [[-2.2742e-02, -1.9000e-02, -8.4317e-03],\n",
            "          [-9.8759e-04,  2.1510e-02,  6.3959e-03],\n",
            "          [-9.4558e-03,  2.6833e-03, -3.1136e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-7.5787e-03, -1.6056e-02, -6.4204e-04],\n",
            "          [-5.5104e-03,  1.4252e-02,  4.5000e-02],\n",
            "          [-9.2800e-03,  2.2351e-02,  4.1728e-02]],\n",
            "\n",
            "         [[ 2.5705e-02,  4.8207e-02,  7.9145e-02],\n",
            "          [-4.4350e-03,  3.8872e-03,  4.1694e-02],\n",
            "          [ 8.0536e-04, -1.0601e-02,  9.2706e-03]],\n",
            "\n",
            "         [[-3.3892e-02,  9.3543e-03,  4.1746e-02],\n",
            "          [-1.6470e-02,  3.9542e-03,  6.2438e-02],\n",
            "          [-3.1055e-02, -3.6302e-03,  7.0817e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-7.1044e-05, -9.0020e-03, -2.6998e-03],\n",
            "          [ 3.0072e-03,  1.1579e-02,  1.5214e-02],\n",
            "          [ 3.4832e-03,  1.1353e-05,  1.6320e-02]],\n",
            "\n",
            "         [[-2.6334e-02,  2.1967e-02, -6.0039e-02],\n",
            "          [ 4.4519e-02,  1.3203e-01, -9.1163e-03],\n",
            "          [ 5.4242e-02,  1.3726e-01,  2.7454e-02]],\n",
            "\n",
            "         [[ 1.7122e-02,  3.7646e-03,  1.4872e-02],\n",
            "          [ 1.2092e-02,  1.1319e-02,  3.4667e-02],\n",
            "          [ 8.1790e-03, -2.0805e-02,  2.7143e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.0111e-02, -1.0526e-02,  2.8394e-02],\n",
            "          [-2.5112e-02, -2.2196e-02,  3.7229e-02],\n",
            "          [-3.8220e-02, -4.6644e-02,  1.5660e-02]],\n",
            "\n",
            "         [[-2.5913e-03, -2.4307e-02,  1.0611e-02],\n",
            "          [-2.1730e-02, -4.3938e-02, -7.1536e-03],\n",
            "          [-2.5171e-02, -5.9467e-02, -2.5577e-02]],\n",
            "\n",
            "         [[ 2.8652e-02,  2.5850e-04,  1.1416e-03],\n",
            "          [ 3.7812e-02, -1.1271e-03,  9.6027e-03],\n",
            "          [ 3.9350e-02,  1.0134e-02,  1.0449e-02]]],\n",
            "\n",
            "\n",
            "        [[[-7.9305e-03,  7.0872e-03,  2.1412e-02],\n",
            "          [-6.0065e-02,  1.4147e-03,  9.7281e-02],\n",
            "          [-6.0130e-02, -2.1725e-02,  3.6863e-02]],\n",
            "\n",
            "         [[ 2.8024e-02,  2.6183e-02, -2.3027e-02],\n",
            "          [ 5.1900e-02, -2.0588e-03, -1.0940e-01],\n",
            "          [-3.2729e-02, -6.2752e-03,  8.0630e-03]],\n",
            "\n",
            "         [[-1.8062e-02, -1.9510e-02,  4.3163e-02],\n",
            "          [ 4.6080e-02,  2.9494e-02,  4.0844e-02],\n",
            "          [ 5.9607e-03, -6.5891e-03, -6.4623e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.2193e-02,  8.4653e-03,  3.6764e-03],\n",
            "          [ 1.7549e-02,  2.1971e-02, -4.5108e-03],\n",
            "          [ 2.1124e-02,  3.4591e-02, -1.6310e-02]],\n",
            "\n",
            "         [[ 3.8144e-02,  4.8395e-02, -9.5556e-02],\n",
            "          [ 1.8923e-02,  1.1341e-02, -7.6311e-02],\n",
            "          [ 4.7358e-03,  3.2138e-02, -7.4777e-02]],\n",
            "\n",
            "         [[-1.9031e-02, -3.2568e-02, -3.8251e-02],\n",
            "          [ 1.0705e-02,  2.3121e-03, -7.5078e-02],\n",
            "          [ 3.3316e-02,  3.5515e-02, -2.1023e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.3330e-01,  7.4683e-02, -3.8624e-03],\n",
            "          [ 9.1377e-02,  8.2415e-02,  3.9469e-02],\n",
            "          [-1.8265e-02, -5.9943e-02,  8.9354e-02]],\n",
            "\n",
            "         [[ 1.5566e-02, -4.1716e-02,  1.0633e-02],\n",
            "          [ 7.2644e-03,  3.1934e-02,  1.2732e-03],\n",
            "          [-2.0851e-02, -3.7593e-03, -7.0170e-02]],\n",
            "\n",
            "         [[-6.6139e-02,  1.0627e-01,  1.9590e-02],\n",
            "          [ 5.4987e-02, -1.5552e-01, -1.8819e-02],\n",
            "          [-4.2554e-03,  4.4964e-02, -2.4632e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-6.1691e-02, -4.5531e-02, -9.1721e-03],\n",
            "          [ 4.3995e-02,  4.5703e-02, -7.0108e-02],\n",
            "          [ 1.1388e-02,  4.4678e-02, -4.5953e-02]],\n",
            "\n",
            "         [[ 4.3432e-03,  2.3194e-02, -2.1895e-02],\n",
            "          [-8.0216e-02, -5.7606e-02, -9.8455e-03],\n",
            "          [-3.3285e-02, -1.1468e-01, -2.3779e-02]],\n",
            "\n",
            "         [[-6.3785e-02, -2.4485e-02, -4.9061e-02],\n",
            "          [-6.1594e-02,  1.0328e-01,  5.9685e-03],\n",
            "          [ 8.1863e-02, -3.0314e-02, -4.6373e-03]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.2496, 0.2198, 0.2756, 0.6073, 0.2654, 0.2942, 0.1136, 0.4425, 0.2868,\n",
            "        0.2974, 0.2506, 0.4103, 0.4855, 0.3383, 0.4670, 0.1772, 0.2171, 0.5025,\n",
            "        0.2263, 0.3667, 0.4867, 0.4586, 0.4652, 0.2200, 0.1510, 0.2761, 0.3813,\n",
            "        0.2803, 0.2382, 0.3953, 0.3032, 0.3163, 0.2025, 0.2323, 0.2003, 0.1661,\n",
            "        0.4690, 0.3476, 0.3414, 0.2274, 0.2485, 0.2356, 0.2726, 0.4657, 0.3429,\n",
            "        0.2465, 0.4674, 0.2812, 0.6241, 0.4152, 0.3403, 0.4218, 0.1152, 0.2985,\n",
            "        0.5802, 0.2795, 0.4706, 0.4517, 0.4303, 0.2749, 0.3427, 0.1137, 0.5069,\n",
            "        0.4370], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 2.2752e-01,  8.6747e-03, -6.7346e-02, -6.8779e-02,  3.5977e-01,\n",
            "        -2.0167e-01, -4.8431e-05,  2.3735e-02,  3.9549e-01,  3.7079e-02,\n",
            "         6.8793e-03,  2.7578e-01, -7.0272e-02, -2.3970e-01, -8.1753e-02,\n",
            "        -9.4132e-02, -1.4544e-01,  3.7301e-02, -3.6174e-01, -3.9561e-01,\n",
            "        -4.0789e-01,  3.5559e-03, -2.7878e-01, -3.5299e-02, -7.0281e-02,\n",
            "         2.1005e-01, -4.6362e-03, -1.9665e-01, -2.8066e-01, -1.6540e-02,\n",
            "         2.6452e-01, -8.9359e-02, -2.1046e-01, -1.3026e-01,  1.7215e-01,\n",
            "         5.3403e-02, -2.2295e-01, -4.8033e-02,  2.4572e-01,  2.0950e-01,\n",
            "         1.6220e-01,  1.1370e-01,  1.1457e-01, -1.4870e-01, -3.2150e-02,\n",
            "        -3.0549e-01,  4.9125e-01,  1.0873e-01,  1.2779e-02,  1.0044e-01,\n",
            "         4.1553e-01, -1.4710e-02,  2.3922e-02,  9.9812e-02, -1.7273e-01,\n",
            "         1.0078e-01, -1.4564e-01, -2.2735e-01,  1.3637e-01,  2.0127e-01,\n",
            "        -5.7430e-02,  2.3530e-01, -1.1299e-01,  3.0933e-01],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 1.9712e-02, -5.2562e-03, -3.7619e-03],\n",
            "          [-1.9635e-02, -1.2336e-02, -3.5196e-02],\n",
            "          [ 5.0761e-02,  7.5668e-02,  4.3344e-02]],\n",
            "\n",
            "         [[ 1.4160e-02, -8.6094e-03, -1.0541e-02],\n",
            "          [-4.2586e-02, -2.3814e-02, -5.4694e-02],\n",
            "          [-1.4018e-03,  4.6720e-02,  5.0898e-02]],\n",
            "\n",
            "         [[ 2.1559e-02,  4.1633e-03, -9.7118e-03],\n",
            "          [-9.3201e-03, -2.5432e-02, -2.8274e-02],\n",
            "          [-3.0107e-02, -4.8230e-02, -2.6001e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.4300e-03,  9.1875e-02,  3.1938e-03],\n",
            "          [-1.7945e-02,  5.7266e-02, -8.4098e-03],\n",
            "          [-3.4961e-02, -2.3296e-02, -3.5089e-02]],\n",
            "\n",
            "         [[ 2.5603e-02, -3.1689e-02, -5.4160e-02],\n",
            "          [ 6.9736e-02, -1.0716e-02, -6.8034e-02],\n",
            "          [ 3.5578e-02,  3.4749e-02, -1.9334e-02]],\n",
            "\n",
            "         [[-6.5420e-02, -4.6427e-03, -2.3362e-02],\n",
            "          [ 7.5833e-02,  9.1174e-03, -4.9701e-02],\n",
            "          [ 6.2944e-02, -9.8735e-02,  3.3158e-02]]],\n",
            "\n",
            "\n",
            "        [[[-9.0557e-03, -3.0753e-02,  1.1953e-02],\n",
            "          [-3.2539e-02, -6.2846e-03, -2.0235e-02],\n",
            "          [ 4.7996e-03, -2.1462e-02, -4.1557e-03]],\n",
            "\n",
            "         [[ 1.7163e-02, -2.3303e-03,  7.3972e-02],\n",
            "          [-3.2105e-02, -7.7536e-02, -1.2648e-02],\n",
            "          [ 3.8985e-02, -4.3170e-02,  1.0904e-02]],\n",
            "\n",
            "         [[-2.9643e-02, -5.8534e-02, -5.9736e-02],\n",
            "          [-2.9437e-02, -3.6441e-02, -1.2380e-02],\n",
            "          [-2.2775e-02, -2.4485e-03, -1.6124e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.6830e-02,  1.4267e-02,  6.2658e-02],\n",
            "          [ 3.0585e-04, -5.3241e-03,  3.2786e-03],\n",
            "          [ 2.1097e-02, -2.3189e-02,  1.2102e-02]],\n",
            "\n",
            "         [[-6.1182e-02, -2.9227e-02,  2.0036e-02],\n",
            "          [-7.6089e-02, -7.7057e-02,  8.6544e-02],\n",
            "          [-3.9228e-02, -3.2361e-02, -8.8970e-02]],\n",
            "\n",
            "         [[-1.3372e-01,  8.8362e-02,  8.3836e-02],\n",
            "          [-1.1688e-02,  4.3156e-01, -3.3629e-03],\n",
            "          [-2.3925e-02, -1.0092e-01, -1.0184e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 8.0165e-02,  4.3042e-02,  2.7325e-03],\n",
            "          [ 3.5269e-02, -1.5504e-02, -3.5011e-02],\n",
            "          [-1.7164e-02, -2.6827e-02, -3.3946e-02]],\n",
            "\n",
            "         [[ 4.5439e-02,  5.1585e-02,  1.8321e-02],\n",
            "          [-3.9647e-02,  2.3956e-02, -2.6609e-02],\n",
            "          [-3.0358e-02, -6.4729e-02,  2.5834e-02]],\n",
            "\n",
            "         [[ 3.8105e-02,  4.0986e-02,  4.1005e-02],\n",
            "          [ 1.7584e-02, -1.6494e-02, -3.2716e-02],\n",
            "          [ 5.5886e-03, -1.7068e-02, -3.0605e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.3694e-01, -1.4074e-01,  5.1423e-02],\n",
            "          [-1.2521e-01, -1.3128e-01,  7.5733e-02],\n",
            "          [-4.5032e-02, -1.7081e-02,  7.1252e-02]],\n",
            "\n",
            "         [[ 6.3381e-02,  1.5874e-02, -2.7322e-02],\n",
            "          [ 8.0356e-02,  3.6104e-02, -2.8506e-02],\n",
            "          [ 2.6638e-02,  2.2021e-02,  3.2345e-02]],\n",
            "\n",
            "         [[-1.2068e-03, -4.6179e-02, -1.5351e-02],\n",
            "          [-1.1276e-02,  1.9200e-02,  3.4336e-02],\n",
            "          [ 1.6540e-02, -7.8592e-03, -2.5392e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 3.3384e-02,  6.9963e-02,  1.0745e-02],\n",
            "          [-1.7518e-02, -5.3524e-02, -6.4960e-02],\n",
            "          [ 3.4248e-04, -4.5557e-02, -4.7336e-02]],\n",
            "\n",
            "         [[-5.1031e-03,  7.9784e-03, -8.6553e-04],\n",
            "          [-1.6557e-03,  1.4661e-02,  5.3365e-03],\n",
            "          [-3.1784e-02, -6.6940e-02, -4.6889e-02]],\n",
            "\n",
            "         [[-1.1775e-02,  7.2759e-03,  7.6622e-03],\n",
            "          [-6.1288e-02, -5.2078e-02, -4.5152e-02],\n",
            "          [-8.6584e-02, -9.7381e-02, -1.0405e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.1243e-02,  6.2456e-02,  2.5188e-02],\n",
            "          [-2.2911e-02, -2.1100e-03, -2.7573e-02],\n",
            "          [ 4.6557e-02,  6.4980e-02,  3.1879e-02]],\n",
            "\n",
            "         [[ 6.2867e-03,  2.4255e-02,  8.9674e-02],\n",
            "          [-7.7718e-03, -5.4311e-02, -4.6843e-02],\n",
            "          [-6.7499e-03, -6.6857e-02, -4.9842e-02]],\n",
            "\n",
            "         [[ 4.7326e-03, -3.9533e-02,  1.1500e-03],\n",
            "          [-2.7957e-02, -1.3466e-01, -6.0753e-02],\n",
            "          [-3.2010e-03,  7.2213e-02,  1.1009e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 2.3763e-02, -1.7876e-02, -7.4843e-03],\n",
            "          [ 1.6239e-02,  5.4479e-04, -3.3735e-02],\n",
            "          [-2.2854e-02, -1.4316e-03,  1.1010e-02]],\n",
            "\n",
            "         [[ 5.2277e-03, -2.5941e-03,  5.9594e-03],\n",
            "          [-2.9058e-03, -7.3409e-03,  3.0652e-02],\n",
            "          [ 7.5540e-02,  6.6445e-03,  2.5518e-03]],\n",
            "\n",
            "         [[-6.5970e-02, -4.1286e-02, -3.0278e-02],\n",
            "          [-3.5108e-02, -3.9099e-02, -1.6818e-02],\n",
            "          [-1.0224e-02, -8.6995e-03, -5.9939e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.1233e-02, -2.4559e-02, -7.4436e-03],\n",
            "          [-4.3734e-03, -3.2864e-02, -3.3453e-02],\n",
            "          [ 8.9269e-03, -1.7646e-02,  3.8375e-04]],\n",
            "\n",
            "         [[-7.8930e-02, -7.2940e-02, -6.7911e-02],\n",
            "          [-8.4146e-02, -8.3657e-02,  5.3666e-02],\n",
            "          [-3.5577e-02, -3.6835e-02,  5.8987e-03]],\n",
            "\n",
            "         [[ 8.3767e-02,  8.0476e-05,  7.2164e-02],\n",
            "          [-6.4219e-02, -1.2661e-01,  4.6026e-02],\n",
            "          [ 9.3033e-02, -4.7521e-02,  3.6777e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 4.1012e-02,  1.3361e-03, -5.8616e-02],\n",
            "          [ 4.2461e-02,  2.9437e-03, -2.0445e-02],\n",
            "          [ 7.6097e-02,  5.2504e-02, -5.5636e-03]],\n",
            "\n",
            "         [[ 2.2046e-02,  4.0888e-03,  1.4645e-02],\n",
            "          [-7.7532e-02, -1.1912e-01, -7.0892e-02],\n",
            "          [-1.0618e-02, -3.2121e-02, -2.3969e-02]],\n",
            "\n",
            "         [[-2.1612e-02, -2.6110e-03, -3.1664e-02],\n",
            "          [-3.2892e-02, -3.9771e-02, -5.1463e-02],\n",
            "          [-2.6150e-02, -3.6554e-02, -2.3315e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.4600e-03,  8.4181e-02,  2.3199e-02],\n",
            "          [ 5.7595e-02,  1.3036e-01,  3.2172e-02],\n",
            "          [-2.2774e-03,  4.2065e-02, -4.8619e-02]],\n",
            "\n",
            "         [[ 3.1533e-02, -4.3655e-02,  2.0361e-02],\n",
            "          [ 3.9973e-03, -5.1430e-02, -6.3839e-02],\n",
            "          [ 6.4002e-03,  4.5347e-02,  4.7346e-02]],\n",
            "\n",
            "         [[-9.1818e-02,  1.0264e-02,  9.6565e-02],\n",
            "          [-2.1635e-03, -2.3452e-02, -5.9038e-02],\n",
            "          [ 1.9402e-02,  2.8854e-02, -9.6113e-02]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.3910, 0.4375, 0.3746, 0.3990, 0.3404, 0.3503, 0.2618, 0.2707, 0.2865,\n",
            "        0.4308, 0.1895, 0.3041, 0.3837, 0.2944, 0.2105, 0.3304, 0.2943, 0.2887,\n",
            "        0.2060, 0.4627, 0.2335, 0.1831, 0.4489, 0.2830, 0.3389, 0.2997, 0.3503,\n",
            "        0.2735, 0.3908, 0.2817, 0.2636, 0.4462, 0.3282, 0.3776, 0.4471, 0.3878,\n",
            "        0.2516, 0.3172, 0.3661, 0.3166, 0.3818, 0.3128, 0.2274, 0.3627, 0.2902,\n",
            "        0.2381, 0.2988, 0.2469, 0.3840, 0.2886, 0.3197, 0.2879, 0.3218, 0.4559,\n",
            "        0.3500, 0.2420, 0.3396, 0.3519, 0.3839, 0.3806, 0.4039, 0.2826, 0.4594,\n",
            "        0.3342], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0997, -0.4755, -0.0474, -0.2698, -0.0834, -0.0072,  0.0474,  0.1022,\n",
            "        -0.0170, -0.1471,  0.2307,  0.1447, -0.1775,  0.0273,  0.1559, -0.1836,\n",
            "         0.1238, -0.1522,  0.0554, -0.2881, -0.2606,  0.2316, -0.3242, -0.0219,\n",
            "        -0.2645,  0.0576, -0.2465,  0.0481, -0.3530,  0.0950, -0.1862, -0.1707,\n",
            "        -0.0161, -0.2604, -0.3145, -0.1083,  0.0659, -0.1427, -0.0570, -0.0076,\n",
            "        -0.3006, -0.0744, -0.0683, -0.1104,  0.0253,  0.0489, -0.2515,  0.1150,\n",
            "        -0.3783,  0.0846, -0.0368,  0.1439, -0.0468, -0.3087, -0.0240,  0.1397,\n",
            "        -0.0908, -0.1795, -0.1129, -0.0793, -0.1491,  0.0594, -0.4433, -0.0138],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-2.1574e-02, -4.5688e-03,  4.5483e-03],\n",
            "          [-8.1870e-03,  4.1740e-02,  2.3010e-02],\n",
            "          [-8.9283e-03,  5.7352e-02,  2.9818e-02]],\n",
            "\n",
            "         [[ 5.8627e-02,  4.2864e-02,  4.4912e-02],\n",
            "          [ 2.2281e-02, -1.2969e-02,  7.6099e-03],\n",
            "          [ 4.5373e-02,  3.0712e-02,  3.7700e-02]],\n",
            "\n",
            "         [[-1.5456e-02, -3.8692e-02, -4.6010e-02],\n",
            "          [-2.3123e-02,  2.8293e-02,  4.7790e-03],\n",
            "          [-2.0328e-02,  1.3756e-02,  2.5883e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.1302e-02,  4.2291e-02,  5.7833e-02],\n",
            "          [ 4.5210e-02,  5.5850e-02,  1.4318e-02],\n",
            "          [ 1.4241e-02,  1.7968e-02,  1.4344e-02]],\n",
            "\n",
            "         [[ 4.6012e-03,  1.2566e-02,  4.8931e-02],\n",
            "          [-6.5754e-03, -2.6431e-02,  1.5855e-02],\n",
            "          [ 1.3192e-02,  1.9011e-02,  1.3842e-02]],\n",
            "\n",
            "         [[ 6.1983e-02,  6.9919e-02,  6.1035e-02],\n",
            "          [ 6.1253e-02,  9.9557e-02,  5.9060e-02],\n",
            "          [ 5.8298e-02,  8.1652e-02,  8.1499e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.0088e-02, -1.2959e-02,  9.7798e-03],\n",
            "          [ 5.5408e-02,  4.3501e-02,  5.6983e-02],\n",
            "          [ 5.3427e-02,  3.5118e-02,  3.6782e-02]],\n",
            "\n",
            "         [[ 2.4442e-03, -3.0207e-02, -1.0377e-02],\n",
            "          [-4.5297e-02, -4.5318e-02,  5.4623e-03],\n",
            "          [-4.4762e-02, -1.5508e-02,  6.9745e-03]],\n",
            "\n",
            "         [[ 3.9658e-02,  3.6838e-02,  5.8796e-03],\n",
            "          [ 2.3207e-02,  3.9240e-03, -2.0887e-02],\n",
            "          [-1.4829e-02,  5.3606e-03,  1.7404e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.2160e-02,  5.9042e-02,  4.8433e-02],\n",
            "          [-2.6464e-02, -8.0667e-03, -1.0359e-02],\n",
            "          [-2.6699e-02, -9.5411e-03, -2.8902e-02]],\n",
            "\n",
            "         [[-2.9235e-02, -3.9078e-02, -4.4955e-02],\n",
            "          [-2.0346e-02, -4.4891e-02, -3.7477e-02],\n",
            "          [ 1.9653e-02, -1.5562e-03, -5.8245e-03]],\n",
            "\n",
            "         [[-5.0696e-02, -4.8902e-02,  9.1631e-03],\n",
            "          [ 5.1668e-03,  2.0509e-02,  6.6874e-02],\n",
            "          [ 2.8934e-02,  4.6717e-02,  2.1371e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 2.1744e-02, -2.8354e-02, -3.2557e-02],\n",
            "          [ 3.0519e-02,  1.8536e-02,  1.5244e-02],\n",
            "          [ 1.3832e-03,  1.7051e-02,  3.2020e-02]],\n",
            "\n",
            "         [[-3.6293e-02,  1.0914e-02,  4.5371e-02],\n",
            "          [ 1.3399e-02,  6.4272e-02,  8.8210e-02],\n",
            "          [ 4.6697e-02,  9.9653e-02,  8.7606e-02]],\n",
            "\n",
            "         [[-2.4336e-02, -2.9627e-02,  1.9537e-02],\n",
            "          [-3.3412e-02, -2.2290e-02, -2.8879e-02],\n",
            "          [ 1.4765e-02,  1.7234e-02, -1.8185e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.9859e-02, -7.1075e-02, -5.8546e-02],\n",
            "          [ 2.2902e-02,  1.1184e-02, -2.3654e-02],\n",
            "          [ 8.1897e-02,  1.1996e-01,  9.3242e-02]],\n",
            "\n",
            "         [[ 3.1984e-02,  7.4931e-02,  6.6020e-02],\n",
            "          [ 2.8490e-02,  1.1931e-01,  1.2100e-01],\n",
            "          [ 7.9259e-04,  4.3812e-02,  4.4648e-02]],\n",
            "\n",
            "         [[ 3.2748e-02,  4.1444e-02, -8.1932e-03],\n",
            "          [ 4.5541e-02,  2.9426e-02, -8.5440e-03],\n",
            "          [ 1.1634e-04,  1.8045e-03,  1.4826e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-4.4144e-02, -8.3106e-02, -5.3073e-02],\n",
            "          [ 3.2124e-02,  1.0286e-02,  2.4409e-02],\n",
            "          [ 6.1606e-03, -1.9455e-02,  4.0534e-02]],\n",
            "\n",
            "         [[ 5.6026e-04,  9.6961e-03,  2.5010e-03],\n",
            "          [ 7.1679e-03, -1.7535e-02, -2.3857e-02],\n",
            "          [-9.8745e-03, -1.8550e-02,  1.7301e-03]],\n",
            "\n",
            "         [[ 4.3882e-03,  4.2049e-02,  7.5950e-02],\n",
            "          [-6.5610e-02, -3.6130e-02, -1.9404e-02],\n",
            "          [-3.8091e-02, -2.6749e-02, -1.3865e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.5593e-02, -4.6050e-02, -2.2809e-02],\n",
            "          [-9.7648e-03,  2.4910e-03,  2.4503e-02],\n",
            "          [ 2.0381e-02,  5.2393e-02,  6.9019e-02]],\n",
            "\n",
            "         [[ 9.3306e-04,  1.2483e-02, -1.1817e-02],\n",
            "          [-1.2627e-02, -1.8756e-02, -1.4144e-03],\n",
            "          [-5.2490e-03, -4.6126e-03, -1.3224e-02]],\n",
            "\n",
            "         [[ 7.4689e-04, -1.0135e-02, -7.8264e-03],\n",
            "          [ 1.2491e-02, -2.5865e-02,  4.0514e-02],\n",
            "          [ 5.8855e-03,  4.5990e-02,  1.0651e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.2262e-02, -1.5378e-02,  1.3862e-03],\n",
            "          [ 4.1166e-02, -2.4944e-02, -2.6686e-02],\n",
            "          [-1.7423e-02,  5.2690e-03, -2.1861e-02]],\n",
            "\n",
            "         [[-3.1207e-02, -3.3025e-02,  2.2114e-02],\n",
            "          [-2.4009e-02,  1.2988e-02,  2.2430e-02],\n",
            "          [ 1.0332e-02,  4.3601e-03,  4.7321e-03]],\n",
            "\n",
            "         [[ 2.0182e-02,  6.1569e-02, -2.8771e-02],\n",
            "          [ 5.8231e-02,  4.6767e-02, -2.8417e-05],\n",
            "          [ 3.7545e-02, -4.5886e-02,  1.5849e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.0431e-03, -3.6082e-03,  7.1986e-03],\n",
            "          [ 2.4895e-02,  6.1671e-03, -3.2427e-02],\n",
            "          [ 7.2338e-03,  2.2406e-03, -5.3330e-02]],\n",
            "\n",
            "         [[ 2.8072e-02, -1.0571e-02, -1.3854e-02],\n",
            "          [-1.0879e-02,  6.1929e-03, -5.6713e-03],\n",
            "          [-2.6083e-02,  8.1861e-03, -3.2873e-02]],\n",
            "\n",
            "         [[-3.1032e-02, -6.0485e-02, -2.5583e-02],\n",
            "          [-4.6239e-02, -2.2805e-02, -7.7678e-03],\n",
            "          [-9.4698e-03,  4.0247e-03, -4.8637e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 2.3128e-02, -5.6038e-02, -3.4572e-02],\n",
            "          [ 1.0638e-03,  5.7929e-02, -7.6970e-03],\n",
            "          [-3.0103e-02,  3.5573e-02, -1.8143e-02]],\n",
            "\n",
            "         [[ 9.6840e-02, -1.1186e-01, -7.8766e-02],\n",
            "          [-1.0444e-01, -1.0851e-01, -1.9553e-01],\n",
            "          [-1.1986e-01, -7.1474e-02,  3.6750e-02]],\n",
            "\n",
            "         [[-2.2194e-02,  6.0298e-03,  5.6914e-02],\n",
            "          [-4.8342e-02,  7.8893e-02, -5.1026e-02],\n",
            "          [-5.1294e-02, -5.7434e-02, -1.9178e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.4896e-02, -8.1267e-02,  5.1794e-02],\n",
            "          [-8.3985e-02, -5.7778e-02,  6.7891e-02],\n",
            "          [ 2.3837e-02,  3.8954e-02,  4.1141e-02]],\n",
            "\n",
            "         [[ 4.6446e-03,  2.7367e-02, -2.3154e-02],\n",
            "          [ 2.0675e-02,  2.3429e-02,  6.4380e-04],\n",
            "          [-5.2222e-02, -1.4854e-02, -2.5150e-02]],\n",
            "\n",
            "         [[ 2.1291e-02,  1.2736e-02,  8.4553e-03],\n",
            "          [-8.2932e-02,  7.2067e-02,  1.3107e-01],\n",
            "          [ 8.5491e-03,  1.3677e-01,  3.9867e-02]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.2560, 0.5690, 0.4042, 0.5130, 0.2178, 0.4940, 0.3315, 0.5510, 0.4354,\n",
            "        0.5291, 0.2081, 0.4735, 0.5945, 0.5645, 0.2761, 0.2571, 0.4853, 0.6240,\n",
            "        0.4370, 0.2308, 0.4970, 0.3157, 0.5706, 0.2162, 0.1932, 0.1448, 0.2218,\n",
            "        0.2389, 0.5871, 0.3501, 0.4109, 0.3199, 0.5808, 0.3281, 0.2723, 0.1971,\n",
            "        0.6139, 0.4075, 0.6304, 0.3874, 0.7605, 0.2111, 0.3071, 0.4603, 0.3099,\n",
            "        0.1914, 0.4431, 0.2537, 0.5745, 0.6459, 0.3914, 0.3090, 0.6782, 0.1937,\n",
            "        0.5814, 0.2570, 0.3514, 0.2124, 0.5794, 0.3415, 0.2051, 0.0715, 0.4090,\n",
            "        0.4416], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1778, -0.1287,  0.0349, -0.1452,  0.1864, -0.1413, -0.4201, -0.1334,\n",
            "         0.2183, -0.1912,  0.0311, -0.0235, -0.1724, -0.0274, -0.0295, -0.1031,\n",
            "         0.0047,  0.0828, -0.1521,  0.0183, -0.2418, -0.0831, -0.0491, -0.0688,\n",
            "        -0.2560,  0.1381, -0.0165,  0.2092, -0.0028, -0.0265, -0.0225,  0.0286,\n",
            "        -0.1065, -0.3698,  0.2862, -0.1036,  0.3080, -0.0894,  0.2772,  0.1136,\n",
            "        -0.3157,  0.0423,  0.0567,  0.2369, -0.0727,  0.0465, -0.0536,  0.1309,\n",
            "         0.0282, -0.1371,  0.1464, -0.0717, -0.3237, -0.1583, -0.0424, -0.1278,\n",
            "        -0.1703,  0.0413,  0.0891,  0.0770, -0.0730,  0.0683, -0.0391,  0.0476],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-7.1555e-02, -1.1031e-01, -1.3711e-01],\n",
            "          [ 7.0593e-02, -1.4782e-02, -1.0053e-01],\n",
            "          [ 1.1938e-01,  8.7330e-02, -8.2206e-03]],\n",
            "\n",
            "         [[-2.3999e-02, -6.3682e-03,  2.4303e-03],\n",
            "          [ 6.1831e-03,  1.8781e-02,  2.5324e-02],\n",
            "          [ 2.3656e-03, -4.0037e-03, -1.1949e-02]],\n",
            "\n",
            "         [[ 6.0344e-03,  6.3784e-03, -1.2247e-02],\n",
            "          [ 7.8854e-03, -1.3464e-02, -4.2702e-02],\n",
            "          [ 1.7380e-02, -1.3862e-02, -4.7145e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.4324e-02,  3.2257e-02,  2.5819e-02],\n",
            "          [ 8.4676e-03, -4.5413e-04, -1.0832e-02],\n",
            "          [-6.7166e-03, -1.5052e-02, -2.6939e-02]],\n",
            "\n",
            "         [[-1.2089e-02, -2.3588e-02, -2.2689e-02],\n",
            "          [ 1.0135e-02,  1.8285e-02, -1.5695e-02],\n",
            "          [ 2.1352e-02,  5.8568e-02,  4.2873e-02]],\n",
            "\n",
            "         [[ 1.4421e-02, -2.8298e-02, -7.0770e-03],\n",
            "          [ 3.0260e-02, -6.6294e-03, -1.6901e-02],\n",
            "          [ 3.9085e-02,  1.4222e-02,  2.2294e-02]]],\n",
            "\n",
            "\n",
            "        [[[-7.7911e-02, -7.3929e-02, -3.6671e-02],\n",
            "          [-3.4903e-02, -6.2355e-02, -3.7793e-02],\n",
            "          [-2.8379e-02, -5.4291e-02, -4.9411e-02]],\n",
            "\n",
            "         [[-1.2970e-02, -2.1825e-02, -2.8767e-04],\n",
            "          [ 7.6444e-03,  1.7653e-02,  1.6660e-02],\n",
            "          [ 3.8337e-02,  2.3006e-02, -1.6620e-03]],\n",
            "\n",
            "         [[-8.7592e-02, -8.4735e-02, -5.5818e-02],\n",
            "          [-7.7731e-02, -8.0311e-02, -3.2554e-02],\n",
            "          [-5.6313e-02, -4.2047e-02,  1.5247e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.2377e-02, -4.0018e-02, -2.9523e-02],\n",
            "          [-1.5294e-02, -1.4165e-02,  2.7086e-03],\n",
            "          [ 1.1652e-02,  2.3886e-02,  2.4413e-02]],\n",
            "\n",
            "         [[ 2.0891e-03, -3.0475e-02, -3.3818e-02],\n",
            "          [ 6.7829e-03,  3.8681e-04, -1.4540e-02],\n",
            "          [-3.1306e-03,  6.7689e-03,  8.4524e-03]],\n",
            "\n",
            "         [[ 3.0586e-02,  4.6281e-02,  3.8359e-04],\n",
            "          [ 5.3079e-02,  6.7488e-02,  3.0547e-02],\n",
            "          [ 2.3374e-02,  4.3993e-02, -3.8713e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 1.3878e-02,  3.2724e-02,  4.6584e-02],\n",
            "          [-8.0647e-03,  1.6209e-03,  1.5153e-02],\n",
            "          [-7.0342e-02, -5.3299e-02, -4.5920e-02]],\n",
            "\n",
            "         [[ 4.6035e-02,  3.5400e-02,  3.4941e-02],\n",
            "          [ 5.8351e-02,  5.4640e-02,  2.7162e-02],\n",
            "          [ 2.6799e-02,  4.5056e-02,  6.6886e-03]],\n",
            "\n",
            "         [[-3.3766e-02, -3.8605e-02, -2.4172e-02],\n",
            "          [-1.8285e-03,  1.0888e-02,  1.1425e-02],\n",
            "          [ 2.2282e-02,  1.4024e-02,  3.6332e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.6330e-02, -6.9552e-02, -8.9737e-02],\n",
            "          [ 3.9766e-02,  1.5501e-02, -2.2695e-02],\n",
            "          [ 1.0290e-01,  1.2294e-01,  6.3867e-02]],\n",
            "\n",
            "         [[-4.2318e-03,  4.9511e-02, -7.6289e-03],\n",
            "          [-2.7720e-02,  7.0398e-03, -9.4052e-03],\n",
            "          [-6.7008e-02, -6.0542e-02, -2.5967e-02]],\n",
            "\n",
            "         [[-5.8560e-03, -1.7573e-02, -3.8016e-02],\n",
            "          [ 2.8579e-03, -4.1603e-03,  1.0113e-02],\n",
            "          [ 2.6243e-02,  3.5200e-02,  3.1143e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-4.4193e-02, -6.5322e-02, -1.7594e-02],\n",
            "          [-9.3970e-02, -5.8291e-02,  1.2093e-02],\n",
            "          [-2.2998e-02,  3.2463e-02,  7.1731e-02]],\n",
            "\n",
            "         [[-4.7220e-03, -3.0125e-03, -1.8075e-02],\n",
            "          [ 1.2667e-02, -8.0509e-03, -1.4605e-02],\n",
            "          [ 7.8220e-03, -1.0720e-02, -2.6515e-02]],\n",
            "\n",
            "         [[-2.5299e-02, -4.9383e-02, -1.2720e-02],\n",
            "          [-5.2206e-02, -4.7233e-02, -4.2470e-03],\n",
            "          [-4.8697e-02, -2.5320e-02,  8.6178e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.7617e-03,  7.8398e-03, -5.9525e-03],\n",
            "          [ 4.0277e-03,  7.3575e-03, -1.1667e-02],\n",
            "          [-3.9997e-02, -3.8038e-02, -5.0469e-02]],\n",
            "\n",
            "         [[-3.8949e-03, -6.8965e-03,  3.4102e-02],\n",
            "          [-6.9814e-03, -4.9762e-02,  5.8711e-02],\n",
            "          [ 1.8361e-02,  2.5874e-02,  8.0028e-02]],\n",
            "\n",
            "         [[-3.3014e-02, -2.1510e-02, -2.1509e-03],\n",
            "          [-4.3894e-02, -3.2009e-02, -1.6265e-02],\n",
            "          [-1.1037e-02,  2.8872e-04,  3.0937e-02]]],\n",
            "\n",
            "\n",
            "        [[[-4.9907e-02, -5.0222e-02, -5.0985e-02],\n",
            "          [ 2.2644e-02, -1.4098e-02, -2.4426e-02],\n",
            "          [ 1.9960e-02,  9.6426e-02,  1.0580e-01]],\n",
            "\n",
            "         [[-3.6873e-02,  2.1413e-03,  8.3469e-03],\n",
            "          [-4.0796e-02, -3.3767e-02, -3.4955e-02],\n",
            "          [ 3.9466e-02,  7.0508e-02,  8.6065e-02]],\n",
            "\n",
            "         [[ 1.4842e-02,  6.6914e-03,  1.4324e-02],\n",
            "          [-3.2621e-02, -4.4027e-02, -2.2269e-02],\n",
            "          [ 7.1982e-03, -1.9187e-02, -4.9348e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.9938e-03,  1.6018e-02,  1.1242e-02],\n",
            "          [-4.7668e-03,  2.1921e-02,  2.2660e-02],\n",
            "          [-2.6753e-02,  2.6917e-04, -5.6827e-03]],\n",
            "\n",
            "         [[-8.7725e-03,  1.0761e-02,  7.3603e-03],\n",
            "          [-1.8010e-05, -1.7926e-02,  4.8229e-03],\n",
            "          [ 4.2431e-02, -1.5764e-02,  2.3554e-02]],\n",
            "\n",
            "         [[-1.3830e-02, -3.0793e-03, -4.0854e-03],\n",
            "          [ 3.3363e-02,  4.2952e-02,  3.5867e-02],\n",
            "          [-3.9653e-02, -3.0855e-02, -4.3189e-02]]],\n",
            "\n",
            "\n",
            "        [[[-3.8617e-02, -3.1549e-03,  2.5739e-03],\n",
            "          [-1.1592e-02,  9.8761e-03,  7.5235e-03],\n",
            "          [-1.9339e-02, -9.8779e-03,  2.1755e-03]],\n",
            "\n",
            "         [[ 1.6889e-04,  1.8302e-03, -8.9537e-03],\n",
            "          [ 5.8343e-03,  1.7360e-02, -1.9029e-02],\n",
            "          [ 5.8642e-03, -7.4307e-04,  1.4667e-03]],\n",
            "\n",
            "         [[-1.6506e-02, -2.8401e-02,  1.3986e-02],\n",
            "          [-2.2922e-02, -4.3484e-02,  1.0471e-02],\n",
            "          [-2.5801e-03, -4.5258e-02,  7.9791e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.5260e-03, -7.6469e-03,  1.3597e-02],\n",
            "          [ 5.5301e-04, -2.9176e-03,  2.2147e-02],\n",
            "          [ 3.2763e-03, -1.0775e-05,  1.3163e-02]],\n",
            "\n",
            "         [[ 5.1756e-03,  1.8495e-02, -8.0268e-03],\n",
            "          [-3.5030e-02,  2.6403e-02, -7.1220e-03],\n",
            "          [-5.2325e-02, -1.1185e-02,  1.9146e-02]],\n",
            "\n",
            "         [[-6.8805e-02,  5.1618e-02,  1.9787e-02],\n",
            "          [ 2.5533e-02, -6.1926e-02,  4.9924e-02],\n",
            "          [ 1.0532e-01, -4.4136e-02,  4.9907e-02]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.3248, 0.3613, 0.2960, 0.2913, 0.3407, 0.3435, 0.3049, 0.3308, 0.3447,\n",
            "        0.3860, 0.3196, 0.2622, 0.2994, 0.2189, 0.2397, 0.3744, 0.3555, 0.1948,\n",
            "        0.3349, 0.2159, 0.3349, 0.3454, 0.3094, 0.3769, 0.3546, 0.3267, 0.3178,\n",
            "        0.3272, 0.3832, 0.2585, 0.2973, 0.3481, 0.2827, 0.2995, 0.3451, 0.3471,\n",
            "        0.3440, 0.3344, 0.3211, 0.3180, 0.2940, 0.3353, 0.3253, 0.3733, 0.3198,\n",
            "        0.2987, 0.1620, 0.3262, 0.3271, 0.3410, 0.3693, 0.3320, 0.3357, 0.2951,\n",
            "        0.3115, 0.3185, 0.3139, 0.2633, 0.3089, 0.3601, 0.2734, 0.3433, 0.3335,\n",
            "        0.3288, 0.2706, 0.2879, 0.3318, 0.3310, 0.3170, 0.2977, 0.3300, 0.3216,\n",
            "        0.3205, 0.3231, 0.3481, 0.3130, 0.2826, 0.2856, 0.3279, 0.3666, 0.3288,\n",
            "        0.3575, 0.3377, 0.2904, 0.3273, 0.3214, 0.3332, 0.3452, 0.1842, 0.3916,\n",
            "        0.3337, 0.2325, 0.3285, 0.3358, 0.2885, 0.3149, 0.3288, 0.2236, 0.3159,\n",
            "        0.2993, 0.3403, 0.3220, 0.3171, 0.2950, 0.2847, 0.3224, 0.3119, 0.2613,\n",
            "        0.3374, 0.3333, 0.3330, 0.2959, 0.4087, 0.2192, 0.2982, 0.4006, 0.3081,\n",
            "        0.3171, 0.2862, 0.2952, 0.3070, 0.3583, 0.3232, 0.3345, 0.3453, 0.3043,\n",
            "        0.3327, 0.3337], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0589, -0.1686, -0.0206,  0.0027, -0.0955, -0.1048,  0.0349, -0.0885,\n",
            "        -0.2053, -0.1764, -0.1224, -0.0364, -0.0785,  0.2088, -0.0403, -0.1820,\n",
            "        -0.1076,  0.2989, -0.0570,  0.2064, -0.0921, -0.1376, -0.1304, -0.1193,\n",
            "        -0.1006, -0.0380, -0.1108, -0.0477, -0.1087,  0.1581, -0.1123, -0.1584,\n",
            "         0.0976, -0.0430, -0.1349, -0.1189, -0.0986, -0.0479, -0.0837, -0.0720,\n",
            "        -0.0836, -0.2442, -0.3376, -0.2124, -0.0693, -0.0651,  0.4979, -0.0811,\n",
            "        -0.1021, -0.0788, -0.1802, -0.1011, -0.1090, -0.0617, -0.0856, -0.0495,\n",
            "        -0.0370,  0.0023, -0.0508, -0.2430,  0.0009, -0.1525, -0.0963, -0.0516,\n",
            "        -0.0473,  0.0884, -0.1028, -0.0907, -0.1086, -0.0379, -0.1030, -0.1609,\n",
            "        -0.0903, -0.0898, -0.1282, -0.0830, -0.0186, -0.0232, -0.0045, -0.2131,\n",
            "        -0.1431, -0.1391, -0.1303, -0.0568, -0.1862, -0.1209, -0.0340, -0.1181,\n",
            "         0.2298, -0.2085, -0.1335,  0.1418, -0.0891, -0.1273,  0.0107, -0.1029,\n",
            "        -0.1025,  0.1562, -0.0937, -0.0657, -0.1245, -0.0451, -0.0707, -0.0447,\n",
            "         0.0715, -0.0484, -0.0312, -0.0437, -0.0927, -0.1465, -0.1151, -0.0183,\n",
            "        -0.1927,  0.2491,  0.0300, -0.1310, -0.0468, -0.0851, -0.0421, -0.0413,\n",
            "        -0.0457, -0.1433, -0.0981, -0.1046, -0.1315, -0.1249, -0.0982, -0.0961],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-0.0074, -0.0098,  0.0028],\n",
            "          [-0.0108,  0.0258,  0.0455],\n",
            "          [-0.0272,  0.0053,  0.0132]],\n",
            "\n",
            "         [[ 0.0354,  0.0251,  0.0078],\n",
            "          [ 0.0040,  0.0199,  0.0274],\n",
            "          [ 0.0353,  0.0355,  0.0133]],\n",
            "\n",
            "         [[ 0.0193, -0.0213, -0.0362],\n",
            "          [-0.0196, -0.0189, -0.0595],\n",
            "          [-0.0218, -0.0077,  0.0039]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0068,  0.0108, -0.0037],\n",
            "          [ 0.0135,  0.0114, -0.0013],\n",
            "          [ 0.0081,  0.0002,  0.0006]],\n",
            "\n",
            "         [[ 0.0077,  0.0077,  0.0044],\n",
            "          [-0.0102, -0.0117, -0.0096],\n",
            "          [-0.0039,  0.0181,  0.0133]],\n",
            "\n",
            "         [[ 0.0124, -0.0269, -0.0120],\n",
            "          [ 0.0268,  0.0264, -0.0215],\n",
            "          [ 0.0129,  0.0028, -0.0054]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0128,  0.0185, -0.0027],\n",
            "          [-0.0168, -0.0123,  0.0355],\n",
            "          [-0.0009,  0.0245,  0.0182]],\n",
            "\n",
            "         [[-0.0067, -0.0207, -0.0144],\n",
            "          [-0.0073,  0.0426,  0.0074],\n",
            "          [ 0.0276,  0.0160,  0.0159]],\n",
            "\n",
            "         [[-0.0229, -0.0206,  0.0236],\n",
            "          [-0.0275, -0.0561, -0.0699],\n",
            "          [ 0.0205,  0.0513,  0.0220]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0272, -0.0390, -0.0395],\n",
            "          [-0.0621, -0.0749, -0.0956],\n",
            "          [-0.0605, -0.0777, -0.0746]],\n",
            "\n",
            "         [[ 0.0287,  0.0293,  0.0287],\n",
            "          [ 0.0079,  0.0471,  0.0146],\n",
            "          [-0.0018,  0.0220,  0.0074]],\n",
            "\n",
            "         [[ 0.0016, -0.0168, -0.0046],\n",
            "          [-0.0081, -0.0255, -0.0524],\n",
            "          [-0.0093, -0.0010, -0.0378]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0047,  0.0061, -0.0071],\n",
            "          [ 0.0236, -0.0931, -0.0793],\n",
            "          [-0.0079, -0.0505, -0.0105]],\n",
            "\n",
            "         [[ 0.0148,  0.0162, -0.0515],\n",
            "          [ 0.0086,  0.0081, -0.0429],\n",
            "          [ 0.0908,  0.0654,  0.0435]],\n",
            "\n",
            "         [[-0.0138, -0.0064,  0.0085],\n",
            "          [ 0.0138, -0.0124,  0.0054],\n",
            "          [ 0.0202, -0.0035,  0.0080]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0009,  0.0185, -0.0306],\n",
            "          [-0.0170,  0.0057, -0.0491],\n",
            "          [-0.0328, -0.0374, -0.0459]],\n",
            "\n",
            "         [[-0.0046,  0.0069, -0.0011],\n",
            "          [-0.0079, -0.0499,  0.0421],\n",
            "          [-0.0752, -0.0048, -0.0058]],\n",
            "\n",
            "         [[ 0.0115, -0.0146,  0.0379],\n",
            "          [ 0.0141,  0.0486,  0.0232],\n",
            "          [ 0.0215, -0.0101,  0.0338]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0264,  0.0040,  0.0075],\n",
            "          [ 0.0636, -0.0320, -0.0018],\n",
            "          [ 0.0262,  0.0076,  0.0495]],\n",
            "\n",
            "         [[-0.0287, -0.0227, -0.0513],\n",
            "          [-0.0260, -0.0487, -0.0140],\n",
            "          [-0.0173, -0.0416, -0.0117]],\n",
            "\n",
            "         [[-0.0350,  0.0356,  0.0347],\n",
            "          [ 0.0183,  0.0436, -0.0263],\n",
            "          [ 0.0178,  0.0356,  0.0113]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0097, -0.0173, -0.0002],\n",
            "          [-0.0199,  0.0820,  0.0129],\n",
            "          [-0.0238, -0.0048,  0.0486]],\n",
            "\n",
            "         [[-0.0244, -0.0258, -0.0353],\n",
            "          [-0.0296, -0.0966, -0.0535],\n",
            "          [-0.0150,  0.0059, -0.0197]],\n",
            "\n",
            "         [[ 0.0068, -0.0368, -0.0255],\n",
            "          [-0.0116, -0.0236, -0.0078],\n",
            "          [ 0.0086,  0.0079, -0.0189]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0329,  0.0297,  0.0111],\n",
            "          [ 0.0390, -0.0090,  0.0226],\n",
            "          [ 0.0078, -0.0280, -0.0230]],\n",
            "\n",
            "         [[ 0.0137,  0.0229, -0.0190],\n",
            "          [ 0.0027,  0.0112,  0.0074],\n",
            "          [ 0.0211,  0.0436,  0.0108]],\n",
            "\n",
            "         [[-0.0237,  0.0221,  0.0004],\n",
            "          [-0.0309,  0.0609,  0.0167],\n",
            "          [-0.0885, -0.0834, -0.0342]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0073, -0.0197,  0.0018],\n",
            "          [ 0.0073, -0.0343, -0.0243],\n",
            "          [-0.0115, -0.0605, -0.0551]],\n",
            "\n",
            "         [[ 0.0030,  0.0026,  0.0171],\n",
            "          [-0.0134, -0.0086,  0.0090],\n",
            "          [ 0.0195,  0.0094,  0.0045]],\n",
            "\n",
            "         [[ 0.0014,  0.0008,  0.0119],\n",
            "          [-0.0024, -0.0107,  0.0126],\n",
            "          [-0.0051, -0.0058, -0.0146]]],\n",
            "\n",
            "\n",
            "        [[[-0.0074,  0.0118, -0.0427],\n",
            "          [ 0.0016, -0.0457, -0.1398],\n",
            "          [-0.0065, -0.0021, -0.0484]],\n",
            "\n",
            "         [[ 0.0075,  0.0527,  0.0388],\n",
            "          [-0.0125,  0.0847,  0.0062],\n",
            "          [ 0.0013, -0.0197, -0.0822]],\n",
            "\n",
            "         [[-0.0249,  0.0166,  0.0169],\n",
            "          [ 0.0087,  0.0214,  0.0117],\n",
            "          [-0.0009,  0.0306,  0.0136]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0078,  0.0220, -0.0175],\n",
            "          [-0.0076, -0.0211, -0.0037],\n",
            "          [ 0.0126, -0.0207, -0.0054]],\n",
            "\n",
            "         [[ 0.0302, -0.0082, -0.0649],\n",
            "          [-0.0238, -0.0954, -0.0530],\n",
            "          [-0.0168, -0.0111,  0.0010]],\n",
            "\n",
            "         [[-0.0245, -0.0847,  0.0251],\n",
            "          [ 0.0106,  0.0387,  0.1400],\n",
            "          [ 0.0155, -0.0095,  0.0041]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.1454, 0.3270, 0.3113, 0.2538, 0.4086, 0.3937, 0.4400, 0.3108, 0.3406,\n",
            "        0.2168, 0.2170, 0.3857, 0.1971, 0.2692, 0.1663, 0.2454, 0.3232, 0.3686,\n",
            "        0.3893, 0.3264, 0.3875, 0.4707, 0.1958, 0.4717, 0.1673, 0.3938, 0.3044,\n",
            "        0.1929, 0.2175, 0.2119, 0.4230, 0.3683, 0.2455, 0.2229, 0.3370, 0.3229,\n",
            "        0.2688, 0.3557, 0.2581, 0.4031, 0.4492, 0.3642, 0.2599, 0.1881, 0.1359,\n",
            "        0.2958, 0.1913, 0.3065, 0.3981, 0.4102, 0.1874, 0.4516, 0.3340, 0.1628,\n",
            "        0.3599, 0.1624, 0.2886, 0.1358, 0.4491, 0.2694, 0.4823, 0.3393, 0.4764,\n",
            "        0.3155, 0.6005, 0.4654, 0.5264, 0.2991, 0.2992, 0.4621, 0.2614, 0.4247,\n",
            "        0.4662, 0.4249, 0.3345, 0.2655, 0.4048, 0.3605, 0.1782, 0.3833, 0.2823,\n",
            "        0.3843, 0.3307, 0.2151, 0.3317, 0.1458, 0.2771, 0.4917, 0.3199, 0.4222,\n",
            "        0.1559, 0.4884, 0.3267, 0.3440, 0.1608, 0.4855, 0.2677, 0.1616, 0.3221,\n",
            "        0.4243, 0.3661, 0.1893, 0.3400, 0.3648, 0.1779, 0.3544, 0.2852, 0.2437,\n",
            "        0.4472, 0.3011, 0.3997, 0.6173, 0.2794, 0.4867, 0.1502, 0.6021, 0.3604,\n",
            "        0.4696, 0.3711, 0.2388, 0.5347, 0.1509, 0.3213, 0.4394, 0.3229, 0.4329,\n",
            "        0.1489, 0.3702], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0246,  0.0593,  0.1347, -0.1089, -0.0470, -0.1359, -0.0550,  0.0509,\n",
            "        -0.0613,  0.0916,  0.0031, -0.0274, -0.0539,  0.0177,  0.0432,  0.0074,\n",
            "         0.0548, -0.0321, -0.0224,  0.0142, -0.2150, -0.1160,  0.0486, -0.1141,\n",
            "         0.1066,  0.0355,  0.0140,  0.0177,  0.0781,  0.1331,  0.0139,  0.0447,\n",
            "         0.1063,  0.0528, -0.0539, -0.1160,  0.1055, -0.1591,  0.0100,  0.1197,\n",
            "         0.0170,  0.0929, -0.0675,  0.0987,  0.1034,  0.0501,  0.0297,  0.0281,\n",
            "        -0.0075, -0.0577, -0.0144, -0.1640,  0.1255,  0.0817,  0.0635,  0.0936,\n",
            "         0.0213,  0.0486, -0.1174,  0.0237, -0.2177,  0.0099, -0.1883,  0.0467,\n",
            "        -0.0829,  0.0585, -0.0306,  0.0509,  0.0541, -0.1671,  0.0115, -0.0302,\n",
            "        -0.1393,  0.0115,  0.0428,  0.1189, -0.1289,  0.0479,  0.0474, -0.0625,\n",
            "         0.0009, -0.0144,  0.0909,  0.1342, -0.0338,  0.0560,  0.0848, -0.0467,\n",
            "         0.0228, -0.0097,  0.1360, -0.2625,  0.0088, -0.0553,  0.0383, -0.0720,\n",
            "         0.0907,  0.1612, -0.1076,  0.1011, -0.0519,  0.0838, -0.0704, -0.0806,\n",
            "        -0.0243,  0.0533,  0.1277,  0.1403, -0.0593, -0.0639, -0.0766, -0.1163,\n",
            "         0.0661, -0.1644,  0.0422, -0.2786, -0.1006, -0.0696, -0.0761,  0.0371,\n",
            "        -0.0247,  0.0916, -0.0200, -0.0176,  0.0298, -0.0373,  0.0466, -0.1371],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 0.0159]],\n",
            "\n",
            "         [[-0.3109]],\n",
            "\n",
            "         [[ 0.0126]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1672]],\n",
            "\n",
            "         [[ 0.0127]],\n",
            "\n",
            "         [[ 0.0132]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0036]],\n",
            "\n",
            "         [[-0.0011]],\n",
            "\n",
            "         [[-0.0083]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0234]],\n",
            "\n",
            "         [[-0.0756]],\n",
            "\n",
            "         [[-0.0126]]],\n",
            "\n",
            "\n",
            "        [[[-0.0419]],\n",
            "\n",
            "         [[ 0.0079]],\n",
            "\n",
            "         [[-0.1662]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0319]],\n",
            "\n",
            "         [[-0.0188]],\n",
            "\n",
            "         [[ 0.0645]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.0287]],\n",
            "\n",
            "         [[ 0.0470]],\n",
            "\n",
            "         [[-0.0523]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0474]],\n",
            "\n",
            "         [[ 0.0586]],\n",
            "\n",
            "         [[ 0.0588]]],\n",
            "\n",
            "\n",
            "        [[[-0.0078]],\n",
            "\n",
            "         [[-0.0203]],\n",
            "\n",
            "         [[ 0.0564]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.7802]],\n",
            "\n",
            "         [[-0.0023]],\n",
            "\n",
            "         [[-0.0259]]],\n",
            "\n",
            "\n",
            "        [[[-0.0283]],\n",
            "\n",
            "         [[-0.0132]],\n",
            "\n",
            "         [[-0.0514]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0236]],\n",
            "\n",
            "         [[-0.0677]],\n",
            "\n",
            "         [[ 0.0268]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.3334,  0.0581,  0.0715,  0.3442,  0.1756,  0.1509,  0.1568,  0.3100,\n",
            "         0.1927,  0.1516,  0.3044,  0.2238,  0.3706,  0.1739,  0.3051,  0.2610,\n",
            "         0.1575,  0.2015,  0.2933,  0.1010,  0.5871,  0.0676,  0.2499,  0.0929,\n",
            "         0.2443,  0.0495,  0.2449,  0.2750,  0.3071,  0.3025,  0.1818,  0.0688,\n",
            "         0.2223,  0.3766,  0.4661,  0.3284,  0.1035,  0.3400,  0.2325,  0.1514,\n",
            "         0.1753,  0.2269,  0.2606,  0.1831,  0.2894,  0.2590,  0.2208,  0.1399,\n",
            "         0.0643,  0.2833,  0.3451,  0.2017,  0.0696,  0.2722,  0.1127,  0.2917,\n",
            "         0.2358,  0.2703,  0.0911,  0.2591,  0.1302,  0.2261,  0.1967,  0.0539,\n",
            "         0.0697,  0.0524,  0.1050,  0.0861,  0.1173,  0.0957,  0.1862,  0.1642,\n",
            "         0.1336,  0.1065,  0.1312,  0.0888,  0.0793,  0.0475,  0.3049,  0.2325,\n",
            "         0.2908,  0.1292,  0.0778,  0.2263,  0.2379,  0.3405,  0.0914,  0.1936,\n",
            "         0.1223,  0.1400,  0.2953,  0.2360,  0.1681,  0.1338,  0.2666,  0.1495,\n",
            "         0.0761,  0.1674,  0.1784,  0.1720,  0.2318,  0.3753,  0.2103,  0.1922,\n",
            "         0.4002,  0.1718,  0.0593,  0.0742,  0.0686,  0.1931,  0.1386,  0.1111,\n",
            "         0.3055,  0.1205,  0.3443,  0.1633,  0.3673,  0.1534,  0.0742,  0.2088,\n",
            "         0.0394,  0.2594,  0.1385, -0.0051,  0.1905,  0.1275,  0.3071,  0.1682],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0246,  0.0593,  0.1347, -0.1089, -0.0470, -0.1359, -0.0550,  0.0509,\n",
            "        -0.0613,  0.0916,  0.0031, -0.0274, -0.0539,  0.0177,  0.0432,  0.0074,\n",
            "         0.0548, -0.0321, -0.0224,  0.0142, -0.2150, -0.1160,  0.0486, -0.1141,\n",
            "         0.1066,  0.0355,  0.0140,  0.0177,  0.0781,  0.1331,  0.0139,  0.0447,\n",
            "         0.1063,  0.0528, -0.0539, -0.1160,  0.1055, -0.1591,  0.0100,  0.1197,\n",
            "         0.0170,  0.0929, -0.0675,  0.0987,  0.1034,  0.0501,  0.0297,  0.0281,\n",
            "        -0.0075, -0.0577, -0.0144, -0.1640,  0.1255,  0.0817,  0.0635,  0.0936,\n",
            "         0.0213,  0.0486, -0.1174,  0.0237, -0.2177,  0.0099, -0.1883,  0.0467,\n",
            "        -0.0829,  0.0585, -0.0306,  0.0509,  0.0541, -0.1671,  0.0115, -0.0302,\n",
            "        -0.1393,  0.0115,  0.0428,  0.1189, -0.1289,  0.0479,  0.0474, -0.0625,\n",
            "         0.0009, -0.0144,  0.0909,  0.1342, -0.0338,  0.0560,  0.0848, -0.0467,\n",
            "         0.0228, -0.0097,  0.1360, -0.2625,  0.0088, -0.0553,  0.0383, -0.0720,\n",
            "         0.0907,  0.1612, -0.1076,  0.1011, -0.0519,  0.0838, -0.0704, -0.0806,\n",
            "        -0.0243,  0.0533,  0.1277,  0.1403, -0.0593, -0.0639, -0.0766, -0.1163,\n",
            "         0.0661, -0.1644,  0.0422, -0.2786, -0.1006, -0.0696, -0.0761,  0.0371,\n",
            "        -0.0247,  0.0916, -0.0200, -0.0176,  0.0298, -0.0373,  0.0466, -0.1371],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-9.9023e-04, -7.7429e-03, -7.9740e-03],\n",
            "          [ 2.4844e-02,  1.8642e-03,  5.8352e-03],\n",
            "          [ 9.5089e-03, -1.6476e-02,  3.9157e-03]],\n",
            "\n",
            "         [[-2.1488e-02, -1.2330e-03, -1.4281e-02],\n",
            "          [-1.7044e-02,  9.5922e-03,  7.0445e-03],\n",
            "          [ 1.0790e-02, -7.2350e-03, -1.1357e-02]],\n",
            "\n",
            "         [[-1.1126e-03,  3.0388e-02,  2.2247e-02],\n",
            "          [-6.1184e-02, -2.3797e-02,  2.3747e-03],\n",
            "          [ 4.0678e-02, -1.0356e-01, -6.0011e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-8.5833e-03,  1.1438e-02,  2.0800e-02],\n",
            "          [-1.6565e-02, -3.9587e-02,  1.2594e-02],\n",
            "          [-1.4314e-03, -5.4257e-03,  3.6794e-02]],\n",
            "\n",
            "         [[-1.3687e-02, -2.9514e-02, -1.4745e-02],\n",
            "          [ 2.8299e-02,  2.2096e-02,  3.4839e-03],\n",
            "          [-4.3521e-03, -2.6706e-03,  1.2258e-04]],\n",
            "\n",
            "         [[ 7.6403e-03,  2.0666e-02,  3.7429e-02],\n",
            "          [ 6.9478e-03,  4.3983e-02,  1.7538e-02],\n",
            "          [-9.7797e-03, -2.4789e-02, -1.1349e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 8.4439e-02,  8.4827e-02, -5.1478e-02],\n",
            "          [ 3.5253e-02, -1.1375e-03, -1.0331e-01],\n",
            "          [-6.4078e-02, -1.2660e-01, -1.2952e-01]],\n",
            "\n",
            "         [[ 1.0628e-03, -1.4083e-02,  4.7109e-03],\n",
            "          [-2.1059e-02, -2.8778e-02,  9.9708e-03],\n",
            "          [ 1.4074e-02,  1.8691e-02,  5.8192e-02]],\n",
            "\n",
            "         [[ 2.2139e-02,  8.9027e-03,  1.4790e-02],\n",
            "          [-1.7497e-02, -5.3924e-03,  2.7834e-02],\n",
            "          [-1.3855e-02, -1.3346e-02,  1.7668e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.8032e-02, -2.3097e-02, -7.1775e-03],\n",
            "          [-3.5089e-02,  1.0861e-02,  1.3640e-02],\n",
            "          [ 6.3449e-04,  9.7476e-03,  7.3670e-03]],\n",
            "\n",
            "         [[-4.4184e-02, -1.6190e-02,  1.2243e-02],\n",
            "          [-4.0349e-02, -1.7894e-02,  2.8911e-02],\n",
            "          [-6.5176e-03, -1.0490e-02,  9.1658e-03]],\n",
            "\n",
            "         [[ 4.3621e-03,  1.3119e-02,  1.8442e-03],\n",
            "          [ 1.1555e-02, -1.3031e-02, -9.5657e-03],\n",
            "          [-2.3314e-02,  1.1609e-03,  2.6771e-03]]],\n",
            "\n",
            "\n",
            "        [[[-2.1180e-02, -6.2213e-03,  1.7609e-03],\n",
            "          [-4.7424e-03,  1.1101e-02,  1.1296e-02],\n",
            "          [-1.4529e-02,  2.9843e-02,  2.4383e-03]],\n",
            "\n",
            "         [[ 6.9183e-03,  9.2937e-03,  3.0078e-02],\n",
            "          [-4.2612e-03,  4.9560e-03, -4.7338e-03],\n",
            "          [ 3.1360e-02,  1.9035e-03, -4.7242e-03]],\n",
            "\n",
            "         [[-3.6726e-02,  5.7285e-03,  1.3919e-01],\n",
            "          [-4.2992e-02,  9.4023e-04,  7.7141e-02],\n",
            "          [-5.0050e-02, -4.9479e-03,  2.4693e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.7203e-02,  7.4712e-03, -4.2659e-02],\n",
            "          [-8.1729e-03, -9.2536e-02, -5.4934e-03],\n",
            "          [-2.5927e-02,  8.3993e-04,  7.4632e-02]],\n",
            "\n",
            "         [[ 1.8076e-02,  4.5272e-03, -1.3757e-02],\n",
            "          [-1.8939e-02, -3.2739e-02, -2.9666e-02],\n",
            "          [-2.0608e-02, -4.6167e-03,  1.3080e-03]],\n",
            "\n",
            "         [[-1.2078e-02, -2.0285e-03, -1.6998e-02],\n",
            "          [-3.4805e-02, -4.9195e-02, -3.1973e-02],\n",
            "          [-2.1021e-02, -5.1164e-03, -4.8522e-03]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 3.1791e-02,  2.2948e-02,  1.0390e-02],\n",
            "          [-1.2628e-02, -2.9320e-03,  4.2645e-03],\n",
            "          [-2.1707e-02, -1.0856e-02,  1.6094e-02]],\n",
            "\n",
            "         [[-1.4525e-03, -1.0131e-02, -4.6862e-04],\n",
            "          [ 2.2130e-02,  2.2736e-02,  5.0183e-03],\n",
            "          [-6.0125e-02, -4.3150e-02, -4.4480e-02]],\n",
            "\n",
            "         [[ 3.0761e-03,  3.4396e-03,  6.0877e-03],\n",
            "          [-1.3683e-02,  4.0576e-03, -2.6544e-02],\n",
            "          [ 6.8231e-02,  6.3474e-02, -9.3660e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.8752e-02,  1.9400e-02,  4.1691e-02],\n",
            "          [ 8.7770e-03,  8.2394e-04,  1.8619e-02],\n",
            "          [ 1.8796e-02,  6.2238e-02, -2.3801e-02]],\n",
            "\n",
            "         [[-2.9788e-02, -3.4598e-02, -2.5225e-02],\n",
            "          [ 8.4234e-03, -2.3222e-02, -9.4612e-03],\n",
            "          [ 6.9035e-03,  6.9737e-02, -1.3359e-02]],\n",
            "\n",
            "         [[ 2.6981e-03, -4.3182e-02, -1.6731e-02],\n",
            "          [ 2.5812e-02, -7.2025e-02, -6.5399e-02],\n",
            "          [ 4.6257e-02,  2.9469e-02, -1.5811e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.1079e-02,  3.8220e-02,  8.3305e-03],\n",
            "          [-5.9912e-03,  3.5584e-02, -1.7534e-03],\n",
            "          [ 1.8735e-02,  7.0859e-03, -3.5151e-03]],\n",
            "\n",
            "         [[-4.5937e-02, -7.4695e-02, -5.3608e-02],\n",
            "          [-8.6266e-03,  9.0894e-03, -3.0345e-02],\n",
            "          [-2.8158e-02, -2.1204e-02, -8.4730e-03]],\n",
            "\n",
            "         [[-7.1772e-02, -6.8582e-02,  2.5544e-02],\n",
            "          [ 5.0363e-02,  2.5269e-02,  5.6668e-02],\n",
            "          [ 2.6238e-03,  1.3871e-03, -8.4692e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.9644e-02,  1.0896e-02, -3.0402e-02],\n",
            "          [ 1.5095e-03,  5.0455e-02,  1.5597e-02],\n",
            "          [-2.1015e-02, -1.0757e-02, -3.4942e-02]],\n",
            "\n",
            "         [[-2.7573e-02,  2.9707e-02, -2.9490e-02],\n",
            "          [ 2.3301e-03, -3.9011e-02,  6.8010e-03],\n",
            "          [ 4.4006e-02,  3.5397e-02,  7.9087e-02]],\n",
            "\n",
            "         [[-2.7480e-02,  5.0337e-02,  1.4290e-02],\n",
            "          [-5.2482e-02, -4.7748e-03,  1.2988e-02],\n",
            "          [-1.8935e-02, -3.0808e-02, -1.7583e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.2280e-02,  4.7408e-02,  3.4054e-02],\n",
            "          [ 2.1445e-02,  3.8987e-03,  4.6985e-04],\n",
            "          [ 1.5159e-02,  8.2067e-03,  3.2426e-02]],\n",
            "\n",
            "         [[ 9.2653e-03,  2.3661e-02,  4.2089e-02],\n",
            "          [ 2.1976e-02,  4.6128e-02,  1.1402e-02],\n",
            "          [ 7.2843e-03,  5.2285e-02,  8.6340e-03]],\n",
            "\n",
            "         [[ 1.4022e-02,  1.2800e-02,  3.5398e-02],\n",
            "          [-4.4398e-02,  1.7399e-02, -1.5838e-02],\n",
            "          [ 3.1712e-02,  5.8679e-02, -9.3244e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.8399e-03,  7.8628e-03, -5.6169e-04],\n",
            "          [ 8.0402e-03,  1.7392e-02,  7.8734e-03],\n",
            "          [-1.7713e-02, -4.5957e-02, -9.8762e-03]],\n",
            "\n",
            "         [[-9.7569e-03, -7.5795e-03, -2.4627e-02],\n",
            "          [-8.2454e-03,  6.3065e-02, -3.2954e-03],\n",
            "          [-7.7549e-03, -1.3404e-04, -8.1337e-03]],\n",
            "\n",
            "         [[ 1.7664e-02,  1.0114e-02,  4.2687e-03],\n",
            "          [-3.7950e-03,  2.6715e-02,  2.0121e-02],\n",
            "          [ 1.6868e-02, -6.6515e-03, -1.1107e-02]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.3323, 0.2908, 0.3246, 0.3435, 0.3011, 0.3054, 0.3041, 0.3539, 0.2862,\n",
            "        0.3601, 0.2970, 0.3381, 0.2565, 0.3276, 0.3030, 0.4085, 0.3519, 0.4218,\n",
            "        0.3055, 0.2551, 0.3425, 0.3215, 0.3366, 0.2700, 0.2849, 0.3954, 0.3166,\n",
            "        0.3286, 0.3515, 0.3953, 0.2768, 0.3625, 0.1988, 0.2717, 0.3355, 0.2797,\n",
            "        0.2510, 0.3832, 0.3266, 0.3263, 0.3681, 0.3401, 0.3651, 0.3391, 0.3071,\n",
            "        0.3231, 0.3691, 0.2410, 0.3536, 0.3189, 0.3238, 0.3611, 0.3086, 0.3309,\n",
            "        0.3886, 0.4362, 0.4550, 0.2962, 0.3071, 0.3386, 0.3317, 0.3228, 0.2393,\n",
            "        0.3147, 0.2738, 0.3218, 0.3198, 0.3411, 0.3611, 0.2833, 0.3035, 0.3183,\n",
            "        0.3146, 0.3890, 0.2607, 0.3479, 0.3236, 0.3709, 0.2592, 0.3742, 0.2555,\n",
            "        0.2966, 0.3505, 0.3165, 0.2808, 0.2660, 0.2817, 0.4795, 0.3372, 0.2723,\n",
            "        0.2955, 0.3225, 0.2470, 0.3160, 0.3515, 0.3131, 0.3372, 0.2837, 0.3540,\n",
            "        0.2897, 0.2490, 0.3019, 0.3114, 0.3510, 0.3022, 0.3617, 0.2859, 0.2831,\n",
            "        0.3243, 0.2769, 0.3314, 0.2394, 0.2932, 0.2788, 0.2686, 0.3194, 0.3542,\n",
            "        0.2683, 0.2955, 0.2924, 0.3538, 0.4256, 0.3603, 0.3013, 0.2763, 0.4354,\n",
            "        0.3991, 0.2694], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1735, -0.2337, -0.3383, -0.0806, -0.1920, -0.0621, -0.1885, -0.2830,\n",
            "        -0.1680, -0.1796, -0.2645, -0.1983, -0.1183, -0.2432, -0.1706, -0.3090,\n",
            "        -0.2661, -0.4040, -0.1949, -0.1392, -0.2449, -0.1242, -0.2012, -0.1901,\n",
            "        -0.1014, -0.3468, -0.2245, -0.3272, -0.3057, -0.3289, -0.1532, -0.1967,\n",
            "        -0.0667, -0.3281, -0.1418, -0.1527, -0.0987, -0.3243, -0.2252, -0.3462,\n",
            "        -0.2284, -0.2263, -0.1810, -0.1564, -0.1730, -0.1507, -0.2913, -0.1643,\n",
            "        -0.1998, -0.1532, -0.2211, -0.2247, -0.0913, -0.1563, -0.2453, -0.4854,\n",
            "        -0.4428, -0.1021, -0.1615, -0.2125, -0.2239, -0.1952, -0.0447, -0.1733,\n",
            "        -0.1178, -0.4775, -0.2110, -0.2305, -0.1795, -0.1582, -0.2008, -0.2041,\n",
            "        -0.1974, -0.2750, -0.0395, -0.2161, -0.2786, -0.2626, -0.0997, -0.2953,\n",
            "        -0.1431, -0.1448, -0.1894, -0.1283, -0.1807, -0.1144, -0.1308, -0.4154,\n",
            "        -0.2324, -0.1376, -0.1154, -0.2099, -0.0966, -0.1669, -0.3835, -0.2545,\n",
            "        -0.1603, -0.1904, -0.2420, -0.1658, -0.1133, -0.1498, -0.1213, -0.2318,\n",
            "        -0.2017, -0.3827, -0.1491, -0.1174, -0.1261, -0.2031, -0.1832, -0.2274,\n",
            "        -0.1281, -0.2557, -0.1400, -0.0723, -0.2212, -0.1486, -0.2914, -0.1116,\n",
            "        -0.2194, -0.4898, -0.3693, -0.1437, -0.1232, -0.3723, -0.6794, -0.1536],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-1.6153e-02,  5.0134e-03, -9.0186e-04],\n",
            "          [-8.8386e-03, -1.9390e-02, -2.4174e-02],\n",
            "          [ 6.3052e-03,  1.0245e-02, -1.3816e-02]],\n",
            "\n",
            "         [[-1.0979e-02,  2.6164e-03,  2.3656e-02],\n",
            "          [-1.7687e-02,  1.9861e-02,  6.4150e-02],\n",
            "          [ 6.0224e-03,  7.6342e-02,  1.0215e-01]],\n",
            "\n",
            "         [[-8.1113e-03,  6.8414e-03,  2.5436e-02],\n",
            "          [-8.0696e-03,  9.2929e-03,  8.2899e-03],\n",
            "          [ 7.7306e-03,  1.2159e-02,  7.1625e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.5175e-02,  6.2196e-03,  2.1798e-02],\n",
            "          [-1.5199e-02, -8.5439e-02, -2.4713e-02],\n",
            "          [-1.8460e-02, -4.9767e-02, -1.6818e-03]],\n",
            "\n",
            "         [[ 3.0728e-02,  3.9962e-02,  3.1253e-02],\n",
            "          [-1.8738e-02, -6.7510e-02, -2.7649e-02],\n",
            "          [ 2.8429e-02,  3.1854e-02,  1.0543e-02]],\n",
            "\n",
            "         [[-1.8320e-02, -1.5854e-02, -1.0685e-02],\n",
            "          [-2.7442e-02, -3.0616e-02, -1.0485e-02],\n",
            "          [-1.5122e-02, -1.0595e-02, -2.5322e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.6868e-03,  3.0996e-02,  4.2763e-02],\n",
            "          [ 4.6537e-02,  4.8606e-02,  2.3800e-03],\n",
            "          [ 1.6654e-02,  1.2900e-02, -1.8230e-02]],\n",
            "\n",
            "         [[-1.0441e-02, -1.5934e-03, -1.6128e-02],\n",
            "          [-1.2799e-02,  4.9570e-03, -1.4585e-02],\n",
            "          [-2.3553e-02, -3.7023e-03, -1.4399e-02]],\n",
            "\n",
            "         [[ 1.0338e-02, -1.7560e-02, -3.3046e-02],\n",
            "          [-3.2090e-02, -5.9258e-03,  2.0201e-03],\n",
            "          [-4.1428e-02,  4.9121e-03,  1.6906e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.9525e-02, -4.6498e-02, -5.9916e-02],\n",
            "          [-2.6670e-02, -1.9079e-02, -2.9419e-02],\n",
            "          [-3.9683e-03,  1.9405e-02,  7.3317e-03]],\n",
            "\n",
            "         [[ 1.4293e-02,  1.5643e-02,  5.8117e-04],\n",
            "          [ 5.1493e-03,  7.4332e-03, -3.6928e-03],\n",
            "          [-1.3522e-02, -8.5536e-03, -2.1259e-03]],\n",
            "\n",
            "         [[-3.0908e-02, -1.9839e-02, -1.9375e-02],\n",
            "          [-1.0368e-02, -2.4294e-02,  2.4103e-04],\n",
            "          [-1.9275e-02, -2.9707e-02, -1.5623e-02]]],\n",
            "\n",
            "\n",
            "        [[[-4.9212e-02, -2.9588e-02,  8.8023e-02],\n",
            "          [ 4.7453e-03,  4.3564e-02,  9.3115e-02],\n",
            "          [ 7.4083e-02,  4.2868e-02, -5.1033e-02]],\n",
            "\n",
            "         [[ 6.6992e-03,  2.1676e-02, -5.4254e-04],\n",
            "          [ 1.9286e-02,  1.0920e-02, -4.5440e-03],\n",
            "          [ 3.1075e-02, -1.7168e-03, -2.7603e-02]],\n",
            "\n",
            "         [[ 6.0096e-02, -2.9359e-02, -5.8911e-02],\n",
            "          [-1.9133e-02, -8.1624e-02, -2.2553e-02],\n",
            "          [ 1.1597e-02,  2.5092e-02,  1.2130e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.4307e-03, -2.3130e-02,  9.6233e-03],\n",
            "          [-4.3785e-02, -2.6735e-02,  2.1993e-02],\n",
            "          [-3.5919e-02, -4.1009e-02, -2.1860e-02]],\n",
            "\n",
            "         [[ 3.3705e-02,  6.2938e-02,  4.3502e-02],\n",
            "          [ 1.1111e-03,  1.9243e-02, -1.9707e-03],\n",
            "          [-1.1493e-02, -5.3445e-02, -9.6676e-03]],\n",
            "\n",
            "         [[-2.6664e-03, -2.6954e-02, -1.7667e-02],\n",
            "          [-8.3382e-03,  8.9920e-03,  8.1260e-04],\n",
            "          [-2.6832e-02, -3.5991e-02, -4.2495e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1.8876e-03, -2.2728e-02, -4.2991e-03],\n",
            "          [-9.2231e-03, -3.4333e-02, -1.3392e-02],\n",
            "          [-1.2774e-02, -1.1435e-02,  1.5617e-02]],\n",
            "\n",
            "         [[ 1.0703e-02,  1.2792e-02,  2.2662e-02],\n",
            "          [ 7.3185e-03, -1.7847e-02,  1.0674e-02],\n",
            "          [-1.5936e-02, -1.9318e-02,  2.1768e-02]],\n",
            "\n",
            "         [[-7.3009e-03,  3.0234e-02, -1.1899e-02],\n",
            "          [-2.6099e-02,  3.7452e-03,  3.2776e-02],\n",
            "          [-3.3101e-02, -7.1923e-03,  1.6559e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.2818e-02, -1.0021e-01, -4.7012e-02],\n",
            "          [ 2.8293e-03,  4.1410e-02, -1.1391e-02],\n",
            "          [-1.1152e-02, -5.5861e-03,  1.9968e-02]],\n",
            "\n",
            "         [[-2.3932e-02, -3.0687e-02, -1.1756e-03],\n",
            "          [ 1.5311e-03, -3.5002e-02, -2.4414e-02],\n",
            "          [-8.7575e-03, -7.7842e-02, -3.8842e-02]],\n",
            "\n",
            "         [[ 2.6107e-02,  1.5406e-02,  1.7569e-02],\n",
            "          [-1.5130e-02, -4.8687e-03,  3.0773e-03],\n",
            "          [-1.3470e-02, -9.3201e-03, -4.8982e-03]]],\n",
            "\n",
            "\n",
            "        [[[-2.0228e-02, -3.0006e-02, -9.8419e-03],\n",
            "          [-3.8676e-02, -3.3481e-02, -7.4265e-03],\n",
            "          [-2.8935e-02, -3.2037e-02,  2.9245e-03]],\n",
            "\n",
            "         [[-1.2900e-02,  3.8046e-03,  1.5940e-02],\n",
            "          [-2.4030e-02,  2.0666e-03,  5.7250e-03],\n",
            "          [ 6.9989e-03,  1.2192e-02,  1.5406e-02]],\n",
            "\n",
            "         [[-1.5018e-02, -9.0988e-03,  2.4450e-02],\n",
            "          [ 1.0039e-02,  1.2561e-02,  2.6997e-02],\n",
            "          [ 2.9556e-02,  1.9463e-02, -2.6584e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.8481e-02,  3.9417e-04,  9.9768e-03],\n",
            "          [-4.5447e-03,  1.2307e-02,  3.5507e-02],\n",
            "          [-1.1873e-03, -2.6185e-03,  1.1547e-02]],\n",
            "\n",
            "         [[ 4.6292e-03, -1.3690e-02, -1.0171e-02],\n",
            "          [ 1.2104e-02,  1.6793e-02,  1.3003e-02],\n",
            "          [ 1.3328e-03,  3.4701e-03,  1.7323e-02]],\n",
            "\n",
            "         [[-8.7332e-05,  5.8646e-03, -3.5117e-03],\n",
            "          [ 3.8112e-03, -7.1828e-03, -1.1407e-02],\n",
            "          [ 1.9705e-02,  2.0556e-02,  5.7084e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 3.6998e-02,  3.2616e-02, -9.4535e-04],\n",
            "          [-2.9484e-02, -2.3441e-02, -2.8085e-02],\n",
            "          [-2.5451e-02,  3.9048e-02,  3.6686e-02]],\n",
            "\n",
            "         [[-1.8732e-02, -1.5352e-02,  1.1149e-02],\n",
            "          [-2.1324e-03, -2.3177e-02,  1.7628e-02],\n",
            "          [-4.0012e-03,  1.5463e-02,  9.2496e-03]],\n",
            "\n",
            "         [[-2.9346e-02,  7.7071e-03, -5.6520e-03],\n",
            "          [-2.3611e-02, -1.9390e-03,  2.0221e-02],\n",
            "          [ 8.0955e-03, -2.3268e-02, -2.8827e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.3532e-02, -2.9092e-02, -4.0045e-02],\n",
            "          [ 2.6530e-03, -2.0568e-02,  1.3075e-02],\n",
            "          [ 1.6061e-02, -5.5725e-02, -4.9167e-02]],\n",
            "\n",
            "         [[-7.9132e-03,  2.1466e-02,  2.0913e-02],\n",
            "          [-1.7259e-02, -2.5851e-02,  2.7177e-03],\n",
            "          [-4.6532e-02, -2.4846e-02, -1.9911e-02]],\n",
            "\n",
            "         [[-5.0350e-02, -2.5574e-02,  1.7763e-02],\n",
            "          [-3.4474e-02,  5.5247e-03, -2.7754e-02],\n",
            "          [-2.0743e-02, -2.2332e-02, -4.3512e-02]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.1194, 0.1625, 0.3084, 0.2931, 0.2957, 0.5263, 0.4038, 0.2024, 0.3401,\n",
            "        0.1982, 0.2559, 0.2311, 0.1630, 0.2891, 0.2248, 0.2311, 0.2417, 0.2187,\n",
            "        0.1922, 0.3103, 0.2015, 0.4802, 0.2481, 0.3898, 0.3204, 0.4035, 0.2617,\n",
            "        0.1551, 0.2256, 0.2117, 0.2708, 0.3537, 0.2505, 0.1843, 0.2465, 0.6501,\n",
            "        0.3898, 0.4289, 0.1799, 0.1604, 0.1775, 0.3600, 0.2694, 0.1283, 0.1662,\n",
            "        0.1716, 0.1837, 0.1710, 0.4178, 0.3249, 0.1759, 0.4717, 0.4115, 0.1995,\n",
            "        0.2025, 0.1492, 0.2860, 0.1072, 0.3649, 0.1906, 0.5369, 0.2400, 0.4411,\n",
            "        0.1702, 0.1993, 0.2045, 0.1972, 0.4041, 0.3034, 0.6168, 0.2284, 0.3228,\n",
            "        0.4547, 0.4370, 0.1570, 0.4057, 0.5791, 0.2338, 0.1586, 0.3130, 0.2201,\n",
            "        0.3195, 0.1166, 0.2517, 0.2184, 0.0989, 0.3116, 0.2613, 0.3277, 0.1778,\n",
            "        0.2718, 0.4174, 0.5140, 0.2136, 0.1905, 0.2898, 0.2472, 0.1341, 0.6212,\n",
            "        0.1810, 0.2394, 0.1417, 0.1759, 0.2827, 0.1987, 0.3775, 0.3749, 0.1274,\n",
            "        0.3656, 0.4305, 0.4212, 0.2673, 0.2016, 0.5098, 0.1449, 0.4408, 0.3583,\n",
            "        0.2503, 0.5682, 0.2518, 0.1392, 0.0617, 0.3406, 0.1313, 0.4586, 0.2914,\n",
            "        0.1326, 0.3915], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1403, -0.0889, -0.4147, -0.2264, -0.0737, -0.3534, -0.3379, -0.0752,\n",
            "        -0.1791,  0.0448, -0.2842, -0.1765, -0.1591, -0.0675, -0.1543, -0.1061,\n",
            "        -0.2334, -0.0981, -0.0908, -0.0567, -0.1908, -0.2055, -0.2704, -0.1883,\n",
            "        -0.3570, -0.1125, -0.1632, -0.0211, -0.1687, -0.2124, -0.1713, -0.0872,\n",
            "        -0.2194, -0.1888, -0.2954, -0.4570, -0.0226, -0.0527,  0.0406, -0.0609,\n",
            "        -0.0456, -0.1176, -0.0145,  0.0318, -0.2046, -0.0953, -0.0496, -0.1051,\n",
            "        -0.0793, -0.1933, -0.1467, -0.3215, -0.3257, -0.2287, -0.0356, -0.1869,\n",
            "        -0.1932, -0.0771,  0.2768, -0.0656, -0.0895, -0.2548, -0.2365,  0.0021,\n",
            "        -0.0987, -0.3178,  0.1613,  0.0006, -0.2347, -0.4150, -0.1310, -0.3142,\n",
            "        -0.2582, -0.5400,  0.0772, -0.2546, -0.4454, -0.0262, -0.0937, -0.2201,\n",
            "        -0.2044, -0.0155, -0.0893, -0.2167,  0.1112, -0.0619, -0.1217, -0.1593,\n",
            "        -0.1317, -0.1717, -0.3729, -0.3354, -0.3414,  0.0358, -0.2067, -0.1087,\n",
            "         0.0141, -0.0338, -0.2129, -0.1122, -0.1627, -0.2000,  0.0908, -0.0041,\n",
            "        -0.1313, -0.2942,  0.0160, -0.1065, -0.1289, -0.1699, -0.1721, -0.1809,\n",
            "        -0.2295, -0.3611, -0.1746, -0.3540, -0.1554, -0.2709, -0.2607,  0.0084,\n",
            "        -0.0311, -0.0022, -0.0831,  0.0380, -0.4893, -0.2749,  0.1245, -0.1272],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-0.0159, -0.0166, -0.0159],\n",
            "          [-0.0053,  0.0151,  0.0099],\n",
            "          [-0.0149,  0.0004, -0.0114]],\n",
            "\n",
            "         [[-0.0095, -0.0186, -0.0061],\n",
            "          [ 0.0098, -0.0123, -0.0053],\n",
            "          [ 0.0071, -0.0161, -0.0071]],\n",
            "\n",
            "         [[-0.0227, -0.0377, -0.0337],\n",
            "          [-0.0316, -0.0580, -0.0391],\n",
            "          [-0.0346, -0.0388, -0.0157]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0049,  0.0080,  0.0144],\n",
            "          [-0.0015,  0.0242,  0.0056],\n",
            "          [-0.0044,  0.0062,  0.0069]],\n",
            "\n",
            "         [[ 0.0160, -0.0120, -0.0013],\n",
            "          [ 0.0096,  0.0057,  0.0016],\n",
            "          [-0.0099, -0.0136, -0.0064]],\n",
            "\n",
            "         [[ 0.0534,  0.0464,  0.0248],\n",
            "          [ 0.0341, -0.0029, -0.0041],\n",
            "          [-0.0140, -0.0046, -0.0142]]],\n",
            "\n",
            "\n",
            "        [[[-0.0012, -0.0186, -0.0345],\n",
            "          [ 0.0050, -0.0117, -0.0333],\n",
            "          [ 0.0060, -0.0162, -0.0175]],\n",
            "\n",
            "         [[ 0.0107,  0.0140, -0.0192],\n",
            "          [ 0.0029,  0.0126,  0.0073],\n",
            "          [-0.0168, -0.0187, -0.0014]],\n",
            "\n",
            "         [[-0.0219,  0.0063,  0.0159],\n",
            "          [-0.0035,  0.0057,  0.0311],\n",
            "          [ 0.0023,  0.0032,  0.0175]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0162, -0.0078,  0.0077],\n",
            "          [-0.0015, -0.0122, -0.0093],\n",
            "          [ 0.0009, -0.0022,  0.0074]],\n",
            "\n",
            "         [[ 0.0029, -0.0236,  0.0058],\n",
            "          [ 0.0143, -0.0169, -0.0062],\n",
            "          [-0.0077, -0.0323, -0.0337]],\n",
            "\n",
            "         [[ 0.0087,  0.0140,  0.0081],\n",
            "          [-0.0034,  0.0105,  0.0150],\n",
            "          [ 0.0189,  0.0309,  0.0256]]],\n",
            "\n",
            "\n",
            "        [[[-0.0358, -0.0226, -0.0144],\n",
            "          [-0.0075, -0.0221,  0.0111],\n",
            "          [-0.0036, -0.0148, -0.0164]],\n",
            "\n",
            "         [[-0.0146, -0.0307, -0.0204],\n",
            "          [-0.0285, -0.0453, -0.0579],\n",
            "          [ 0.0288, -0.0152, -0.0245]],\n",
            "\n",
            "         [[ 0.0174,  0.0199, -0.0046],\n",
            "          [ 0.0178,  0.0236,  0.0136],\n",
            "          [ 0.0293,  0.0434,  0.0186]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0079, -0.0122, -0.0158],\n",
            "          [-0.0107, -0.0314, -0.0075],\n",
            "          [-0.0070, -0.0173, -0.0308]],\n",
            "\n",
            "         [[-0.0103, -0.0118, -0.0171],\n",
            "          [-0.0264, -0.0015,  0.0280],\n",
            "          [-0.0089,  0.0054,  0.0097]],\n",
            "\n",
            "         [[ 0.0145, -0.0315, -0.0197],\n",
            "          [-0.0149, -0.0177,  0.0077],\n",
            "          [ 0.0125,  0.0071, -0.0062]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0053,  0.0255,  0.0071],\n",
            "          [ 0.0197,  0.0270,  0.0420],\n",
            "          [ 0.0412,  0.0223,  0.0350]],\n",
            "\n",
            "         [[ 0.0026,  0.0041,  0.0106],\n",
            "          [ 0.0046,  0.0203,  0.0081],\n",
            "          [ 0.0145, -0.0030, -0.0197]],\n",
            "\n",
            "         [[-0.0040,  0.0255,  0.0023],\n",
            "          [-0.0152,  0.0260,  0.0083],\n",
            "          [-0.0021,  0.0269,  0.0032]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0055, -0.0049,  0.0183],\n",
            "          [ 0.0085,  0.0023,  0.0077],\n",
            "          [ 0.0082,  0.0096,  0.0215]],\n",
            "\n",
            "         [[-0.0177,  0.0099, -0.0129],\n",
            "          [-0.0127,  0.0096, -0.0124],\n",
            "          [ 0.0090,  0.0493,  0.0362]],\n",
            "\n",
            "         [[ 0.0123,  0.0184, -0.0178],\n",
            "          [ 0.0058, -0.0058, -0.0117],\n",
            "          [ 0.0034, -0.0103, -0.0425]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0316,  0.0765,  0.0443],\n",
            "          [ 0.0940,  0.1480,  0.1510],\n",
            "          [ 0.0665,  0.1386,  0.1132]],\n",
            "\n",
            "         [[-0.0282, -0.0041, -0.0200],\n",
            "          [-0.0193, -0.0012,  0.0107],\n",
            "          [-0.0165, -0.0028,  0.0008]],\n",
            "\n",
            "         [[-0.0122, -0.0322, -0.0153],\n",
            "          [-0.0009, -0.0123, -0.0039],\n",
            "          [-0.0193, -0.0107, -0.0211]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0030,  0.0066, -0.0003],\n",
            "          [ 0.0146, -0.0167,  0.0142],\n",
            "          [ 0.0227,  0.0120,  0.0072]],\n",
            "\n",
            "         [[ 0.0039, -0.0197,  0.0111],\n",
            "          [-0.0251, -0.0384, -0.0447],\n",
            "          [-0.0359, -0.0981, -0.0684]],\n",
            "\n",
            "         [[-0.0085,  0.0023,  0.0031],\n",
            "          [ 0.0038,  0.0182,  0.0069],\n",
            "          [ 0.0089,  0.0080,  0.0126]]],\n",
            "\n",
            "\n",
            "        [[[-0.0130, -0.0090,  0.0011],\n",
            "          [-0.0260, -0.0195, -0.0094],\n",
            "          [ 0.0048,  0.0024,  0.0106]],\n",
            "\n",
            "         [[-0.0025, -0.0140, -0.0286],\n",
            "          [-0.0024,  0.0011, -0.0233],\n",
            "          [ 0.0123,  0.0008,  0.0014]],\n",
            "\n",
            "         [[-0.0490, -0.0439, -0.0579],\n",
            "          [-0.0359, -0.0365, -0.0386],\n",
            "          [-0.0410, -0.0333, -0.0137]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0118, -0.0081, -0.0158],\n",
            "          [-0.0272, -0.0285,  0.0075],\n",
            "          [-0.0244,  0.0139,  0.0061]],\n",
            "\n",
            "         [[-0.0273,  0.0197,  0.0222],\n",
            "          [-0.0376,  0.0208,  0.0187],\n",
            "          [-0.0446, -0.0044, -0.0168]],\n",
            "\n",
            "         [[ 0.0199,  0.0268,  0.0120],\n",
            "          [ 0.0203,  0.0215,  0.0025],\n",
            "          [ 0.0002, -0.0076, -0.0196]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.2856, 0.2425, 0.3032, 0.3168, 0.3011, 0.3475, 0.3076, 0.3105, 0.3646,\n",
            "        0.3255, 0.2195, 0.3167, 0.2674, 0.3104, 0.3026, 0.3443, 0.2915, 0.3379,\n",
            "        0.2887, 0.2996, 0.3588, 0.3164, 0.2882, 0.2917, 0.3492, 0.3749, 0.3587,\n",
            "        0.3166, 0.2756, 0.2978, 0.3364, 0.2893, 0.3106, 0.2506, 0.3460, 0.3621,\n",
            "        0.2570, 0.3695, 0.2935, 0.3286, 0.3243, 0.3188, 0.3093, 0.3314, 0.3550,\n",
            "        0.2978, 0.2737, 0.3023, 0.3179, 0.2831, 0.3065, 0.3390, 0.3053, 0.3099,\n",
            "        0.3017, 0.3472, 0.3034, 0.2935, 0.3352, 0.3676, 0.3163, 0.3404, 0.3078,\n",
            "        0.2819, 0.3794, 0.3083, 0.2778, 0.3363, 0.2284, 0.3259, 0.2790, 0.3072,\n",
            "        0.2975, 0.3847, 0.3372, 0.2253, 0.2827, 0.3737, 0.2796, 0.3485, 0.3879,\n",
            "        0.3288, 0.3340, 0.3335, 0.2756, 0.3500, 0.2897, 0.2798, 0.2907, 0.3220,\n",
            "        0.3824, 0.3522, 0.3278, 0.3689, 0.3147, 0.3600, 0.3123, 0.2519, 0.2355,\n",
            "        0.3211, 0.3203, 0.3345, 0.2768, 0.3341, 0.3153, 0.3175, 0.2224, 0.2956,\n",
            "        0.3206, 0.2658, 0.3662, 0.2715, 0.3655, 0.3427, 0.2820, 0.2754, 0.4669,\n",
            "        0.3090, 0.3468, 0.3144, 0.3220, 0.2765, 0.3301, 0.3219, 0.3152, 0.2813,\n",
            "        0.2497, 0.3514, 0.3264, 0.3014, 0.2734, 0.3522, 0.3831, 0.3028, 0.2940,\n",
            "        0.2825, 0.3099, 0.2373, 0.2705, 0.4189, 0.2985, 0.3841, 0.2754, 0.3091,\n",
            "        0.3169, 0.2824, 0.2749, 0.3493, 0.4018, 0.3108, 0.2176, 0.2821, 0.3199,\n",
            "        0.3358, 0.2468, 0.3332, 0.2876, 0.2964, 0.2385, 0.3451, 0.3081, 0.2760,\n",
            "        0.2533, 0.2576, 0.3092, 0.2950, 0.3089, 0.3113, 0.3475, 0.3172, 0.2474,\n",
            "        0.3371, 0.3450, 0.3189, 0.3150, 0.3008, 0.2694, 0.3730, 0.3235, 0.2988,\n",
            "        0.2812, 0.3245, 0.3630, 0.2843, 0.3533, 0.3451, 0.3244, 0.3524, 0.3118,\n",
            "        0.3429, 0.3215, 0.2748, 0.3287, 0.3656, 0.2901, 0.2523, 0.3284, 0.2523,\n",
            "        0.3426, 0.2851, 0.2918, 0.2497, 0.5159, 0.3026, 0.2743, 0.2379, 0.3524,\n",
            "        0.3394, 0.2264, 0.2652, 0.3759, 0.3777, 0.2459, 0.3046, 0.3067, 0.3775,\n",
            "        0.2976, 0.3552, 0.2696, 0.2649, 0.2872, 0.2985, 0.2867, 0.3676, 0.3494,\n",
            "        0.3823, 0.3246, 0.3567, 0.2662, 0.3357, 0.2935, 0.2987, 0.2664, 0.3019,\n",
            "        0.3175, 0.2436, 0.3274, 0.2764, 0.2466, 0.2876, 0.3060, 0.3157, 0.3329,\n",
            "        0.2984, 0.2961, 0.3309, 0.3729, 0.3238, 0.3491, 0.3342, 0.3037, 0.3578,\n",
            "        0.2849, 0.2827, 0.2809, 0.3249], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0915,  0.0189, -0.1235, -0.0613, -0.1003, -0.1306, -0.1473, -0.1079,\n",
            "        -0.2438, -0.1113,  0.1361, -0.1477,  0.0387, -0.0907,  0.0352, -0.1851,\n",
            "        -0.1319, -0.1746, -0.0815, -0.1004, -0.3394, -0.1712, -0.0807, -0.1228,\n",
            "        -0.2263, -0.1503, -0.2314, -0.2327, -0.0854, -0.0802, -0.0716, -0.0839,\n",
            "        -0.0592,  0.0358, -0.0322, -0.2197,  0.0027, -0.1471, -0.0264, -0.1886,\n",
            "        -0.2417, -0.1494, -0.1904, -0.1089, -0.2657, -0.1362, -0.0487, -0.1340,\n",
            "        -0.0930, -0.0064, -0.1721, -0.1476, -0.1714,  0.0336, -0.1011, -0.1761,\n",
            "        -0.1184, -0.0482, -0.3260, -0.1555, -0.0169, -0.2373, -0.1015, -0.1051,\n",
            "        -0.2738, -0.1917, -0.0503, -0.1098,  0.1484, -0.2282, -0.0700, -0.1427,\n",
            "        -0.1417, -0.3096, -0.2043,  0.0269, -0.0779, -0.0842, -0.0464, -0.1429,\n",
            "        -0.3917,  0.0257, -0.1779, -0.0993, -0.0507, -0.2222, -0.0951, -0.0861,\n",
            "        -0.0743, -0.1666, -0.2054, -0.1782, -0.1150, -0.2525, -0.0694, -0.0536,\n",
            "        -0.0499, -0.0311,  0.1212, -0.0988, -0.1570, -0.3093, -0.0797, -0.0994,\n",
            "        -0.1774, -0.0505,  0.0766, -0.0480, -0.1278, -0.0651, -0.1737,  0.0303,\n",
            "        -0.1334, -0.2435, -0.0746, -0.0365, -0.1843, -0.0887, -0.1924, -0.1110,\n",
            "        -0.1458, -0.0895, -0.0956, -0.2042, -0.1338, -0.0637, -0.0699, -0.1656,\n",
            "        -0.1521, -0.1317, -0.0826, -0.2470, -0.1174, -0.1475, -0.0840, -0.0681,\n",
            "        -0.1789,  0.0288, -0.0362, -0.3005, -0.1441, -0.0812, -0.0492, -0.0657,\n",
            "        -0.1249, -0.1104,  0.0187, -0.1351, -0.1944, -0.0909,  0.2067, -0.1081,\n",
            "        -0.2499, -0.0999,  0.0507, -0.1899, -0.0369, -0.1432,  0.1279, -0.1782,\n",
            "        -0.1172, -0.0099,  0.0785, -0.0681, -0.0365, -0.1596, -0.1606, -0.0922,\n",
            "        -0.1773, -0.1788,  0.0306, -0.1101, -0.1355, -0.2244, -0.0860, -0.1232,\n",
            "        -0.0927, -0.1666, -0.1393, -0.0898, -0.0614, -0.1740, -0.2503, -0.0593,\n",
            "        -0.1272, -0.1422, -0.0743, -0.2208, -0.2207, -0.2742, -0.1302, -0.0916,\n",
            "        -0.1696, -0.2481, -0.1524,  0.0410, -0.1077,  0.0408, -0.1915, -0.0697,\n",
            "        -0.1049, -0.0110, -0.3257, -0.1336, -0.1021,  0.0128, -0.2717, -0.1245,\n",
            "         0.0288, -0.1025, -0.2405, -0.1476,  0.1008, -0.0220, -0.0983, -0.4417,\n",
            "        -0.0774, -0.3207, -0.0272, -0.0726, -0.0608, -0.0430, -0.0872, -0.1280,\n",
            "        -0.1608, -0.1529, -0.1745, -0.1702, -0.0486, -0.1459, -0.0552, -0.0808,\n",
            "        -0.0264, -0.0952, -0.1126, -0.0452, -0.0837, -0.0331,  0.0127, -0.0865,\n",
            "        -0.1446, -0.0732, -0.2160, -0.0952, -0.1297, -0.2008, -0.2135, -0.2204,\n",
            "        -0.2381, -0.1787, -0.1386, -0.1901, -0.0981, -0.0850, -0.0761, -0.0586],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-0.0093, -0.0339, -0.0119],\n",
            "          [-0.0246, -0.0798, -0.0487],\n",
            "          [-0.0435, -0.0801, -0.0653]],\n",
            "\n",
            "         [[-0.0289,  0.0002, -0.0286],\n",
            "          [ 0.0099,  0.0103, -0.0177],\n",
            "          [-0.0107,  0.0028, -0.0125]],\n",
            "\n",
            "         [[-0.0147,  0.0226,  0.0044],\n",
            "          [ 0.0155,  0.0109, -0.0040],\n",
            "          [-0.0208, -0.0180, -0.0173]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0003, -0.0041, -0.0008],\n",
            "          [-0.0217, -0.0223, -0.0299],\n",
            "          [ 0.0105,  0.0035, -0.0114]],\n",
            "\n",
            "         [[ 0.0097,  0.0184,  0.0370],\n",
            "          [ 0.0037,  0.0104,  0.0152],\n",
            "          [ 0.0084,  0.0183,  0.0302]],\n",
            "\n",
            "         [[ 0.0014,  0.0084,  0.0097],\n",
            "          [ 0.0265,  0.0415,  0.0553],\n",
            "          [ 0.0169,  0.0610,  0.0563]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0117,  0.0165,  0.0051],\n",
            "          [ 0.0294,  0.0204,  0.0216],\n",
            "          [ 0.0079,  0.0133,  0.0117]],\n",
            "\n",
            "         [[-0.0153, -0.0213, -0.0090],\n",
            "          [-0.0292, -0.0516, -0.0436],\n",
            "          [-0.0045, -0.0372, -0.0420]],\n",
            "\n",
            "         [[ 0.0003,  0.0398, -0.0001],\n",
            "          [ 0.0129,  0.0332,  0.0163],\n",
            "          [-0.0096, -0.0057, -0.0170]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0447,  0.0082,  0.0169],\n",
            "          [-0.0066, -0.0371, -0.0059],\n",
            "          [-0.0239, -0.0607, -0.0289]],\n",
            "\n",
            "         [[-0.0057, -0.0423, -0.0219],\n",
            "          [-0.0228, -0.0314, -0.0583],\n",
            "          [-0.0196, -0.0530, -0.0485]],\n",
            "\n",
            "         [[ 0.0065,  0.0033,  0.0093],\n",
            "          [ 0.0010, -0.0049, -0.0110],\n",
            "          [-0.0160, -0.0101, -0.0146]]],\n",
            "\n",
            "\n",
            "        [[[-0.0061, -0.0067, -0.0069],\n",
            "          [-0.0052, -0.0089, -0.0143],\n",
            "          [-0.0115, -0.0171, -0.0237]],\n",
            "\n",
            "         [[ 0.0399,  0.0167,  0.0210],\n",
            "          [ 0.0165, -0.0262, -0.0116],\n",
            "          [ 0.0059, -0.0206, -0.0153]],\n",
            "\n",
            "         [[ 0.0060,  0.0242,  0.0207],\n",
            "          [ 0.0050, -0.0062,  0.0148],\n",
            "          [ 0.0099, -0.0279, -0.0054]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0097, -0.0017, -0.0101],\n",
            "          [-0.0006,  0.0434,  0.0381],\n",
            "          [ 0.0038,  0.0450,  0.0392]],\n",
            "\n",
            "         [[ 0.0134,  0.0069,  0.0196],\n",
            "          [ 0.0068,  0.0250,  0.0152],\n",
            "          [ 0.0018, -0.0044,  0.0037]],\n",
            "\n",
            "         [[-0.0174, -0.0163, -0.0240],\n",
            "          [-0.0197, -0.0174, -0.0178],\n",
            "          [-0.0300, -0.0137, -0.0211]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0066,  0.0061,  0.0284],\n",
            "          [-0.0130, -0.0509, -0.0204],\n",
            "          [-0.0154, -0.0149, -0.0181]],\n",
            "\n",
            "         [[ 0.0123,  0.0325,  0.0227],\n",
            "          [-0.0042, -0.0181,  0.0022],\n",
            "          [ 0.0029, -0.0174,  0.0032]],\n",
            "\n",
            "         [[-0.0110,  0.0111, -0.0142],\n",
            "          [ 0.0082,  0.0301,  0.0416],\n",
            "          [ 0.0006,  0.0002,  0.0213]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0008,  0.0266,  0.0118],\n",
            "          [-0.0159, -0.0074, -0.0051],\n",
            "          [ 0.0042, -0.0069, -0.0073]],\n",
            "\n",
            "         [[-0.0070,  0.0010,  0.0019],\n",
            "          [ 0.0061,  0.0510, -0.0035],\n",
            "          [-0.0081, -0.0303, -0.0174]],\n",
            "\n",
            "         [[-0.0100,  0.0003, -0.0011],\n",
            "          [-0.0015, -0.0297, -0.0195],\n",
            "          [-0.0010,  0.0049,  0.0151]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0161,  0.0126,  0.0055],\n",
            "          [-0.0132, -0.0196, -0.0208],\n",
            "          [-0.0077, -0.0230, -0.0202]],\n",
            "\n",
            "         [[-0.0019,  0.0530,  0.0134],\n",
            "          [ 0.0027,  0.0249,  0.0167],\n",
            "          [-0.0192, -0.0188, -0.0200]],\n",
            "\n",
            "         [[ 0.0081,  0.0300,  0.0006],\n",
            "          [ 0.0036,  0.0254, -0.0104],\n",
            "          [-0.0010, -0.0193, -0.0121]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0163,  0.0251,  0.0290],\n",
            "          [-0.0073, -0.0014,  0.0124],\n",
            "          [-0.0211, -0.0347, -0.0195]],\n",
            "\n",
            "         [[ 0.0165,  0.0519,  0.0494],\n",
            "          [ 0.0058,  0.0199,  0.0363],\n",
            "          [-0.0049, -0.0165, -0.0130]],\n",
            "\n",
            "         [[-0.0102, -0.0308, -0.0340],\n",
            "          [ 0.0055, -0.0109,  0.0005],\n",
            "          [ 0.0382,  0.0209,  0.0326]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0184, -0.0031,  0.0103],\n",
            "          [-0.0072, -0.0013, -0.0070],\n",
            "          [ 0.0217,  0.0011, -0.0033]],\n",
            "\n",
            "         [[-0.0166,  0.0002, -0.0092],\n",
            "          [ 0.0150,  0.0584,  0.0218],\n",
            "          [-0.0136,  0.0181,  0.0164]],\n",
            "\n",
            "         [[ 0.0219,  0.0316,  0.0183],\n",
            "          [-0.0021, -0.0156,  0.0203],\n",
            "          [ 0.0053,  0.0005,  0.0157]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0045, -0.0025,  0.0017],\n",
            "          [-0.0052, -0.0088,  0.0137],\n",
            "          [-0.0107,  0.0010,  0.0170]],\n",
            "\n",
            "         [[-0.0060, -0.0368,  0.0031],\n",
            "          [ 0.0102,  0.0291, -0.0007],\n",
            "          [ 0.0099,  0.0600,  0.0272]],\n",
            "\n",
            "         [[-0.0092, -0.0183,  0.0062],\n",
            "          [-0.0319, -0.0294, -0.0149],\n",
            "          [-0.0148, -0.0123, -0.0236]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.3212, 0.2124, 0.2661, 0.3594, 0.2785, 0.2582, 0.3108, 0.3096, 0.3348,\n",
            "        0.2992, 0.2545, 0.2458, 0.3133, 0.4159, 0.2997, 0.3070, 0.3135, 0.4418,\n",
            "        0.3743, 0.2570, 0.2943, 0.3078, 0.2738, 0.3948, 0.2928, 0.3572, 0.3435,\n",
            "        0.5379, 0.4243, 0.3908, 0.2745, 0.2798, 0.3217, 0.1956, 0.2751, 0.3187,\n",
            "        0.3507, 0.2751, 0.1919, 0.3307, 0.2850, 0.3038, 0.2179, 0.2652, 0.2944,\n",
            "        0.2138, 0.2184, 0.2948, 0.3262, 0.3759, 0.2557, 0.3796, 0.2950, 0.3386,\n",
            "        0.3243, 0.3070, 0.3331, 0.2302, 0.3036, 0.3377, 0.2922, 0.2204, 0.3267,\n",
            "        0.3198, 0.4023, 0.2987, 0.4860, 0.2854, 0.2716, 0.4341, 0.2834, 0.2296,\n",
            "        0.2507, 0.3120, 0.3673, 0.3244, 0.3380, 0.3272, 0.2868, 0.2877, 0.3210,\n",
            "        0.2332, 0.3379, 0.2767, 0.2942, 0.2672, 0.4401, 0.2908, 0.3771, 0.2789,\n",
            "        0.3056, 0.3276, 0.3871, 0.2453, 0.2559, 0.2783, 0.3168, 0.3410, 0.2318,\n",
            "        0.3577, 0.5036, 0.3557, 0.2475, 0.1852, 0.2273, 0.3602, 0.2919, 0.3928,\n",
            "        0.4423, 0.2052, 0.2524, 0.2189, 0.4113, 0.3611, 0.4284, 0.2333, 0.3504,\n",
            "        0.7001, 0.3754, 0.2874, 0.3702, 0.3174, 0.3640, 0.2889, 0.4155, 0.2479,\n",
            "        0.2898, 0.3740, 0.4926, 0.2808, 0.2388, 0.3473, 0.1868, 0.2837, 0.3090,\n",
            "        0.3614, 0.2797, 0.6871, 0.2854, 0.2937, 0.3128, 0.4863, 0.2193, 0.2871,\n",
            "        0.2554, 0.4175, 0.3044, 0.3230, 0.3343, 0.4947, 0.3924, 0.2264, 0.2657,\n",
            "        0.4193, 0.3483, 0.3551, 0.2877, 0.2559, 0.2459, 0.2775, 0.3842, 0.2949,\n",
            "        0.3510, 0.1926, 0.3101, 0.3417, 0.3931, 0.3918, 0.3239, 0.2851, 0.4583,\n",
            "        0.2669, 0.2663, 0.4433, 0.3221, 0.3655, 0.3336, 0.4393, 0.3970, 0.3727,\n",
            "        0.3523, 0.3586, 0.3286, 0.4181, 0.2955, 0.3050, 0.2988, 0.4320, 0.2309,\n",
            "        0.3826, 0.2270, 0.2228, 0.3206, 0.3273, 0.2627, 0.3087, 0.2920, 0.2328,\n",
            "        0.4144, 0.4075, 0.3264, 0.3583, 0.3014, 0.3150, 0.4438, 0.4042, 0.2028,\n",
            "        0.3855, 0.2570, 0.2361, 0.2343, 0.3312, 0.2303, 0.3744, 0.4727, 0.3601,\n",
            "        0.2754, 0.1987, 0.3027, 0.3427, 0.2994, 0.2533, 0.2639, 0.3460, 0.3847,\n",
            "        0.4368, 0.3786, 0.3123, 0.2591, 0.3979, 0.2577, 0.3131, 0.2934, 0.3027,\n",
            "        0.2942, 0.2266, 0.2806, 0.2977, 0.1858, 0.2788, 0.2504, 0.3948, 0.3496,\n",
            "        0.2429, 0.2155, 0.2683, 0.4100, 0.3495, 0.4243, 0.2627, 0.3329, 0.2849,\n",
            "        0.3924, 0.3728, 0.2655, 0.3338], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-2.6385e-02,  9.9545e-02, -6.8362e-03, -8.7678e-02,  7.8322e-03,\n",
            "         4.0653e-02, -3.0738e-02,  5.9561e-03,  1.6938e-03,  4.7769e-02,\n",
            "         6.3004e-02,  3.5760e-02, -5.0405e-02,  2.1372e-02, -9.0327e-03,\n",
            "        -3.3702e-02, -4.5529e-02, -1.9238e-01, -6.7608e-02,  7.7464e-02,\n",
            "        -3.3957e-02, -7.9947e-02,  1.3138e-01, -1.2730e-01, -6.2825e-02,\n",
            "        -5.4973e-03, -9.1520e-02, -1.7570e-01, -8.2958e-03, -9.4468e-02,\n",
            "         2.4915e-03, -3.1894e-02, -1.5755e-02,  1.4372e-01, -3.4770e-03,\n",
            "         1.0756e-02, -5.1074e-02,  3.5848e-02,  8.7819e-02, -4.5240e-02,\n",
            "        -4.5785e-02,  1.4690e-02,  6.8685e-02,  1.6806e-02, -4.7715e-02,\n",
            "         5.6814e-02,  4.5985e-02, -5.0668e-02,  5.8914e-03, -1.0335e-01,\n",
            "         1.0281e-02, -1.0520e-01, -1.6557e-02, -1.9213e-02, -3.4522e-02,\n",
            "         2.0138e-02, -1.3621e-01,  3.9602e-02, -8.8277e-03, -1.0837e-02,\n",
            "        -2.9803e-02,  7.2119e-02, -6.6893e-02, -9.4287e-03, -3.0956e-02,\n",
            "        -2.6700e-02, -1.4184e-01,  1.1900e-01,  6.6929e-02, -2.1375e-01,\n",
            "         4.2720e-02,  4.7782e-02,  3.3905e-02,  1.3857e-04, -1.4822e-01,\n",
            "        -2.3684e-02, -7.4337e-02, -6.8427e-02, -2.0076e-02,  1.4697e-02,\n",
            "        -3.9620e-02,  1.9393e-02, -6.9640e-02, -5.5779e-02,  7.9664e-03,\n",
            "         2.3639e-02, -2.5781e-01,  6.3564e-03, -1.0041e-01,  2.8047e-02,\n",
            "         1.5223e-02, -4.8413e-02, -1.5355e-01,  1.0493e-01,  4.9892e-02,\n",
            "         6.5689e-02, -5.4139e-02,  7.7265e-03,  9.4090e-02, -1.9978e-02,\n",
            "        -2.3561e-01, -6.2284e-02,  3.3363e-02,  1.1017e-01,  7.6999e-02,\n",
            "        -3.2478e-02,  4.8089e-02, -1.4995e-01, -1.6503e-01,  1.2304e-01,\n",
            "         7.1157e-02,  5.8895e-02, -4.8155e-02, -9.7220e-02, -1.8604e-01,\n",
            "         8.5326e-02, -5.1602e-02, -3.0799e-01, -6.0410e-02, -7.7131e-02,\n",
            "        -2.7282e-01,  2.8950e-02, -1.3280e-01,  1.7264e-02, -3.9184e-02,\n",
            "         5.4153e-02, -3.7219e-02, -1.5283e-01, -1.7663e-01,  8.3937e-02,\n",
            "         6.9329e-02, -8.2645e-02,  1.1178e-01, -5.0772e-02, -4.4772e-02,\n",
            "        -3.7455e-02,  3.0361e-02, -3.7824e-01,  1.4857e-02,  6.8245e-03,\n",
            "        -5.2090e-02, -2.9501e-01,  8.9860e-02,  2.9646e-02,  1.9930e-02,\n",
            "        -8.3483e-02, -9.6416e-02, -2.3830e-02,  3.4911e-02, -2.6626e-01,\n",
            "        -1.6184e-01,  7.3552e-02,  2.7568e-02, -1.1087e-01, -1.0322e-02,\n",
            "        -9.7499e-02,  1.3995e-02,  1.0802e-02,  7.8444e-02,  1.3057e-02,\n",
            "        -3.9476e-02,  2.4805e-02, -7.7445e-02, -2.8419e-02,  1.0412e-02,\n",
            "        -4.2330e-02, -1.6634e-01, -9.4908e-02, -3.4328e-02,  4.5544e-02,\n",
            "        -3.0001e-01, -6.9376e-03,  1.4103e-02, -2.6155e-01, -7.3598e-02,\n",
            "        -1.0627e-01, -1.0544e-02, -7.1160e-02, -1.0339e-01, -2.9787e-02,\n",
            "        -1.4277e-01, -5.1710e-02, -5.7126e-02, -5.4392e-02, -4.2260e-02,\n",
            "        -8.5120e-03,  1.5887e-02, -6.5358e-02, -6.1346e-02, -1.4502e-01,\n",
            "         3.9939e-02,  8.1640e-02, -7.7597e-03, -3.4062e-02,  3.1969e-02,\n",
            "        -4.4756e-02, -7.0306e-02,  1.0213e-01, -1.7993e-01, -2.1173e-01,\n",
            "        -5.9797e-02, -1.1596e-01,  3.9271e-02, -4.5443e-02, -1.8446e-01,\n",
            "        -1.0848e-01,  5.5781e-02, -6.3649e-02,  1.6825e-02,  1.6623e-04,\n",
            "         7.9866e-02, -6.7240e-02,  7.9827e-02, -3.9905e-03, -1.9016e-01,\n",
            "         2.0026e-02,  7.3181e-02,  1.0323e-01, -2.6431e-02,  2.3963e-02,\n",
            "        -4.4247e-02,  2.2896e-02,  2.3444e-02, -2.3481e-02,  1.0516e-02,\n",
            "        -2.1494e-01, -1.2810e-01, -1.8338e-02, -6.0221e-04, -5.1624e-02,\n",
            "         5.6575e-02, -5.4297e-02,  1.4146e-02, -4.9852e-02,  6.7255e-02,\n",
            "         5.1713e-02, -4.0266e-03,  3.5057e-02,  8.2781e-02,  1.0034e-02,\n",
            "         5.9230e-02, -2.0429e-01, -7.6169e-02,  4.1432e-02,  7.7517e-02,\n",
            "         7.6005e-02, -1.5919e-01, -8.3613e-02, -1.6625e-01,  2.2968e-03,\n",
            "        -6.8489e-02,  3.8119e-02, -9.8731e-02, -2.0290e-02,  1.5394e-02,\n",
            "        -1.0548e-01], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 0.0081]],\n",
            "\n",
            "         [[-0.0192]],\n",
            "\n",
            "         [[-0.0173]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0128]],\n",
            "\n",
            "         [[ 0.0025]],\n",
            "\n",
            "         [[ 0.0054]]],\n",
            "\n",
            "\n",
            "        [[[-0.0143]],\n",
            "\n",
            "         [[-0.0554]],\n",
            "\n",
            "         [[-0.0346]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0275]],\n",
            "\n",
            "         [[ 0.0360]],\n",
            "\n",
            "         [[ 0.0240]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0076]],\n",
            "\n",
            "         [[ 0.0207]],\n",
            "\n",
            "         [[-0.0101]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0278]],\n",
            "\n",
            "         [[ 0.0064]],\n",
            "\n",
            "         [[-0.0022]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0336]],\n",
            "\n",
            "         [[-0.0424]],\n",
            "\n",
            "         [[ 0.0226]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0330]],\n",
            "\n",
            "         [[-0.0009]],\n",
            "\n",
            "         [[-0.0177]]],\n",
            "\n",
            "\n",
            "        [[[-0.0114]],\n",
            "\n",
            "         [[-0.0183]],\n",
            "\n",
            "         [[ 0.0076]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0151]],\n",
            "\n",
            "         [[ 0.0332]],\n",
            "\n",
            "         [[ 0.0002]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0063]],\n",
            "\n",
            "         [[-0.0200]],\n",
            "\n",
            "         [[ 0.0010]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0191]],\n",
            "\n",
            "         [[ 0.0455]],\n",
            "\n",
            "         [[ 0.0078]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.0674, 0.0514, 0.0385, 0.1692, 0.0604, 0.0460, 0.1209, 0.1110, 0.0418,\n",
            "        0.0387, 0.0442, 0.0707, 0.0790, 0.1094, 0.0959, 0.0544, 0.1032, 0.2190,\n",
            "        0.0459, 0.0372, 0.1410, 0.0587, 0.0360, 0.0955, 0.1657, 0.1024, 0.1417,\n",
            "        0.0580, 0.0536, 0.0716, 0.0865, 0.1110, 0.0511, 0.0515, 0.0809, 0.1154,\n",
            "        0.0777, 0.0449, 0.0490, 0.1056, 0.1457, 0.0744, 0.0530, 0.0600, 0.1026,\n",
            "        0.0486, 0.0408, 0.1312, 0.0639, 0.1062, 0.0915, 0.1476, 0.0900, 0.0742,\n",
            "        0.1069, 0.0776, 0.1423, 0.0495, 0.0974, 0.0661, 0.1292, 0.0548, 0.1145,\n",
            "        0.0950, 0.0921, 0.1579, 0.0496, 0.0236, 0.0398, 0.0935, 0.0291, 0.0653,\n",
            "        0.0885, 0.1190, 0.1692, 0.0692, 0.1316, 0.0606, 0.0480, 0.0654, 0.1082,\n",
            "        0.0624, 0.1103, 0.1106, 0.1076, 0.0400, 0.0723, 0.0947, 0.0662, 0.0464,\n",
            "        0.0444, 0.1727, 0.0921, 0.0345, 0.0451, 0.0374, 0.0940, 0.0818, 0.0397,\n",
            "        0.0452, 0.0985, 0.1095, 0.1072, 0.0506, 0.0444, 0.0755, 0.0420, 0.1046,\n",
            "        0.1172, 0.0447, 0.0459, 0.0409, 0.0539, 0.1036, 0.0741, 0.0311, 0.1086,\n",
            "        0.1746, 0.0777, 0.0689, 0.1100, 0.0489, 0.1048, 0.1097, 0.1025, 0.0448,\n",
            "        0.0675, 0.0707, 0.1364, 0.0438, 0.0346, 0.1769, 0.0667, 0.1155, 0.0628,\n",
            "        0.0873, 0.0406, 0.2890, 0.0703, 0.0428, 0.1173, 0.1049, 0.0611, 0.0469,\n",
            "        0.0400, 0.0744, 0.1003, 0.1012, 0.0599, 0.1078, 0.1512, 0.0322, 0.0430,\n",
            "        0.0977, 0.0951, 0.0838, 0.0958, 0.0448, 0.0263, 0.0425, 0.1154, 0.0771,\n",
            "        0.1781, 0.0300, 0.0699, 0.0724, 0.1600, 0.0893, 0.1130, 0.0534, 0.1359,\n",
            "        0.0375, 0.0809, 0.1145, 0.1232, 0.0942, 0.0880, 0.0346, 0.0996, 0.0461,\n",
            "        0.0694, 0.0630, 0.1590, 0.0509, 0.1254, 0.0590, 0.0744, 0.1084, 0.0514,\n",
            "        0.0931, 0.0848, 0.0240, 0.0279, 0.0993, 0.0612, 0.0599, 0.1095, 0.0508,\n",
            "        0.0658, 0.1162, 0.0833, 0.1651, 0.0505, 0.1231, 0.1228, 0.1038, 0.0369,\n",
            "        0.0756, 0.0415, 0.1192, 0.0292, 0.0839, 0.0577, 0.0951, 0.0944, 0.0309,\n",
            "        0.0390, 0.0604, 0.0672, 0.0501, 0.0383, 0.0946, 0.0958, 0.0501, 0.0243,\n",
            "        0.1074, 0.1908, 0.0693, 0.1376, 0.1151, 0.0329, 0.0647, 0.0616, 0.1106,\n",
            "        0.0358, 0.0721, 0.0851, 0.0375, 0.0368, 0.0947, 0.0464, 0.1666, 0.1049,\n",
            "        0.0755, 0.0398, 0.0249, 0.1528, 0.1167, 0.0886, 0.0540, 0.0726, 0.0736,\n",
            "        0.0797, 0.0854, 0.0609, 0.1263], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-2.6385e-02,  9.9545e-02, -6.8362e-03, -8.7678e-02,  7.8322e-03,\n",
            "         4.0653e-02, -3.0738e-02,  5.9561e-03,  1.6938e-03,  4.7769e-02,\n",
            "         6.3004e-02,  3.5760e-02, -5.0405e-02,  2.1372e-02, -9.0327e-03,\n",
            "        -3.3702e-02, -4.5529e-02, -1.9238e-01, -6.7608e-02,  7.7464e-02,\n",
            "        -3.3957e-02, -7.9947e-02,  1.3138e-01, -1.2730e-01, -6.2825e-02,\n",
            "        -5.4973e-03, -9.1520e-02, -1.7570e-01, -8.2958e-03, -9.4468e-02,\n",
            "         2.4915e-03, -3.1894e-02, -1.5755e-02,  1.4372e-01, -3.4770e-03,\n",
            "         1.0756e-02, -5.1074e-02,  3.5848e-02,  8.7819e-02, -4.5240e-02,\n",
            "        -4.5785e-02,  1.4690e-02,  6.8685e-02,  1.6806e-02, -4.7715e-02,\n",
            "         5.6814e-02,  4.5985e-02, -5.0668e-02,  5.8914e-03, -1.0335e-01,\n",
            "         1.0281e-02, -1.0520e-01, -1.6557e-02, -1.9213e-02, -3.4522e-02,\n",
            "         2.0138e-02, -1.3621e-01,  3.9602e-02, -8.8277e-03, -1.0837e-02,\n",
            "        -2.9803e-02,  7.2119e-02, -6.6893e-02, -9.4287e-03, -3.0956e-02,\n",
            "        -2.6700e-02, -1.4184e-01,  1.1900e-01,  6.6929e-02, -2.1375e-01,\n",
            "         4.2720e-02,  4.7782e-02,  3.3905e-02,  1.3857e-04, -1.4822e-01,\n",
            "        -2.3684e-02, -7.4337e-02, -6.8427e-02, -2.0076e-02,  1.4697e-02,\n",
            "        -3.9620e-02,  1.9393e-02, -6.9640e-02, -5.5779e-02,  7.9664e-03,\n",
            "         2.3639e-02, -2.5781e-01,  6.3564e-03, -1.0041e-01,  2.8047e-02,\n",
            "         1.5223e-02, -4.8413e-02, -1.5355e-01,  1.0493e-01,  4.9892e-02,\n",
            "         6.5689e-02, -5.4139e-02,  7.7265e-03,  9.4090e-02, -1.9978e-02,\n",
            "        -2.3561e-01, -6.2284e-02,  3.3363e-02,  1.1017e-01,  7.6999e-02,\n",
            "        -3.2478e-02,  4.8089e-02, -1.4995e-01, -1.6503e-01,  1.2304e-01,\n",
            "         7.1157e-02,  5.8895e-02, -4.8155e-02, -9.7220e-02, -1.8604e-01,\n",
            "         8.5326e-02, -5.1602e-02, -3.0799e-01, -6.0410e-02, -7.7131e-02,\n",
            "        -2.7282e-01,  2.8950e-02, -1.3280e-01,  1.7264e-02, -3.9184e-02,\n",
            "         5.4153e-02, -3.7219e-02, -1.5283e-01, -1.7663e-01,  8.3937e-02,\n",
            "         6.9329e-02, -8.2645e-02,  1.1178e-01, -5.0772e-02, -4.4772e-02,\n",
            "        -3.7455e-02,  3.0361e-02, -3.7824e-01,  1.4857e-02,  6.8245e-03,\n",
            "        -5.2090e-02, -2.9501e-01,  8.9860e-02,  2.9646e-02,  1.9930e-02,\n",
            "        -8.3483e-02, -9.6416e-02, -2.3830e-02,  3.4911e-02, -2.6626e-01,\n",
            "        -1.6184e-01,  7.3552e-02,  2.7568e-02, -1.1087e-01, -1.0322e-02,\n",
            "        -9.7499e-02,  1.3995e-02,  1.0802e-02,  7.8444e-02,  1.3057e-02,\n",
            "        -3.9476e-02,  2.4805e-02, -7.7445e-02, -2.8419e-02,  1.0412e-02,\n",
            "        -4.2330e-02, -1.6634e-01, -9.4908e-02, -3.4328e-02,  4.5544e-02,\n",
            "        -3.0001e-01, -6.9376e-03,  1.4103e-02, -2.6155e-01, -7.3598e-02,\n",
            "        -1.0627e-01, -1.0544e-02, -7.1160e-02, -1.0339e-01, -2.9787e-02,\n",
            "        -1.4277e-01, -5.1710e-02, -5.7126e-02, -5.4392e-02, -4.2260e-02,\n",
            "        -8.5120e-03,  1.5887e-02, -6.5358e-02, -6.1346e-02, -1.4502e-01,\n",
            "         3.9939e-02,  8.1640e-02, -7.7597e-03, -3.4062e-02,  3.1969e-02,\n",
            "        -4.4756e-02, -7.0306e-02,  1.0213e-01, -1.7993e-01, -2.1173e-01,\n",
            "        -5.9797e-02, -1.1596e-01,  3.9271e-02, -4.5443e-02, -1.8446e-01,\n",
            "        -1.0848e-01,  5.5781e-02, -6.3649e-02,  1.6825e-02,  1.6623e-04,\n",
            "         7.9866e-02, -6.7240e-02,  7.9827e-02, -3.9905e-03, -1.9016e-01,\n",
            "         2.0026e-02,  7.3181e-02,  1.0323e-01, -2.6431e-02,  2.3963e-02,\n",
            "        -4.4247e-02,  2.2896e-02,  2.3444e-02, -2.3481e-02,  1.0516e-02,\n",
            "        -2.1494e-01, -1.2810e-01, -1.8338e-02, -6.0221e-04, -5.1624e-02,\n",
            "         5.6575e-02, -5.4297e-02,  1.4146e-02, -4.9852e-02,  6.7255e-02,\n",
            "         5.1713e-02, -4.0266e-03,  3.5057e-02,  8.2781e-02,  1.0034e-02,\n",
            "         5.9230e-02, -2.0429e-01, -7.6169e-02,  4.1432e-02,  7.7517e-02,\n",
            "         7.6005e-02, -1.5919e-01, -8.3613e-02, -1.6625e-01,  2.2968e-03,\n",
            "        -6.8489e-02,  3.8119e-02, -9.8731e-02, -2.0290e-02,  1.5394e-02,\n",
            "        -1.0548e-01], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 4.8367e-02,  4.8045e-02,  3.8471e-02],\n",
            "          [ 4.9888e-02,  5.5208e-02,  5.6701e-02],\n",
            "          [ 2.4192e-02,  1.3436e-02,  2.4655e-02]],\n",
            "\n",
            "         [[-3.6542e-03, -3.1100e-03,  4.9227e-03],\n",
            "          [-1.2114e-03,  3.4020e-03,  1.9846e-02],\n",
            "          [-2.1704e-02, -2.1158e-02, -2.8686e-03]],\n",
            "\n",
            "         [[-1.2536e-02, -2.0486e-02, -2.3154e-02],\n",
            "          [-1.3515e-02, -2.3781e-02, -2.5515e-02],\n",
            "          [ 1.0584e-02,  7.2999e-03, -5.2329e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.3596e-02, -1.8328e-02, -5.0577e-02],\n",
            "          [ 1.6590e-02,  5.0719e-02,  2.1919e-02],\n",
            "          [-1.9203e-02, -8.8315e-03, -2.0335e-02]],\n",
            "\n",
            "         [[-7.6949e-03, -1.5848e-02,  1.5841e-03],\n",
            "          [-6.2470e-03, -1.3135e-02,  6.9092e-03],\n",
            "          [-3.3791e-03,  1.7889e-03,  3.7373e-03]],\n",
            "\n",
            "         [[-6.6310e-03,  5.8503e-03, -5.8571e-04],\n",
            "          [-2.4600e-02, -8.9747e-03, -7.2466e-03],\n",
            "          [-1.7566e-02, -8.5829e-03, -7.5220e-03]]],\n",
            "\n",
            "\n",
            "        [[[-2.3679e-02, -9.4399e-03, -1.1688e-02],\n",
            "          [-2.4777e-02, -1.7326e-02, -3.1489e-02],\n",
            "          [-3.3683e-03,  9.7571e-03, -5.1527e-03]],\n",
            "\n",
            "         [[-3.0809e-02, -4.0685e-02, -2.2731e-02],\n",
            "          [-5.1065e-03, -1.6457e-02, -1.8804e-02],\n",
            "          [ 5.0382e-02,  5.2054e-02,  3.9185e-02]],\n",
            "\n",
            "         [[-3.7790e-02, -4.2234e-02, -2.9703e-02],\n",
            "          [-6.4766e-03,  2.6967e-03, -8.1736e-03],\n",
            "          [ 3.7747e-02,  5.5416e-02,  2.5806e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.7275e-02, -4.5364e-02, -3.9567e-02],\n",
            "          [ 8.9827e-03,  1.6150e-02,  1.1675e-02],\n",
            "          [-9.7209e-03, -3.6449e-02, -1.6842e-02]],\n",
            "\n",
            "         [[ 1.7824e-02,  1.5013e-02,  1.0225e-02],\n",
            "          [ 5.4044e-03,  1.1664e-02,  6.4623e-03],\n",
            "          [ 2.1803e-02,  4.1795e-02,  1.9234e-02]],\n",
            "\n",
            "         [[-2.6730e-04,  1.5218e-03, -5.0352e-03],\n",
            "          [ 2.5761e-02,  2.7110e-02, -9.3395e-04],\n",
            "          [-1.1949e-02, -7.5204e-03, -3.9370e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.7447e-02, -1.8358e-02, -2.6020e-02],\n",
            "          [-1.4074e-02, -1.1302e-02, -1.4814e-02],\n",
            "          [-3.1460e-03, -1.8674e-02, -9.3350e-03]],\n",
            "\n",
            "         [[-5.1125e-03, -4.8036e-03,  1.8139e-02],\n",
            "          [-1.0524e-02, -1.5152e-02,  2.3904e-03],\n",
            "          [ 8.7093e-03,  9.3810e-03,  2.4203e-03]],\n",
            "\n",
            "         [[-7.6392e-03, -8.1496e-03, -1.5331e-02],\n",
            "          [-8.0622e-03, -1.3383e-02, -1.3938e-02],\n",
            "          [-1.6904e-02, -3.0059e-02, -1.8659e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.8390e-02, -2.6080e-03,  9.3782e-03],\n",
            "          [-6.4662e-04, -1.3146e-02,  1.0045e-02],\n",
            "          [-2.2293e-03, -1.4097e-02,  1.7385e-02]],\n",
            "\n",
            "         [[ 3.0293e-04,  2.9622e-03,  1.0030e-02],\n",
            "          [-5.7588e-03, -1.6943e-03,  6.9988e-03],\n",
            "          [ 9.8134e-03,  1.4197e-02,  5.9742e-03]],\n",
            "\n",
            "         [[ 2.8753e-03, -1.7814e-03,  1.0873e-02],\n",
            "          [ 1.5230e-02,  4.5867e-03,  1.6860e-02],\n",
            "          [ 1.9536e-03,  1.9503e-02,  1.2168e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 1.3983e-02,  2.4598e-03, -7.4604e-03],\n",
            "          [-2.2250e-02, -1.2757e-02, -2.8846e-03],\n",
            "          [-1.0911e-02,  7.5499e-03,  8.6910e-03]],\n",
            "\n",
            "         [[-4.8463e-03, -8.3250e-03,  1.3420e-02],\n",
            "          [-6.2502e-03, -7.3982e-03,  1.1153e-02],\n",
            "          [ 4.0391e-03, -9.0354e-03, -7.5441e-03]],\n",
            "\n",
            "         [[-5.1627e-03, -8.9529e-03, -1.2414e-02],\n",
            "          [-4.9261e-03, -3.5488e-03,  2.1501e-03],\n",
            "          [-1.1709e-02, -1.4984e-02, -1.9216e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.5428e-02, -7.6036e-04, -1.3522e-03],\n",
            "          [-3.4856e-02, -7.4478e-04, -6.5064e-03],\n",
            "          [-9.1655e-03, -2.8467e-02, -4.8924e-02]],\n",
            "\n",
            "         [[ 1.2207e-02,  1.0519e-02, -8.4421e-03],\n",
            "          [-2.5495e-02,  2.8140e-03,  1.6165e-03],\n",
            "          [-1.8831e-02,  1.2268e-02,  1.5439e-02]],\n",
            "\n",
            "         [[-1.3684e-02, -4.1732e-03,  1.2609e-02],\n",
            "          [-6.8834e-04,  5.9757e-03, -1.0183e-02],\n",
            "          [ 2.1559e-04, -1.3462e-02, -3.0114e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.6186e-02, -6.4926e-02, -4.3146e-02],\n",
            "          [-2.1790e-02, -4.9106e-02, -3.4568e-02],\n",
            "          [ 4.0506e-02,  4.2449e-02,  6.1562e-02]],\n",
            "\n",
            "         [[ 3.5715e-03, -1.0916e-02, -2.2922e-02],\n",
            "          [-2.4831e-03,  6.4555e-03, -1.1316e-02],\n",
            "          [ 1.6662e-03, -1.9145e-02, -2.3007e-02]],\n",
            "\n",
            "         [[-7.1243e-03, -4.2783e-05,  4.9363e-03],\n",
            "          [-1.5832e-02,  4.0474e-03,  4.5135e-04],\n",
            "          [-4.7967e-03, -7.2164e-04, -1.7230e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.1589e-02,  7.7814e-04,  6.3205e-03],\n",
            "          [ 1.1360e-02, -6.2076e-03, -2.7689e-02],\n",
            "          [ 2.6392e-02,  2.3775e-03, -1.4937e-02]],\n",
            "\n",
            "         [[-1.1237e-02, -2.6285e-03,  9.1537e-03],\n",
            "          [-8.2120e-03, -2.2236e-02,  3.2917e-04],\n",
            "          [ 5.5909e-03, -1.3858e-03,  6.8947e-03]],\n",
            "\n",
            "         [[-1.4783e-02, -1.0367e-02, -2.7472e-02],\n",
            "          [-4.1090e-02, -3.8532e-02, -3.9202e-02],\n",
            "          [-2.1614e-02, -3.4340e-02, -1.8542e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.9492e-02, -1.6098e-02, -3.1792e-02],\n",
            "          [ 2.5374e-02,  4.6815e-02,  2.7513e-02],\n",
            "          [ 3.5903e-02,  3.1892e-02,  2.6156e-02]],\n",
            "\n",
            "         [[ 1.6856e-02,  1.5645e-02,  1.4189e-02],\n",
            "          [ 2.2550e-02,  3.0456e-02,  1.6739e-02],\n",
            "          [-2.3615e-04, -7.9501e-03, -1.9666e-03]],\n",
            "\n",
            "         [[-7.9060e-03, -4.7390e-03,  1.6030e-03],\n",
            "          [ 1.3802e-03, -8.5837e-03,  6.9451e-03],\n",
            "          [ 1.1407e-02, -5.9877e-03,  1.3759e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.0124e-03,  2.9951e-02,  1.1915e-02],\n",
            "          [-4.3412e-02, -3.1776e-03, -2.7705e-02],\n",
            "          [-1.6183e-02, -1.1247e-02, -3.5084e-02]],\n",
            "\n",
            "         [[ 2.9837e-02,  5.9935e-02,  2.4631e-02],\n",
            "          [-1.9571e-03,  2.2415e-02, -1.5499e-02],\n",
            "          [ 1.6075e-02,  1.7850e-02, -1.8412e-02]],\n",
            "\n",
            "         [[-4.3712e-03, -4.9032e-02, -2.1335e-02],\n",
            "          [-5.2598e-03, -2.8579e-02, -2.2090e-02],\n",
            "          [ 8.5126e-03,  2.0862e-03,  2.3301e-02]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.2480, 0.1972, 0.2279, 0.2709, 0.3296, 0.2640, 0.2710, 0.3475, 0.2388,\n",
            "        0.2904, 0.2769, 0.3045, 0.2268, 0.2634, 0.2999, 0.2397, 0.2724, 0.2723,\n",
            "        0.2133, 0.3806, 0.2767, 0.2403, 0.2406, 0.2917, 0.2675, 0.2305, 0.2394,\n",
            "        0.3123, 0.2984, 0.3353, 0.2234, 0.1919, 0.3168, 0.2626, 0.2901, 0.2918,\n",
            "        0.3455, 0.2561, 0.2434, 0.2298, 0.3318, 0.3481, 0.2032, 0.2478, 0.2478,\n",
            "        0.2483, 0.3252, 0.2567, 0.2685, 0.1977, 0.2541, 0.4079, 0.2480, 0.2076,\n",
            "        0.2276, 0.2683, 0.2098, 0.2056, 0.2010, 0.3560, 0.2384, 0.3284, 0.1952,\n",
            "        0.2445, 0.2848, 0.3742, 0.2746, 0.2117, 0.3859, 0.4785, 0.3005, 0.2848,\n",
            "        0.3762, 0.2903, 0.2126, 0.1776, 0.2778, 0.3878, 0.3123, 0.1974, 0.2679,\n",
            "        0.2300, 0.2474, 0.2320, 0.2635, 0.2819, 0.2296, 0.3194, 0.3814, 0.2503,\n",
            "        0.2269, 0.2676, 0.3431, 0.3799, 0.3787, 0.2968, 0.3021, 0.2575, 0.3007,\n",
            "        0.1939, 0.1950, 0.3217, 0.3623, 0.2171, 0.2486, 0.2266, 0.2133, 0.2851,\n",
            "        0.2715, 0.2720, 0.3107, 0.2174, 0.2675, 0.2387, 0.3434, 0.2761, 0.2084,\n",
            "        0.2975, 0.3178, 0.2818, 0.2858, 0.3498, 0.2675, 0.2638, 0.3159, 0.2879,\n",
            "        0.1873, 0.2986, 0.3584, 0.2570, 0.1815, 0.2758, 0.2640, 0.2486, 0.2567,\n",
            "        0.2252, 0.3420, 0.2910, 0.2898, 0.2902, 0.2404, 0.2381, 0.3633, 0.2690,\n",
            "        0.3810, 0.2947, 0.2743, 0.4644, 0.3133, 0.2444, 0.3477, 0.3001, 0.1977,\n",
            "        0.2301, 0.2513, 0.2660, 0.3271, 0.1622, 0.2274, 0.2225, 0.3596, 0.3215,\n",
            "        0.1997, 0.2215, 0.2706, 0.2831, 0.2621, 0.3710, 0.2730, 0.2903, 0.1893,\n",
            "        0.2140, 0.2460, 0.3141, 0.2424, 0.3699, 0.2364, 0.2420, 0.2948, 0.2497,\n",
            "        0.2760, 0.2686, 0.2895, 0.3857, 0.1398, 0.2832, 0.3362, 0.2522, 0.2823,\n",
            "        0.2381, 0.2311, 0.3274, 0.4078, 0.2648, 0.2525, 0.3388, 0.3251, 0.2420,\n",
            "        0.2856, 0.3605, 0.2603, 0.2294, 0.2483, 0.2171, 0.2353, 0.4117, 0.2588,\n",
            "        0.2888, 0.1972, 0.2408, 0.2755, 0.3031, 0.2457, 0.2744, 0.3564, 0.2546,\n",
            "        0.3673, 0.2883, 0.2590, 0.3021, 0.2890, 0.3505, 0.2092, 0.2953, 0.3222,\n",
            "        0.2925, 0.2574, 0.3012, 0.3893, 0.2211, 0.2226, 0.3258, 0.3205, 0.2975,\n",
            "        0.2323, 0.3323, 0.2812, 0.2702, 0.2300, 0.2846, 0.3318, 0.2292, 0.3498,\n",
            "        0.2622, 0.3581, 0.4003, 0.2924, 0.3049, 0.3478, 0.2845, 0.2742, 0.2019,\n",
            "        0.2466, 0.2988, 0.2044, 0.2691], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1332, -0.0644, -0.3239, -0.2390, -0.3262, -0.1796, -0.2087, -0.3208,\n",
            "        -0.1874, -0.2988, -0.2099, -0.2283, -0.2141, -0.2460, -0.2768, -0.1351,\n",
            "        -0.2498, -0.2393, -0.1223, -0.4590, -0.2172, -0.1220, -0.2101, -0.1779,\n",
            "        -0.2426, -0.1546, -0.1549, -0.3716, -0.2817, -0.3886, -0.1545, -0.0687,\n",
            "        -0.3412, -0.2261, -0.1961, -0.2242, -0.2984, -0.1381, -0.2251, -0.1658,\n",
            "        -0.4534, -0.3226, -0.0977, -0.1349, -0.2619, -0.1428, -0.3960, -0.1633,\n",
            "        -0.2101, -0.1161, -0.1448, -0.5502, -0.2179, -0.1246,  0.0502, -0.1902,\n",
            "        -0.1047, -0.1000, -0.1411, -0.3124, -0.2190, -0.3062, -0.1247, -0.1557,\n",
            "        -0.2973, -0.3825, -0.1951, -0.1381, -0.5761, -0.3879, -0.2808, -0.2542,\n",
            "        -0.3470, -0.2460, -0.1091, -0.0562, -0.1833, -0.4956, -0.3059, -0.0988,\n",
            "        -0.2255, -0.1958, -0.1320, -0.1738, -0.2287, -0.1926, -0.0924, -0.3427,\n",
            "        -0.5489, -0.2431, -0.1935, -0.1641, -0.2503, -0.3274, -0.4008, -0.2824,\n",
            "        -0.2694, -0.1939, -0.2413, -0.0309, -0.0880, -0.3421, -0.3104, -0.1102,\n",
            "        -0.1539, -0.1233, -0.1780, -0.2715, -0.2005, -0.1846, -0.2843, -0.1117,\n",
            "        -0.1816, -0.2119, -0.3304, -0.2267, -0.1413, -0.3376, -0.2674, -0.2524,\n",
            "        -0.2554, -0.4735, -0.2342, -0.2130, -0.3282, -0.1966, -0.1063, -0.2615,\n",
            "        -0.4234, -0.1374, -0.0811, -0.3069, -0.1538, -0.1453, -0.1612, -0.1631,\n",
            "        -0.3759, -0.2608, -0.2382, -0.2499, -0.1485, -0.1487, -0.4328, -0.1377,\n",
            "        -0.2781, -0.2259, -0.2072, -0.4165, -0.3582, -0.1382, -0.3598, -0.2672,\n",
            "        -0.2090, -0.0177, -0.1279, -0.2812, -0.3621,  0.0476, -0.2232, -0.1272,\n",
            "        -0.3237, -0.3008, -0.1119, -0.0839, -0.2426, -0.2000, -0.1873, -0.4685,\n",
            "        -0.2000, -0.3462, -0.0706, -0.1973, -0.3548, -0.1975, -0.3537, -0.3546,\n",
            "        -0.1433, -0.2052, -0.2722, -0.1528, -0.2798, -0.1945, -0.2474, -0.4910,\n",
            "         0.1322, -0.2378, -0.5166, -0.3959, -0.2354, -0.1266, -0.0810, -0.4132,\n",
            "        -0.5576, -0.2238, -0.1563, -0.3950, -0.3283, -0.0846, -0.3103, -0.3130,\n",
            "        -0.1498, -0.1396, -0.0972, -0.1620, -0.1631, -0.6364, -0.1350, -0.2345,\n",
            "        -0.1049, -0.1625, -0.2878, -0.2450, -0.1468, -0.2035, -0.5358, -0.1683,\n",
            "        -0.5524, -0.2511, -0.1230, -0.2305, -0.1925, -0.3759, -0.1014, -0.1697,\n",
            "        -0.4002, -0.2980, -0.3035, -0.1563, -0.4660, -0.1155, -0.1665, -0.3382,\n",
            "        -0.2935, -0.3122, -0.3015, -0.3261, -0.2542, -0.2037, -0.0955, -0.2070,\n",
            "        -0.4370, -0.2051, -0.4205, -0.3125, -0.4845, -0.3528, -0.2624, -0.2894,\n",
            "        -0.3976, -0.2107, -0.1791, -0.1075, -0.1213, -0.3022,  0.0516, -0.1928],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-4.2568e-02, -2.6148e-02, -2.2019e-02],\n",
            "          [-1.7334e-02, -7.5950e-03, -7.2384e-03],\n",
            "          [-1.7876e-03,  2.3800e-02,  1.4873e-02]],\n",
            "\n",
            "         [[-2.8277e-03, -5.0644e-03, -4.9442e-03],\n",
            "          [ 1.2117e-03,  1.4908e-02,  1.6013e-02],\n",
            "          [ 1.4391e-02,  3.3109e-02,  5.0061e-02]],\n",
            "\n",
            "         [[-3.4891e-03, -4.4437e-03,  2.6589e-03],\n",
            "          [ 1.5105e-02,  2.6303e-02,  2.6802e-02],\n",
            "          [ 3.9232e-02,  5.0057e-02,  4.6637e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.2877e-02,  1.5454e-02, -2.4483e-02],\n",
            "          [ 3.1145e-02,  3.4944e-02,  1.3296e-02],\n",
            "          [-1.7674e-04,  7.3297e-03, -5.7174e-03]],\n",
            "\n",
            "         [[-2.1781e-02, -3.7379e-02, -1.3382e-02],\n",
            "          [ 1.8976e-02,  1.4155e-02, -6.5395e-03],\n",
            "          [ 2.6831e-02,  3.6354e-02,  1.1450e-02]],\n",
            "\n",
            "         [[ 3.1603e-02,  3.3933e-02,  3.1575e-02],\n",
            "          [-1.0098e-02, -1.2657e-02,  1.1674e-02],\n",
            "          [ 1.0325e-02,  7.9424e-05,  1.5911e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 2.5937e-02,  6.2590e-03,  6.0798e-03],\n",
            "          [-4.5745e-03, -3.5188e-02, -2.9249e-02],\n",
            "          [ 2.1366e-02,  2.0480e-03,  6.2699e-03]],\n",
            "\n",
            "         [[-2.9549e-03, -1.3679e-03, -8.6876e-03],\n",
            "          [ 7.9988e-03,  1.2888e-03, -5.9629e-03],\n",
            "          [-9.1481e-03, -2.1914e-02, -4.1572e-02]],\n",
            "\n",
            "         [[ 7.6390e-03,  3.0253e-03,  2.7817e-04],\n",
            "          [ 7.0329e-03,  1.1914e-02, -2.4419e-03],\n",
            "          [-8.2131e-03, -9.7848e-05, -1.9223e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.4498e-03,  5.1611e-03,  3.7416e-03],\n",
            "          [ 3.2110e-04,  8.3762e-03,  3.6612e-03],\n",
            "          [ 9.3343e-03,  8.1829e-03,  1.1234e-03]],\n",
            "\n",
            "         [[-6.6849e-02, -5.9871e-02, -3.3931e-02],\n",
            "          [ 2.2337e-02,  3.1932e-02,  3.7244e-02],\n",
            "          [ 9.3296e-03,  3.7222e-02,  1.4052e-02]],\n",
            "\n",
            "         [[-2.0643e-03,  1.2408e-02, -3.1072e-03],\n",
            "          [-8.2882e-03,  1.3917e-02, -2.0680e-02],\n",
            "          [-1.9329e-02,  1.1953e-02, -2.3436e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.7788e-03, -3.5982e-03, -1.2592e-03],\n",
            "          [-1.5320e-02, -1.0690e-02, -2.0311e-02],\n",
            "          [-3.4649e-04, -2.2188e-03, -1.5021e-02]],\n",
            "\n",
            "         [[-2.8952e-02, -3.3958e-02, -2.5437e-02],\n",
            "          [-1.5919e-04,  1.5204e-02,  3.4554e-02],\n",
            "          [ 3.6892e-02,  7.0144e-02,  7.3610e-02]],\n",
            "\n",
            "         [[ 1.0721e-02,  2.1531e-03, -5.6155e-03],\n",
            "          [ 1.1754e-02, -4.8546e-03, -5.5013e-03],\n",
            "          [-3.7388e-04, -9.7639e-03, -1.5029e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.5622e-02,  9.8976e-03,  3.4725e-03],\n",
            "          [ 1.4711e-02,  7.0707e-03, -9.1826e-03],\n",
            "          [ 7.0986e-03,  6.3087e-03, -3.5893e-03]],\n",
            "\n",
            "         [[-6.4518e-03, -6.7673e-03,  1.1635e-02],\n",
            "          [ 1.4707e-02,  2.3831e-02,  4.9396e-02],\n",
            "          [ 1.8897e-02,  3.4981e-02,  4.5488e-02]],\n",
            "\n",
            "         [[ 1.5900e-02,  3.3369e-02,  2.6194e-02],\n",
            "          [ 1.0616e-02,  1.8515e-02,  3.0190e-03],\n",
            "          [ 1.1004e-02,  2.5503e-02,  1.3654e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-2.1231e-02, -1.2804e-02, -1.5498e-02],\n",
            "          [ 7.6750e-03,  1.2120e-02,  1.5099e-02],\n",
            "          [ 1.8536e-02,  2.5110e-02,  2.5283e-02]],\n",
            "\n",
            "         [[ 7.4059e-03, -3.0540e-03, -1.5475e-03],\n",
            "          [-8.4415e-03, -2.2002e-02, -3.4099e-03],\n",
            "          [ 9.1918e-03,  2.2617e-03, -1.4260e-02]],\n",
            "\n",
            "         [[-5.2568e-03, -5.3507e-03, -3.2230e-03],\n",
            "          [-1.5805e-02,  6.0508e-03, -1.5917e-03],\n",
            "          [-8.9323e-03,  2.6483e-03,  5.0508e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.9826e-02, -2.1209e-03,  1.4889e-02],\n",
            "          [ 5.7275e-02,  3.5549e-02,  6.0175e-03],\n",
            "          [ 2.3347e-02, -2.2153e-02, -2.5497e-02]],\n",
            "\n",
            "         [[-1.3985e-02, -6.4766e-02, -1.7286e-02],\n",
            "          [ 1.1704e-02,  1.0714e-02,  4.6278e-02],\n",
            "          [-1.0038e-02, -3.5707e-03,  2.2691e-02]],\n",
            "\n",
            "         [[-8.3342e-03, -1.3070e-03, -1.0049e-02],\n",
            "          [ 3.2605e-02,  5.3259e-02,  2.2172e-02],\n",
            "          [ 3.7339e-02,  6.1155e-02,  4.4555e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.6584e-02, -1.3850e-02, -1.4604e-02],\n",
            "          [-1.7604e-02, -2.1268e-02, -1.6734e-02],\n",
            "          [-6.0039e-04,  3.8569e-03,  1.2837e-02]],\n",
            "\n",
            "         [[ 1.7623e-02,  2.3706e-02,  2.7633e-02],\n",
            "          [-2.2841e-02, -1.9576e-02, -1.6551e-02],\n",
            "          [-8.0822e-03,  4.3779e-03, -5.3622e-03]],\n",
            "\n",
            "         [[ 1.5582e-02,  3.7879e-02,  2.3555e-02],\n",
            "          [-6.4632e-03,  9.8620e-03,  1.2121e-02],\n",
            "          [-1.3743e-02, -6.1246e-03, -2.7332e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.5037e-03, -1.2064e-02, -9.0989e-03],\n",
            "          [-4.7911e-04, -2.8339e-03,  2.1365e-03],\n",
            "          [-6.2077e-03, -2.6615e-03,  1.1215e-02]],\n",
            "\n",
            "         [[-8.1794e-03, -2.2417e-02, -3.4012e-02],\n",
            "          [-2.8553e-02, -2.9546e-02, -4.4372e-02],\n",
            "          [-5.0348e-02, -3.4973e-02, -5.2028e-02]],\n",
            "\n",
            "         [[ 1.4728e-02,  3.2834e-02,  2.6312e-02],\n",
            "          [ 1.3449e-02,  2.6407e-02,  2.6924e-02],\n",
            "          [ 2.5572e-02,  3.4316e-02,  2.6184e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 7.2026e-03, -2.3931e-03,  2.2182e-03],\n",
            "          [ 4.2555e-03, -6.4084e-03,  7.8548e-03],\n",
            "          [ 2.0510e-02,  1.8644e-02,  2.3280e-02]],\n",
            "\n",
            "         [[-1.2471e-02,  1.3008e-02,  1.0010e-02],\n",
            "          [-1.7496e-03,  6.1331e-03,  4.3366e-03],\n",
            "          [ 5.2269e-03,  1.5111e-02, -8.1881e-03]],\n",
            "\n",
            "         [[-3.7337e-02,  1.9923e-02, -2.4149e-02],\n",
            "          [-4.9487e-02, -1.0510e-02, -4.2107e-02],\n",
            "          [-5.7684e-03, -4.8632e-03, -1.8332e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.2013e-03, -1.5208e-02, -1.6507e-02],\n",
            "          [-8.8276e-03, -1.8698e-02, -1.6637e-03],\n",
            "          [-1.2015e-02,  2.9667e-03,  6.2300e-03]],\n",
            "\n",
            "         [[-1.8341e-02, -9.0521e-03,  2.6030e-02],\n",
            "          [ 3.5930e-02,  5.3049e-02,  5.8487e-02],\n",
            "          [-1.3661e-02, -3.6888e-03, -7.1606e-03]],\n",
            "\n",
            "         [[-1.2594e-02, -4.0898e-02,  1.7162e-03],\n",
            "          [-1.7420e-02, -4.3435e-02, -1.3183e-02],\n",
            "          [-3.7506e-02, -5.5707e-02, -3.0051e-02]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.1971, 0.1771, 0.1303, 0.1995, 0.1839, 0.0934, 0.2333, 0.2236, 0.1654,\n",
            "        0.1280, 0.0842, 0.1085, 0.3168, 0.2032, 0.3246, 0.2184, 0.3208, 0.2824,\n",
            "        0.3408, 0.3339, 0.3307, 0.5571, 0.2821, 0.3081, 0.2114, 0.2971, 0.2361,\n",
            "        0.5500, 0.1221, 0.3381, 0.1528, 0.1544, 0.1982, 0.0582, 0.1812, 0.2489,\n",
            "        0.1954, 0.0705, 0.0918, 0.1328, 0.2616, 0.2013, 0.0720, 0.1573, 0.1919,\n",
            "        0.0813, 0.1170, 0.2504, 0.2863, 0.3032, 0.1476, 0.3696, 0.1870, 0.2097,\n",
            "        0.1907, 0.2364, 0.1642, 0.1079, 0.2531, 0.1703, 0.1266, 0.0814, 0.2407,\n",
            "        0.2609, 0.2705, 0.2128, 0.5007, 0.2375, 0.0802, 0.2896, 0.1776, 0.0887,\n",
            "        0.1094, 0.1834, 0.2812, 0.1971, 0.2021, 0.3443, 0.1411, 0.1362, 0.2676,\n",
            "        0.1618, 0.2723, 0.2727, 0.2528, 0.0982, 0.4707, 0.2239, 0.3649, 0.1987,\n",
            "        0.0815, 0.2543, 0.3322, 0.1561, 0.2336, 0.1294, 0.2570, 0.1700, 0.1374,\n",
            "        0.2215, 0.5015, 0.3132, 0.1487, 0.1174, 0.0916, 0.2130, 0.1393, 0.3057,\n",
            "        0.5634, 0.1018, 0.0994, 0.0492, 0.4427, 0.3142, 0.4002, 0.1334, 0.2174,\n",
            "        0.5522, 0.2806, 0.2784, 0.4333, 0.2602, 0.3788, 0.1827, 0.2664, 0.1077,\n",
            "        0.3001, 0.2428, 0.5130, 0.0829, 0.1254, 0.1996, 0.1451, 0.2253, 0.1467,\n",
            "        0.3712, 0.0794, 0.5425, 0.2058, 0.2103, 0.1288, 0.4993, 0.1815, 0.1845,\n",
            "        0.4154, 0.3817, 0.2054, 0.2205, 0.1471, 0.4964, 0.4202, 0.0801, 0.0623,\n",
            "        0.3536, 0.2760, 0.3840, 0.1632, 0.1402, 0.2674, 0.0844, 0.2305, 0.2259,\n",
            "        0.2146, 0.4181, 0.2821, 0.2926, 0.3416, 0.4640, 0.3025, 0.3732, 0.5871,\n",
            "        0.0616, 0.2797, 0.3042, 0.2173, 0.3550, 0.2096, 0.2449, 0.3428, 0.2868,\n",
            "        0.3543, 0.4667, 0.3220, 0.3805, 0.2632, 0.2160, 0.1924, 0.4074, 0.4966,\n",
            "        0.3623, 0.1670, 0.1321, 0.2374, 0.2118, 0.1522, 0.1668, 0.3836, 0.0983,\n",
            "        0.3729, 0.3943, 0.4353, 0.2270, 0.1508, 0.3133, 0.3850, 0.5774, 0.1892,\n",
            "        0.2822, 0.0907, 0.2364, 0.0964, 0.2360, 0.0699, 0.2938, 0.5100, 0.3348,\n",
            "        0.2339, 0.1145, 0.2155, 0.2266, 0.2829, 0.2341, 0.1891, 0.2906, 0.2681,\n",
            "        0.3876, 0.3915, 0.1844, 0.1889, 0.4405, 0.1405, 0.3460, 0.2724, 0.2567,\n",
            "        0.2785, 0.1148, 0.1607, 0.1754, 0.0883, 0.1649, 0.1268, 0.2356, 0.2811,\n",
            "        0.0766, 0.1424, 0.1683, 0.3979, 0.2685, 0.6383, 0.1087, 0.3180, 0.1760,\n",
            "        0.3634, 0.2615, 0.1999, 0.2541], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0162, -0.2033,  0.0294, -0.1697, -0.1840, -0.0309, -0.2039, -0.1426,\n",
            "        -0.0443, -0.0886, -0.0647, -0.0968, -0.0380, -0.2073, -0.3061,  0.1443,\n",
            "        -0.3079, -0.1232, -0.1627, -0.0980, -0.2471, -0.2837, -0.1201, -0.2893,\n",
            "        -0.2303, -0.3562, -0.0825, -0.3483,  0.0707, -0.1321, -0.1074, -0.1451,\n",
            "         0.0235,  0.0225, -0.1885, -0.2507, -0.2461,  0.0631, -0.0023, -0.1209,\n",
            "        -0.2581, -0.1640, -0.0172, -0.1143, -0.2096, -0.0158,  0.0128, -0.1332,\n",
            "        -0.3139, -0.2294, -0.1527, -0.3503,  0.2086,  0.0785, -0.1597, -0.1990,\n",
            "         0.0346,  0.0388, -0.1269,  0.1019,  0.0981, -0.0390, -0.2537, -0.1356,\n",
            "        -0.1796, -0.2422, -0.4517, -0.3124, -0.0177, -0.2615, -0.1567,  0.0212,\n",
            "        -0.0753, -0.1426, -0.2788,  0.0062, -0.1895, -0.2327, -0.1298, -0.1200,\n",
            "        -0.1917, -0.0987, -0.1916, -0.1666, -0.2729,  0.1287, -0.4620, -0.2259,\n",
            "        -0.2270,  0.1939,  0.0230, -0.3303, -0.3202, -0.1292, -0.0716,  0.0048,\n",
            "        -0.2579, -0.0116, -0.0557, -0.1229, -0.4804, -0.2351, -0.1367, -0.0578,\n",
            "        -0.0537, -0.2743, -0.0827, -0.1922, -0.3481, -0.0358, -0.1094,  0.0138,\n",
            "        -0.1888, -0.2592, -0.3293, -0.0820, -0.1839, -0.1636, -0.3163, -0.0246,\n",
            "        -0.1667, -0.1653, -0.3076, -0.2229, -0.1834, -0.0536, -0.0621, -0.1752,\n",
            "        -0.5243, -0.1933, -0.1119, -0.2283, -0.0437, -0.1777, -0.1300, -0.2519,\n",
            "        -0.0456, -0.6305, -0.1364, -0.2138,  0.0406, -0.5287, -0.2014, -0.1442,\n",
            "        -0.1930, -0.3033,  0.1030, -0.1499, -0.2297, -0.5301, -0.2543, -0.0417,\n",
            "         0.0429, -0.3218, -0.1611, -0.2562, -0.1187, -0.1001,  0.0225,  0.0996,\n",
            "        -0.2138, -0.2019,  0.0808, -0.0121, -0.2364, -0.3247, -0.1482, -0.4846,\n",
            "        -0.3449, -0.1365, -0.6664,  0.0418, -0.2807, -0.0961, -0.2378, -0.1834,\n",
            "        -0.1890, -0.0377, -0.3056, -0.1843, -0.1357, -0.3038, -0.2680, -0.4143,\n",
            "        -0.2633, -0.1750, -0.1856, -0.2405, -0.1082, -0.2250, -0.1268, -0.1094,\n",
            "         0.0594, -0.1419, -0.1178, -0.1602, -0.0328, -0.0194, -0.1985,  0.0470,\n",
            "        -0.1887, -0.2776, -0.0930, -0.4092, -0.3378, -0.7252,  0.0260, -0.1829,\n",
            "         0.0561, -0.2227, -0.0026, -0.3218, -0.0093, -0.2843, -0.5121, -0.2337,\n",
            "        -0.0836, -0.0818, -0.1296, -0.2090,  0.0169, -0.1899, -0.1892, -0.3075,\n",
            "        -0.3108, -0.2986, -0.4712, -0.1823, -0.1893, -0.3131, -0.0876, -0.1166,\n",
            "        -0.2995, -0.0831, -0.3427, -0.0772, -0.1460, -0.1611,  0.0203, -0.0627,\n",
            "        -0.0610, -0.2574, -0.1383,  0.0470, -0.0302, -0.1638, -0.3323, -0.1741,\n",
            "        -0.6307, -0.0772, -0.2123, -0.1559, -0.0459, -0.2416, -0.0143, -0.2079],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-1.1645e-02, -1.9010e-02, -2.1876e-02],\n",
            "          [ 2.0482e-02,  2.3962e-02,  2.9161e-02],\n",
            "          [ 4.3672e-02,  3.3278e-02,  4.9908e-02]],\n",
            "\n",
            "         [[-7.4040e-03,  2.8083e-03, -4.7339e-03],\n",
            "          [ 6.9030e-03,  1.4271e-02, -3.6954e-03],\n",
            "          [-3.1341e-03,  1.3736e-02,  1.6127e-03]],\n",
            "\n",
            "         [[ 1.8676e-02, -1.0553e-02, -1.4233e-02],\n",
            "          [ 8.9944e-03, -2.5068e-03, -1.2145e-02],\n",
            "          [-4.9455e-03, -2.9206e-02, -9.6385e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.2655e-02,  1.7691e-02,  9.8264e-04],\n",
            "          [ 7.4271e-03,  7.6115e-03,  1.1135e-02],\n",
            "          [ 2.3242e-02,  1.1058e-02,  4.0498e-03]],\n",
            "\n",
            "         [[ 1.8557e-02,  1.2472e-02,  1.7220e-02],\n",
            "          [-4.8544e-03,  8.3627e-03,  2.2811e-02],\n",
            "          [-5.1675e-03,  2.3264e-02,  3.4068e-02]],\n",
            "\n",
            "         [[ 2.4934e-02,  2.2373e-02,  4.2614e-02],\n",
            "          [ 1.3486e-02,  1.6760e-03,  1.3019e-02],\n",
            "          [-6.2821e-03, -1.5112e-03, -8.9229e-03]]],\n",
            "\n",
            "\n",
            "        [[[-9.8089e-04, -6.3011e-03,  5.9932e-03],\n",
            "          [ 1.5936e-02,  1.3394e-02,  2.9934e-02],\n",
            "          [ 2.3149e-02,  2.0709e-02,  2.5485e-02]],\n",
            "\n",
            "         [[-2.0015e-02, -3.3349e-02, -8.0396e-03],\n",
            "          [-7.2800e-03, -1.2187e-02, -2.0389e-04],\n",
            "          [-1.3138e-02, -2.0427e-02, -1.6286e-02]],\n",
            "\n",
            "         [[-6.7681e-03,  5.0045e-03, -2.6683e-03],\n",
            "          [-2.1073e-02,  2.8275e-04, -1.8205e-02],\n",
            "          [-1.7382e-02, -5.0244e-03, -3.0386e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.1035e-02, -2.2964e-02, -1.1028e-02],\n",
            "          [-6.3256e-03, -4.1667e-03, -1.7323e-02],\n",
            "          [-1.3611e-02, -2.3468e-02, -1.6436e-02]],\n",
            "\n",
            "         [[ 7.3663e-03,  6.6219e-03,  5.2776e-03],\n",
            "          [-3.5464e-03,  3.2750e-03, -9.1126e-03],\n",
            "          [ 3.5593e-04, -1.0151e-02, -1.9123e-02]],\n",
            "\n",
            "         [[ 1.8193e-03,  8.8087e-03,  5.1361e-03],\n",
            "          [ 3.1915e-03,  2.5287e-02,  2.4939e-02],\n",
            "          [ 1.3968e-02,  1.9613e-02,  2.2382e-02]]],\n",
            "\n",
            "\n",
            "        [[[-4.1548e-03,  8.8964e-03,  2.0143e-03],\n",
            "          [ 1.1327e-02,  1.3251e-02,  1.4014e-02],\n",
            "          [ 7.2196e-03,  1.3045e-02,  2.4827e-02]],\n",
            "\n",
            "         [[-1.5025e-02,  5.0530e-03,  7.4766e-03],\n",
            "          [-2.4685e-02, -1.6732e-02, -1.0888e-02],\n",
            "          [-2.8064e-02, -1.1875e-02, -3.4120e-03]],\n",
            "\n",
            "         [[ 2.8449e-02,  1.4594e-02,  6.9441e-03],\n",
            "          [ 2.4799e-02,  1.9453e-02,  1.1294e-02],\n",
            "          [-1.0787e-02, -2.1006e-02, -1.0372e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.4967e-02,  8.2449e-03,  2.0244e-03],\n",
            "          [ 1.4287e-02, -6.3867e-03, -8.0757e-03],\n",
            "          [ 2.7547e-02,  1.0791e-02,  1.6567e-02]],\n",
            "\n",
            "         [[ 3.6191e-02,  3.8918e-02,  3.9028e-02],\n",
            "          [-8.3489e-04,  1.3273e-02,  2.0172e-02],\n",
            "          [-2.0652e-02, -5.4010e-03,  1.7147e-03]],\n",
            "\n",
            "         [[ 2.0373e-04,  3.5919e-03,  8.5592e-03],\n",
            "          [ 6.2363e-03, -9.3086e-05,  1.2940e-02],\n",
            "          [ 1.3152e-02,  1.0732e-02,  1.9896e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1.7400e-02, -6.7019e-03, -9.1787e-03],\n",
            "          [-9.9672e-03,  2.6298e-04,  3.3439e-03],\n",
            "          [ 1.5721e-02,  1.4216e-02,  2.0509e-02]],\n",
            "\n",
            "         [[ 2.1410e-02,  3.6914e-02,  2.8239e-02],\n",
            "          [ 3.8158e-02,  4.8944e-02,  3.4652e-02],\n",
            "          [ 3.1723e-02,  4.4208e-02,  4.0035e-02]],\n",
            "\n",
            "         [[-3.3437e-03, -1.0482e-02, -5.3990e-03],\n",
            "          [-5.3186e-03,  1.1394e-02,  1.7593e-03],\n",
            "          [-5.6652e-03, -6.6373e-03, -1.3492e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.7099e-02, -1.8145e-03, -1.3040e-02],\n",
            "          [-2.2750e-02, -3.6062e-03, -8.0294e-03],\n",
            "          [-1.6087e-02, -1.0175e-02, -1.3529e-02]],\n",
            "\n",
            "         [[ 4.1701e-04, -5.1785e-03, -2.1884e-02],\n",
            "          [ 2.6919e-03,  8.9139e-03, -1.4217e-04],\n",
            "          [-7.3746e-03, -6.6853e-03, -2.3725e-02]],\n",
            "\n",
            "         [[ 1.9425e-02,  1.3175e-02,  1.7511e-02],\n",
            "          [ 1.8235e-02,  4.4286e-02,  2.3767e-02],\n",
            "          [ 2.6504e-02,  3.3104e-02,  1.9696e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.0177e-02, -1.0701e-02, -2.0428e-02],\n",
            "          [-1.7986e-02,  5.9928e-03, -1.0584e-03],\n",
            "          [-1.8794e-02, -1.8773e-03, -6.9449e-03]],\n",
            "\n",
            "         [[-2.8498e-03,  1.6427e-03,  1.4575e-04],\n",
            "          [-5.4403e-03,  8.3667e-03, -9.4164e-03],\n",
            "          [-4.4999e-03,  5.4902e-03,  2.4863e-03]],\n",
            "\n",
            "         [[-1.3356e-02, -2.1525e-02,  5.3421e-04],\n",
            "          [-1.9160e-02, -2.4645e-02, -1.3791e-02],\n",
            "          [-6.1991e-03, -1.3174e-02, -3.6783e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.3993e-03, -2.7823e-03,  7.6715e-03],\n",
            "          [-2.0649e-02, -1.2731e-02, -9.4138e-03],\n",
            "          [-1.3678e-03, -3.4410e-02, -2.6984e-02]],\n",
            "\n",
            "         [[-3.5651e-04,  2.0102e-03,  1.4130e-02],\n",
            "          [-1.3073e-02, -1.6616e-02, -1.2690e-02],\n",
            "          [-3.5934e-02, -4.1700e-02, -3.3968e-02]],\n",
            "\n",
            "         [[ 2.0470e-02,  8.0159e-04, -1.1607e-03],\n",
            "          [ 9.5101e-03,  3.0336e-02,  2.7362e-02],\n",
            "          [ 1.5588e-02,  3.2851e-02,  1.3015e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.5574e-02, -3.2971e-02, -3.1939e-02],\n",
            "          [-2.2502e-02, -5.7187e-03, -5.6729e-03],\n",
            "          [-2.7309e-02, -1.6981e-02,  1.2832e-04]],\n",
            "\n",
            "         [[-1.1925e-02, -2.9479e-02, -2.0437e-02],\n",
            "          [-2.4408e-02, -2.2069e-02, -1.9965e-03],\n",
            "          [-2.3279e-02, -5.5140e-03,  2.5630e-02]],\n",
            "\n",
            "         [[-1.6100e-02, -8.2417e-03,  1.5266e-04],\n",
            "          [-2.6195e-03, -8.2754e-03, -2.9435e-02],\n",
            "          [-2.7493e-03, -2.4889e-02, -2.3583e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.7985e-02,  1.8594e-02,  8.9198e-04],\n",
            "          [-1.7319e-02,  7.8735e-03, -2.8659e-03],\n",
            "          [ 3.8596e-03,  2.9061e-02,  2.4188e-02]],\n",
            "\n",
            "         [[-2.6735e-02, -1.4391e-02, -4.0148e-02],\n",
            "          [-2.6728e-02, -2.4455e-02, -6.9176e-03],\n",
            "          [-5.7244e-02, -2.1995e-04,  5.5438e-02]],\n",
            "\n",
            "         [[ 2.3487e-02,  2.7157e-03, -8.4719e-04],\n",
            "          [ 1.7886e-02,  5.4860e-03,  2.8059e-02],\n",
            "          [ 4.6468e-03,  1.8598e-02,  1.3761e-03]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.2427, 0.2232, 0.2511, 0.2288, 0.2074, 0.2905, 0.2482, 0.3102, 0.2749,\n",
            "        0.2892, 0.2448, 0.1759, 0.2426, 0.2780, 0.2315, 0.2631, 0.3383, 0.2785,\n",
            "        0.2536, 0.2989, 0.2335, 0.2812, 0.3486, 0.2778, 0.2280, 0.2547, 0.3032,\n",
            "        0.2468, 0.2512, 0.2973, 0.2577, 0.3200, 0.2385, 0.2714, 0.2532, 0.2625,\n",
            "        0.3344, 0.2626, 0.1838, 0.2839, 0.2187, 0.2666, 0.2858, 0.2471, 0.2915,\n",
            "        0.2332, 0.2637, 0.2691, 0.2432, 0.2384, 0.2356, 0.2525, 0.2564, 0.2451,\n",
            "        0.2529, 0.2522, 0.2800, 0.3165, 0.2340, 0.2634, 0.2569, 0.1942, 0.2621,\n",
            "        0.2205, 0.2301, 0.2323, 0.2811, 0.1897, 0.2280, 0.3472, 0.2717, 0.3191,\n",
            "        0.2440, 0.2719, 0.2781, 0.2262, 0.3444, 0.2648, 0.2725, 0.2851, 0.2039,\n",
            "        0.2935, 0.2742, 0.2774, 0.2654, 0.2430, 0.2721, 0.2708, 0.3085, 0.2895,\n",
            "        0.2596, 0.2147, 0.3119, 0.3449, 0.2262, 0.2814, 0.2326, 0.2712, 0.2637,\n",
            "        0.2323, 0.3333, 0.2714, 0.2991, 0.2747, 0.2515, 0.2394, 0.2709, 0.2836,\n",
            "        0.2866, 0.2408, 0.2560, 0.2048, 0.2394, 0.2813, 0.3267, 0.2761, 0.2123,\n",
            "        0.2715, 0.2540, 0.2771, 0.3209, 0.1905, 0.3989, 0.2676, 0.2357, 0.2169,\n",
            "        0.3216, 0.3596, 0.2838, 0.2648, 0.2702, 0.2469, 0.2442, 0.2553, 0.2599,\n",
            "        0.2693, 0.2399, 0.2700, 0.2063, 0.2711, 0.2834, 0.2781, 0.2529, 0.2013,\n",
            "        0.2343, 0.2082, 0.3063, 0.1635, 0.2673, 0.2197, 0.2787, 0.2724, 0.2744,\n",
            "        0.2287, 0.2969, 0.2662, 0.2982, 0.2396, 0.3039, 0.2319, 0.2773, 0.2661,\n",
            "        0.2898, 0.2489, 0.3060, 0.2612, 0.2937, 0.3045, 0.2999, 0.2580, 0.2093,\n",
            "        0.2714, 0.2993, 0.2679, 0.2963, 0.2754, 0.2580, 0.2566, 0.2634, 0.2325,\n",
            "        0.2442, 0.2934, 0.2398, 0.2631, 0.2851, 0.2870, 0.2239, 0.2410, 0.2676,\n",
            "        0.2681, 0.2638, 0.2732, 0.2812, 0.2203, 0.2670, 0.2764, 0.2550, 0.3160,\n",
            "        0.2888, 0.2615, 0.2178, 0.2485, 0.2414, 0.2798, 0.2872, 0.2767, 0.2551,\n",
            "        0.2429, 0.2459, 0.3288, 0.3024, 0.2912, 0.2625, 0.3019, 0.2643, 0.2721,\n",
            "        0.2108, 0.2368, 0.2269, 0.1988, 0.2830, 0.2569, 0.2349, 0.2755, 0.2442,\n",
            "        0.2717, 0.2747, 0.2785, 0.2516, 0.2227, 0.2783, 0.2465, 0.2652, 0.2641,\n",
            "        0.2960, 0.2671, 0.2679, 0.2537, 0.2847, 0.2507, 0.2525, 0.2024, 0.2311,\n",
            "        0.2618, 0.2764, 0.3031, 0.2452, 0.2716, 0.2273, 0.2295, 0.2611, 0.2329,\n",
            "        0.2690, 0.2753, 0.2737, 0.2590, 0.2421, 0.2685, 0.3392, 0.3073, 0.1371,\n",
            "        0.3650, 0.2980, 0.2460, 0.2487, 0.2912, 0.2704, 0.2560, 0.2213, 0.2569,\n",
            "        0.2661, 0.2367, 0.2742, 0.2847, 0.3055, 0.2671, 0.2819, 0.2791, 0.2401,\n",
            "        0.2549, 0.2210, 0.3507, 0.2852, 0.2162, 0.2821, 0.2369, 0.2905, 0.2826,\n",
            "        0.2300, 0.2745, 0.2437, 0.2522, 0.2489, 0.2395, 0.2851, 0.2887, 0.2621,\n",
            "        0.2500, 0.2689, 0.2427, 0.3010, 0.3067, 0.2861, 0.2387, 0.2462, 0.2859,\n",
            "        0.2550, 0.2630, 0.2442, 0.2145, 0.2898, 0.2282, 0.2327, 0.2242, 0.2738,\n",
            "        0.2485, 0.2379, 0.3058, 0.2798, 0.2761, 0.2252, 0.2866, 0.2660, 0.3250,\n",
            "        0.2612, 0.2767, 0.3205, 0.2932, 0.3183, 0.2939, 0.3103, 0.2553, 0.2981,\n",
            "        0.3667, 0.3086, 0.2254, 0.2352, 0.2348, 0.2555, 0.2597, 0.2369, 0.3017,\n",
            "        0.2776, 0.2728, 0.3174, 0.2785, 0.2721, 0.2637, 0.2702, 0.3633, 0.2869,\n",
            "        0.2675, 0.3405, 0.2587, 0.2732, 0.2747, 0.2821, 0.2750, 0.2630, 0.2018,\n",
            "        0.2358, 0.3034, 0.3155, 0.3013, 0.2775, 0.2511, 0.2945, 0.1605, 0.2825,\n",
            "        0.2964, 0.2194, 0.2061, 0.2332, 0.2348, 0.2663, 0.2543, 0.2927, 0.2215,\n",
            "        0.2521, 0.2827, 0.1993, 0.2453, 0.2597, 0.2654, 0.2757, 0.2650, 0.2444,\n",
            "        0.2949, 0.2308, 0.3071, 0.1904, 0.3024, 0.2786, 0.3659, 0.2966, 0.2746,\n",
            "        0.2449, 0.2201, 0.2564, 0.2853, 0.2392, 0.2457, 0.2467, 0.2374, 0.2664,\n",
            "        0.2460, 0.3182, 0.1793, 0.2379, 0.2596, 0.2847, 0.2452, 0.1974, 0.2388,\n",
            "        0.2949, 0.2879, 0.2786, 0.2765, 0.3296, 0.2530, 0.2690, 0.2547, 0.2333,\n",
            "        0.2348, 0.2690, 0.2718, 0.2679, 0.2516, 0.2710, 0.2366, 0.2601, 0.2764,\n",
            "        0.2880, 0.2008, 0.2637, 0.2263, 0.2511, 0.2604, 0.2805, 0.2989, 0.2965,\n",
            "        0.2597, 0.2767, 0.2553, 0.2959, 0.2512, 0.2925, 0.3008, 0.2423, 0.2394,\n",
            "        0.2708, 0.3704, 0.2879, 0.2532, 0.2248, 0.2023, 0.2279, 0.2366, 0.3082,\n",
            "        0.2980, 0.2909, 0.2777, 0.4293, 0.2658, 0.2940, 0.2418, 0.2816, 0.3247,\n",
            "        0.2647, 0.2216, 0.2758, 0.2421, 0.2078, 0.2332, 0.2271, 0.2611, 0.3650,\n",
            "        0.2017, 0.2598, 0.2160, 0.2641, 0.1408, 0.2664, 0.2502, 0.2553, 0.2227,\n",
            "        0.2417, 0.2696, 0.2388, 0.2833, 0.2333, 0.2667, 0.2224, 0.2691, 0.2710,\n",
            "        0.2459, 0.2674, 0.2430, 0.2593, 0.1851, 0.2950, 0.3664, 0.2212, 0.3026,\n",
            "        0.1840, 0.3443, 0.2140, 0.3717, 0.2360, 0.3081, 0.2638, 0.2233],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1986, -0.1593, -0.2054, -0.1598, -0.1268, -0.3226, -0.1597, -0.3477,\n",
            "        -0.2497, -0.2730, -0.2319, -0.0286, -0.1899, -0.2813, -0.1733, -0.2412,\n",
            "        -0.3712, -0.2747, -0.2053, -0.2585, -0.1535, -0.2748, -0.3241, -0.2525,\n",
            "        -0.1906, -0.2252, -0.3436, -0.2202, -0.1664, -0.2716, -0.1920, -0.3399,\n",
            "        -0.2026, -0.2972, -0.2616, -0.2238, -0.2486, -0.2606, -0.0893, -0.3572,\n",
            "        -0.1283, -0.2583, -0.2450, -0.1523, -0.3165, -0.1445, -0.2522, -0.1963,\n",
            "        -0.1794, -0.1071, -0.1662, -0.2053, -0.2530, -0.1447, -0.2517, -0.2062,\n",
            "        -0.2817, -0.3376, -0.1382, -0.2389, -0.2557, -0.0156, -0.2169, -0.1763,\n",
            "        -0.1486, -0.2122, -0.2002, -0.0716, -0.2089, -0.3580, -0.2588, -0.3599,\n",
            "        -0.1528, -0.2107, -0.2925, -0.1855, -0.3970, -0.1257, -0.2574, -0.2412,\n",
            "        -0.0863, -0.3065, -0.2701, -0.3380, -0.2485, -0.1935, -0.2987, -0.2279,\n",
            "        -0.3600, -0.2764, -0.2480, -0.1208, -0.3378, -0.2661, -0.1677, -0.2470,\n",
            "        -0.2152, -0.2591, -0.1936, -0.1543, -0.4117, -0.1570, -0.2372, -0.2997,\n",
            "        -0.2124, -0.2034, -0.1848, -0.3070, -0.3438, -0.1839, -0.1937, -0.0916,\n",
            "        -0.2338, -0.3558, -0.1967, -0.3303, -0.1398, -0.2177, -0.1665, -0.1857,\n",
            "        -0.3115, -0.1049, -0.4229, -0.2408, -0.1320, -0.1631, -0.3378, -0.3300,\n",
            "        -0.3183, -0.2268, -0.2787, -0.1950, -0.1950, -0.1463, -0.2437, -0.2297,\n",
            "        -0.1282, -0.2164, -0.1179, -0.2437, -0.2611, -0.2656, -0.1948, -0.1208,\n",
            "        -0.1668, -0.1351, -0.2713, -0.0560, -0.2243, -0.1318, -0.2356, -0.2720,\n",
            "        -0.2051, -0.1736, -0.2891, -0.2627, -0.3358, -0.1779, -0.2309, -0.1477,\n",
            "        -0.2685, -0.1882, -0.2629, -0.1983, -0.3522, -0.1905, -0.2778, -0.3395,\n",
            "        -0.2895, -0.2240, -0.1150, -0.2462, -0.2426, -0.2581, -0.3133, -0.2315,\n",
            "        -0.2271, -0.2077, -0.2109, -0.1371, -0.1323, -0.2529, -0.1716, -0.2532,\n",
            "        -0.2277, -0.2084, -0.1803, -0.1868, -0.2404, -0.2166, -0.2197, -0.2870,\n",
            "        -0.3062, -0.1507, -0.1054, -0.2199, -0.2415, -0.3310, -0.2700, -0.1568,\n",
            "        -0.1449, -0.2610, -0.1828, -0.2648, -0.3134, -0.2937, -0.2687, -0.2115,\n",
            "        -0.2164, -0.4522, -0.2999, -0.3032, -0.2292, -0.3099, -0.2642, -0.2695,\n",
            "        -0.1441, -0.1671, -0.1570, -0.1415, -0.2222, -0.1736, -0.1481, -0.2573,\n",
            "        -0.2060, -0.1703, -0.2360, -0.1770, -0.2132, -0.2016, -0.3001, -0.1518,\n",
            "        -0.2086, -0.2805, -0.2698, -0.2292, -0.1293, -0.2514, -0.2600, -0.2454,\n",
            "        -0.1744, -0.1029, -0.1679, -0.2353, -0.2007, -0.3363, -0.1640, -0.2430,\n",
            "        -0.1699, -0.1697, -0.1837, -0.1625, -0.2415, -0.2687, -0.2305, -0.2029,\n",
            "        -0.2209, -0.2240, -0.2675, -0.3233,  0.1462, -0.4777, -0.2376, -0.1489,\n",
            "        -0.1462, -0.3055, -0.2234, -0.1697, -0.1952, -0.2131, -0.2340, -0.2039,\n",
            "        -0.3054, -0.2596, -0.3470, -0.2176, -0.2706, -0.2897, -0.1729, -0.2300,\n",
            "        -0.1066, -0.3556, -0.2912, -0.1777, -0.2007, -0.1699, -0.3009, -0.3046,\n",
            "        -0.1693, -0.2602, -0.2053, -0.1810, -0.1808, -0.1730, -0.3757, -0.1808,\n",
            "        -0.1805, -0.1895, -0.2643, -0.2075, -0.2365, -0.1975, -0.3064, -0.1984,\n",
            "        -0.1811, -0.3676, -0.1198, -0.1485, -0.1770, -0.0781, -0.2052, -0.1360,\n",
            "        -0.1417, -0.1691, -0.2395, -0.1785, -0.1747, -0.2484, -0.2717, -0.3096,\n",
            "        -0.1465, -0.2239, -0.2584, -0.3572, -0.2311, -0.2878, -0.3841, -0.3475,\n",
            "        -0.3896, -0.1891, -0.2861, -0.2431, -0.2837, -0.4365, -0.3353, -0.1802,\n",
            "        -0.1976, -0.1529, -0.1978, -0.2535, -0.1954, -0.2667, -0.2813, -0.2487,\n",
            "        -0.3070, -0.2339, -0.2212, -0.1925, -0.2224, -0.4178, -0.3151, -0.2663,\n",
            "        -0.3581, -0.1935, -0.2385, -0.2424, -0.1850, -0.2265, -0.1803, -0.0777,\n",
            "        -0.1492, -0.3361, -0.4133, -0.3123, -0.2745, -0.1247, -0.3102,  0.0041,\n",
            "        -0.1981, -0.3301, -0.2047, -0.1053, -0.1653, -0.1634, -0.1116, -0.2314,\n",
            "        -0.3191, -0.1818, -0.2657, -0.2220, -0.1029, -0.1999, -0.2702, -0.2139,\n",
            "        -0.2256, -0.2653, -0.1630, -0.3322, -0.1617, -0.3446,  0.0288, -0.2456,\n",
            "        -0.3171, -0.3580, -0.2857, -0.2520, -0.2031, -0.1522, -0.2203, -0.3490,\n",
            "        -0.1685, -0.1424, -0.1602, -0.1553, -0.3057, -0.2420, -0.3536, -0.0551,\n",
            "        -0.0987, -0.2272, -0.2619, -0.2035, -0.0906, -0.1976, -0.3040, -0.2732,\n",
            "        -0.3161, -0.2102, -0.3384, -0.1740, -0.1475, -0.1842, -0.1823, -0.1151,\n",
            "        -0.2183, -0.2010, -0.2659, -0.2205, -0.2567, -0.1633, -0.2213, -0.2658,\n",
            "        -0.2938, -0.1069, -0.2522, -0.1103, -0.2216, -0.2244, -0.2908, -0.2176,\n",
            "        -0.3605, -0.2374, -0.2391, -0.2251, -0.2256, -0.1339, -0.1970, -0.2970,\n",
            "        -0.2206, -0.2051, -0.2229, -0.3602, -0.2923, -0.2498, -0.1466, -0.0979,\n",
            "        -0.1686, -0.2158, -0.2881, -0.3002, -0.2760, -0.2496, -0.3536, -0.2868,\n",
            "        -0.3251, -0.1847, -0.3062, -0.3861, -0.2650, -0.1339, -0.1846, -0.1630,\n",
            "        -0.0630, -0.1717, -0.1415, -0.1906, -0.4611, -0.1391, -0.1920, -0.1369,\n",
            "        -0.1647, -0.0055, -0.2598, -0.2653, -0.2319, -0.1780, -0.1913, -0.2055,\n",
            "        -0.1891, -0.2625, -0.1633, -0.2497, -0.1696, -0.1907, -0.2431, -0.1825,\n",
            "        -0.2607, -0.1943, -0.2361, -0.0581, -0.2758, -0.2593, -0.1466, -0.3589,\n",
            "        -0.0439, -0.3440, -0.1089, -0.4219, -0.1503, -0.2792, -0.3035, -0.1156],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 1.6218e-04, -1.4720e-02, -1.7000e-02],\n",
            "          [-1.2850e-02, -3.3085e-02, -3.6656e-02],\n",
            "          [ 2.7812e-02,  1.7691e-02, -1.8369e-02]],\n",
            "\n",
            "         [[ 1.0528e-02,  3.1379e-02,  2.4801e-02],\n",
            "          [-1.2698e-02, -2.9453e-02, -1.1834e-02],\n",
            "          [-9.4094e-03, -8.9462e-03, -3.1349e-02]],\n",
            "\n",
            "         [[-7.8447e-03, -2.9256e-02,  5.3590e-03],\n",
            "          [-1.3791e-02, -1.1116e-02,  5.0388e-03],\n",
            "          [-2.4919e-03,  7.3514e-03,  5.4013e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.0276e-03, -1.0275e-02, -2.9986e-02],\n",
            "          [-3.8465e-03,  1.9549e-03, -1.6291e-02],\n",
            "          [-1.8100e-03,  8.3778e-03, -8.5481e-03]],\n",
            "\n",
            "         [[-1.8196e-02, -1.3533e-02, -1.7457e-02],\n",
            "          [ 2.2457e-02,  5.7402e-02,  1.9325e-02],\n",
            "          [-2.4977e-02, -3.2113e-02, -8.1780e-03]],\n",
            "\n",
            "         [[ 3.6550e-03,  4.9358e-03, -5.7597e-03],\n",
            "          [-1.6875e-02,  1.3999e-04,  3.7629e-04],\n",
            "          [-2.6272e-03,  1.0947e-03,  1.1145e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 1.4018e-02,  3.9198e-03, -1.7189e-03],\n",
            "          [-1.3175e-03,  4.3503e-04, -1.1798e-02],\n",
            "          [-9.8003e-03, -1.7693e-02, -1.9910e-02]],\n",
            "\n",
            "         [[-1.4957e-02, -1.9796e-02, -2.8724e-02],\n",
            "          [ 5.8908e-03, -1.5228e-02, -5.6715e-03],\n",
            "          [ 2.9284e-03, -1.8028e-02, -7.1433e-03]],\n",
            "\n",
            "         [[-1.1625e-02, -3.3804e-02, -1.0025e-02],\n",
            "          [-1.6606e-02, -5.5716e-02, -2.3204e-02],\n",
            "          [-2.5758e-02, -4.3135e-02, -2.5901e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.5007e-02, -1.4333e-02, -2.5937e-03],\n",
            "          [-2.3078e-02, -1.5820e-02, -2.2818e-03],\n",
            "          [-4.1318e-03, -8.0353e-03, -2.3236e-03]],\n",
            "\n",
            "         [[-1.8531e-02, -1.8004e-02, -2.8084e-02],\n",
            "          [-3.6680e-02, -6.8641e-02, -5.2469e-02],\n",
            "          [-1.1712e-02, -2.4334e-02, -1.6733e-02]],\n",
            "\n",
            "         [[-2.2078e-02, -2.9163e-02, -3.8717e-03],\n",
            "          [-7.0301e-03,  1.6718e-02,  5.4339e-03],\n",
            "          [-1.3131e-02,  1.1999e-02, -1.7480e-02]]],\n",
            "\n",
            "\n",
            "        [[[-5.2378e-03, -3.4890e-03, -2.0851e-03],\n",
            "          [ 1.5306e-02, -2.1752e-02, -8.7682e-03],\n",
            "          [ 2.2460e-02,  9.9175e-03, -3.3635e-03]],\n",
            "\n",
            "         [[ 7.4677e-03, -9.1762e-03, -9.2569e-05],\n",
            "          [ 1.9441e-04,  1.2344e-03, -8.9978e-03],\n",
            "          [-5.1243e-04,  2.1850e-04, -4.8828e-03]],\n",
            "\n",
            "         [[ 1.7078e-02,  3.3955e-03,  9.3503e-03],\n",
            "          [ 2.0334e-02, -1.0621e-04, -8.2017e-05],\n",
            "          [ 1.0706e-02, -1.8414e-03,  1.0828e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.2008e-02,  2.3494e-02,  2.5386e-02],\n",
            "          [ 1.9307e-02,  2.3924e-02,  2.8972e-02],\n",
            "          [ 9.9003e-03,  2.0158e-02,  2.2655e-02]],\n",
            "\n",
            "         [[-9.8395e-03, -1.1114e-02, -3.7696e-03],\n",
            "          [-2.9508e-02, -3.6956e-02, -1.8228e-02],\n",
            "          [-1.3663e-03, -2.5845e-03,  1.0352e-02]],\n",
            "\n",
            "         [[-7.3867e-03, -2.5413e-02, -2.1942e-02],\n",
            "          [-1.6699e-02, -1.5133e-02, -1.3030e-02],\n",
            "          [-2.0090e-02,  3.7970e-03, -1.0341e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1.6157e-02, -1.6883e-02, -2.8328e-04],\n",
            "          [-7.7759e-03, -2.4465e-03, -1.4641e-02],\n",
            "          [ 2.4639e-02,  3.9862e-02,  2.1048e-02]],\n",
            "\n",
            "         [[ 2.4491e-03, -9.3885e-03, -1.1786e-02],\n",
            "          [ 2.5301e-02,  2.5625e-04,  7.1335e-03],\n",
            "          [ 2.2342e-02,  1.9042e-02,  7.2526e-03]],\n",
            "\n",
            "         [[-1.4652e-02, -2.7802e-02, -4.3564e-03],\n",
            "          [-1.7961e-02, -4.3846e-02,  2.7409e-03],\n",
            "          [-4.7968e-03, -8.4231e-03,  1.2070e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.0171e-02, -3.3546e-02, -1.6728e-02],\n",
            "          [-1.7847e-02, -5.1713e-02, -2.6780e-02],\n",
            "          [-1.3145e-03, -4.3181e-03, -9.6373e-03]],\n",
            "\n",
            "         [[-5.3917e-03, -2.0410e-04,  2.7798e-03],\n",
            "          [-9.6882e-04, -2.5141e-02,  1.4804e-02],\n",
            "          [ 2.8748e-02,  9.0832e-03,  4.2548e-02]],\n",
            "\n",
            "         [[-1.5698e-02, -1.9303e-02, -9.1469e-03],\n",
            "          [-2.0025e-02, -1.1131e-02, -3.3902e-02],\n",
            "          [-5.7436e-03, -7.3640e-03, -1.0044e-02]]],\n",
            "\n",
            "\n",
            "        [[[-8.8612e-03, -4.5370e-03, -1.2354e-02],\n",
            "          [-5.9245e-03, -1.7058e-02, -2.8041e-02],\n",
            "          [-1.0435e-02,  7.6695e-04, -1.0578e-02]],\n",
            "\n",
            "         [[ 9.5200e-03, -5.1975e-03,  1.2947e-02],\n",
            "          [ 4.4305e-03, -2.3992e-02, -8.4569e-04],\n",
            "          [ 4.6608e-03,  9.6787e-03,  8.2174e-03]],\n",
            "\n",
            "         [[ 5.1559e-03,  4.4635e-04, -7.9934e-03],\n",
            "          [ 3.3069e-03,  1.4450e-02,  8.9234e-03],\n",
            "          [ 6.3402e-03,  1.9043e-02,  1.9021e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.6964e-03, -1.3777e-02,  6.0539e-03],\n",
            "          [-1.5745e-03, -2.3391e-02, -1.0052e-02],\n",
            "          [ 9.5183e-03, -1.2251e-02,  2.2436e-03]],\n",
            "\n",
            "         [[ 1.0375e-02,  3.5875e-03, -5.7940e-04],\n",
            "          [ 7.0412e-03, -1.0673e-02, -4.9120e-03],\n",
            "          [-2.6034e-03,  1.1306e-02,  7.0696e-03]],\n",
            "\n",
            "         [[-1.7509e-02, -2.3182e-02, -1.7897e-02],\n",
            "          [-1.7769e-03,  1.9672e-03, -7.3220e-03],\n",
            "          [-6.6833e-03,  9.8286e-03,  2.0653e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 2.8375e-02, -8.1936e-03,  1.8009e-02],\n",
            "          [ 1.5829e-02, -1.3571e-02, -1.9335e-02],\n",
            "          [ 4.0766e-03, -1.5722e-02, -5.0620e-02]],\n",
            "\n",
            "         [[-5.5310e-03, -1.8996e-02, -7.9436e-03],\n",
            "          [ 1.3825e-03, -4.9608e-02,  1.7256e-03],\n",
            "          [ 7.6629e-03, -7.6101e-03,  1.2541e-02]],\n",
            "\n",
            "         [[ 1.8052e-02,  3.1718e-02,  4.2556e-03],\n",
            "          [-3.6760e-03,  3.0490e-03, -1.2264e-02],\n",
            "          [-8.9404e-03, -1.6604e-02,  1.6348e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.3192e-03,  1.8204e-02,  1.8114e-02],\n",
            "          [-6.1202e-03,  1.5905e-03,  2.0264e-02],\n",
            "          [-1.1471e-02, -1.5697e-02,  9.0871e-03]],\n",
            "\n",
            "         [[ 3.7707e-03,  8.0599e-03,  1.8290e-02],\n",
            "          [ 1.7257e-02,  6.9638e-03,  1.8746e-02],\n",
            "          [ 1.0751e-02,  1.3663e-02, -1.0081e-03]],\n",
            "\n",
            "         [[ 1.9711e-02, -1.4569e-02, -2.4663e-02],\n",
            "          [ 2.5966e-03, -2.4807e-02,  9.3861e-03],\n",
            "          [-1.2876e-03,  1.3974e-03,  1.3434e-02]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.4474, 0.5138, 0.4335, 0.3421, 0.3855, 0.3495, 0.3741, 0.5836, 0.4327,\n",
            "        0.5043, 0.4618, 0.3866, 0.3498, 0.4798, 0.3310, 0.3913, 0.3880, 0.5225,\n",
            "        0.3975, 0.3292, 0.4151, 0.4458, 0.3970, 0.3614, 0.3914, 0.4633, 0.3463,\n",
            "        0.3644, 0.3272, 0.4584, 0.4280, 0.4538, 0.4030, 0.4673, 0.4209, 0.3987,\n",
            "        0.4233, 0.3876, 0.4212, 0.3460, 0.3522, 0.3744, 0.4550, 0.2888, 0.4590,\n",
            "        0.4817, 0.4450, 0.5110, 0.4052, 0.4247, 0.3558, 0.3075, 0.4462, 0.4724,\n",
            "        0.4253, 0.3884, 0.4492, 0.3727, 0.4630, 0.3985, 0.3512, 0.3665, 0.3860,\n",
            "        0.5082, 0.4022, 0.3458, 0.4805, 0.5390, 0.4223, 0.4275, 0.4590, 0.4736,\n",
            "        0.3673, 0.5405, 0.3243, 0.5178, 0.4743, 0.3506, 0.3759, 0.4328, 0.3867,\n",
            "        0.4591, 0.3843, 0.4982, 0.5288, 0.3946, 0.4589, 0.3197, 0.4676, 0.4806,\n",
            "        0.4308, 0.4235, 0.3284, 0.3877, 0.4140, 0.4469, 0.4041, 0.4407, 0.4356,\n",
            "        0.5120, 0.5059, 0.4628, 0.4585, 0.3311, 0.3424, 0.4150, 0.5170, 0.4593,\n",
            "        0.5228, 0.4252, 0.4214, 0.4995, 0.4098, 0.5380, 0.4874, 0.3719, 0.4649,\n",
            "        0.4320, 0.3277, 0.3743, 0.4360, 0.4838, 0.4399, 0.3763, 0.4150, 0.5147,\n",
            "        0.5012, 0.4382, 0.3655, 0.4037, 0.4498, 0.4720, 0.3914, 0.3237, 0.3208,\n",
            "        0.3224, 0.4291, 0.4009, 0.3947, 0.3779, 0.4349, 0.4120, 0.3274, 0.4334,\n",
            "        0.3740, 0.4189, 0.4288, 0.3071, 0.4260, 0.3410, 0.4375, 0.4407, 0.3750,\n",
            "        0.5853, 0.4518, 0.5045, 0.3005, 0.4968, 0.4155, 0.3755, 0.5514, 0.4146,\n",
            "        0.4677, 0.1404, 0.5001, 0.4193, 0.4246, 0.4452, 0.5109, 0.4488, 0.4574,\n",
            "        0.3896, 0.4145, 0.4497, 0.4245, 0.3971, 0.3957, 0.4072, 0.5305, 0.4986,\n",
            "        0.3733, 0.4280, 0.3469, 0.4178, 0.3766, 0.4029, 0.3814, 0.4493, 0.5132,\n",
            "        0.4080, 0.4155, 0.3635, 0.4391, 0.3489, 0.4228, 0.4833, 0.3494, 0.4406,\n",
            "        0.3795, 0.4298, 0.4910, 0.3878, 0.6299, 0.4322, 0.5436, 0.4140, 0.4312,\n",
            "        0.3161, 0.3612, 0.3597, 0.4281, 0.4506, 0.4294, 0.3646, 0.4110, 0.4038,\n",
            "        0.4098, 0.3901, 0.3928, 0.5421, 0.3629, 0.4078, 0.4586, 0.4217, 0.3953,\n",
            "        0.3997, 0.3838, 0.4374, 0.3576, 0.4217, 0.4128, 0.3904, 0.4137, 0.5145,\n",
            "        0.4039, 0.3577, 0.4429, 0.5639, 0.3848, 0.6104, 0.4482, 0.6203, 0.5336,\n",
            "        0.3480, 0.5401, 0.6044, 0.4077, 0.3469, 0.4281, 0.4631, 0.5948, 0.3479,\n",
            "        0.3689, 0.3658, 0.3191, 0.5492, 0.3410, 0.5386, 0.4041, 0.3373, 0.4186,\n",
            "        0.5187, 0.3933, 0.3188, 0.3502, 0.3736, 0.4238, 0.4752, 0.3322, 0.5078,\n",
            "        0.4317, 0.5318, 0.4413, 0.5510, 0.5648, 0.4130, 0.4017, 0.4304, 0.4077,\n",
            "        0.4285, 0.4360, 0.3749, 0.4261, 0.3905, 0.3030, 0.3412, 0.3768, 0.4507,\n",
            "        0.3127, 0.4592, 0.4298, 0.3936, 0.3106, 0.3869, 0.3594, 0.4046, 0.4722,\n",
            "        0.4373, 0.3902, 0.3515, 0.4448, 0.4299, 0.4347, 0.4693, 0.4807, 0.2549,\n",
            "        0.4171, 0.4387, 0.4156, 0.3976, 0.4092, 0.4953, 0.4824, 0.3468, 0.4382,\n",
            "        0.4179, 0.4668, 0.3299, 0.5986, 0.4949, 0.4167, 0.4996, 0.4528, 0.4550,\n",
            "        0.4945, 0.3415, 0.4658, 0.4356, 0.3976, 0.5439, 0.4643, 0.5122, 0.4669,\n",
            "        0.4463, 0.4810, 0.3492, 0.3961, 0.3593, 0.4053, 0.3878, 0.3959, 0.5001,\n",
            "        0.2808, 0.5470, 0.4448, 0.4894, 0.4621, 0.3417, 0.3485, 0.5060, 0.3637,\n",
            "        0.3774, 0.3248, 0.4520, 0.3936, 0.3403, 0.4660, 0.4114, 0.3643, 0.4196,\n",
            "        0.3903, 0.5128, 0.4221, 0.4115, 0.4240, 0.3610, 0.4999, 0.3672, 0.4721,\n",
            "        0.4252, 0.5590, 0.4694, 0.7322, 0.5849, 0.4749, 0.4426, 0.3934, 0.3909,\n",
            "        0.4576, 0.3636, 0.4146, 0.4129, 0.5081, 0.3681, 0.3652, 0.4254, 0.2945,\n",
            "        0.4142, 0.3145, 0.4304, 0.4252, 0.3493, 0.4257, 0.5133, 0.3261, 0.4367,\n",
            "        0.3637, 0.3712, 0.4183, 0.3772, 0.4418, 0.4231, 0.4133, 0.4731, 0.4955,\n",
            "        0.4046, 0.4079, 0.4719, 0.3875, 0.4673, 0.4129, 0.4569, 0.3530, 0.4793,\n",
            "        0.3844, 0.3785, 0.3343, 0.4351, 0.6512, 0.4295, 0.4122, 0.3788, 0.3692,\n",
            "        0.4343, 0.4214, 0.3873, 0.4566, 0.4456, 0.4107, 0.4596, 0.7082, 0.4452,\n",
            "        0.3515, 0.4785, 0.4217, 0.5756, 0.4312, 0.4047, 0.4043, 0.4764, 0.5489,\n",
            "        0.4430, 0.5559, 0.3744, 0.3951, 0.4376, 0.4752, 0.4340, 0.4399, 0.3586,\n",
            "        0.4161, 0.3930, 0.4599, 0.4354, 0.3448, 0.4649, 0.4442, 0.4275, 0.3881,\n",
            "        0.3247, 0.4909, 0.3426, 0.3989, 0.4320, 0.3363, 0.3991, 0.4732, 0.3514,\n",
            "        0.4736, 0.4244, 0.4603, 0.3298, 0.4357, 0.4353, 0.3742, 0.4191, 0.3880,\n",
            "        0.4212, 0.4527, 0.7213, 0.3969, 0.5217, 0.3786, 0.3512, 0.5318, 0.4138,\n",
            "        0.3243, 0.3244, 0.3652, 0.4774, 0.3997, 0.2800, 0.4562, 0.4463, 0.4816,\n",
            "        0.4290, 0.4399, 0.4633, 0.3575, 0.4774, 0.3105, 0.4356, 0.3797, 0.4304,\n",
            "        0.4261, 0.3740, 0.3370, 0.3917, 0.3637, 0.4347, 0.5235, 0.3845],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1759, -0.2156, -0.2047, -0.1695, -0.1628, -0.1473, -0.2158, -0.2905,\n",
            "        -0.1112, -0.2196, -0.1020, -0.1549, -0.1989, -0.0445, -0.1508, -0.1920,\n",
            "        -0.2114, -0.1655, -0.1854, -0.1733, -0.1289, -0.2376, -0.1965, -0.1965,\n",
            "        -0.1776, -0.1774, -0.1760, -0.1546, -0.1648, -0.2599, -0.1752, -0.2498,\n",
            "        -0.1741, -0.2410, -0.2498, -0.2938, -0.1496, -0.1578, -0.1800, -0.1851,\n",
            "        -0.1516, -0.1345, -0.2746, -0.1248, -0.2246, -0.2531, -0.2398, -0.1859,\n",
            "        -0.1739, -0.2393, -0.1214, -0.1803, -0.2729, -0.2617, -0.1855, -0.2316,\n",
            "        -0.2333, -0.1860, -0.2097, -0.0692, -0.1912, -0.2078, -0.1084, -0.2810,\n",
            "        -0.1303, -0.1654, -0.2119, -0.3641, -0.2951, -0.2384, -0.1632, -0.1892,\n",
            "        -0.1792, -0.2031, -0.1770, -0.2738, -0.3324, -0.1725, -0.1793, -0.2638,\n",
            "        -0.2207, -0.1609, -0.1534, -0.1414, -0.2992, -0.1450, -0.1838, -0.1779,\n",
            "        -0.1422, -0.2198, -0.1900, -0.1580, -0.1666, -0.2490, -0.1569, -0.1718,\n",
            "        -0.1660, -0.1972, -0.2287, -0.2366, -0.2230, -0.1543, -0.2030, -0.1431,\n",
            "        -0.1363, -0.2015, -0.1804, -0.2093, -0.2964, -0.1984, -0.2683, -0.2216,\n",
            "        -0.2147, -0.3404, -0.2668, -0.1890, -0.1733, -0.2226, -0.1772, -0.1698,\n",
            "        -0.1095, -0.2180, -0.1154, -0.1654, -0.1910, -0.3535, -0.3112, -0.2161,\n",
            "        -0.1496, -0.1667, -0.2849, -0.2207, -0.1529, -0.1807, -0.2118, -0.1869,\n",
            "        -0.1376, -0.1770, -0.1861, -0.1969, -0.1741, -0.3011, -0.0787, -0.2017,\n",
            "        -0.1947, -0.2247, -0.2459, -0.1058, -0.1401, -0.1213, -0.1199, -0.1760,\n",
            "        -0.2156, -0.3307, -0.3515, -0.2366, -0.1185, -0.2155, -0.1751, -0.1892,\n",
            "        -0.3365, -0.1598, -0.2554,  0.0644, -0.2856, -0.1198, -0.1583, -0.2297,\n",
            "        -0.3352, -0.1987, -0.2686, -0.1632, -0.2461, -0.2900, -0.2428, -0.1449,\n",
            "        -0.1900, -0.2149, -0.1541, -0.2917, -0.2504, -0.2213, -0.0463, -0.1547,\n",
            "        -0.1511, -0.1527, -0.1735, -0.1931, -0.1987, -0.2239, -0.2086, -0.2688,\n",
            "        -0.1845, -0.1797, -0.1833, -0.3880, -0.1539, -0.1553, -0.1567, -0.2238,\n",
            "        -0.1511, -0.2540, -0.2849, -0.1826, -0.2687, -0.2328, -0.2108, -0.2410,\n",
            "        -0.1022, -0.1507, -0.1978, -0.1734, -0.2282, -0.0985, -0.1847, -0.1770,\n",
            "        -0.1576, -0.1937, -0.1643, -0.2822, -0.1866, -0.2754, -0.2266, -0.2169,\n",
            "        -0.1352, -0.2194, -0.1060, -0.2139, -0.1322, -0.1889, -0.2130, -0.1913,\n",
            "        -0.2364, -0.1402, -0.2228, -0.2354, -0.1632, -0.1905, -0.1428, -0.1177,\n",
            "        -0.2419, -0.2733, -0.2963, -0.1600, -0.3558, -0.3673, -0.2201, -0.1505,\n",
            "        -0.2084, -0.0870, -0.2052, -0.2070, -0.1986, -0.2299, -0.0745, -0.1765,\n",
            "        -0.1412, -0.2180, -0.1450, -0.1426, -0.1452, -0.2916, -0.0871, -0.1359,\n",
            "        -0.2003, -0.1125, -0.2588, -0.1988, -0.2028, -0.2443, -0.0864, -0.3415,\n",
            "        -0.2579, -0.2343, -0.3552, -0.1859, -0.1153, -0.1732, -0.1780, -0.1909,\n",
            "        -0.2018, -0.1886, -0.2751, -0.1501,  0.1165, -0.1891, -0.1845, -0.2037,\n",
            "        -0.0339, -0.3464, -0.1956, -0.1962, -0.1537, -0.1902, -0.1431, -0.3022,\n",
            "        -0.1780, -0.1971, -0.2118, -0.0952, -0.1711, -0.2409, -0.2184, -0.2114,\n",
            "        -0.2042, -0.0566, -0.0700, -0.2081, -0.1872, -0.2079, -0.1540, -0.2266,\n",
            "        -0.1981, -0.1679, -0.2022, -0.2010, -0.1051, -0.1705, -0.2139,  0.0396,\n",
            "        -0.1077, -0.2745, -0.2690, -0.2603, -0.2819, -0.1917, -0.1940, -0.2944,\n",
            "        -0.1822, -0.2903, -0.1064, -0.2076, -0.2648, -0.3032, -0.2878, -0.1579,\n",
            "        -0.0071, -0.2142, -0.2022, -0.1516, -0.1123,  0.0246, -0.0978, -0.1382,\n",
            "        -0.1800, -0.3214, -0.2179, -0.1369, -0.0800,  0.0117, -0.1839, -0.1926,\n",
            "        -0.1614, -0.2769, -0.1909, -0.2101, -0.2305, -0.2055, -0.2017, -0.2741,\n",
            "        -0.1005, -0.3152, -0.1121, -0.1700, -0.1364, -0.2157, -0.2673, -0.1584,\n",
            "        -0.1997, -0.1745, -0.1886, -0.2307, -0.2024, -0.3376, -0.2266, -0.2355,\n",
            "        -0.2133, -0.2346, -0.2412, -0.2358, -0.1265, -0.2341, -0.1887, -0.1646,\n",
            "        -0.1417, -0.1882, -0.1076, -0.3048, -0.1162, -0.1651, -0.2046, -0.1833,\n",
            "        -0.3102, -0.1778, -0.1575, -0.2676, -0.1777, -0.1569, -0.1741, -0.1892,\n",
            "        -0.3028, -0.1457, -0.2179, -0.2226, -0.1609, -0.1423, -0.2683, -0.2920,\n",
            "        -0.1740, -0.2079, -0.1940, -0.2679, -0.1973, -0.1951, -0.1665, -0.2286,\n",
            "        -0.1903, -0.2667, -0.4010, -0.2550, -0.1817, -0.2025, -0.1589, -0.2476,\n",
            "        -0.0573, -0.2203, -0.2084, -0.1587, -0.1212, -0.1795, -0.3449, -0.1662,\n",
            "        -0.2523, -0.2435, -0.2878, -0.2797, -0.1897, -0.2113, -0.1943, -0.2050,\n",
            "        -0.1694, -0.2243, -0.2987, -0.1328, -0.1428, -0.2399, -0.1593, -0.1999,\n",
            "        -0.3225, -0.1860, -0.1763, -0.2691, -0.2097, -0.2396, -0.1140, -0.1897,\n",
            "        -0.1870, -0.1829, -0.2615, -0.2073, -0.1858, -0.0598, -0.1915, -0.2183,\n",
            "        -0.2088, -0.1742, -0.2715, -0.1999, -0.2117, -0.2492, -0.1717, -0.1566,\n",
            "        -0.1669, -0.3015, -0.1685, -0.2434, -0.2297, -0.1947, -0.2860, -0.3288,\n",
            "        -0.2197, -0.1862, -0.1755, -0.0987, -0.1756, -0.1304, -0.1555, -0.1679,\n",
            "        -0.2222, -0.2819, -0.2652, -0.0947, -0.2412, -0.2731, -0.2572, -0.2604,\n",
            "        -0.2934, -0.2470, -0.1820, -0.2740, -0.1336, -0.1698, -0.1919, -0.1796,\n",
            "        -0.2325, -0.1352, -0.1077, -0.2184, -0.1539, -0.2015, -0.3243, -0.1713],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 0.0057]],\n",
            "\n",
            "         [[ 0.0020]],\n",
            "\n",
            "         [[ 0.0167]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0085]],\n",
            "\n",
            "         [[-0.0274]],\n",
            "\n",
            "         [[ 0.0097]]],\n",
            "\n",
            "\n",
            "        [[[-0.0271]],\n",
            "\n",
            "         [[-0.0157]],\n",
            "\n",
            "         [[ 0.0543]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0206]],\n",
            "\n",
            "         [[-0.0308]],\n",
            "\n",
            "         [[ 0.0013]]],\n",
            "\n",
            "\n",
            "        [[[-0.0523]],\n",
            "\n",
            "         [[-0.0353]],\n",
            "\n",
            "         [[ 0.0394]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0382]],\n",
            "\n",
            "         [[-0.0264]],\n",
            "\n",
            "         [[-0.0443]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0494]],\n",
            "\n",
            "         [[ 0.0436]],\n",
            "\n",
            "         [[ 0.0103]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0072]],\n",
            "\n",
            "         [[ 0.0014]],\n",
            "\n",
            "         [[-0.0669]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0533]],\n",
            "\n",
            "         [[-0.0148]],\n",
            "\n",
            "         [[-0.0480]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0055]],\n",
            "\n",
            "         [[ 0.0429]],\n",
            "\n",
            "         [[ 0.0129]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0300]],\n",
            "\n",
            "         [[-0.0092]],\n",
            "\n",
            "         [[ 0.0090]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0101]],\n",
            "\n",
            "         [[-0.0111]],\n",
            "\n",
            "         [[-0.0080]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.1694,  0.3368,  0.2993,  0.3745,  0.1513,  0.1781,  0.3167,  0.3947,\n",
            "         0.1858,  0.2068,  0.1090,  0.2042,  0.2955,  0.0765,  0.2023,  0.2487,\n",
            "         0.3295,  0.3349,  0.2532,  0.2739,  0.1661,  0.3432,  0.3424,  0.2969,\n",
            "         0.2226,  0.0993,  0.3328,  0.2349,  0.2894,  0.2296,  0.2719,  0.3945,\n",
            "         0.1990,  0.2564,  0.2557,  0.3541,  0.1848,  0.2513,  0.3101,  0.2782,\n",
            "         0.2109,  0.2441,  0.3282,  0.3248,  0.2499,  0.1873,  0.2643,  0.3949,\n",
            "         0.1962,  0.2587,  0.1708,  0.3381,  0.2238,  0.2498,  0.2787,  0.3783,\n",
            "         0.3445,  0.2681,  0.2956,  0.1146,  0.2688,  0.3479,  0.1295,  0.2843,\n",
            "         0.1552,  0.3026,  0.2738,  0.1891,  0.3568,  0.2302,  0.2199,  0.2070,\n",
            "         0.2119,  0.0971,  0.2482,  0.2264,  0.3555,  0.3113,  0.2386,  0.2654,\n",
            "         0.2975,  0.2666,  0.2180,  0.1451,  0.2460,  0.1734,  0.2358,  0.2891,\n",
            "         0.2091,  0.1971,  0.2185,  0.2008,  0.2461,  0.3726,  0.2028,  0.1993,\n",
            "         0.3652,  0.2258,  0.2606,  0.1900,  0.2764,  0.2011,  0.1973,  0.2958,\n",
            "         0.3222,  0.4117,  0.1475,  0.2674,  0.1928,  0.3615,  0.2774,  0.2143,\n",
            "         0.2688,  0.4286,  0.2560,  0.2777,  0.1339,  0.5103,  0.3238,  0.2417,\n",
            "         0.1529,  0.1843,  0.0579,  0.2288,  0.1797,  0.2803,  0.2279,  0.1579,\n",
            "         0.3196,  0.1842,  0.3378,  0.1688,  0.1654,  0.3049,  0.3533,  0.2948,\n",
            "         0.1140,  0.2503,  0.1892,  0.2647,  0.2405,  0.3880,  0.1933,  0.1918,\n",
            "         0.2511,  0.2901,  0.3151,  0.3252,  0.1296,  0.2491,  0.1417,  0.1295,\n",
            "         0.3062,  0.2836,  0.3483,  0.2306,  0.2741,  0.2700,  0.1873,  0.2431,\n",
            "         0.3526,  0.3546,  0.2721,  0.2708,  0.3065,  0.0832,  0.2968,  0.2286,\n",
            "         0.3276,  0.2695,  0.2452,  0.2444,  0.2857,  0.3365,  0.2784,  0.2933,\n",
            "         0.3397,  0.2231,  0.2330,  0.1486,  0.3846,  0.3104,  0.1724,  0.1724,\n",
            "         0.3466,  0.2978,  0.2582,  0.1879,  0.2419,  0.2249,  0.2720,  0.3735,\n",
            "         0.4259,  0.3754,  0.1731,  0.3698,  0.2349,  0.2694,  0.3148,  0.1658,\n",
            "         0.1181,  0.2994,  0.4018,  0.2126,  0.3864,  0.2955,  0.1848,  0.3686,\n",
            "         0.1972,  0.3265,  0.2319,  0.1676,  0.1756,  0.2367,  0.2139,  0.1974,\n",
            "         0.2561,  0.2619,  0.2170,  0.2284,  0.3486,  0.4500,  0.2563,  0.2559,\n",
            "         0.2814,  0.1797,  0.1736,  0.2013,  0.3411,  0.2245,  0.1385,  0.2284,\n",
            "         0.2230,  0.2566,  0.2301,  0.3639,  0.1380,  0.2381,  0.2590,  0.0830,\n",
            "         0.1863,  0.1267,  0.4501,  0.2741,  0.2590,  0.2782,  0.2248,  0.2718,\n",
            "         0.1949,  0.1815,  0.2969,  0.3168,  0.3389,  0.2790,  0.1594,  0.2752,\n",
            "         0.2947,  0.2909,  0.1418,  0.3336,  0.1953,  0.2646,  0.0879,  0.2553,\n",
            "         0.3335,  0.1943,  0.2777,  0.2386,  0.3676,  0.3042,  0.1234,  0.2615,\n",
            "         0.2548,  0.3224,  0.3462,  0.2090,  0.2142,  0.2054,  0.2115,  0.2153,\n",
            "         0.2163,  0.2509,  0.2429,  0.3326, -0.0527,  0.2244,  0.2319,  0.2674,\n",
            "         0.1103,  0.2320,  0.2822,  0.3234,  0.2818,  0.2093,  0.2261,  0.2900,\n",
            "         0.3127,  0.3456,  0.2592,  0.1677,  0.3924,  0.2694,  0.1997,  0.2973,\n",
            "         0.3324,  0.2270,  0.0656,  0.2964,  0.1948,  0.2383,  0.3021,  0.2510,\n",
            "         0.3117,  0.3185,  0.1721,  0.1867,  0.1665,  0.2851,  0.3512, -0.0486,\n",
            "         0.1558,  0.2213,  0.3281,  0.3861,  0.2375,  0.3057,  0.1178,  0.2681,\n",
            "         0.1921,  0.2211,  0.1679,  0.2877,  0.2495,  0.2451,  0.2678,  0.2393,\n",
            "         0.0988,  0.2778,  0.2465,  0.1747,  0.1005,  0.0502,  0.2809,  0.2810,\n",
            "         0.1716,  0.2114,  0.2213,  0.2817,  0.1506,  0.0769,  0.2381,  0.2411,\n",
            "         0.2942,  0.2543,  0.2556,  0.3451,  0.2948,  0.3040,  0.3204,  0.2757,\n",
            "         0.1657,  0.2941,  0.1301,  0.1854,  0.2866,  0.3198,  0.2127,  0.3608,\n",
            "         0.3440,  0.0954,  0.2586,  0.1709,  0.2007,  0.1967,  0.1972,  0.1942,\n",
            "         0.3201,  0.3484,  0.3437,  0.3153,  0.2020,  0.3251,  0.3227,  0.3038,\n",
            "         0.2634,  0.2364,  0.2492,  0.3080,  0.2591,  0.2391,  0.2720,  0.2601,\n",
            "         0.3210,  0.1818,  0.3526,  0.3579,  0.2861,  0.2526,  0.1642,  0.2897,\n",
            "         0.3996,  0.2651,  0.2031,  0.2502,  0.3694,  0.2085,  0.2804,  0.2233,\n",
            "         0.2309,  0.1609,  0.2369,  0.2116,  0.3549,  0.1635,  0.1642,  0.3072,\n",
            "         0.3077,  0.2152,  0.2821,  0.2857,  0.1701,  0.2305,  0.2134,  0.3189,\n",
            "         0.1061,  0.2628,  0.2608,  0.1749,  0.0820,  0.1815,  0.3566,  0.1204,\n",
            "         0.3159,  0.1595,  0.3790,  0.3272,  0.2086,  0.3096,  0.2253,  0.1456,\n",
            "         0.1346,  0.2304,  0.2913,  0.2727,  0.2027,  0.2688,  0.1958,  0.2277,\n",
            "         0.3036,  0.3250,  0.3000,  0.3328,  0.2417,  0.2665,  0.2473,  0.0913,\n",
            "         0.2503,  0.2543,  0.3710,  0.3321,  0.3693,  0.1099,  0.1701,  0.1758,\n",
            "         0.3888,  0.2206,  0.2766,  0.2813,  0.1755,  0.2616,  0.1544,  0.2519,\n",
            "         0.1945,  0.2452,  0.3405,  0.2446,  0.2426,  0.1822,  0.3002,  0.3037,\n",
            "         0.3118,  0.2414,  0.2326,  0.1303,  0.3081,  0.0979,  0.2776,  0.2918,\n",
            "         0.3848,  0.1789,  0.3622,  0.3005,  0.1923,  0.2672,  0.1663,  0.2998,\n",
            "         0.2710,  0.2040,  0.2565,  0.2289,  0.2552,  0.2121,  0.3532,  0.2293,\n",
            "         0.2510,  0.3085,  0.2368,  0.3000,  0.2111,  0.3456,  0.3422,  0.1576],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1759, -0.2156, -0.2047, -0.1695, -0.1628, -0.1473, -0.2158, -0.2905,\n",
            "        -0.1112, -0.2196, -0.1020, -0.1549, -0.1989, -0.0445, -0.1508, -0.1920,\n",
            "        -0.2114, -0.1655, -0.1854, -0.1733, -0.1289, -0.2376, -0.1965, -0.1965,\n",
            "        -0.1776, -0.1774, -0.1760, -0.1546, -0.1648, -0.2599, -0.1752, -0.2498,\n",
            "        -0.1741, -0.2410, -0.2498, -0.2938, -0.1496, -0.1578, -0.1800, -0.1851,\n",
            "        -0.1516, -0.1345, -0.2746, -0.1248, -0.2246, -0.2531, -0.2398, -0.1859,\n",
            "        -0.1739, -0.2393, -0.1214, -0.1803, -0.2729, -0.2617, -0.1855, -0.2316,\n",
            "        -0.2333, -0.1860, -0.2097, -0.0692, -0.1912, -0.2078, -0.1084, -0.2810,\n",
            "        -0.1303, -0.1654, -0.2119, -0.3641, -0.2951, -0.2384, -0.1632, -0.1892,\n",
            "        -0.1792, -0.2031, -0.1770, -0.2738, -0.3324, -0.1725, -0.1793, -0.2638,\n",
            "        -0.2207, -0.1609, -0.1534, -0.1414, -0.2992, -0.1450, -0.1838, -0.1779,\n",
            "        -0.1422, -0.2198, -0.1900, -0.1580, -0.1666, -0.2490, -0.1569, -0.1718,\n",
            "        -0.1660, -0.1972, -0.2287, -0.2366, -0.2230, -0.1543, -0.2030, -0.1431,\n",
            "        -0.1363, -0.2015, -0.1804, -0.2093, -0.2964, -0.1984, -0.2683, -0.2216,\n",
            "        -0.2147, -0.3404, -0.2668, -0.1890, -0.1733, -0.2226, -0.1772, -0.1698,\n",
            "        -0.1095, -0.2180, -0.1154, -0.1654, -0.1910, -0.3535, -0.3112, -0.2161,\n",
            "        -0.1496, -0.1667, -0.2849, -0.2207, -0.1529, -0.1807, -0.2118, -0.1869,\n",
            "        -0.1376, -0.1770, -0.1861, -0.1969, -0.1741, -0.3011, -0.0787, -0.2017,\n",
            "        -0.1947, -0.2247, -0.2459, -0.1058, -0.1401, -0.1213, -0.1199, -0.1760,\n",
            "        -0.2156, -0.3307, -0.3515, -0.2366, -0.1185, -0.2155, -0.1751, -0.1892,\n",
            "        -0.3365, -0.1598, -0.2554,  0.0644, -0.2856, -0.1198, -0.1583, -0.2297,\n",
            "        -0.3352, -0.1987, -0.2686, -0.1632, -0.2461, -0.2900, -0.2428, -0.1449,\n",
            "        -0.1900, -0.2149, -0.1541, -0.2917, -0.2504, -0.2213, -0.0463, -0.1547,\n",
            "        -0.1511, -0.1527, -0.1735, -0.1931, -0.1987, -0.2239, -0.2086, -0.2688,\n",
            "        -0.1845, -0.1797, -0.1833, -0.3880, -0.1539, -0.1553, -0.1567, -0.2238,\n",
            "        -0.1511, -0.2540, -0.2849, -0.1826, -0.2687, -0.2328, -0.2108, -0.2410,\n",
            "        -0.1022, -0.1507, -0.1978, -0.1734, -0.2282, -0.0985, -0.1847, -0.1770,\n",
            "        -0.1576, -0.1937, -0.1643, -0.2822, -0.1866, -0.2754, -0.2266, -0.2169,\n",
            "        -0.1352, -0.2194, -0.1060, -0.2139, -0.1322, -0.1889, -0.2130, -0.1913,\n",
            "        -0.2364, -0.1402, -0.2228, -0.2354, -0.1632, -0.1905, -0.1428, -0.1177,\n",
            "        -0.2419, -0.2733, -0.2963, -0.1600, -0.3558, -0.3673, -0.2201, -0.1505,\n",
            "        -0.2084, -0.0870, -0.2052, -0.2070, -0.1986, -0.2299, -0.0745, -0.1765,\n",
            "        -0.1412, -0.2180, -0.1450, -0.1426, -0.1452, -0.2916, -0.0871, -0.1359,\n",
            "        -0.2003, -0.1125, -0.2588, -0.1988, -0.2028, -0.2443, -0.0864, -0.3415,\n",
            "        -0.2579, -0.2343, -0.3552, -0.1859, -0.1153, -0.1732, -0.1780, -0.1909,\n",
            "        -0.2018, -0.1886, -0.2751, -0.1501,  0.1165, -0.1891, -0.1845, -0.2037,\n",
            "        -0.0339, -0.3464, -0.1956, -0.1962, -0.1537, -0.1902, -0.1431, -0.3022,\n",
            "        -0.1780, -0.1971, -0.2118, -0.0952, -0.1711, -0.2409, -0.2184, -0.2114,\n",
            "        -0.2042, -0.0566, -0.0700, -0.2081, -0.1872, -0.2079, -0.1540, -0.2266,\n",
            "        -0.1981, -0.1679, -0.2022, -0.2010, -0.1051, -0.1705, -0.2139,  0.0396,\n",
            "        -0.1077, -0.2745, -0.2690, -0.2603, -0.2819, -0.1917, -0.1940, -0.2944,\n",
            "        -0.1822, -0.2903, -0.1064, -0.2076, -0.2648, -0.3032, -0.2878, -0.1579,\n",
            "        -0.0071, -0.2142, -0.2022, -0.1516, -0.1123,  0.0246, -0.0978, -0.1382,\n",
            "        -0.1800, -0.3214, -0.2179, -0.1369, -0.0800,  0.0117, -0.1839, -0.1926,\n",
            "        -0.1614, -0.2769, -0.1909, -0.2101, -0.2305, -0.2055, -0.2017, -0.2741,\n",
            "        -0.1005, -0.3152, -0.1121, -0.1700, -0.1364, -0.2157, -0.2673, -0.1584,\n",
            "        -0.1997, -0.1745, -0.1886, -0.2307, -0.2024, -0.3376, -0.2266, -0.2355,\n",
            "        -0.2133, -0.2346, -0.2412, -0.2358, -0.1265, -0.2341, -0.1887, -0.1646,\n",
            "        -0.1417, -0.1882, -0.1076, -0.3048, -0.1162, -0.1651, -0.2046, -0.1833,\n",
            "        -0.3102, -0.1778, -0.1575, -0.2676, -0.1777, -0.1569, -0.1741, -0.1892,\n",
            "        -0.3028, -0.1457, -0.2179, -0.2226, -0.1609, -0.1423, -0.2683, -0.2920,\n",
            "        -0.1740, -0.2079, -0.1940, -0.2679, -0.1973, -0.1951, -0.1665, -0.2286,\n",
            "        -0.1903, -0.2667, -0.4010, -0.2550, -0.1817, -0.2025, -0.1589, -0.2476,\n",
            "        -0.0573, -0.2203, -0.2084, -0.1587, -0.1212, -0.1795, -0.3449, -0.1662,\n",
            "        -0.2523, -0.2435, -0.2878, -0.2797, -0.1897, -0.2113, -0.1943, -0.2050,\n",
            "        -0.1694, -0.2243, -0.2987, -0.1328, -0.1428, -0.2399, -0.1593, -0.1999,\n",
            "        -0.3225, -0.1860, -0.1763, -0.2691, -0.2097, -0.2396, -0.1140, -0.1897,\n",
            "        -0.1870, -0.1829, -0.2615, -0.2073, -0.1858, -0.0598, -0.1915, -0.2183,\n",
            "        -0.2088, -0.1742, -0.2715, -0.1999, -0.2117, -0.2492, -0.1717, -0.1566,\n",
            "        -0.1669, -0.3015, -0.1685, -0.2434, -0.2297, -0.1947, -0.2860, -0.3288,\n",
            "        -0.2197, -0.1862, -0.1755, -0.0987, -0.1756, -0.1304, -0.1555, -0.1679,\n",
            "        -0.2222, -0.2819, -0.2652, -0.0947, -0.2412, -0.2731, -0.2572, -0.2604,\n",
            "        -0.2934, -0.2470, -0.1820, -0.2740, -0.1336, -0.1698, -0.1919, -0.1796,\n",
            "        -0.2325, -0.1352, -0.1077, -0.2184, -0.1539, -0.2015, -0.3243, -0.1713],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-8.0284e-03, -5.7776e-03,  6.4154e-03],\n",
            "          [ 5.0498e-03, -6.7796e-03,  1.2691e-02],\n",
            "          [ 1.3331e-02,  1.4523e-02,  2.4522e-02]],\n",
            "\n",
            "         [[-1.9876e-03,  1.2466e-02,  1.0494e-02],\n",
            "          [-1.9364e-02, -1.6696e-02, -1.1857e-02],\n",
            "          [-1.1569e-02, -3.7674e-03, -3.4679e-03]],\n",
            "\n",
            "         [[-1.1440e-02, -1.3884e-02,  1.1559e-03],\n",
            "          [-1.7906e-02, -2.9349e-02, -1.3876e-02],\n",
            "          [-1.4057e-02, -2.6989e-02, -2.3963e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-6.3040e-03, -3.1167e-03, -1.3304e-02],\n",
            "          [ 7.1623e-03,  6.4669e-03,  1.6063e-02],\n",
            "          [-1.0750e-02, -1.0480e-02, -6.1070e-03]],\n",
            "\n",
            "         [[ 7.4484e-03,  6.3878e-03, -1.2579e-02],\n",
            "          [-7.7356e-03,  1.8112e-03, -1.7890e-02],\n",
            "          [-2.9142e-03,  7.7705e-03, -9.7314e-03]],\n",
            "\n",
            "         [[ 2.1760e-02,  2.2364e-02,  2.2731e-02],\n",
            "          [ 2.6681e-02,  2.9127e-02,  3.3356e-02],\n",
            "          [ 1.2892e-02, -3.5818e-03,  5.3022e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.0597e-02, -9.1551e-03, -2.3418e-02],\n",
            "          [-1.0768e-02, -3.3171e-03, -1.8559e-02],\n",
            "          [-1.8607e-02, -4.2634e-03, -1.5591e-02]],\n",
            "\n",
            "         [[-2.6090e-02, -2.2517e-02, -3.0593e-02],\n",
            "          [-3.9406e-02, -2.6639e-02, -2.8202e-02],\n",
            "          [-2.6143e-02, -1.9647e-02, -2.1466e-02]],\n",
            "\n",
            "         [[-3.5259e-03,  1.6623e-03, -6.5624e-03],\n",
            "          [-5.0597e-03, -8.7162e-04, -5.3742e-03],\n",
            "          [-7.9651e-03, -9.7778e-03, -1.0736e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.8492e-02, -3.6799e-03,  1.0043e-02],\n",
            "          [-5.2974e-03, -2.0757e-02, -1.5120e-02],\n",
            "          [ 2.1435e-02,  6.4916e-03,  4.7660e-03]],\n",
            "\n",
            "         [[-1.8810e-02, -6.0469e-04, -7.6999e-03],\n",
            "          [-1.7697e-02, -7.8692e-03, -1.6543e-02],\n",
            "          [-1.7206e-02, -2.4746e-02, -3.0270e-02]],\n",
            "\n",
            "         [[-3.1191e-02, -1.4363e-02,  2.2032e-03],\n",
            "          [-1.2033e-02, -2.3699e-03, -1.6630e-02],\n",
            "          [-1.2905e-02, -1.5363e-02, -3.6297e-03]]],\n",
            "\n",
            "\n",
            "        [[[-3.2648e-02, -4.8158e-03, -2.0476e-02],\n",
            "          [-2.5846e-02, -1.4660e-03, -2.8170e-02],\n",
            "          [-2.6640e-02,  4.3022e-03, -2.7636e-02]],\n",
            "\n",
            "         [[-6.3289e-03, -1.5401e-02, -1.3096e-03],\n",
            "          [-1.7499e-02, -2.6212e-02, -2.3646e-02],\n",
            "          [-7.3207e-03, -1.5592e-02, -8.9578e-03]],\n",
            "\n",
            "         [[ 8.9701e-04, -6.6914e-03, -5.3129e-03],\n",
            "          [-1.1727e-03, -1.0726e-02, -9.0103e-03],\n",
            "          [ 3.2311e-03, -4.5854e-03,  4.3512e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.1822e-02, -3.6889e-02, -2.2588e-02],\n",
            "          [-1.3054e-02, -3.4191e-02, -2.7238e-02],\n",
            "          [-1.2383e-02, -2.3452e-02, -2.2486e-02]],\n",
            "\n",
            "         [[ 6.8177e-03,  2.1561e-02,  1.3674e-02],\n",
            "          [ 3.1192e-03,  1.0660e-02,  1.0409e-02],\n",
            "          [ 8.0477e-03, -4.6817e-03, -4.3912e-03]],\n",
            "\n",
            "         [[-1.1983e-02, -1.6201e-02, -2.2626e-02],\n",
            "          [-1.3461e-02, -7.0928e-03, -1.4384e-02],\n",
            "          [-2.4456e-02,  1.4885e-02,  1.2247e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-2.6347e-02, -2.9923e-02, -3.7810e-02],\n",
            "          [-1.5663e-02, -4.1126e-03, -1.1482e-02],\n",
            "          [-1.3415e-02, -1.5432e-02, -1.8204e-02]],\n",
            "\n",
            "         [[-3.8392e-03, -1.1093e-02, -8.0841e-04],\n",
            "          [-5.9634e-03, -5.9165e-03, -9.3332e-03],\n",
            "          [-2.2761e-03,  5.4781e-03, -5.6050e-03]],\n",
            "\n",
            "         [[-1.8406e-03, -2.8134e-03,  8.3246e-03],\n",
            "          [-1.2453e-03,  2.1453e-04,  7.4868e-03],\n",
            "          [ 1.3450e-02,  3.0599e-02,  2.6405e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.5268e-04,  2.3897e-03,  6.2558e-03],\n",
            "          [-1.4338e-02, -2.3146e-02, -1.9024e-02],\n",
            "          [-2.7306e-02, -3.0079e-02, -3.1762e-02]],\n",
            "\n",
            "         [[ 1.4584e-02,  4.3430e-03,  1.2053e-02],\n",
            "          [-6.1130e-03, -2.8539e-02, -1.8268e-02],\n",
            "          [-1.6844e-02, -4.7816e-02, -2.6274e-02]],\n",
            "\n",
            "         [[-1.8850e-02, -9.3396e-03,  7.8905e-03],\n",
            "          [-1.5322e-03,  8.3153e-03,  1.7783e-02],\n",
            "          [-8.3318e-03, -1.5759e-02, -1.2061e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 9.9578e-03,  7.4573e-03, -1.8738e-03],\n",
            "          [-1.7752e-03, -6.8015e-04, -7.4443e-03],\n",
            "          [-1.8319e-02, -1.4264e-02, -7.1446e-03]],\n",
            "\n",
            "         [[ 7.8524e-03, -2.6520e-03, -1.7556e-02],\n",
            "          [ 4.5240e-03, -4.8661e-03, -1.5215e-02],\n",
            "          [-5.0211e-03, -1.1864e-02, -1.4846e-02]],\n",
            "\n",
            "         [[ 2.9163e-02,  1.0344e-02,  2.4736e-02],\n",
            "          [ 1.2012e-02, -1.0346e-02,  3.5472e-03],\n",
            "          [ 8.2238e-03, -1.8237e-02, -5.4892e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-8.8434e-03, -4.3184e-03, -5.7536e-03],\n",
            "          [ 7.7230e-03, -4.1936e-04,  7.7260e-03],\n",
            "          [ 1.3536e-02,  1.5705e-02,  2.0893e-02]],\n",
            "\n",
            "         [[ 1.6743e-03,  1.9720e-03,  2.1567e-02],\n",
            "          [-8.0074e-03, -4.6606e-03,  4.0560e-03],\n",
            "          [-1.6688e-02, -1.3754e-02, -1.1708e-02]],\n",
            "\n",
            "         [[-9.7959e-03, -9.4502e-03, -9.3443e-03],\n",
            "          [ 6.9547e-03, -3.9134e-05,  6.2691e-03],\n",
            "          [-1.3193e-02,  9.3272e-04,  1.4579e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.4963e-03,  5.5133e-04,  1.1571e-02],\n",
            "          [ 1.0174e-02,  1.7889e-03,  1.1035e-02],\n",
            "          [ 7.0212e-03,  1.4651e-03,  1.2769e-03]],\n",
            "\n",
            "         [[-1.3021e-02,  6.4109e-03, -1.5199e-02],\n",
            "          [ 2.4775e-02,  2.1926e-02,  3.3679e-02],\n",
            "          [ 2.6471e-04, -3.0235e-03,  1.1690e-02]],\n",
            "\n",
            "         [[-2.9665e-02, -1.5314e-02, -1.7500e-02],\n",
            "          [-1.8339e-02, -2.0845e-02, -1.5494e-02],\n",
            "          [-1.6086e-03,  1.0831e-02, -1.4309e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-7.7044e-03, -2.1100e-02, -2.2816e-02],\n",
            "          [ 5.7688e-03,  1.9362e-04,  7.7105e-04],\n",
            "          [-6.1357e-03,  9.7275e-03, -2.5464e-03]],\n",
            "\n",
            "         [[ 1.1043e-02,  2.4205e-02,  3.4213e-02],\n",
            "          [ 2.9181e-02,  2.6904e-02,  4.5372e-02],\n",
            "          [-2.1594e-02, -1.1072e-03, -7.8312e-03]],\n",
            "\n",
            "         [[-8.3287e-03, -7.9521e-03, -5.3358e-03],\n",
            "          [-6.2527e-04, -5.3243e-03, -8.6296e-03],\n",
            "          [ 3.6094e-03, -1.2544e-03, -4.3801e-03]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.2587, 0.3073, 0.2595, 0.3223, 0.2662, 0.2652, 0.2575, 0.2660, 0.2766,\n",
            "        0.2414, 0.3045, 0.2853, 0.2821, 0.2880, 0.3094, 0.3444, 0.3155, 0.4129,\n",
            "        0.2110, 0.2903, 0.2496, 0.2601, 0.2967, 0.3033, 0.4152, 0.2719, 0.3661,\n",
            "        0.3251, 0.3898, 0.3346, 0.2753, 0.2712, 0.2414, 0.3351, 0.3394, 0.3167,\n",
            "        0.3360, 0.2666, 0.2109, 0.2705, 0.2587, 0.3070, 0.2720, 0.2316, 0.2885,\n",
            "        0.2884, 0.2955, 0.3057, 0.3043, 0.2596, 0.2673, 0.1929, 0.3136, 0.3593,\n",
            "        0.2622, 0.2931, 0.3295, 0.2514, 0.3208, 0.2798, 0.3259, 0.2939, 0.2390,\n",
            "        0.3105, 0.3471, 0.2812, 0.2148, 0.2997, 0.3061, 0.2740, 0.2791, 0.3790,\n",
            "        0.3592, 0.3247, 0.2995, 0.2735, 0.3356, 0.2703, 0.3255, 0.3127, 0.2783,\n",
            "        0.2702, 0.3900, 0.2942, 0.2899, 0.3461, 0.3432, 0.4685, 0.2634, 0.2553,\n",
            "        0.3019, 0.3961, 0.2742, 0.2995, 0.3858, 0.2785, 0.3212, 0.3109, 0.3642,\n",
            "        0.2193, 0.2643, 0.2333, 0.3151, 0.3102, 0.2936, 0.2374, 0.2419, 0.2976,\n",
            "        0.3335, 0.2619, 0.3984, 0.2721, 0.2718, 0.2678, 0.2757, 0.2445, 0.3508,\n",
            "        0.2174, 0.3309, 0.2653, 0.2564, 0.1748, 0.3177, 0.2751, 0.2067, 0.2905,\n",
            "        0.2762, 0.3329, 0.2738, 0.3224, 0.2199, 0.2997, 0.2206, 0.3213, 0.2760,\n",
            "        0.3927, 0.3174, 0.2698, 0.2988, 0.2610, 0.2550, 0.2788, 0.4445, 0.2862,\n",
            "        0.3606, 0.3279, 0.2869, 0.3294, 0.2244, 0.2338, 0.1754, 0.2318, 0.3186,\n",
            "        0.3322, 0.2255, 0.3041, 0.2837, 0.3276, 0.2392, 0.3668, 0.1971, 0.2946,\n",
            "        0.3613, 0.2736, 0.2554, 0.2860, 0.2511, 0.3490, 0.3253, 0.2934, 0.2027,\n",
            "        0.2580, 0.2200, 0.3089, 0.3074, 0.3332, 0.2943, 0.3375, 0.2330, 0.2611,\n",
            "        0.3383, 0.2837, 0.3546, 0.3093, 0.3791, 0.2197, 0.2648, 0.2830, 0.2587,\n",
            "        0.3588, 0.2830, 0.3971, 0.3194, 0.3066, 0.2754, 0.2647, 0.0970, 0.2182,\n",
            "        0.2334, 0.2624, 0.1829, 0.2933, 0.2747, 0.3001, 0.2996, 0.3107, 0.3256,\n",
            "        0.2940, 0.3901, 0.2790, 0.3030, 0.2838, 0.3010, 0.3044, 0.3479, 0.3087,\n",
            "        0.2611, 0.1958, 0.2941, 0.2558, 0.2889, 0.3148, 0.2516, 0.2664, 0.2862,\n",
            "        0.3940, 0.2933, 0.2781, 0.3796, 0.3022, 0.2583, 0.3021, 0.2784, 0.2967,\n",
            "        0.2994, 0.3856, 0.3277, 0.2587, 0.2539, 0.2824, 0.2634, 0.1489, 0.2205,\n",
            "        0.3929, 0.3401, 0.2717, 0.2789, 0.2917, 0.3177, 0.1992, 0.3684, 0.3120,\n",
            "        0.3201, 0.2810, 0.2302, 0.2779, 0.2865, 0.2858, 0.2713, 0.1601, 0.2496,\n",
            "        0.2895, 0.3154, 0.3443, 0.3285, 0.3444, 0.3251, 0.3235, 0.3375, 0.2282,\n",
            "        0.2128, 0.1795, 0.3077, 0.3005, 0.2775, 0.3054, 0.2914, 0.3535, 0.2871,\n",
            "        0.2669, 0.3961, 0.2674, 0.3898, 0.3183, 0.3242, 0.2789, 0.1911, 0.2569,\n",
            "        0.3427, 0.2464, 0.2778, 0.2098, 0.3019, 0.3145, 0.3271, 0.2914, 0.2619,\n",
            "        0.2643, 0.3039, 0.2520, 0.2099, 0.3643, 0.2915, 0.1957, 0.3286, 0.2355,\n",
            "        0.3210, 0.2982, 0.3388, 0.3450, 0.3716, 0.2898, 0.2846, 0.2805, 0.2219,\n",
            "        0.2910, 0.2681, 0.3163, 0.1964, 0.3176, 0.3092, 0.2706, 0.2505, 0.2508,\n",
            "        0.3166, 0.3583, 0.1563, 0.2608, 0.2892, 0.3401, 0.2891, 0.3126, 0.2172,\n",
            "        0.2459, 0.2651, 0.4052, 0.2986, 0.3026, 0.3773, 0.2262, 0.2675, 0.2900,\n",
            "        0.3759, 0.3201, 0.2567, 0.3443, 0.2348, 0.3057, 0.2347, 0.3277, 0.2938,\n",
            "        0.2746, 0.2805, 0.2421, 0.3590, 0.2622, 0.2773, 0.2396, 0.2134, 0.2727,\n",
            "        0.2984, 0.2744, 0.2591, 0.2628, 0.3568, 0.2009, 0.3220, 0.2868, 0.2561,\n",
            "        0.3113, 0.2138, 0.3136, 0.2745, 0.3046, 0.3042, 0.1972, 0.2815, 0.2542,\n",
            "        0.2983, 0.2613, 0.2668, 0.3142, 0.2930, 0.3800, 0.1966, 0.2948, 0.3363,\n",
            "        0.2713, 0.3625, 0.2909, 0.2695, 0.3111, 0.3242, 0.3009, 0.3231, 0.3051,\n",
            "        0.2012, 0.2716, 0.3692, 0.2694, 0.1481, 0.2858, 0.2819, 0.2391, 0.2867,\n",
            "        0.3466, 0.3431, 0.2365, 0.3357, 0.1685, 0.2925, 0.3092, 0.3127, 0.1883,\n",
            "        0.2561, 0.3086, 0.1732, 0.2989, 0.3235, 0.2693, 0.2630, 0.2913, 0.2786,\n",
            "        0.3124, 0.3098, 0.2695, 0.2403, 0.2906, 0.2784, 0.2654, 0.3485, 0.3939,\n",
            "        0.3033, 0.3145, 0.2622, 0.1540, 0.2790, 0.2967, 0.1954, 0.2632, 0.2957,\n",
            "        0.2581, 0.3231, 0.2795, 0.2859, 0.3139, 0.2488, 0.2404, 0.3714, 0.2649,\n",
            "        0.2267, 0.2878, 0.3462, 0.3063, 0.3180, 0.1726, 0.3153, 0.2625, 0.3020,\n",
            "        0.2996, 0.3632, 0.1541, 0.3192, 0.2200, 0.2894, 0.2622, 0.2534, 0.2935,\n",
            "        0.3208, 0.2231, 0.2743, 0.3023, 0.2829, 0.2394, 0.2506, 0.3512, 0.3366,\n",
            "        0.2666, 0.2930, 0.3049, 0.2321, 0.3397, 0.2727, 0.2900, 0.3146, 0.2682,\n",
            "        0.3094, 0.3718, 0.3387, 0.3202, 0.2423, 0.2745, 0.2966, 0.2500, 0.2329,\n",
            "        0.3419, 0.2928, 0.3536, 0.3739, 0.1935, 0.2670, 0.2846, 0.2583, 0.3783,\n",
            "        0.2826, 0.2929, 0.2728, 0.3645, 0.2770, 0.2756, 0.2523, 0.2500],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1668, -0.3019, -0.2187, -0.2917, -0.1971, -0.2325, -0.1869, -0.1857,\n",
            "        -0.2474, -0.1629, -0.2448, -0.2508, -0.1895, -0.2651, -0.3250, -0.3811,\n",
            "        -0.2953, -0.4963, -0.0294, -0.2724, -0.2007, -0.2220, -0.2945, -0.2579,\n",
            "        -0.5152, -0.1994, -0.5016, -0.2736, -0.4528, -0.3968, -0.2281, -0.1772,\n",
            "        -0.1293, -0.2655, -0.3252, -0.3232, -0.3337, -0.1901, -0.0692, -0.2196,\n",
            "        -0.2132, -0.2565, -0.1646, -0.1567, -0.2087, -0.2178, -0.2480, -0.2767,\n",
            "        -0.3071, -0.1988, -0.1985, -0.0235, -0.2458, -0.4156, -0.1660, -0.1923,\n",
            "        -0.3328, -0.1481, -0.3047, -0.2277, -0.3182, -0.2744, -0.1643, -0.3365,\n",
            "        -0.4050, -0.2082, -0.0621, -0.2671, -0.2809, -0.2185, -0.2148, -0.4465,\n",
            "        -0.3376, -0.3213, -0.2921, -0.1998, -0.3369, -0.2092, -0.2831, -0.2893,\n",
            "        -0.1719, -0.2189, -0.4016, -0.2484, -0.2070, -0.3849, -0.3753, -0.5874,\n",
            "        -0.1637, -0.1748, -0.2217, -0.5067, -0.2496, -0.2117, -0.4291, -0.1944,\n",
            "        -0.3089, -0.2621, -0.4096, -0.0602, -0.2009, -0.1316, -0.3336, -0.2627,\n",
            "        -0.2320, -0.0910, -0.1560, -0.2889, -0.3286, -0.1628, -0.5128, -0.2036,\n",
            "        -0.1726, -0.1844, -0.2285, -0.1925, -0.3432, -0.0929, -0.3138, -0.1912,\n",
            "        -0.1926, -0.0342, -0.3268, -0.1699, -0.0828, -0.2417, -0.2069, -0.3870,\n",
            "        -0.2210, -0.2867, -0.0526, -0.3092, -0.0655, -0.2594, -0.2160, -0.5062,\n",
            "        -0.2905, -0.2125, -0.3124, -0.2128, -0.1946, -0.2520, -0.5475, -0.2321,\n",
            "        -0.3350, -0.3473, -0.2158, -0.3603, -0.0759, -0.1472, -0.0327, -0.1404,\n",
            "        -0.3128, -0.3063, -0.1120, -0.2664, -0.2700, -0.3112, -0.1519, -0.3843,\n",
            "        -0.0645, -0.2373, -0.4227, -0.2546, -0.1611, -0.2350, -0.1524, -0.3494,\n",
            "        -0.3453, -0.2081, -0.0918, -0.2025, -0.1246, -0.2533, -0.2768, -0.3156,\n",
            "        -0.2530, -0.3957, -0.0981, -0.1257, -0.3697, -0.2333, -0.3664, -0.2829,\n",
            "        -0.4320, -0.0836, -0.1583, -0.2395, -0.1818, -0.4408, -0.2376, -0.4450,\n",
            "        -0.3232, -0.2787, -0.1858, -0.2137,  0.0481, -0.1058, -0.1093, -0.2035,\n",
            "        -0.0496, -0.2117, -0.1598, -0.2389, -0.2830, -0.2878, -0.3406, -0.2560,\n",
            "        -0.4468, -0.2444, -0.2492, -0.2222, -0.2792, -0.3005, -0.4180, -0.2568,\n",
            "        -0.1872, -0.0270, -0.2645, -0.1873, -0.3022, -0.3400, -0.1803, -0.1810,\n",
            "        -0.2079, -0.4775, -0.2047, -0.1878, -0.4504, -0.2516, -0.1657, -0.2765,\n",
            "        -0.2329, -0.2446, -0.2956, -0.4163, -0.2816, -0.1571, -0.2199, -0.2125,\n",
            "        -0.1684,  0.0356, -0.0914, -0.4484, -0.3535, -0.2212, -0.2550, -0.2509,\n",
            "        -0.2702, -0.0599, -0.3505, -0.2924, -0.2360, -0.2339, -0.1259, -0.2597,\n",
            "        -0.2267, -0.1978, -0.1371, -0.0129, -0.1175, -0.2527, -0.3099, -0.3231,\n",
            "        -0.3468, -0.3553, -0.3537, -0.3315, -0.3713, -0.1091, -0.0959, -0.0258,\n",
            "        -0.2756, -0.2808, -0.2012, -0.2812, -0.1991, -0.3948, -0.2257, -0.2469,\n",
            "        -0.4211, -0.2110, -0.4670, -0.3069, -0.3549, -0.2337, -0.0612, -0.1321,\n",
            "        -0.2968, -0.1870, -0.2316, -0.0686, -0.3113, -0.2895, -0.3149, -0.2686,\n",
            "        -0.2081, -0.2096, -0.3011, -0.1810, -0.0227, -0.3873, -0.2665, -0.0225,\n",
            "        -0.2973, -0.0973, -0.2980, -0.3219, -0.2926, -0.3196, -0.4332, -0.1980,\n",
            "        -0.2117, -0.2302, -0.0980, -0.2344, -0.2154, -0.2921, -0.0350, -0.3361,\n",
            "        -0.2620, -0.2188, -0.1566, -0.1795, -0.2726, -0.4103,  0.0413, -0.1507,\n",
            "        -0.2552, -0.3137, -0.2466, -0.2961, -0.0938, -0.1481, -0.2129, -0.5480,\n",
            "        -0.2915, -0.2802, -0.5077, -0.1306, -0.1862, -0.2400, -0.4362, -0.3017,\n",
            "        -0.1633, -0.3447, -0.1047, -0.2846, -0.1244, -0.3036, -0.2404, -0.2333,\n",
            "        -0.2494, -0.1866, -0.3294, -0.1677, -0.2540, -0.1295, -0.0512, -0.1966,\n",
            "        -0.2801, -0.1702, -0.1879, -0.1850, -0.3274, -0.0369, -0.2979, -0.2612,\n",
            "        -0.1889, -0.3270, -0.1377, -0.2787, -0.2201, -0.2417, -0.2834, -0.0555,\n",
            "        -0.2538, -0.1040, -0.2660, -0.1644, -0.1723, -0.2672, -0.2797, -0.4214,\n",
            "        -0.0378, -0.2386, -0.3498, -0.2435, -0.4348, -0.2554, -0.1719, -0.2836,\n",
            "        -0.3316, -0.2787, -0.2879, -0.2640, -0.0560, -0.1789, -0.4195, -0.2152,\n",
            "         0.0567, -0.2359, -0.2249, -0.0911, -0.2644, -0.3875, -0.3317, -0.1415,\n",
            "        -0.3425, -0.0020, -0.1941, -0.2821, -0.2809, -0.0965, -0.1841, -0.2971,\n",
            "        -0.0173, -0.3043, -0.3013, -0.1729, -0.1872, -0.2683, -0.2033, -0.3059,\n",
            "        -0.2939, -0.2163, -0.1889, -0.2581, -0.2296, -0.2066, -0.3462, -0.4298,\n",
            "        -0.2600, -0.3095, -0.1800, -0.0116, -0.2124, -0.2552, -0.0523, -0.2216,\n",
            "        -0.2605, -0.2134, -0.2867, -0.2556, -0.2275, -0.3437, -0.1698, -0.1560,\n",
            "        -0.4120, -0.2067, -0.1159, -0.2408, -0.3093, -0.2621, -0.2593, -0.0135,\n",
            "        -0.3099, -0.2179, -0.2766, -0.2400, -0.3934,  0.0072, -0.2982, -0.0930,\n",
            "        -0.2166, -0.1635, -0.1827, -0.2308, -0.2525, -0.0991, -0.2325, -0.2938,\n",
            "        -0.2480, -0.0934, -0.1911, -0.3772, -0.3369, -0.1606, -0.2752, -0.3005,\n",
            "        -0.1372, -0.2990, -0.2156, -0.2622, -0.3160, -0.1342, -0.2903, -0.3865,\n",
            "        -0.2916, -0.3243, -0.2051, -0.2656, -0.2359, -0.1508, -0.1063, -0.3595,\n",
            "        -0.2312, -0.3046, -0.4178, -0.0276, -0.2204, -0.2426, -0.1616, -0.4789,\n",
            "        -0.1713, -0.2802, -0.2305, -0.4327, -0.2413, -0.1862, -0.1486, -0.1507],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 2.8729e-04,  4.2632e-03, -2.0266e-03],\n",
            "          [ 1.9513e-04,  2.4381e-03, -5.8632e-03],\n",
            "          [ 4.4803e-03,  8.6577e-03,  8.5538e-04]],\n",
            "\n",
            "         [[-1.1335e-02, -1.3195e-02, -1.0305e-02],\n",
            "          [-4.9507e-03, -4.5898e-03, -3.1041e-03],\n",
            "          [-7.5883e-03, -8.3795e-03, -8.9239e-03]],\n",
            "\n",
            "         [[-1.1914e-02, -1.2104e-02, -1.0167e-02],\n",
            "          [-1.2093e-02, -1.1557e-02, -8.9600e-03],\n",
            "          [-1.2515e-02, -9.3296e-03, -6.4079e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-9.3573e-03, -1.0662e-02, -1.2672e-02],\n",
            "          [-8.0600e-03, -8.5423e-03, -1.2121e-02],\n",
            "          [-8.1498e-03, -8.8037e-03, -1.0611e-02]],\n",
            "\n",
            "         [[ 4.2632e-03,  5.6461e-03,  2.8460e-03],\n",
            "          [ 4.7070e-03,  6.2550e-03,  7.5862e-03],\n",
            "          [ 1.1504e-02,  1.1518e-02,  1.0728e-02]],\n",
            "\n",
            "         [[-6.2455e-03, -9.1693e-03, -9.6664e-03],\n",
            "          [-4.2935e-03, -6.5311e-03, -5.0513e-03],\n",
            "          [-3.1141e-03, -5.0124e-03, -5.8122e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 2.7483e-03,  3.7146e-04,  3.3262e-05],\n",
            "          [-4.5675e-03, -6.6689e-03, -6.4447e-03],\n",
            "          [-6.7610e-03, -7.3204e-03, -9.5855e-03]],\n",
            "\n",
            "         [[-1.4630e-02, -1.2320e-02, -1.4457e-02],\n",
            "          [-8.6197e-03, -5.8059e-03, -1.1075e-02],\n",
            "          [-6.2154e-03, -6.8218e-03, -9.3805e-03]],\n",
            "\n",
            "         [[ 1.0879e-03,  4.3850e-04, -1.9456e-03],\n",
            "          [-1.2517e-03,  3.2917e-04, -2.1435e-03],\n",
            "          [ 4.8136e-03,  2.5333e-03,  5.1504e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.4644e-02,  1.7434e-02,  2.0734e-02],\n",
            "          [ 2.3101e-02,  1.3487e-02,  2.0728e-02],\n",
            "          [ 1.9381e-02,  1.5243e-02,  1.7340e-02]],\n",
            "\n",
            "         [[ 1.2212e-02,  1.2448e-02,  1.5048e-02],\n",
            "          [ 5.2993e-03,  4.0090e-03,  9.3927e-03],\n",
            "          [ 6.6766e-03,  2.4941e-03,  8.3288e-03]],\n",
            "\n",
            "         [[ 3.1040e-02,  2.8243e-02,  3.2319e-02],\n",
            "          [ 3.8608e-02,  3.3099e-02,  3.8652e-02],\n",
            "          [ 2.5839e-02,  2.6524e-02,  2.4995e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.1761e-03,  4.5553e-03,  2.0612e-03],\n",
            "          [ 4.9747e-03,  1.1420e-02,  8.5734e-03],\n",
            "          [ 4.8583e-03,  1.1469e-02,  1.0039e-02]],\n",
            "\n",
            "         [[-6.2547e-05,  6.5336e-04,  9.4747e-04],\n",
            "          [ 5.0603e-03,  7.7136e-03,  6.5484e-03],\n",
            "          [-4.8432e-04,  2.3057e-03,  2.9219e-03]],\n",
            "\n",
            "         [[-3.2788e-02, -2.7615e-02, -3.2608e-02],\n",
            "          [-3.6296e-02, -2.8170e-02, -3.0277e-02],\n",
            "          [-3.6814e-02, -3.1547e-02, -3.0231e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.2998e-03, -2.8590e-04, -4.9266e-03],\n",
            "          [-7.0530e-03, -2.3684e-04, -1.5838e-03],\n",
            "          [-6.9291e-03,  4.8084e-04, -3.1548e-03]],\n",
            "\n",
            "         [[ 1.1854e-02,  8.4836e-03,  1.3839e-02],\n",
            "          [ 2.8741e-03, -9.7358e-05,  4.4888e-03],\n",
            "          [-2.5515e-03, -2.7788e-03, -3.2464e-03]],\n",
            "\n",
            "         [[-1.2408e-02, -1.5001e-02, -1.3377e-02],\n",
            "          [-1.4540e-02, -1.8537e-02, -1.7392e-02],\n",
            "          [-6.7315e-03, -9.5205e-03, -9.0692e-03]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 3.0369e-03,  1.9542e-03,  1.7140e-03],\n",
            "          [-7.6240e-03, -2.8765e-03, -5.1760e-03],\n",
            "          [-9.3019e-03, -4.8800e-03, -4.2932e-03]],\n",
            "\n",
            "         [[ 4.4836e-03,  2.4909e-03,  1.5746e-03],\n",
            "          [ 1.2065e-02,  1.2936e-02,  1.0344e-02],\n",
            "          [ 1.9010e-02,  1.7459e-02,  1.5988e-02]],\n",
            "\n",
            "         [[-1.4914e-03, -8.1727e-03, -8.0671e-03],\n",
            "          [-6.6247e-03, -6.2421e-03, -9.2717e-03],\n",
            "          [-8.7991e-03, -7.7528e-03, -8.6336e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.8040e-02, -1.5366e-02, -1.5334e-02],\n",
            "          [-1.3148e-02, -1.2180e-02, -1.0915e-02],\n",
            "          [-1.4545e-02, -1.4756e-02, -1.1787e-02]],\n",
            "\n",
            "         [[ 3.5762e-03,  6.6073e-03, -1.4055e-03],\n",
            "          [ 4.3975e-03,  7.8375e-03,  8.8085e-05],\n",
            "          [-5.0697e-03, -5.6633e-04, -5.9284e-03]],\n",
            "\n",
            "         [[-1.9234e-03, -8.8012e-03, -5.8821e-03],\n",
            "          [ 3.6685e-03, -1.3784e-03, -3.2117e-03],\n",
            "          [-4.7037e-04,  1.5340e-04, -3.4046e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.8305e-02, -1.7735e-02, -2.1683e-02],\n",
            "          [-1.6598e-02, -1.2508e-02, -2.0530e-02],\n",
            "          [-1.0800e-02, -9.8670e-03, -1.7195e-02]],\n",
            "\n",
            "         [[ 2.0721e-02,  2.2466e-02,  2.5049e-02],\n",
            "          [ 1.8682e-02,  1.3160e-02,  2.3696e-02],\n",
            "          [ 2.2104e-02,  1.7261e-02,  2.4877e-02]],\n",
            "\n",
            "         [[-5.7091e-03, -2.6876e-03, -9.2260e-04],\n",
            "          [-9.4530e-03, -7.0543e-03, -6.2770e-03],\n",
            "          [-4.5806e-03, -2.7182e-03, -2.5823e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.4150e-02,  1.4002e-02,  1.6559e-02],\n",
            "          [ 2.1363e-02,  1.4359e-02,  1.5854e-02],\n",
            "          [ 2.5786e-02,  2.7233e-02,  2.5104e-02]],\n",
            "\n",
            "         [[-4.6450e-03,  1.2419e-03, -1.8768e-03],\n",
            "          [ 1.3005e-03,  4.0888e-03, -6.5483e-04],\n",
            "          [-7.9783e-03, -6.6539e-03, -8.9957e-03]],\n",
            "\n",
            "         [[ 1.1494e-02,  2.6621e-02,  1.5649e-02],\n",
            "          [ 6.5960e-03,  1.7290e-02,  7.5466e-03],\n",
            "          [-8.0256e-03,  4.6246e-03, -5.7808e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 1.4232e-02,  1.1769e-02,  9.4342e-03],\n",
            "          [ 6.2592e-03,  5.1087e-03,  2.3311e-03],\n",
            "          [-1.9694e-03,  2.7110e-03, -2.8945e-03]],\n",
            "\n",
            "         [[-7.0772e-03,  1.0365e-03, -5.8451e-03],\n",
            "          [-9.1879e-03, -3.1388e-03, -8.1517e-03],\n",
            "          [-8.0300e-03, -5.1313e-03, -9.5734e-03]],\n",
            "\n",
            "         [[ 2.4314e-02,  1.8942e-02,  2.4256e-02],\n",
            "          [ 2.0090e-02,  1.1472e-02,  1.5993e-02],\n",
            "          [ 2.2910e-02,  2.0622e-02,  2.3820e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.6375e-02, -1.6928e-02, -1.9019e-02],\n",
            "          [-9.7367e-03, -1.1274e-02, -1.0261e-02],\n",
            "          [-1.2310e-02, -1.5931e-02, -1.4151e-02]],\n",
            "\n",
            "         [[ 4.7098e-03, -4.5205e-04,  2.8042e-03],\n",
            "          [ 2.1428e-03, -4.6175e-03, -1.6818e-03],\n",
            "          [-1.3336e-03, -5.5009e-03, -2.6237e-03]],\n",
            "\n",
            "         [[-1.4367e-02, -1.3520e-02, -1.1387e-02],\n",
            "          [-4.7420e-03, -1.7309e-03, -2.6426e-03],\n",
            "          [ 5.1448e-03,  7.0428e-03,  5.0202e-03]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([1.8419, 1.8307, 1.7650, 1.8288, 1.9505, 1.8026, 1.9536, 2.2790, 1.7662,\n",
            "        1.8902, 1.7768, 1.7749, 1.9055, 1.7328, 1.8762, 1.8211, 1.7967, 2.3428,\n",
            "        1.7985, 1.7271, 1.7915, 1.9512, 1.8928, 1.9017, 1.8784, 1.9809, 1.8569,\n",
            "        1.7830, 1.8911, 1.8859, 1.7764, 1.9832, 1.8389, 1.7616, 1.8728, 1.8753,\n",
            "        1.9008, 1.8209, 1.7039, 1.7377, 1.7786, 1.6944, 1.7829, 1.7815, 1.7594,\n",
            "        1.8428, 1.9238, 2.0871, 1.8980, 1.8413, 1.8471, 1.8584, 1.7640, 1.8453,\n",
            "        1.7606, 1.9504, 1.9620, 1.8755, 1.9424, 1.8731, 1.8674, 1.9422, 1.8750,\n",
            "        1.9208, 1.7464, 1.8558, 1.6539, 2.0660, 2.0298, 1.9174, 1.8972, 1.7589,\n",
            "        1.7551, 1.9560, 1.7909, 1.7971, 1.7851, 1.7733, 1.8061, 1.7949, 1.8169,\n",
            "        1.8089, 1.8641, 2.1542, 1.7739, 1.7913, 1.8022, 1.7155, 1.7679, 1.7704,\n",
            "        1.6266, 1.8645, 1.9076, 1.8576, 1.6924, 1.8020, 1.7100, 1.7713, 1.8572,\n",
            "        1.7103, 2.0664, 1.9054, 1.9422, 1.8078, 1.7412, 1.6061, 1.9105, 1.8947,\n",
            "        1.7954, 1.8989, 1.8239, 1.7619, 1.7951, 1.8149, 1.8539, 1.8502, 1.7095,\n",
            "        2.1831, 1.8599, 1.8252, 1.8193, 1.8460, 1.7968, 1.6229, 1.8450, 1.8290,\n",
            "        1.8706, 1.9293, 1.6881, 1.9725, 1.8981, 1.8925, 1.8851, 1.8445, 1.9764,\n",
            "        2.0674, 1.8384, 1.8414, 1.8762, 1.7931, 1.7131, 1.9644, 1.7854, 1.9369,\n",
            "        1.8972, 1.8940, 1.8700, 1.7967, 1.8775, 1.9409, 1.7391, 1.7944, 1.9678,\n",
            "        1.7678, 1.6851, 1.9414, 1.9663, 1.9882, 1.7915, 1.8141, 1.8325, 2.1200,\n",
            "        1.9256, 2.3592, 2.0304, 1.9594, 1.7334, 1.9048, 1.8221, 1.7811, 1.9084,\n",
            "        1.8053, 1.9171, 1.9644, 1.8256, 1.6432, 1.9173, 1.9094, 1.9923, 1.7963,\n",
            "        1.9077, 1.7619, 2.1724, 1.7931, 1.7564, 1.8889, 1.9832, 1.9136, 1.8035,\n",
            "        1.8419, 1.8278, 1.8057, 1.9063, 1.8646, 1.7848, 1.8230, 1.7986, 1.7091,\n",
            "        1.7724, 1.7939, 1.7611, 1.9325, 2.0162, 1.7295, 2.0196, 1.8876, 1.8325,\n",
            "        1.8225, 1.7870, 1.9160, 1.7197, 1.7170, 1.9133, 1.7770, 1.9943, 1.8389,\n",
            "        1.8070, 1.8516, 1.7857, 1.9648, 1.9553, 1.9232, 1.8086, 1.8114, 1.7141,\n",
            "        1.8058, 1.8532, 1.9255, 1.7682, 1.8314, 1.8495, 1.8296, 1.8278, 1.8819,\n",
            "        1.7698, 1.7838, 1.7807, 1.9974, 1.6994, 1.9483, 1.7793, 1.8029, 2.2210,\n",
            "        1.6455, 1.8357, 2.1706, 1.9204, 1.7414, 1.7809, 1.8648, 1.9145, 1.8849,\n",
            "        1.8346, 1.9368, 1.8169, 2.2302, 1.8262, 2.0651, 1.9888, 1.8169, 1.8462,\n",
            "        1.9681, 1.8083, 1.8595, 1.8539, 1.7699, 1.9001, 1.7285, 1.7553, 1.8924,\n",
            "        1.7829, 1.9428, 1.8724, 1.7228, 2.0548, 1.7732, 1.8561, 1.7699, 1.9269,\n",
            "        1.8171, 2.4075, 1.7257, 1.7819, 1.7244, 1.8521, 1.8302, 1.8797, 1.7617,\n",
            "        1.9650, 1.9807, 1.7102, 1.7486, 1.8350, 1.9919, 1.8505, 1.9000, 1.8269,\n",
            "        1.9787, 1.7635, 1.6071, 1.7998, 1.9545, 1.7348, 1.7140, 1.8851, 1.7981,\n",
            "        1.9100, 1.8315, 1.7864, 1.9165, 1.8839, 1.9017, 1.9334, 1.7405, 1.7661,\n",
            "        1.8015, 1.9987, 1.7622, 1.9107, 1.8444, 1.7128, 1.8726, 1.8529, 1.9270,\n",
            "        1.8769, 1.7261, 1.8393, 1.9075, 1.7953, 1.8246, 1.7605, 2.0470, 1.9221,\n",
            "        1.9205, 1.8910, 1.7666, 1.6801, 1.8308, 1.8845, 1.8339, 1.8238, 1.7616,\n",
            "        1.6114, 1.8411, 1.7437, 1.8423, 1.9540, 1.7465, 1.7741, 1.8746, 1.8856,\n",
            "        1.7740, 1.7603, 1.7682, 1.8396, 1.6869, 1.8080, 1.8836, 1.8283, 1.8341,\n",
            "        1.8522, 1.9749, 1.8707, 1.7719, 1.8993, 1.8108, 1.8480, 1.8267, 1.8731,\n",
            "        1.9576, 1.8347, 1.9509, 1.9641, 1.7997, 1.7652, 1.9253, 1.7126, 1.7551,\n",
            "        1.9427, 1.8559, 1.9163, 1.7681, 1.7803, 1.8500, 1.8535, 1.8865, 1.7599,\n",
            "        2.0692, 1.8021, 1.7077, 1.8890, 1.9457, 1.8516, 1.7882, 1.8356, 1.8472,\n",
            "        1.6708, 1.7435, 1.9080, 1.9653, 2.0401, 1.8935, 1.8450, 1.7536, 1.7733,\n",
            "        1.8135, 1.8534, 1.9368, 1.7348, 1.8738, 1.9632, 1.9033, 1.7422, 1.7842,\n",
            "        1.8516, 2.0218, 1.7044, 1.8793, 1.8655, 1.8516, 1.8002, 1.8687, 1.8460,\n",
            "        1.7589, 1.8174, 1.9830, 1.9034, 2.1222, 1.8460, 1.9209, 1.8893, 1.9422,\n",
            "        1.8489, 1.8396, 1.9953, 2.0865, 1.8253, 1.7700, 1.8035, 1.7535, 1.8923,\n",
            "        1.8620, 1.8627, 1.7264, 1.8140, 1.9613, 1.8812, 1.8729, 2.0050, 1.7092,\n",
            "        1.7726, 1.9410, 1.8381, 1.8366, 1.7276, 1.8796, 1.7548, 1.9536, 1.8062,\n",
            "        1.8883, 2.0278, 1.8775, 1.9446, 1.8676, 1.8423, 1.7798, 1.9403, 1.8375,\n",
            "        2.0473, 1.9507, 1.8337, 1.8184, 1.7791, 1.8993, 1.8781, 1.8691, 1.8493,\n",
            "        1.7623, 1.9458, 1.7564, 1.7448, 1.8633, 1.6863, 1.8062, 1.8702, 2.0048,\n",
            "        1.8504, 1.8964, 1.9489, 1.8264, 1.9019, 1.8196, 1.9712, 1.8969, 1.8652,\n",
            "        1.8709, 1.6984, 1.8677, 1.8846, 1.9256, 1.8620, 1.6366, 1.8434, 1.7506,\n",
            "        1.8438, 1.5788, 1.9316, 1.9535, 1.7878, 1.7354, 2.0920, 1.9456],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.2371, 0.3433, 0.3279, 0.4642, 0.2233, 0.2370, 0.2176, 0.3793, 0.3140,\n",
            "        0.2803, 0.2434, 0.2116, 0.2478, 0.2435, 0.2298, 0.3172, 0.2725, 0.6511,\n",
            "        0.2925, 0.2281, 0.2279, 0.4254, 0.2342, 0.3328, 0.2632, 0.2176, 0.3180,\n",
            "        0.3893, 0.1387, 0.2274, 0.3379, 0.0767, 0.2253, 0.2504, 0.1990, 0.1951,\n",
            "        0.2566, 0.3253, 0.2797, 0.3149, 0.2373, 0.2533, 0.1956, 0.3236, 0.2093,\n",
            "        0.2333, 0.2300, 0.5019, 0.2830, 0.1885, 0.3264, 0.2722, 0.2369, 0.2430,\n",
            "        0.3625, 0.2165, 0.4700, 0.3047, 0.3675, 0.2641, 0.1979, 0.2664, 0.3448,\n",
            "        0.2005, 0.2450, 0.4351, 0.2689, 0.1632, 0.3087, 0.1209, 0.2153, 0.1592,\n",
            "        0.2960, 0.1423, 0.2951, 0.2706, 0.2007, 0.2939, 0.2210, 0.2243, 0.2465,\n",
            "        0.3910, 0.4599, 0.5417, 0.2147, 0.3469, 0.2703, 0.2229, 0.3645, 0.2647,\n",
            "        0.2421, 0.2492, 0.1666, 0.2763, 0.2560, 0.2151, 0.3363, 0.2767, 0.2516,\n",
            "        0.2988, 0.2622, 0.3499, 0.3001, 0.3907, 0.3184, 0.2233, 0.2649, 0.2110,\n",
            "        0.2034, 0.2752, 0.2314, 0.3480, 0.2238, 0.2892, 0.1991, 0.2923, 0.3259,\n",
            "        0.0722, 0.3039, 0.3041, 0.3803, 0.2568, 0.2382, 0.3057, 0.2652, 0.1532,\n",
            "        0.2110, 0.2567, 0.3148, 0.2746, 0.1833, 0.1950, 0.1116, 0.2279, 0.3705,\n",
            "        0.2477, 0.2000, 0.3060, 0.2548, 0.2468, 0.3028, 0.1921, 0.2952, 0.1980,\n",
            "        0.2135, 0.1583, 0.1586, 0.3944, 0.2352, 0.3947, 0.2740, 0.2861, 0.1856,\n",
            "        0.2702, 0.2986, 0.1728, 0.2658, 0.2696, 0.2028, 0.1838, 0.3176, 0.6246,\n",
            "        0.2631, 0.3855, 0.2074, 0.2317, 0.4171, 0.2044, 0.2926, 0.3506, 0.2305,\n",
            "        0.2400, 0.1420, 0.1093, 0.2757, 0.3253, 0.2334, 0.1650, 0.4026, 0.2066,\n",
            "        0.1790, 0.3032, 0.5658, 0.3246, 0.3834, 0.3254, 0.1772, 0.2909, 0.2350,\n",
            "        0.2519, 0.1968, 0.2003, 0.3213, 0.4802, 0.2543, 0.2578, 0.3280, 0.2270,\n",
            "        0.3044, 0.2273, 0.2447, 0.2527, 0.4136, 0.2588, 0.3589, 0.2688, 0.2115,\n",
            "        0.2022, 0.3186, 0.3740, 0.1785, 0.2074, 0.2346, 0.3566, 0.2623, 0.2620,\n",
            "        0.2880, 0.1462, 0.1896, 0.2777, 0.1852, 0.3240, 0.2748, 0.2164, 0.3066,\n",
            "        0.1845, 0.3992, 0.1695, 0.4411, 0.2812, 0.2730, 0.2784, 0.1861, 0.3589,\n",
            "        0.1934, 0.3320, 0.3350, 0.2655, 0.2740, 0.3185, 0.2633, 0.2458, 0.2003,\n",
            "        0.2809, 0.3049, 0.2050, 0.2904, 0.2381, 0.3278, 0.3484, 0.4293, 0.2422,\n",
            "        0.2859, 0.1864, 0.2954, 0.5634, 0.2081, 0.3743, 0.2902, 0.3820, 0.3069,\n",
            "        0.2101, 0.2750, 0.2878, 0.1870, 0.3015, 0.1661, 0.2998, 0.3101, 0.2522,\n",
            "        0.2419, 0.1758, 0.2681, 0.2812, 0.1495, 0.2868, 0.3157, 0.2587, 0.2437,\n",
            "        0.1467, 0.5416, 0.2490, 0.2831, 0.2783, 0.1614, 0.1963, 0.2034, 0.2364,\n",
            "        0.2527, 0.1573, 0.3184, 0.2841, 0.1613, 0.1489, 0.2850, 0.1625, 0.3277,\n",
            "        0.4936, 0.2780, 0.3178, 0.1743, 0.2158, 0.2222, 0.2821, 0.4267, 0.2713,\n",
            "        0.1778, 0.3067, 0.2270, 0.1772, 0.3897, 0.2923, 0.4843, 0.2345, 0.2327,\n",
            "        0.2740, 0.2700, 0.2804, 0.4035, 0.1501, 0.3329, 0.3286, 0.2803, 0.2309,\n",
            "        0.1738, 0.3270, 0.3097, 0.1808, 0.2384, 0.2107, 0.3240, 0.3346, 0.2236,\n",
            "        0.2061, 0.2687, 0.2360, 0.3338, 0.2694, 0.3203, 0.2895, 0.1884, 0.1491,\n",
            "        0.3957, 0.5167, 0.3407, 0.1854, 0.1816, 0.2626, 0.1855, 0.2219, 0.1482,\n",
            "        0.2584, 0.2458, 0.2616, 0.2396, 0.2402, 0.2423, 0.3463, 0.2731, 0.1524,\n",
            "        0.2514, 0.2760, 0.1734, 0.2715, 0.4052, 0.2252, 0.3676, 0.3070, 0.3127,\n",
            "        0.1836, 0.4330, 0.2203, 0.2073, 0.2803, 0.2984, 0.2191, 0.3272, 0.2267,\n",
            "        0.2749, 0.3056, 0.4566, 0.2962, 0.3528, 0.3236, 0.4220, 0.2715, 0.2256,\n",
            "        0.2903, 0.1829, 0.3994, 0.2820, 0.2471, 0.1647, 0.3654, 0.4504, 0.2685,\n",
            "        0.2992, 0.2825, 0.2435, 0.2212, 0.4300, 0.4342, 0.1988, 0.2863, 0.3398,\n",
            "        0.2444, 0.2905, 0.2559, 0.2586, 0.1702, 0.1906, 0.2536, 0.2978, 0.2498,\n",
            "        0.3777, 0.2252, 0.2472, 0.2243, 0.1732, 0.2194, 0.2091, 0.2820, 0.2898,\n",
            "        0.2887, 0.3292, 0.1644, 0.2962, 0.3279, 0.2535, 0.2795, 0.2238, 0.2607,\n",
            "        0.1937, 0.2680, 0.2418, 0.5193, 0.2502, 0.3147, 0.2166, 0.2313, 0.2027,\n",
            "        0.1880, 0.2180, 0.3826, 0.3871, 0.2358, 0.3556, 0.2272, 0.3272, 0.3442,\n",
            "        0.3154, 0.1993, 0.3135, 0.2254, 0.3048, 0.2658, 0.3337, 0.2679, 0.2670,\n",
            "        0.2363, 0.4347, 0.1931, 0.1995, 0.2072, 0.3202, 0.2667, 0.2305, 0.2383,\n",
            "        0.2246, 0.2562, 0.2837, 0.4046, 0.2786, 0.2243, 0.1591, 0.1923, 0.1894,\n",
            "        0.2496, 0.1140, 0.3128, 0.3197, 0.3530, 0.2999, 0.2115, 0.4718, 0.2979,\n",
            "        0.3472, 0.2890, 0.4740, 0.2230, 0.3630, 0.4015, 0.2446, 0.1897, 0.1460,\n",
            "        0.1874, 0.2734, 0.2366, 0.3001, 0.2359, 0.2688, 0.3256, 0.2749, 0.2848,\n",
            "        0.2299, 0.3001, 0.4818, 0.3074, 0.3164, 0.3114, 0.3549, 0.2859],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.0185, -0.0705, -0.0518,  ..., -0.0390,  0.1735, -0.0410],\n",
            "        [-0.0818, -0.0944,  0.0174,  ...,  0.2028, -0.0248,  0.0372],\n",
            "        [-0.0332, -0.0566, -0.0242,  ..., -0.0344, -0.0227,  0.0197],\n",
            "        ...,\n",
            "        [-0.0103,  0.0033, -0.0359,  ..., -0.0279, -0.0115,  0.0128],\n",
            "        [-0.0359, -0.0353, -0.0296,  ..., -0.0330, -0.0110, -0.0513],\n",
            "        [ 0.0021, -0.0248, -0.0829,  ...,  0.0417, -0.0500,  0.0663]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-2.6341e-03,  3.0005e-03,  6.5581e-04, -2.6909e-02,  6.3637e-03,\n",
            "         1.3260e-02, -1.1178e-02,  2.0639e-02, -3.6373e-03, -1.2325e-02,\n",
            "        -1.2629e-02, -7.2057e-03, -1.9321e-02, -2.4960e-02, -1.1885e-02,\n",
            "        -8.3259e-03, -9.5745e-03, -1.6658e-02,  9.1804e-03, -1.5354e-02,\n",
            "         7.1358e-03,  3.0737e-02,  1.3239e-02, -7.7528e-03,  4.7448e-03,\n",
            "         1.1175e-02,  1.5949e-02, -1.6712e-02, -1.0130e-03, -3.7167e-03,\n",
            "         6.5269e-03, -1.2041e-02,  9.0427e-03, -8.3279e-04,  8.8647e-03,\n",
            "        -2.6307e-02, -1.4588e-02,  2.9433e-03,  2.9718e-03, -1.9125e-02,\n",
            "        -4.7922e-03,  1.3828e-02,  9.8802e-03, -1.8417e-02,  1.9734e-02,\n",
            "         1.6941e-03,  1.2420e-02, -5.5842e-03, -1.0612e-02,  3.9847e-04,\n",
            "         4.2733e-03, -1.3298e-02,  2.0661e-02,  1.6963e-02,  2.7952e-03,\n",
            "         7.4214e-04,  1.3168e-02,  3.2213e-03,  1.0458e-02,  1.6511e-02,\n",
            "         9.1717e-04,  3.9388e-03, -5.6534e-03,  1.9372e-02,  7.5238e-03,\n",
            "         1.3437e-02, -1.3185e-02, -1.0026e-02,  7.1920e-03, -2.3166e-03,\n",
            "        -1.8895e-02,  1.2519e-02,  1.9583e-03,  7.3836e-03, -9.6664e-03,\n",
            "         2.0189e-02,  7.6652e-03,  1.8529e-02,  1.5710e-02,  1.8582e-02,\n",
            "        -6.9314e-03,  1.7090e-02,  9.1268e-03, -3.8876e-02, -2.4116e-02,\n",
            "        -6.8715e-03, -1.1648e-02,  7.8817e-03,  1.8046e-03,  2.8480e-02,\n",
            "        -1.9379e-02, -1.6295e-02,  1.0468e-02, -1.3027e-02, -8.4211e-03,\n",
            "        -2.9210e-02, -2.4856e-03, -8.7141e-03, -1.6397e-02, -9.3054e-03,\n",
            "        -1.5931e-02, -2.6346e-02, -1.1091e-03,  2.2589e-02,  2.1387e-03,\n",
            "        -2.3212e-02, -1.4085e-02, -5.6224e-03, -2.0090e-02, -3.0284e-02,\n",
            "        -4.9574e-02,  2.3283e-02,  1.4954e-02, -7.7501e-03, -3.9482e-03,\n",
            "        -3.7629e-02, -2.4220e-02, -1.0194e-02, -7.7038e-03, -4.1312e-03,\n",
            "        -2.9553e-03, -6.2174e-03, -1.2076e-02, -7.0168e-03, -3.8948e-03,\n",
            "        -1.6953e-02, -2.4585e-02,  5.5353e-03, -8.3370e-03, -7.0759e-03,\n",
            "        -2.4023e-02, -6.3686e-03,  7.3420e-04,  5.2883e-03, -2.2181e-02,\n",
            "        -2.6972e-02, -1.7990e-02, -1.6393e-02,  2.1485e-03, -1.6122e-02,\n",
            "        -1.6112e-02,  6.5931e-03, -2.0045e-02,  6.4149e-03, -1.2601e-02,\n",
            "        -7.6238e-03,  1.1411e-02, -4.5084e-02, -9.2018e-03, -1.5563e-02,\n",
            "        -1.3590e-02, -1.4374e-03, -1.9466e-02,  2.0737e-02, -1.0476e-02,\n",
            "         6.3229e-03,  8.3229e-03, -1.0791e-02, -1.8903e-02,  5.8624e-03,\n",
            "        -2.0189e-03,  3.2436e-02,  4.0581e-02, -4.0820e-05,  1.0886e-02,\n",
            "        -1.6544e-02, -5.3365e-04, -2.2903e-02,  4.6295e-03, -4.8402e-03,\n",
            "         1.0187e-02,  1.7954e-02,  4.8211e-03,  6.1831e-03,  1.4419e-02,\n",
            "        -1.2094e-02, -8.7460e-03,  1.9488e-03,  1.4685e-02,  1.2464e-02,\n",
            "         7.0523e-03, -4.1783e-03,  1.2048e-02, -2.0199e-02,  9.9144e-03,\n",
            "         1.3978e-02, -1.0321e-03,  5.7394e-03,  1.4019e-03,  6.0113e-04,\n",
            "        -5.5790e-04,  2.4424e-02,  2.3076e-02, -1.4610e-02,  1.1185e-02,\n",
            "         3.4608e-02,  1.6944e-02,  4.3295e-03, -2.5606e-02,  1.2279e-02,\n",
            "        -2.5810e-02,  8.5365e-03,  2.0437e-02,  2.2557e-02,  2.2966e-02,\n",
            "         8.8420e-03, -1.3894e-02,  3.8719e-03, -9.3046e-03,  2.3220e-02,\n",
            "        -1.4949e-02,  6.9258e-03,  5.0070e-03, -1.7302e-02,  1.0364e-03,\n",
            "        -1.0223e-02, -9.6949e-03,  3.4534e-02,  6.1337e-03,  1.1582e-02,\n",
            "        -2.0529e-02, -2.1956e-02,  5.3109e-03,  3.4101e-02, -5.8079e-03,\n",
            "         2.9406e-02, -8.7954e-03, -5.2505e-03, -1.9088e-02,  3.0350e-02,\n",
            "         1.8445e-02, -2.1225e-02,  1.8432e-02,  1.3832e-02,  1.7848e-02,\n",
            "        -4.4762e-03,  3.5858e-02,  2.1762e-02,  1.0880e-02,  4.0255e-02,\n",
            "        -2.0049e-03, -3.0348e-03,  9.3293e-03, -1.6304e-02,  9.6253e-04,\n",
            "         1.8673e-02, -1.6567e-02,  1.4964e-02, -3.7206e-03, -7.6734e-03,\n",
            "        -7.9254e-06,  3.9732e-03, -9.5979e-03, -1.6833e-02,  5.8524e-05,\n",
            "        -6.4126e-03,  8.2977e-03,  4.8207e-03, -1.1467e-03,  4.8869e-03,\n",
            "         1.7349e-02,  3.9222e-03, -7.8080e-03,  1.6051e-02,  9.8802e-03,\n",
            "        -1.0144e-02,  2.0912e-02, -6.3203e-03, -2.3139e-02,  1.1646e-03,\n",
            "         2.2468e-02, -6.6953e-03,  1.8311e-02,  1.4623e-02, -1.1654e-02,\n",
            "        -1.4306e-02,  1.2974e-02, -9.6865e-03, -6.2351e-03,  1.3180e-02,\n",
            "         6.7543e-03,  4.6418e-02, -2.7962e-02, -1.5111e-02,  2.8716e-02,\n",
            "         9.1991e-03, -5.3710e-03, -6.0361e-03, -7.2140e-03, -9.2421e-03,\n",
            "         1.8536e-03, -3.1078e-03, -8.4004e-03, -1.6766e-02,  4.0936e-03,\n",
            "         6.2426e-03, -1.2470e-03, -1.2919e-02,  3.5819e-03,  1.1006e-02,\n",
            "        -1.3282e-02,  2.6395e-03,  8.9953e-03,  6.5421e-03, -1.2031e-02,\n",
            "         1.7149e-02,  1.7949e-02, -1.0581e-02, -2.6962e-02, -1.3564e-02,\n",
            "        -9.7173e-03, -2.1176e-03,  3.5370e-02,  1.8392e-02,  2.6676e-02,\n",
            "        -1.0594e-03, -3.3949e-03, -4.8838e-03,  1.3427e-02, -1.3948e-02,\n",
            "        -1.9559e-02, -2.3295e-02, -3.7834e-02, -1.4637e-02, -2.1323e-02,\n",
            "        -3.0952e-02, -3.0822e-02,  1.9438e-03,  2.8637e-03, -2.1198e-02,\n",
            "         1.0448e-02, -1.1316e-02, -4.2609e-03,  2.2647e-02, -1.2867e-02,\n",
            "        -1.1018e-02,  1.2336e-02, -2.0057e-02, -2.1837e-02, -6.8067e-03,\n",
            "        -1.0488e-02, -2.6298e-02, -9.9579e-03, -1.2966e-02, -2.4832e-03,\n",
            "        -7.0940e-03,  1.7997e-02, -6.4257e-03,  7.9069e-03, -1.2287e-02,\n",
            "         9.8176e-03,  2.6674e-03, -2.1524e-02,  2.7511e-03, -9.2075e-03,\n",
            "        -1.7541e-02, -1.7103e-03, -1.4588e-02,  4.4247e-03,  3.4405e-02,\n",
            "         1.2725e-02,  3.0885e-02,  1.3090e-03, -7.0084e-05, -2.3165e-03,\n",
            "        -3.7989e-03, -1.1148e-02,  1.7210e-02, -6.7575e-03, -1.3694e-04,\n",
            "        -8.9166e-03, -1.6281e-02, -4.4920e-03,  1.1332e-02, -1.5909e-03,\n",
            "        -8.8193e-03, -4.9399e-03,  4.5732e-03, -1.0949e-02,  1.2890e-02,\n",
            "        -6.6586e-03, -2.5605e-03,  2.7965e-03,  1.1225e-02, -2.2055e-02,\n",
            "        -3.9271e-03, -6.6467e-03, -1.8840e-02, -2.1687e-02, -7.4066e-04,\n",
            "        -2.7281e-02,  5.0448e-03, -2.0709e-02, -3.4103e-02, -2.2374e-02,\n",
            "        -1.6656e-02, -2.7916e-02, -9.8977e-03,  5.5252e-03,  1.6013e-02,\n",
            "        -1.4895e-02,  3.5091e-03,  9.0003e-03, -8.3982e-03, -3.7479e-02,\n",
            "         2.0727e-02, -5.8799e-03,  9.1768e-03, -2.0297e-02, -7.3148e-03,\n",
            "        -1.6966e-03, -1.4029e-03,  3.2229e-03,  2.9212e-02,  1.2487e-02,\n",
            "        -2.0100e-02,  2.1170e-02, -2.5300e-02,  3.1815e-02, -1.0645e-03,\n",
            "        -1.0449e-02, -2.3419e-02,  1.4564e-02,  2.1245e-02,  1.6530e-02,\n",
            "        -3.2436e-03, -2.0437e-02, -3.6982e-02, -8.7213e-03,  5.4575e-03,\n",
            "         1.1048e-03,  2.2012e-03,  2.9512e-03, -5.9939e-05,  5.5785e-04,\n",
            "        -3.6906e-03,  5.3763e-03, -2.4765e-02,  9.2729e-03,  9.6081e-03,\n",
            "         9.1647e-03,  9.0880e-03,  7.4842e-03, -1.1946e-02,  2.1395e-02,\n",
            "         2.7922e-02,  1.4692e-02, -2.4958e-03,  2.8887e-02,  1.3422e-02,\n",
            "         1.7173e-03,  2.5018e-03, -2.1253e-02, -8.2424e-04,  4.2183e-03,\n",
            "         8.5981e-03,  1.8735e-02,  8.5622e-03, -8.8255e-03,  1.7462e-02,\n",
            "        -1.3693e-02,  2.1955e-03,  1.0772e-02,  2.8693e-03,  3.1032e-02,\n",
            "         8.5460e-03, -1.4198e-02, -2.2472e-03,  1.8740e-02, -1.2905e-02,\n",
            "         4.0370e-02, -7.7538e-04,  1.8671e-03,  7.2793e-03, -2.6508e-02,\n",
            "        -1.7609e-02, -2.4142e-02,  2.9577e-03, -1.5917e-02,  1.6273e-03,\n",
            "         1.1132e-02,  1.4574e-02, -8.1919e-03, -7.6581e-03, -1.8452e-02,\n",
            "        -9.0419e-03,  4.0883e-03,  4.4482e-02, -2.3664e-02, -5.2547e-03,\n",
            "        -1.9529e-02,  3.2860e-03,  5.4667e-03, -4.9558e-03,  7.6805e-03,\n",
            "        -3.3026e-03, -2.6248e-03, -1.1094e-02,  2.3922e-02,  1.8079e-02,\n",
            "        -1.8135e-02,  5.2204e-03, -1.3559e-02,  1.9448e-02,  1.0981e-02,\n",
            "         2.6869e-02, -6.6801e-03, -8.9389e-04, -3.4924e-03, -1.9667e-02,\n",
            "        -1.8511e-02, -7.6262e-04, -1.6382e-02, -1.5862e-02, -1.3717e-02,\n",
            "         1.7528e-02, -1.1419e-03, -6.3346e-03, -1.1118e-02,  1.3159e-02,\n",
            "        -2.3464e-02,  2.7993e-04, -3.6273e-04,  2.3797e-02, -2.7353e-03,\n",
            "        -2.2223e-02,  1.3415e-02,  1.0443e-02, -2.3512e-02,  1.6832e-02,\n",
            "         4.3699e-03, -1.3243e-02, -2.8605e-03,  5.4212e-03,  1.9924e-03,\n",
            "        -6.8664e-04, -3.9092e-04,  1.7806e-02,  1.8391e-02,  2.8473e-02,\n",
            "        -3.3835e-02, -1.0778e-02, -1.2371e-02, -1.9110e-03, -1.6381e-03,\n",
            "         1.7288e-02, -3.9813e-03, -1.5167e-02, -1.0781e-02,  5.3808e-03,\n",
            "        -3.3947e-04,  3.3885e-04, -1.0162e-02, -4.0266e-03, -3.4751e-03,\n",
            "         4.2359e-03, -1.4677e-03,  1.3207e-02,  7.5580e-03,  1.9397e-04,\n",
            "         3.0048e-03,  8.6283e-03, -1.1193e-02,  3.8466e-02, -2.6220e-02,\n",
            "        -2.0251e-02, -6.3872e-03,  2.1906e-02, -7.3400e-03,  5.2753e-03,\n",
            "        -1.1709e-02,  8.4009e-03,  2.8530e-03, -4.7220e-03,  2.3118e-02,\n",
            "        -7.6039e-03,  2.8136e-03, -1.1701e-02, -4.4118e-03,  1.1846e-02,\n",
            "        -1.7632e-03, -1.2260e-02, -2.1210e-03,  1.2072e-02,  6.7523e-03,\n",
            "        -1.9128e-04, -2.5105e-02,  1.2693e-02,  1.6062e-02,  8.1264e-03,\n",
            "         1.3857e-03,  3.0087e-03, -1.4111e-02,  1.9784e-02, -9.2301e-04,\n",
            "        -1.8428e-02,  7.8059e-03,  1.5319e-02, -1.2768e-02, -9.0166e-03,\n",
            "         1.8031e-02,  2.4853e-02,  1.7788e-02,  8.8640e-03, -9.4422e-03,\n",
            "        -1.3652e-03,  1.2932e-02,  9.0133e-03,  1.6655e-02, -5.4321e-03,\n",
            "         2.7480e-02, -3.1781e-02, -1.3331e-02,  5.5792e-03, -1.3278e-02,\n",
            "        -1.9219e-02, -1.3307e-02,  4.2390e-03,  3.0246e-02, -8.1990e-03,\n",
            "         8.3008e-03,  1.8993e-02,  1.0643e-02,  3.1324e-02,  1.9283e-02,\n",
            "         3.3642e-03,  1.9669e-02,  2.2673e-03, -1.9630e-02,  2.0147e-02,\n",
            "        -1.1433e-02, -7.6073e-03,  1.5071e-02, -3.0395e-03, -9.3430e-03,\n",
            "        -4.1657e-03,  2.2972e-03, -5.0985e-03, -1.4499e-02, -2.7673e-02,\n",
            "        -3.8721e-02,  5.4249e-03,  1.3504e-02, -1.2811e-03,  3.7465e-02,\n",
            "         1.5154e-03,  2.4035e-02, -2.0557e-02,  9.8406e-03,  1.0352e-02,\n",
            "         3.8597e-02, -1.1905e-02, -2.1718e-02,  8.3778e-03,  1.4691e-02,\n",
            "         2.2631e-02, -3.7629e-03,  1.5570e-02, -9.3990e-03,  5.3536e-03,\n",
            "         1.9584e-02, -1.1156e-02,  1.5190e-02,  5.4622e-03,  2.2995e-02,\n",
            "         2.9260e-02, -1.5236e-03,  6.6009e-03, -3.1939e-02, -1.0486e-02,\n",
            "        -4.4617e-03,  3.1853e-02,  1.3736e-02,  1.3561e-02,  7.0907e-03,\n",
            "        -1.6753e-02, -2.5470e-02,  1.9752e-02,  2.6715e-02, -4.6859e-03,\n",
            "         1.7682e-02,  3.2496e-02,  1.4553e-02,  2.6101e-02,  1.1341e-02,\n",
            "        -2.2271e-03,  3.5237e-02, -1.1892e-02, -1.8683e-02, -5.5245e-03,\n",
            "        -7.0732e-03, -5.2670e-03,  7.5946e-03, -1.8465e-02, -1.6897e-02,\n",
            "         1.0127e-02,  1.3006e-02, -1.8251e-03,  6.6651e-04, -1.1207e-02,\n",
            "         1.3563e-02, -1.8153e-02, -2.6487e-02,  6.0652e-03,  3.9711e-02,\n",
            "        -1.4285e-02,  1.8001e-02, -1.4039e-02, -1.8762e-02, -1.1778e-02,\n",
            "        -1.6449e-02,  9.0423e-03, -7.2730e-03,  1.7517e-02, -7.3016e-04,\n",
            "         1.0212e-02,  2.3785e-02,  8.1286e-03,  8.0260e-03,  1.1922e-02,\n",
            "         6.2416e-03, -2.4625e-02,  2.9461e-02, -1.4183e-02, -1.8672e-02,\n",
            "        -1.4057e-02,  7.9872e-03, -2.1081e-02, -2.7560e-02, -3.5690e-03,\n",
            "         1.5993e-03,  9.4720e-03,  1.8272e-02, -2.3742e-03,  1.1843e-03,\n",
            "        -5.7722e-04,  8.7818e-03,  2.7804e-03,  6.7973e-04,  1.5877e-02,\n",
            "        -7.0359e-03,  2.5487e-03, -1.7925e-02,  8.4912e-03,  4.3375e-03,\n",
            "         2.4508e-02,  3.6686e-03,  1.0252e-02, -1.3396e-02,  4.5706e-04,\n",
            "         1.0313e-02,  1.5229e-02,  3.9907e-02, -8.0809e-03,  1.3760e-02,\n",
            "        -6.5863e-03,  6.6066e-03, -3.1480e-02,  2.4665e-02,  3.4374e-03,\n",
            "         2.0973e-02,  1.9384e-02, -2.0880e-02,  7.1465e-03,  1.0406e-02,\n",
            "         2.2273e-05, -1.9182e-02,  6.3135e-03, -1.6891e-04,  8.8664e-03,\n",
            "        -4.7666e-03, -1.4493e-02,  3.2176e-03,  7.3346e-03,  2.0694e-02,\n",
            "        -4.9972e-04,  1.8820e-02,  3.9147e-02, -2.7095e-02, -1.8293e-02,\n",
            "        -1.9868e-02, -9.4048e-03,  4.1552e-03,  5.3837e-03, -4.6663e-03,\n",
            "        -1.3019e-02, -2.4452e-02, -8.9231e-03, -1.4603e-02,  2.5529e-03,\n",
            "        -3.0766e-02,  1.1169e-02, -6.8113e-03, -7.5967e-03, -9.3191e-03,\n",
            "         1.4919e-03, -2.3428e-03,  4.4398e-04, -1.0810e-02,  8.8498e-03,\n",
            "        -2.1022e-02, -8.0380e-03, -1.0818e-02, -6.4815e-03, -2.0681e-03,\n",
            "         2.2326e-02, -1.9234e-02,  4.0844e-03,  7.7233e-04,  1.7226e-03,\n",
            "        -1.7454e-02, -1.3190e-02, -7.4112e-03, -1.7550e-03,  1.2926e-03,\n",
            "        -6.7029e-03, -7.0588e-03,  6.2745e-03, -1.8068e-02, -9.4855e-03,\n",
            "        -2.0856e-02,  8.9604e-03,  2.1294e-02,  1.7025e-02,  2.1015e-02,\n",
            "         8.8233e-03, -9.8277e-03, -2.2293e-02,  2.4295e-02, -1.1174e-02,\n",
            "        -7.5753e-03,  6.1182e-03, -2.0653e-02, -1.6264e-02,  2.6457e-02,\n",
            "        -1.4782e-02,  1.8654e-02,  2.5488e-02,  2.4106e-02,  4.7888e-03,\n",
            "         2.3329e-02,  3.5806e-04,  2.5154e-02,  1.7094e-02,  1.7803e-02,\n",
            "         2.4687e-02,  9.0085e-03,  2.3610e-03,  2.6088e-02, -1.4110e-02,\n",
            "        -5.4212e-04,  8.9498e-04,  2.1150e-02,  4.8484e-03, -3.0503e-02,\n",
            "        -7.5025e-03, -3.3718e-02, -2.8913e-02,  1.5691e-02,  6.2047e-03,\n",
            "        -1.0853e-02,  1.9524e-02, -1.6188e-02,  8.9890e-03,  9.1894e-03,\n",
            "        -2.8592e-03, -1.0911e-02,  1.0848e-02,  4.8784e-02, -1.9687e-03,\n",
            "         2.6843e-02, -4.8715e-03,  1.3489e-02, -1.4523e-02, -2.7585e-02,\n",
            "         6.1228e-03,  4.8171e-03,  2.1566e-03, -3.7561e-02,  3.0775e-02,\n",
            "         1.9977e-02,  1.8480e-02,  3.0368e-03,  9.3825e-03,  4.5243e-04,\n",
            "         6.1650e-02, -8.6416e-03, -2.6913e-02,  6.3527e-03,  7.7985e-03,\n",
            "         1.3180e-02, -1.6666e-03,  2.0865e-02,  9.9480e-03,  8.8136e-03,\n",
            "         1.4841e-02,  3.3211e-03,  3.6342e-03,  2.8740e-02, -2.2120e-02,\n",
            "        -7.1567e-03,  1.0352e-02,  1.6433e-02,  1.1683e-02, -5.8058e-03,\n",
            "        -6.9297e-04,  2.6578e-02,  8.7967e-03, -3.1689e-02,  1.8949e-02,\n",
            "        -8.5859e-03,  3.4228e-02, -1.5237e-02, -5.9709e-03,  1.1069e-03,\n",
            "        -1.8394e-02, -1.9246e-02, -3.6361e-02,  3.9839e-03,  4.1237e-02,\n",
            "         1.3816e-02, -7.3304e-03,  3.8832e-03,  2.4367e-03, -2.1625e-02,\n",
            "        -1.4523e-02, -1.6281e-04,  6.2566e-04, -1.6798e-02,  2.3083e-02,\n",
            "         9.7114e-03, -8.2207e-03,  1.1595e-03, -2.0983e-02, -6.6540e-03,\n",
            "        -1.4097e-02,  3.4067e-03, -7.7575e-03, -1.4738e-02, -2.1343e-02,\n",
            "         5.4123e-03,  3.9747e-03, -4.6185e-03, -1.5462e-02, -7.6229e-03,\n",
            "         1.2211e-02, -4.8453e-03, -8.8757e-03, -1.0275e-02,  7.3482e-03,\n",
            "        -6.0349e-03,  2.3658e-03,  2.1053e-02, -8.5688e-03, -1.1630e-02,\n",
            "        -2.7332e-02, -2.0648e-02,  4.4952e-03, -1.8649e-02, -1.1564e-02,\n",
            "         4.5905e-04, -6.1831e-03, -2.4435e-02, -7.1187e-03, -1.4394e-02,\n",
            "        -2.3544e-03,  2.1556e-02,  2.2924e-02, -1.3725e-02,  7.7785e-03,\n",
            "        -8.5513e-03,  2.4221e-02,  3.8192e-03,  7.0947e-04,  1.6114e-02,\n",
            "         2.5932e-02,  1.8108e-02,  2.9306e-02,  1.6773e-03, -3.0166e-03,\n",
            "         3.2015e-02, -1.4034e-02,  2.7365e-02, -1.8858e-02,  2.5832e-03,\n",
            "         1.3498e-02, -1.3502e-02, -1.4940e-02, -1.0904e-02,  1.8642e-02,\n",
            "         4.2593e-03, -1.6742e-02, -1.2638e-02, -4.5468e-02, -5.0823e-03,\n",
            "        -2.5093e-02,  6.7847e-03, -1.7868e-02, -7.8250e-04, -6.3448e-03],\n",
            "       requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace the classifier layer (fully connected layer)\n",
        "num_ftrs = model.fc.in_features\n",
        "print(f'Total Input Feature in Fully Connected Layer: {num_ftrs}')\n",
        "\n",
        "model.fc = nn.Linear(num_ftrs, num_classes)       #Replace output feature 1000 to 10, cause CIFAR10 has 10 classes\n",
        "print(model.fc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d1Wv0ddDfoh",
        "outputId": "f936663a-38d4-43ff-caa0-8ae326d109f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Input Feature in Fully Connected Layer: 512\n",
            "Linear(in_features=512, out_features=10, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNzfrjetEZJa",
        "outputId": "a20f5868-389e-47a9-e619-2a0791348631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Print Model summary\n",
        "summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xe_ZHmoGUWA",
        "outputId": "142cbe7c-8f48-4a61-9745-99974e0c4f0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
            "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
            "             ReLU-14           [-1, 64, 56, 56]               0\n",
            "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
            "             ReLU-17           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
            "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
            "             ReLU-21          [-1, 128, 28, 28]               0\n",
            "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
            "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
            "             ReLU-26          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
            "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
            "             ReLU-30          [-1, 128, 28, 28]               0\n",
            "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
            "             ReLU-33          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
            "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
            "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
            "             ReLU-37          [-1, 256, 14, 14]               0\n",
            "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
            "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
            "             ReLU-42          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
            "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
            "             ReLU-46          [-1, 256, 14, 14]               0\n",
            "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
            "             ReLU-49          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
            "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
            "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-53            [-1, 512, 7, 7]               0\n",
            "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
            "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
            "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-58            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
            "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-62            [-1, 512, 7, 7]               0\n",
            "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-65            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
            "           Linear-68                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 11,181,642\n",
            "Trainable params: 5,130\n",
            "Non-trainable params: 11,176,512\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 62.79\n",
            "Params size (MB): 42.65\n",
            "Estimated Total Size (MB): 106.01\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Optimizer and Loss Function\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)     #Only train the classifier layer\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "4njhMh71SZxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training Function\n",
        "def train_model(model, train_loader, epochs):\n",
        "  model.train()   #Set the model to training mode\n",
        "  for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in train_loader:\n",
        "      data, target = data.to(device), target.to(device)     #Move the data and target to cuda enabled GPU\n",
        "      optimizer.zero_grad()     #Reset Gradients\n",
        "      output = model(data)\n",
        "      loss = criterion(output, target)\n",
        "      loss.backward()     #Backpropagation(Calculate derivative of loss with respect to parameters)\n",
        "      optimizer.step()    #Update weight\n",
        "      print(f'Loss of Item: {loss.item()}')     # retrieve the scalar value from a loss tensor\n",
        "      print(f'Batch Size of Tensor Data: {data.size(0)}')   #batch size of a tensor data\n",
        "      total_loss += loss.item() * data.size(0)      #Accumulates total loss (not per batch).\n",
        "      pred = output.argmax(dim=1)                   #Gets predicted class from Output(logits).\n",
        "      correct += pred.eq(target).sum().item()       #Counts correct predictions.\n",
        "    print(f'Total Loss: {total_loss}')\n",
        "    print(f'Correct Predictions: {correct}')\n",
        "    avg_loss = total_loss / len(train_loader.dataset)\n",
        "    accuracy = correct / len(train_loader.dataset)\n",
        "\n",
        "    print(f'Epoch: [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "FVCKjEojU1he"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate the model\n",
        "train_model(model, train_loader, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUqCfXFE4107",
        "outputId": "5a28655b-48f1-45f1-936e-9f2786e5e723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss of Item: 2.401623249053955\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 2.3376846313476562\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 2.2183098793029785\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 2.330643653869629\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 2.1595098972320557\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 2.2020909786224365\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 2.0914981365203857\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 2.1416847705841064\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 2.0309975147247314\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 2.048609495162964\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 2.0119271278381348\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.8622747659683228\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.8672726154327393\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.8489923477172852\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.890444040298462\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.8699382543563843\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.7832902669906616\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.7451152801513672\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.7346800565719604\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.7087140083312988\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.7023199796676636\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.595698595046997\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.5892319679260254\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.598736047744751\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.5924843549728394\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.5918835401535034\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.6314209699630737\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.4668315649032593\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.4108144044876099\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.5452643632888794\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.4743303060531616\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.4652302265167236\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.4028606414794922\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.4055620431900024\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.4532310962677002\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.3467469215393066\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.3420768976211548\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.3519465923309326\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.3588651418685913\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.2290951013565063\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.3138951063156128\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.2924007177352905\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.2944494485855103\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.2250125408172607\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.2486732006072998\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.142520785331726\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.134661316871643\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.1944838762283325\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.050167202949524\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.1901901960372925\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.1495174169540405\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.255771517753601\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.1134589910507202\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.1068111658096313\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.2751846313476562\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.10554838180542\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.213275671005249\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.2249891757965088\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.099273681640625\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.0776808261871338\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.0431901216506958\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.0522024631500244\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.103440761566162\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.1344457864761353\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.1148569583892822\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.0966665744781494\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.1518404483795166\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.025421142578125\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.015592098236084\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.0414209365844727\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9711891412734985\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.0087645053863525\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.0385420322418213\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9184236526489258\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.971875786781311\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9514318704605103\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.0355721712112427\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.074680209159851\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9075921177864075\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9824249744415283\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9689950942993164\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9273871183395386\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.0548409223556519\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.0645904541015625\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9650365114212036\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9779012799263\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.083035945892334\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.0052441358566284\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9972540140151978\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9617953300476074\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.0252002477645874\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8824065327644348\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8162631392478943\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.0040801763534546\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.00174880027771\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9797606468200684\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.937816858291626\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7931773066520691\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9499991536140442\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.0137699842453003\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9543987512588501\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8516300916671753\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9999166131019592\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.895721971988678\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9912688732147217\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.906955897808075\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8913811445236206\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.0330309867858887\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9512702226638794\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.002652645111084\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8663702607154846\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8182578086853027\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9054787755012512\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.790064811706543\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8788235783576965\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8065434098243713\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7913611531257629\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8403838276863098\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9133436679840088\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9028446674346924\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8830811381340027\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7840710878372192\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.0040271282196045\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8859008550643921\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9566600918769836\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7658379673957825\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8970362544059753\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9009182453155518\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9255764484405518\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.0200718641281128\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9188758134841919\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7462632656097412\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8807541131973267\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7518726587295532\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.875792384147644\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9278959035873413\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7768351435661316\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7625769972801208\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8677244782447815\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9021944403648376\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8407180309295654\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.75483638048172\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8906828165054321\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7023308277130127\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8218734264373779\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8360509872436523\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9056248068809509\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8674886226654053\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.870681643486023\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7914682626724243\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.88873690366745\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7326026558876038\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7719322443008423\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7982268929481506\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8704269528388977\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7949697375297546\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7906975150108337\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9601091742515564\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6934931874275208\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8190609216690063\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.877434492111206\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7148030400276184\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8856465816497803\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7743699550628662\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5777552127838135\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8906237483024597\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8254677653312683\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8152562975883484\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8192707896232605\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7115600109100342\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8505548238754272\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7265107035636902\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.891478955745697\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6723477244377136\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8318087458610535\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.697969913482666\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7261556386947632\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7263575792312622\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7173699140548706\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6770328283309937\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7771400213241577\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8050143718719482\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.701181948184967\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6288660764694214\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8335663676261902\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8367772102355957\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7815447449684143\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7476601600646973\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8515533804893494\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7534621953964233\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7621841430664062\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7493745684623718\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6642146706581116\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7177594304084778\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7888233661651611\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7536395192146301\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7732680439949036\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5780177116394043\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6839040517807007\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8467671275138855\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8173745274543762\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6103878021240234\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7787584662437439\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6672662496566772\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7980176210403442\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7034658193588257\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6498184204101562\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.789323091506958\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7479498386383057\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6042903661727905\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8021272420883179\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7741035223007202\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8139391541481018\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.84372878074646\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7261421084403992\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6874356269836426\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7821313142776489\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7787370681762695\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8723050951957703\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6650658845901489\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8372384309768677\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7020121216773987\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7537381649017334\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6844727396965027\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7792763113975525\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.871020495891571\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6944385766983032\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7431446313858032\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8159046769142151\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8731111884117126\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6047350764274597\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7537040710449219\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7214544415473938\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.807569682598114\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7033798694610596\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.806868314743042\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8288582563400269\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8607858419418335\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8876619935035706\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7814610600471497\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7310599088668823\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6951971650123596\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7038968205451965\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7425435781478882\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8586007952690125\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8359272480010986\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6805426478385925\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.807083010673523\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7642452716827393\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5962562561035156\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6532134413719177\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8672266006469727\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8091357350349426\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6105321645736694\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6361611485481262\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7741960287094116\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7960007786750793\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7261320948600769\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9335431456565857\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6296613812446594\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6415166258811951\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5561279654502869\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5881645679473877\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7259937524795532\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7266526818275452\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6958330273628235\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6530312299728394\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7020702362060547\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6955787539482117\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7095478177070618\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6724024415016174\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8050794005393982\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7588730454444885\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6838642954826355\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.66943359375\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7367894053459167\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6354208588600159\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7014345526695251\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6471930146217346\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6384646892547607\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7082061767578125\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6997640132904053\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7234336137771606\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7151386737823486\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6851578950881958\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6396371126174927\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6367985606193542\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6655578017234802\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7650653123855591\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6674275994300842\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5845091938972473\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.790604293346405\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6191418170928955\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8083706498146057\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7198691964149475\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6265342831611633\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6988738179206848\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6299399733543396\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6677201390266418\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7425108551979065\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6580296158790588\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6060143709182739\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7223990559577942\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.715259313583374\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.663292646408081\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6509235501289368\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6982961297035217\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6495600342750549\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6850663423538208\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7074097990989685\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6074899435043335\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6726586222648621\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6834226846694946\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6801843643188477\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4769238829612732\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7701537609100342\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7249630689620972\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6705808639526367\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8185251951217651\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6956760883331299\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6625246405601501\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6934100985527039\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.463955283164978\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7256917953491211\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8132244348526001\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6533656120300293\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6662352681159973\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7591578960418701\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6007198691368103\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7359532117843628\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7230848670005798\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7671968936920166\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8059775829315186\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6824593544006348\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5545682907104492\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7295079231262207\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6554607152938843\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6420844793319702\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6672539114952087\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7087322473526001\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.707010805606842\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7001417875289917\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6673957705497742\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.65703284740448\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6293047070503235\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5985282063484192\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5670437216758728\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6958364844322205\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5834337472915649\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6768471002578735\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6268647909164429\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6623878479003906\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6498363614082336\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5941511392593384\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5661863684654236\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7348366975784302\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6809309720993042\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7275083065032959\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6576501131057739\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6324656009674072\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.562545120716095\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6804089546203613\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6179824471473694\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.704608142375946\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6922093033790588\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6150916218757629\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5724018812179565\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6001462936401367\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6290816068649292\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.757568359375\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6415742635726929\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7143979668617249\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.46797606348991394\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6767104864120483\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8443566560745239\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6739183068275452\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5168136954307556\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5663835406303406\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6100613474845886\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7155201435089111\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7323613166809082\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7345414161682129\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6238241195678711\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.588930070400238\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5047199726104736\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6523731350898743\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6196218729019165\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6736908555030823\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5239097476005554\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7105349898338318\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6870828866958618\n",
            "Batch Size of Tensor Data: 80\n",
            "Total Loss: 44610.370038986206\n",
            "Correct Predictions: 35988\n",
            "Epoch: [1/5], Loss: 0.8922, Accuracy: 0.7198\n",
            "Loss of Item: 0.6496888399124146\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.48366114497184753\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6841400265693665\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6449612379074097\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6821046471595764\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6682026982307434\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5622355341911316\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5827540159225464\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7484622597694397\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6767417788505554\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5873562097549438\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5864941477775574\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6457502245903015\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7006376385688782\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7092107534408569\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.663335919380188\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7240607738494873\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.75111323595047\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6695026159286499\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8252657055854797\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7016103267669678\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.619464635848999\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6355568170547485\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6350259780883789\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6321712136268616\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6501461267471313\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5645249485969543\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6132006049156189\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7702264189720154\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6953492164611816\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5773383975028992\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.558693528175354\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5548546314239502\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5977907180786133\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6740853786468506\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6725572943687439\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6450855135917664\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6797159314155579\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6702300310134888\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5338846445083618\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7101192474365234\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5828302502632141\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7875339984893799\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7235537171363831\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6678481101989746\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7006063461303711\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6313542723655701\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6285679340362549\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6621193289756775\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6002742052078247\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6335341930389404\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5991873145103455\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8125548362731934\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6100943684577942\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6474708318710327\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.68010014295578\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.757301390171051\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6602090001106262\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6480749845504761\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6146292686462402\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.62171870470047\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5306538343429565\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6255109906196594\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5978065133094788\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6382918953895569\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5441460013389587\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6360411047935486\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.598816454410553\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6272714734077454\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6263684630393982\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7618783712387085\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8648983836174011\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6997405290603638\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5141608119010925\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6385202407836914\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5770266652107239\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6873835325241089\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5979392528533936\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6555488109588623\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5408284068107605\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6151898503303528\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6064608097076416\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6359087228775024\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5880129933357239\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5850025415420532\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7622045278549194\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6669794321060181\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6521164178848267\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7419725656509399\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5756062269210815\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6813979744911194\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5934721231460571\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6515172719955444\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6314841508865356\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5780307650566101\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.571591317653656\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7662431001663208\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6313694715499878\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6412959098815918\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6564269661903381\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5030699372291565\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7845802307128906\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6316768527030945\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5273576378822327\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6000645160675049\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5623292922973633\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5000861287117004\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6007245182991028\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7014859318733215\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6695725917816162\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6527268290519714\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7090528607368469\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6613022089004517\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6258007287979126\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7832149863243103\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6947112679481506\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6763908267021179\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5843297243118286\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6481213569641113\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5542652010917664\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6609635949134827\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6441768407821655\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7300527691841125\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.530637800693512\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6340367197990417\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6071860194206238\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6227288246154785\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5863893032073975\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5108115077018738\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6278044581413269\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7156508564949036\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5194925665855408\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8912159204483032\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6440050601959229\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6577176451683044\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6734665632247925\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6130917072296143\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7090405225753784\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.590898334980011\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5358568429946899\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5645676255226135\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6266231536865234\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6390841007232666\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7042924165725708\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5845925211906433\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5802059173583984\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4782536029815674\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.49418291449546814\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6249943375587463\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6620652079582214\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6674623489379883\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6617713570594788\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.536558985710144\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5867847800254822\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5615237355232239\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7528504133224487\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6828869581222534\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5729964971542358\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6430566310882568\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7515512704849243\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6466966867446899\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6181446313858032\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5137519240379333\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5795615315437317\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5895737409591675\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.605693519115448\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6624614596366882\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7832039594650269\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7255437970161438\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6680530309677124\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5232815146446228\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5312910676002502\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6086030602455139\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6924269199371338\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6315949559211731\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6855103373527527\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6821375489234924\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6122026443481445\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5742655396461487\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4998331367969513\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6648280024528503\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6487605571746826\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5526965260505676\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5579673051834106\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6502670645713806\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.47946345806121826\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6571252942085266\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7208365201950073\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4895055294036865\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5856249928474426\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5128242373466492\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6735710501670837\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6066269278526306\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6093877553939819\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5459295511245728\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5879729986190796\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7314906716346741\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7240694165229797\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5410058498382568\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5976356267929077\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5013402104377747\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5713533163070679\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5815047025680542\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.685213565826416\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6004458665847778\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5803093910217285\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6043773889541626\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.681642472743988\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6462840437889099\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5940262675285339\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6058269143104553\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5934467911720276\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6083804368972778\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5362515449523926\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6671401858329773\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6953932046890259\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6112802624702454\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4510483741760254\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.49975618720054626\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6108285784721375\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6306307911872864\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6688095331192017\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6784099340438843\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6440817713737488\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6132065057754517\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6446825861930847\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6365342140197754\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5379814505577087\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.48616597056388855\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6563450694084167\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5589857697486877\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4897772967815399\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6103968024253845\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.600912868976593\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.768203616142273\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6007380485534668\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.46000951528549194\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.46578189730644226\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.656845211982727\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5297870635986328\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6541538238525391\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5878110527992249\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8058763742446899\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6849387884140015\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6245123147964478\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6002299785614014\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5991034507751465\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5673325657844543\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5198683142662048\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6515126824378967\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5569033622741699\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5444342494010925\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5274969935417175\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5268443822860718\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7008540630340576\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.762366533279419\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5447636246681213\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.58857262134552\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5516971945762634\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5795015096664429\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6465156674385071\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5416474342346191\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4712529480457306\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5979645252227783\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6317815184593201\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6182752251625061\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6579604744911194\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5934811234474182\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.713297963142395\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5796060562133789\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5651900768280029\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5260114073753357\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6327470541000366\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5633087158203125\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6193094253540039\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7010491490364075\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6530255675315857\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.626012921333313\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7182275056838989\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7058534622192383\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.639981746673584\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5519500970840454\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.772611141204834\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5445062518119812\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7090161442756653\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5605167746543884\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5758956074714661\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5258495807647705\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.532286524772644\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5172489881515503\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5470163822174072\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.45061826705932617\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4801706075668335\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5539017915725708\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6550282835960388\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5791025757789612\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5464658141136169\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.535320520401001\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6050006151199341\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6716496348381042\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7862461805343628\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5190415978431702\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5238698720932007\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5924969911575317\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6046270132064819\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.45514753460884094\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5535486340522766\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6856682300567627\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7245440483093262\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6776408553123474\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6902646422386169\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6755523681640625\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6822397112846375\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6148848533630371\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.621852695941925\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5948375463485718\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6926875114440918\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6279730796813965\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6557909250259399\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6184948682785034\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5778629779815674\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5919482707977295\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5725156664848328\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6814970374107361\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7871944308280945\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5880470275878906\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5478461384773254\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.592621386051178\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5533602237701416\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.60965496301651\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5052667856216431\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9362868070602417\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7091317772865295\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6772887110710144\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6093109846115112\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.556717038154602\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6976363658905029\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6656395196914673\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5290464758872986\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6441547274589539\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7956257462501526\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5948546528816223\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5681700706481934\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6344025731086731\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5997456312179565\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6534034013748169\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6217644810676575\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6210438013076782\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6444106101989746\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6223189830780029\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6078211665153503\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6142985820770264\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6563745737075806\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6511898040771484\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5519863963127136\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6661109924316406\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4774468243122101\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6951247453689575\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6254667639732361\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6517167091369629\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5942062139511108\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7626072764396667\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4974689483642578\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7440512180328369\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6016802787780762\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5927243828773499\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7882925271987915\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6007097363471985\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6321678757667542\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6528302431106567\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6680809855461121\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5776798725128174\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5775284767150879\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5187996625900269\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7320014238357544\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6507633328437805\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6125638484954834\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5488802790641785\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6336576342582703\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5957460403442383\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5969745516777039\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7467086315155029\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.648070752620697\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.45737916231155396\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6355717182159424\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5776657462120056\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6260157823562622\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5462182760238647\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6806528568267822\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6242358088493347\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5527125597000122\n",
            "Batch Size of Tensor Data: 80\n",
            "Total Loss: 31211.066705703735\n",
            "Correct Predictions: 39556\n",
            "Epoch: [2/5], Loss: 0.6242, Accuracy: 0.7911\n",
            "Loss of Item: 0.567015528678894\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7918792366981506\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5297282934188843\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5244438052177429\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.42322012782096863\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6029019951820374\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5332813858985901\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5666578412055969\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4705013036727905\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.627690851688385\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5428497791290283\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5055387020111084\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7749245762825012\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4934992492198944\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6592156887054443\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6715375781059265\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5478788018226624\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4812975823879242\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5516119599342346\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5903615355491638\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6538952589035034\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5162637829780579\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6347792148590088\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6603614687919617\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.579443097114563\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6657004356384277\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.631011962890625\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5699640512466431\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5409950017929077\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6537905335426331\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5896040797233582\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6768102049827576\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.674865186214447\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.49986299872398376\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5685868263244629\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5546833872795105\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4880472421646118\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5846331119537354\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6052500605583191\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6741577386856079\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5952746272087097\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5031372308731079\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5071890354156494\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5510731935501099\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5362804532051086\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7438889741897583\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.49398574233055115\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5770238041877747\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5166390538215637\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7468616366386414\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5439899563789368\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.48276782035827637\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6543750762939453\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5701341032981873\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.634045422077179\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6951585412025452\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6533918976783752\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5178835391998291\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.56132972240448\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.41363444924354553\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.49463939666748047\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.755075216293335\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.43644434213638306\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6573283672332764\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5101742744445801\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7545634508132935\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5534758567810059\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5557664036750793\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5919105410575867\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.47808560729026794\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6800926327705383\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7071243524551392\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6207516193389893\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.539003312587738\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6460171937942505\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5698665380477905\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5218186378479004\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5444356203079224\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.48438677191734314\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.41925641894340515\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5879905223846436\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.567640483379364\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.564974308013916\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.46184879541397095\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7042967677116394\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5931828022003174\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6154800653457642\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5686565041542053\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6246792674064636\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7110220193862915\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6368678212165833\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5636289715766907\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4814787805080414\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7276710271835327\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6713430881500244\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5131738781929016\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5981960892677307\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6403697729110718\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4916648268699646\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6105054020881653\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6329347491264343\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5605687499046326\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5590959787368774\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6758524775505066\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5550000071525574\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6586556434631348\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.46121126413345337\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5285987257957458\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6179173588752747\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6589208841323853\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6397654414176941\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.659397304058075\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.616248607635498\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.46048325300216675\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6955650448799133\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5689792633056641\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6293407678604126\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6065082550048828\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.603156566619873\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6379958391189575\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5452383756637573\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5952854156494141\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5753607153892517\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7654653787612915\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5232008099555969\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6698310375213623\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5592294335365295\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6598732471466064\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6307517886161804\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5604661107063293\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.53144371509552\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5412705540657043\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5606254935264587\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6532735824584961\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6377172470092773\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6895341277122498\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6663531064987183\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5498200058937073\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.592939019203186\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6778022646903992\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5392942428588867\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6160281896591187\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4932413399219513\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7726441621780396\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.616496205329895\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6005436182022095\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6234254240989685\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5032249689102173\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4107285141944885\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5369668006896973\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.514120876789093\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.48486730456352234\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5191892981529236\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.753655195236206\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5114144682884216\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5624375343322754\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6894311904907227\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6106765270233154\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5005868673324585\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6135964393615723\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6639463305473328\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3425460755825043\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6187723875045776\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6018984317779541\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5131207704544067\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6242237687110901\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6265529990196228\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.40713343024253845\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6623196005821228\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4627091884613037\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5348323583602905\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5929404497146606\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5978763699531555\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4954105615615845\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6103464961051941\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5743669867515564\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6543803215026855\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6523503661155701\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5841732621192932\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5666017532348633\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.46959102153778076\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5991034507751465\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6090535521507263\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5738286972045898\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6324782371520996\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.606434166431427\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6034952402114868\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.41689014434814453\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6236311197280884\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5041953921318054\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5014623999595642\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6596532464027405\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6546748876571655\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6691980361938477\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5063708424568176\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6669201850891113\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6183649301528931\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5592963099479675\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6554042100906372\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.48796072602272034\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.556760847568512\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6178566217422485\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5577309727668762\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5819694995880127\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5705133080482483\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5419960021972656\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5273008346557617\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5582323670387268\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5806961059570312\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.49233996868133545\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4636041522026062\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6674427390098572\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5653782486915588\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.54288250207901\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6100875735282898\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5639464855194092\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5777236223220825\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.46691185235977173\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6971219778060913\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.723279595375061\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6359336972236633\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5583513379096985\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5712910890579224\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7301791310310364\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5455397367477417\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5030166506767273\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4845859110355377\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6298422813415527\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6520207524299622\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5045956969261169\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6090936660766602\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6755386590957642\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7249364852905273\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6044094562530518\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6414702534675598\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.46215158700942993\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.47187384963035583\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6450274586677551\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.591535210609436\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.592494785785675\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5807967185974121\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5966311693191528\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4967918395996094\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5739070177078247\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4595549404621124\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6252020597457886\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.585369884967804\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5247204899787903\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5220245122909546\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6851621270179749\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.43477338552474976\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4694029688835144\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6871403455734253\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5371443629264832\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6069145798683167\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5781571865081787\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5805318355560303\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.42921313643455505\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.661725640296936\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6708065271377563\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5833698511123657\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5195621252059937\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6285951733589172\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.586966872215271\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5538485646247864\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.710797131061554\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7605477571487427\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6067801713943481\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5550442934036255\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5628448724746704\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4654214382171631\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4881265163421631\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6335757374763489\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.46457377076148987\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7491090893745422\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5584578514099121\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5719566941261292\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5825023055076599\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.573710560798645\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6063128113746643\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6385673880577087\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6393048167228699\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4941304624080658\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5959374904632568\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.588811993598938\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6461586952209473\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6132198572158813\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7190647721290588\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5282756686210632\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4911063313484192\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5371460318565369\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6565232276916504\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5430351495742798\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6938412189483643\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5829883813858032\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5423523783683777\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5324520468711853\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7902474403381348\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.49663087725639343\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4965294897556305\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.42748919129371643\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5661770105361938\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.630335807800293\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5140889286994934\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.538744330406189\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6625733971595764\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5674591660499573\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5370506048202515\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5297514796257019\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6642389297485352\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4954850673675537\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4833473563194275\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6462893486022949\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4354884624481201\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5365771055221558\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.598652720451355\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6359314322471619\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6325966119766235\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7438898086547852\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5190359354019165\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.60943204164505\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5965127348899841\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.644042432308197\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5979380011558533\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6790773868560791\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.500738263130188\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7499863505363464\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5576437711715698\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6501824259757996\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5347904562950134\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.48841890692710876\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5898167490959167\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.49104395508766174\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.48503199219703674\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6051644086837769\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.49723872542381287\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5511802434921265\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.586895763874054\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.47697651386260986\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5217944979667664\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5617732405662537\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5476574301719666\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6052466034889221\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7248300313949585\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7187781929969788\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5373809933662415\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6054485440254211\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6474665403366089\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5104389190673828\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6122285723686218\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5347093343734741\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6148555278778076\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5933643579483032\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.574329674243927\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5260805487632751\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7173048257827759\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7991481423377991\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5826921463012695\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6120507717132568\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7171845436096191\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5610010027885437\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5532849431037903\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8568471670150757\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7123938202857971\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6288688778877258\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4769001603126526\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5275089740753174\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5390295386314392\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8483374118804932\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5185141563415527\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5418159365653992\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5718047618865967\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5520879030227661\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6673173308372498\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4647962749004364\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6002295017242432\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5168057084083557\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5189694166183472\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.604483425617218\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5910413861274719\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.45340514183044434\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6051011085510254\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6297194957733154\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.49615874886512756\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5291449427604675\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5643481612205505\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.47478875517845154\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5443270802497864\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5928110480308533\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6179291009902954\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6979573369026184\n",
            "Batch Size of Tensor Data: 80\n",
            "Total Loss: 29213.514477729797\n",
            "Correct Predictions: 39964\n",
            "Epoch: [3/5], Loss: 0.5843, Accuracy: 0.7993\n",
            "Loss of Item: 0.6055142879486084\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4852037727832794\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5967621803283691\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5648188591003418\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7375262379646301\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5107546448707581\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7528447508811951\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4779496490955353\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5307799577713013\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5358989834785461\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6825730204582214\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5966722369194031\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.43402549624443054\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.521857500076294\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5782274007797241\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5729571580886841\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5844624042510986\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.579197347164154\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5286675691604614\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5458146333694458\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6532827019691467\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5819605588912964\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.657495379447937\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6526350975036621\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6010307669639587\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6343567371368408\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7524662613868713\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5975349545478821\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5490567088127136\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5960196852684021\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5041999220848083\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7160204648971558\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5815110206604004\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6485379934310913\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6048287153244019\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5740730166435242\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6310932040214539\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6103093028068542\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5332368612289429\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6170892119407654\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7544817924499512\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5468369126319885\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6828673481941223\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4819660186767578\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6291242837905884\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6319869756698608\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5812845230102539\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5841833353042603\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6903035044670105\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6165277361869812\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6407042741775513\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5687586069107056\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.585598886013031\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7108941078186035\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6227452754974365\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6294522285461426\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7062798142433167\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.438949316740036\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5173972249031067\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5222402811050415\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5036154389381409\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5126429200172424\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6329578757286072\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6143073439598083\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6931913495063782\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5190746784210205\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6599690914154053\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.494005411863327\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5586447715759277\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4797773063182831\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5876002907752991\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6208525896072388\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5662254095077515\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7351865768432617\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5516198873519897\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5096513032913208\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5587291121482849\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5725581049919128\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6103737354278564\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6187600493431091\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5632743239402771\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7491323947906494\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5711148977279663\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5614438056945801\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6041463613510132\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.79637610912323\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5610553622245789\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5762340426445007\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5721347332000732\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5932145118713379\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6376694440841675\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5898257493972778\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5204028487205505\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5504652857780457\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5481607913970947\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5495193600654602\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5514034032821655\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5472716689109802\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5727017521858215\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4656231105327606\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5734775066375732\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6230918169021606\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6876437664031982\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6274011135101318\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5263880491256714\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5176816582679749\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.44048482179641724\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.463640421628952\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5869184136390686\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.46252208948135376\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6347647905349731\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.559396505355835\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.39556851983070374\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.554125189781189\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.43548646569252014\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5272669196128845\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5169609189033508\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.64300936460495\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6308806538581848\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7270525097846985\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.47036048769950867\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6658679246902466\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6351824402809143\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6300393342971802\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5277138948440552\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6048604249954224\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5220188498497009\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5909960865974426\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6791093349456787\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6316794753074646\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.45145145058631897\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6266589760780334\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5045962333679199\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5136617422103882\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.49694693088531494\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4683496356010437\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.42341482639312744\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7009956240653992\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5594647526741028\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6384873986244202\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.665887176990509\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5900537371635437\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6320667266845703\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5267145037651062\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.559411883354187\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.48533543944358826\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5803030133247375\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6045034527778625\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5423834919929504\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5849117636680603\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5085719227790833\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6426404118537903\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5336036682128906\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5340261459350586\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6207892298698425\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.49909883737564087\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.49540746212005615\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6268356442451477\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6844807863235474\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7285297513008118\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7457612752914429\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6043694615364075\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5619722008705139\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.46125441789627075\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.41063717007637024\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6008181571960449\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5194599032402039\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6185904741287231\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.44179603457450867\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6363681554794312\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5031664967536926\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5829792022705078\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5194867253303528\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5415811538696289\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7821985483169556\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.503783106803894\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6452902555465698\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.606631875038147\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5817200541496277\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5829773545265198\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5149771571159363\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.45052570104599\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4408087432384491\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.46651479601860046\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4495035409927368\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.44396284222602844\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5914033055305481\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.627899169921875\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6406778693199158\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.48330363631248474\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6888658404350281\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5010790824890137\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.602551281452179\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5372739434242249\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.39016950130462646\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5999253988265991\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7647493481636047\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6063482165336609\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.45298469066619873\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5388549566268921\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5957012176513672\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5533073544502258\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4966929256916046\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5631921291351318\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6917140483856201\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4064805209636688\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4375351667404175\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5981349349021912\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5986872911453247\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5956372618675232\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5876481533050537\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.512627363204956\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5710778832435608\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.40614980459213257\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5348009467124939\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5446209907531738\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6170898675918579\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6044963002204895\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4929373562335968\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5290766954421997\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5120580196380615\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5497254729270935\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.496561735868454\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.554669201374054\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5867611765861511\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4979400932788849\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6160423755645752\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6249781847000122\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.557618260383606\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6116414666175842\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.450676292181015\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5729788541793823\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6149595975875854\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6208314895629883\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.45705994963645935\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4671967327594757\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5802062749862671\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.48220905661582947\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5584267377853394\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5035671591758728\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5665015578269958\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5621067881584167\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6134313941001892\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6076288223266602\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.702613890171051\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5920507907867432\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.37260016798973083\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.524556040763855\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.49713972210884094\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.49560654163360596\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6947776079177856\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5184981822967529\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4556446075439453\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5064988136291504\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3606121242046356\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6248685717582703\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6117887496948242\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5291045904159546\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5802698731422424\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5616322159767151\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6524375081062317\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6558560729026794\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.41291868686676025\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5848397612571716\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4519205391407013\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4489341080188751\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7472741007804871\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6229334473609924\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6167783141136169\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5442414879798889\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6425374746322632\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5374411344528198\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7188567519187927\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4679632782936096\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5735989809036255\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4919660985469818\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4747227430343628\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4590713381767273\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6396265029907227\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6887763738632202\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5162166357040405\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5464663505554199\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6116806268692017\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.625297486782074\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.448803573846817\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5304027795791626\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6818568706512451\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5226396322250366\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4963667690753937\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5332532525062561\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5456777811050415\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5746106505393982\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5663794875144958\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.697477400302887\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7112253904342651\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.46577462553977966\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6445213556289673\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.40954604744911194\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5741442441940308\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5834172964096069\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5796246528625488\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4597453474998474\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6262036561965942\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6469621062278748\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6204954981803894\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5037518739700317\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5860036611557007\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7395058870315552\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6070088148117065\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6882157921791077\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5932692289352417\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5515340566635132\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.38398072123527527\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5784220099449158\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5722712278366089\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5099498629570007\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6026819348335266\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5790004134178162\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7026529312133789\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7089099884033203\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6222478747367859\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.46796172857284546\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6645048260688782\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6191457509994507\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5674737095832825\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.555338442325592\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5698010921478271\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6318339705467224\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5609406232833862\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4859095811843872\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5817580223083496\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5961529612541199\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4595279097557068\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4580136239528656\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4961616098880768\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.44636592268943787\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5044108033180237\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.46060115098953247\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5066596865653992\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6485826969146729\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5453429222106934\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5595030784606934\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6594624519348145\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.595913290977478\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5101430416107178\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6989147067070007\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7273797988891602\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5330085158348083\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4120033085346222\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7392045855522156\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5615230798721313\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7084136605262756\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6730977892875671\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6598873734474182\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4574536979198456\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5847705602645874\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5709025263786316\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.39582231640815735\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4927044212818146\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6230775713920593\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6404151320457458\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5173143744468689\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5267236232757568\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.44629356265068054\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5456687808036804\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4870937466621399\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.43797388672828674\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.569888174533844\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5207571983337402\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5557659864425659\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6152617335319519\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4240405857563019\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5894200205802917\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6525387167930603\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5594033598899841\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5727499127388\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5896217823028564\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.46525833010673523\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7658764123916626\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4862346351146698\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6064670085906982\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5039059519767761\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5578259825706482\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5368523597717285\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5584546327590942\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5401245355606079\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5717560052871704\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5681921243667603\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6296342611312866\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.48766207695007324\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6526230573654175\n",
            "Batch Size of Tensor Data: 80\n",
            "Total Loss: 28473.140573501587\n",
            "Correct Predictions: 40168\n",
            "Epoch: [4/5], Loss: 0.5695, Accuracy: 0.8034\n",
            "Loss of Item: 0.37891846895217896\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6843346953392029\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4976294934749603\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4992930293083191\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5692692995071411\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5481622815132141\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7003926634788513\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5559777021408081\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3703387379646301\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6239021420478821\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6547335982322693\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5551069974899292\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5075310468673706\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.49426156282424927\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.48795509338378906\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.466481477022171\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5602211952209473\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3705807328224182\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5997828245162964\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4813368022441864\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5161805152893066\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.755180299282074\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5895485877990723\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4943636357784271\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.576181173324585\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5222577452659607\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6349889039993286\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4676866829395294\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4393635094165802\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5020776987075806\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.41784733533859253\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6105840802192688\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5283937454223633\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7458436489105225\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.46481338143348694\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5462620854377747\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.43629252910614014\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.48988333344459534\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5441243052482605\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5057238340377808\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5532413721084595\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5697897672653198\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6672453284263611\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6598384380340576\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5897824168205261\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6134671568870544\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.435546875\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5523280501365662\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.483720600605011\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.501425564289093\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5974487662315369\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6323356032371521\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.628903865814209\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5865237712860107\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5161199569702148\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5341228246688843\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5372183322906494\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7088731527328491\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5292222499847412\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5779279470443726\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5907527804374695\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.43164509534835815\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6807411909103394\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5130746364593506\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5919777750968933\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6013901829719543\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.45634567737579346\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5642480850219727\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5310533046722412\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4729808568954468\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5903832316398621\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5461430549621582\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5434820055961609\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4647202491760254\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4686805307865143\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.47487664222717285\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5367594957351685\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5225017070770264\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4676855206489563\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6217296719551086\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6276117563247681\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6322954297065735\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5452188849449158\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5118495225906372\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5618931651115417\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5631995797157288\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5976251363754272\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5268769860267639\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5753151774406433\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5087862610816956\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7714390158653259\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6780611276626587\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5522928833961487\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.48515835404396057\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.613471269607544\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.672307014465332\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6891404986381531\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5137925744056702\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6531745195388794\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5272784233093262\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5197833180427551\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.49820107221603394\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6705167293548584\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6709166169166565\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5880957841873169\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4405785799026489\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5042406916618347\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6568263173103333\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6689765453338623\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4588628113269806\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6665704250335693\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5140567421913147\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4700859487056732\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4711305797100067\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6654156446456909\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.586587131023407\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6403852105140686\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5954840779304504\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.692190408706665\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6063916087150574\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6085207462310791\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5889188051223755\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.49023711681365967\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5303090810775757\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.48964303731918335\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5346742272377014\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5750803351402283\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5945096015930176\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6852812767028809\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6258253455162048\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5986015200614929\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5866524577140808\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5470647215843201\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5125025510787964\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4641358256340027\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6434880495071411\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6417409181594849\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6492344737052917\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.47571977972984314\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6284129023551941\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5983433127403259\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7123865485191345\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5856049656867981\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4142068922519684\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6664183139801025\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.47881051898002625\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5971306562423706\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5690309405326843\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.49813053011894226\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6122885346412659\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6464376449584961\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.787123441696167\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4810687303543091\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.496035099029541\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.499815970659256\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5538759827613831\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5645785927772522\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7734805345535278\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5535012483596802\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4777810871601105\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.48722296953201294\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5891244411468506\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5149743556976318\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5720993876457214\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5931667685508728\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6326740980148315\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4718225300312042\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4771052300930023\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5534738302230835\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6373952627182007\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7109564542770386\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6389046907424927\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5361828804016113\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5819770097732544\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.57437664270401\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5242688655853271\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.48512354493141174\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6121768355369568\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5439589023590088\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5583339333534241\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.45272591710090637\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5166950225830078\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5197470784187317\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4931255280971527\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7051151990890503\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4632563889026642\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5755773782730103\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5966940522193909\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4223300814628601\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4415556490421295\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4095038175582886\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5728779435157776\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.43323400616645813\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4846141040325165\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.514639139175415\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.42516428232192993\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.45368698239326477\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4870920777320862\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7501946687698364\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5570205450057983\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5367032885551453\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5050719976425171\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.37421345710754395\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6212682723999023\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7191525101661682\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5292539000511169\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6617157459259033\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.49993348121643066\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4766748249530792\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6894664168357849\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5009962320327759\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4631654620170593\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6494275331497192\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6134815812110901\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5216128826141357\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5055604577064514\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5407894849777222\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.49865928292274475\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6480833888053894\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.664501965045929\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6020578145980835\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5288599133491516\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.655373752117157\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.40136006474494934\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5062414407730103\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.56751549243927\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5549360513687134\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5234975814819336\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5725686550140381\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6006818413734436\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5654913187026978\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6709302663803101\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.48582255840301514\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7145090699195862\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5610037446022034\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4876568615436554\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6634583473205566\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5196973085403442\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.529282808303833\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5075463652610779\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.591725766658783\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6014233231544495\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5422571301460266\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.44538766145706177\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6380411982536316\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.635013222694397\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5457907915115356\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5618972182273865\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5357269048690796\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5079237818717957\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6238175630569458\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4860254228115082\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.48212385177612305\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4143650531768799\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.45765769481658936\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5275421142578125\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5133609175682068\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4662247598171234\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5807841420173645\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5738984942436218\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5110912322998047\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.49752962589263916\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.47914570569992065\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4619777798652649\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5148932933807373\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.37695854902267456\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.495598703622818\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5099971294403076\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6661852598190308\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5336959958076477\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.46713030338287354\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4189344346523285\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5506033897399902\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5725690126419067\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5486750602722168\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4061891734600067\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.568581223487854\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.45126351714134216\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6059953570365906\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5757118463516235\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6624476909637451\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5979695916175842\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6680448055267334\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.542986273765564\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6403108835220337\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4749406576156616\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6318210959434509\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6496959328651428\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6364471316337585\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5052114725112915\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5321409106254578\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.640213131904602\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.49738967418670654\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5341778993606567\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6125187277793884\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5687311887741089\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5608283877372742\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4956933259963989\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5936405658721924\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5411231517791748\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6237760782241821\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6979101300239563\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4609565734863281\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6296907067298889\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5291075110435486\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5088316202163696\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.46153637766838074\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.730463445186615\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6901148557662964\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3999176323413849\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5285505056381226\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4022649824619293\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5352401733398438\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4706106185913086\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4892203211784363\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.62943434715271\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5633729696273804\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.47311919927597046\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.42012348771095276\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.592077910900116\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6134041547775269\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.52564537525177\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.41832801699638367\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5734077095985413\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4728344976902008\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.49683767557144165\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5666513442993164\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5769968628883362\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5308363437652588\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4688382148742676\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5812982320785522\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5315806865692139\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5199518203735352\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6957045793533325\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.48292526602745056\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7877622246742249\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.47074833512306213\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6276198029518127\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.48212185502052307\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5571646690368652\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5393019914627075\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6101887822151184\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.549901008605957\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4664117693901062\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5220432281494141\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6402443051338196\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6060306429862976\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5981010794639587\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5354700684547424\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.587357223033905\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.49036139249801636\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5217760801315308\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4742206633090973\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5233813524246216\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6226539611816406\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5267801284790039\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5829763412475586\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6633586287498474\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5161227583885193\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5191138386726379\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6356332898139954\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6271663308143616\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.44908347725868225\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.630379319190979\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6355898976325989\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.46674028038978577\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5864503383636475\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5310009717941284\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7298824191093445\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5246105790138245\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6516764760017395\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.508536696434021\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5335673689842224\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5089637637138367\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6596763134002686\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4674820005893707\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5580666661262512\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.46765780448913574\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5918338298797607\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5657474398612976\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5384286642074585\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5878989100456238\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.45908111333847046\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6118754744529724\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6177263855934143\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5137601494789124\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5525674819946289\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6082845330238342\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.587861955165863\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5713021159172058\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.680823802947998\n",
            "Batch Size of Tensor Data: 80\n",
            "Total Loss: 27753.4725189209\n",
            "Correct Predictions: 40393\n",
            "Epoch: [5/5], Loss: 0.5551, Accuracy: 0.8079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation Function\n",
        "def evaluate_model(model, test_loader):\n",
        "  model.eval()      #Set Model to evaluation mode\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():           #used to disable gradient computation within a block of code\n",
        "    for data, target in test_loader:\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      output = model(data)\n",
        "      print(f'Output: {output}')\n",
        "      loss = criterion(output, target)\n",
        "      test_loss += loss.item() * data.size(0)\n",
        "      pred = output.argmax(dim=1)       #retrieve the indices of the maximum values along a specified dimension in a tensor.\n",
        "      print(f'Prediction: {pred}')\n",
        "      print(f'Target: {target}')\n",
        "      correct += pred.eq(target).sum().item()\n",
        "\n",
        "  avg_loss = test_loss / len(test_loader.dataset)\n",
        "  accuracy = correct / len(test_loader.dataset)\n",
        "\n",
        "  print(f'Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "id": "IqItyWjchkms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate the model\n",
        "evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkSyZaXx4z3z",
        "outputId": "243b6e7d-1371-425e-cbaa-bbe8ba2a2f8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: tensor([[-4.8580, -5.4461, -3.3562,  ..., -5.2471, -6.4713, -5.7082],\n",
            "        [ 0.0781, -0.9834, -1.7896,  ..., -3.8417,  3.2115, -7.1185],\n",
            "        [-0.8739, -2.5350, -5.0243,  ..., -4.3409,  4.3292, -0.0714],\n",
            "        ...,\n",
            "        [-1.0129, -5.3064,  1.5976,  ..., -3.5764, -3.4406, -7.2658],\n",
            "        [-1.9168, -5.9966, -3.3778,  ..., -3.4628,  6.7709, -3.8583],\n",
            "        [-4.7395, -4.5417, -1.6218,  ..., -4.6037, -4.9550, -6.5695]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 8, 8, 0, 6, 6, 9, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6, 7, 0, 4, 9,\n",
            "        5, 3, 3, 0, 9, 6, 6, 5, 4, 5, 9, 2, 4, 9, 9, 5, 0, 6, 5, 3, 0, 9, 3, 9,\n",
            "        7, 6, 9, 8, 0, 3, 8, 8, 7, 3, 5, 3, 7, 3, 6, 3, 6, 2, 1, 0, 3, 7, 2, 3,\n",
            "        8, 8, 9, 2, 9, 3, 5, 0, 8, 1, 1, 7, 3, 7, 2, 7, 8, 9, 0, 3, 8, 6, 4, 6,\n",
            "        6, 0, 0, 7, 4, 5, 6, 3, 1, 1, 3, 6, 0, 7, 4, 0, 6, 2, 1, 3, 0, 7, 3, 3,\n",
            "        8, 3, 1, 2, 8, 2, 8, 3], device='cuda:0')\n",
            "Target: tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6, 7, 0, 4, 9,\n",
            "        5, 2, 4, 0, 9, 6, 6, 5, 4, 5, 9, 2, 4, 1, 9, 5, 4, 6, 5, 6, 0, 9, 3, 9,\n",
            "        7, 6, 9, 8, 0, 3, 8, 8, 7, 7, 4, 6, 7, 3, 6, 3, 6, 2, 1, 2, 3, 7, 2, 6,\n",
            "        8, 8, 0, 2, 9, 3, 3, 8, 8, 1, 1, 7, 2, 5, 2, 7, 8, 9, 0, 3, 8, 6, 4, 6,\n",
            "        6, 0, 0, 7, 4, 5, 6, 3, 1, 1, 3, 6, 8, 7, 4, 0, 6, 2, 1, 3, 0, 4, 2, 7,\n",
            "        8, 3, 1, 2, 8, 0, 8, 3], device='cuda:0')\n",
            "Output: tensor([[-5.2622, -7.0640, -1.4886,  ..., -2.9407, -8.2162, -4.4186],\n",
            "        [-0.4732, -4.9391,  1.1136,  ..., -2.4132, -5.6125, -6.5290],\n",
            "        [-3.6011, -7.8597,  1.2731,  ..., -0.2024, -6.9382, -6.6028],\n",
            "        ...,\n",
            "        [-3.6844,  2.9413, -5.5254,  ..., -2.6653, -4.1096,  0.7113],\n",
            "        [-6.2166, -6.3282, -0.8337,  ..., -0.3149, -9.4064, -7.2958],\n",
            "        [-0.7841, -3.6651, -6.2626,  ..., -4.1507, -0.1303, -3.1034]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 2, 4, 1, 8, 9, 3, 2, 9, 7, 2, 8, 6, 5, 6, 7, 8, 2, 6, 5, 3, 3, 8, 4,\n",
            "        6, 0, 0, 3, 3, 9, 3, 4, 0, 1, 3, 6, 0, 4, 8, 4, 5, 8, 9, 9, 9, 8, 9, 9,\n",
            "        3, 7, 3, 9, 0, 5, 2, 4, 3, 8, 6, 3, 7, 8, 5, 8, 7, 1, 4, 2, 8, 8, 7, 6,\n",
            "        5, 1, 8, 7, 1, 3, 0, 5, 7, 9, 5, 4, 5, 9, 8, 0, 7, 9, 8, 2, 7, 5, 9, 4,\n",
            "        3, 9, 3, 4, 3, 6, 5, 1, 3, 8, 8, 0, 4, 9, 5, 3, 1, 1, 8, 9, 0, 3, 1, 9,\n",
            "        2, 3, 5, 3, 9, 1, 4, 8], device='cuda:0')\n",
            "Target: tensor([5, 2, 4, 1, 8, 9, 1, 2, 9, 7, 2, 9, 6, 5, 6, 3, 8, 7, 6, 2, 5, 2, 8, 9,\n",
            "        6, 0, 0, 5, 2, 9, 5, 4, 2, 1, 6, 6, 8, 4, 8, 4, 5, 0, 9, 9, 9, 8, 9, 9,\n",
            "        3, 7, 5, 0, 0, 5, 2, 2, 3, 8, 6, 3, 4, 0, 5, 8, 0, 1, 7, 2, 8, 8, 7, 8,\n",
            "        5, 1, 8, 7, 1, 3, 0, 5, 7, 9, 7, 4, 5, 9, 8, 0, 7, 9, 8, 2, 7, 6, 9, 4,\n",
            "        3, 9, 6, 4, 7, 6, 5, 1, 5, 8, 8, 0, 4, 0, 5, 5, 1, 1, 8, 9, 0, 3, 1, 9,\n",
            "        2, 2, 5, 3, 9, 9, 4, 0], device='cuda:0')\n",
            "Output: tensor([[-6.4603, -7.1487, -3.5355,  ..., -1.1160, -7.8128, -6.2001],\n",
            "        [ 3.0161, -3.5000, -1.5370,  ..., -2.9445, -1.5358, -3.0837],\n",
            "        [ 1.7151, -3.4142, -4.9939,  ..., -2.8158, -0.9129, -7.6712],\n",
            "        ...,\n",
            "        [-2.5551,  0.6342, -6.4112,  ..., -3.2458, -1.7242,  4.1853],\n",
            "        [ 3.9391, -3.0541, -0.1465,  ..., -6.3658, -3.9686, -3.8840],\n",
            "        [ 1.1337,  0.3713, -4.5023,  ..., -2.4903, -0.8966,  3.7436]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 0, 0, 9, 8, 1, 5, 3, 0, 3, 8, 4, 7, 0, 2, 3, 6, 3, 8, 3, 0, 3, 4, 3,\n",
            "        9, 0, 6, 1, 9, 9, 1, 0, 7, 9, 1, 2, 4, 1, 3, 4, 6, 0, 0, 6, 6, 6, 3, 2,\n",
            "        6, 0, 8, 2, 1, 4, 8, 6, 0, 0, 4, 0, 4, 7, 3, 5, 3, 5, 7, 3, 4, 7, 7, 3,\n",
            "        4, 6, 1, 9, 3, 6, 6, 9, 3, 8, 0, 7, 2, 6, 0, 5, 8, 5, 7, 6, 8, 9, 9, 1,\n",
            "        8, 2, 2, 2, 5, 2, 8, 0, 9, 5, 8, 1, 9, 4, 1, 3, 8, 1, 4, 7, 9, 4, 2, 7,\n",
            "        7, 7, 2, 6, 6, 9, 0, 9], device='cuda:0')\n",
            "Target: tensor([3, 0, 0, 9, 8, 1, 5, 7, 0, 8, 2, 4, 7, 0, 2, 3, 6, 3, 8, 5, 0, 3, 4, 3,\n",
            "        9, 0, 6, 1, 0, 9, 1, 0, 7, 9, 1, 2, 6, 9, 3, 4, 6, 0, 0, 6, 6, 6, 3, 2,\n",
            "        6, 1, 8, 2, 1, 6, 8, 6, 8, 0, 4, 0, 7, 7, 5, 5, 3, 5, 2, 3, 4, 1, 7, 5,\n",
            "        4, 6, 1, 9, 3, 6, 6, 9, 3, 8, 0, 7, 2, 6, 2, 5, 8, 5, 4, 6, 8, 9, 9, 1,\n",
            "        0, 2, 2, 7, 3, 2, 8, 0, 9, 5, 8, 1, 9, 4, 1, 3, 8, 1, 4, 7, 9, 4, 2, 7,\n",
            "        0, 7, 0, 6, 6, 9, 0, 9], device='cuda:0')\n",
            "Output: tensor([[-4.9708, -3.9999, -1.7233,  ..., -0.2700, -3.3456, -5.2408],\n",
            "        [ 3.3382, -0.3553, -1.5620,  ..., -3.9375,  2.2195, -4.3811],\n",
            "        [-4.0897, -5.1907, -4.1283,  ...,  3.0693, -5.9843, -4.3107],\n",
            "        ...,\n",
            "        [-4.6493, -5.1265, -0.3735,  ..., -1.6191, -3.7452, -4.6785],\n",
            "        [-4.1197, -7.0493, -4.3593,  ..., -6.1364, -3.5327, -4.6066],\n",
            "        [-5.3209, -6.2072, -0.0512,  ..., -4.5370, -5.7988, -7.2065]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([5, 0, 7, 2, 5, 5, 1, 2, 6, 2, 1, 6, 4, 3, 0, 3, 9, 8, 7, 8, 8, 2, 0, 1,\n",
            "        8, 7, 7, 0, 3, 6, 1, 9, 0, 7, 5, 7, 4, 5, 0, 3, 2, 9, 3, 4, 9, 2, 2, 5,\n",
            "        3, 7, 3, 7, 2, 5, 3, 1, 1, 2, 9, 9, 5, 7, 3, 0, 2, 2, 2, 1, 7, 3, 5, 5,\n",
            "        4, 4, 3, 2, 5, 6, 1, 4, 3, 4, 4, 3, 4, 8, 5, 7, 8, 0, 3, 7, 4, 0, 5, 4,\n",
            "        9, 6, 8, 5, 5, 9, 9, 9, 3, 0, 1, 0, 8, 1, 1, 8, 0, 2, 2, 0, 4, 3, 5, 6,\n",
            "        9, 4, 7, 9, 0, 4, 5, 6], device='cuda:0')\n",
            "Target: tensor([2, 8, 7, 2, 2, 5, 1, 2, 6, 2, 9, 6, 2, 3, 0, 3, 9, 8, 7, 8, 8, 4, 0, 1,\n",
            "        8, 2, 7, 9, 3, 6, 1, 9, 0, 7, 3, 7, 4, 5, 0, 0, 2, 9, 3, 4, 0, 6, 2, 5,\n",
            "        3, 7, 3, 7, 2, 5, 3, 1, 1, 4, 9, 9, 5, 7, 5, 0, 2, 2, 2, 9, 7, 3, 9, 4,\n",
            "        3, 5, 4, 6, 5, 6, 1, 4, 3, 4, 4, 3, 7, 8, 3, 7, 8, 0, 5, 7, 6, 0, 5, 4,\n",
            "        8, 6, 8, 5, 5, 9, 9, 9, 5, 0, 1, 0, 8, 1, 1, 8, 0, 2, 2, 0, 4, 6, 5, 4,\n",
            "        9, 4, 7, 9, 9, 4, 5, 6], device='cuda:0')\n",
            "Output: tensor([[-5.1363, -7.5853, -2.4085,  ..., -3.5454, -6.6216, -6.9803],\n",
            "        [-2.8390,  2.5886, -5.9083,  ..., -1.5678, -3.9766, -1.5151],\n",
            "        [-5.0525, -0.0431, -5.3670,  ..., -3.7845, -5.4642, -4.4361],\n",
            "        ...,\n",
            "        [-3.1585, -2.1056, -0.6167,  ..., -4.6924, -4.5274, -4.6469],\n",
            "        [ 5.0633, -0.8396, -3.8029,  ..., -3.2612,  0.2054, -1.5990],\n",
            "        [-4.6971, -6.0883, -3.1912,  ..., -1.2769, -7.7837, -3.2362]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([6, 1, 1, 3, 8, 9, 7, 8, 5, 7, 8, 7, 0, 5, 4, 0, 4, 6, 9, 3, 9, 5, 6, 6,\n",
            "        6, 6, 9, 0, 1, 7, 6, 7, 5, 9, 1, 6, 3, 8, 5, 7, 8, 5, 9, 4, 6, 4, 3, 2,\n",
            "        0, 4, 6, 2, 2, 3, 9, 7, 8, 2, 3, 7, 1, 5, 6, 6, 8, 9, 7, 5, 4, 8, 8, 4,\n",
            "        0, 9, 3, 4, 8, 9, 6, 9, 2, 6, 1, 4, 7, 3, 3, 3, 9, 5, 0, 2, 1, 6, 4, 2,\n",
            "        3, 9, 6, 1, 8, 8, 5, 8, 6, 6, 2, 1, 7, 0, 1, 4, 7, 1, 9, 4, 4, 1, 2, 5,\n",
            "        6, 3, 7, 6, 8, 3, 0, 5], device='cuda:0')\n",
            "Target: tensor([6, 1, 5, 3, 8, 9, 5, 8, 5, 7, 0, 7, 0, 5, 0, 0, 4, 6, 9, 0, 9, 5, 6, 6,\n",
            "        6, 2, 9, 0, 1, 7, 6, 7, 5, 9, 1, 6, 2, 5, 5, 5, 8, 5, 9, 4, 6, 4, 3, 2,\n",
            "        0, 7, 6, 2, 2, 3, 9, 7, 9, 2, 6, 7, 1, 3, 6, 6, 8, 9, 7, 5, 4, 0, 8, 4,\n",
            "        0, 9, 3, 4, 8, 9, 6, 9, 2, 6, 1, 4, 7, 3, 5, 3, 8, 5, 0, 2, 1, 6, 4, 3,\n",
            "        3, 9, 6, 9, 8, 8, 5, 8, 6, 6, 2, 1, 7, 7, 1, 2, 7, 9, 9, 4, 4, 1, 2, 5,\n",
            "        6, 8, 7, 6, 8, 3, 0, 5], device='cuda:0')\n",
            "Output: tensor([[ -4.9226,  -6.9114,  -0.3583,  ...,  -2.3056,  -5.9560,  -7.5011],\n",
            "        [ -5.8455,  -3.6401,  -2.3498,  ...,  -3.8469,  -3.6462,  -6.5720],\n",
            "        [ -0.4256,  -2.4838,  -1.2161,  ...,  -4.3136,  -1.0010,  -5.1390],\n",
            "        ...,\n",
            "        [ -7.6485,  -8.6995,   6.9331,  ...,  -2.3089,  -6.8716, -10.6130],\n",
            "        [ -1.2104,  -1.1513,  -3.5384,  ...,  -0.4716,  -2.7958,  -1.7483],\n",
            "        [ -2.2080,  -3.5657,  -2.3926,  ...,  -3.1890,  -3.4509,   2.6661]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 3, 0, 7, 9, 1, 3, 4, 4, 5, 3, 9, 3, 6, 9, 9, 1, 1, 4, 1, 9, 4, 7, 4,\n",
            "        3, 0, 9, 0, 1, 3, 6, 3, 6, 3, 0, 8, 6, 9, 0, 2, 9, 6, 4, 8, 9, 6, 9, 6,\n",
            "        3, 0, 3, 2, 3, 7, 8, 3, 8, 2, 7, 5, 7, 2, 5, 8, 7, 4, 2, 9, 8, 8, 6, 0,\n",
            "        8, 7, 4, 3, 3, 8, 4, 9, 7, 8, 8, 1, 8, 0, 1, 2, 3, 5, 7, 0, 7, 9, 1, 4,\n",
            "        9, 0, 1, 3, 2, 1, 0, 7, 9, 7, 7, 6, 2, 3, 9, 2, 9, 1, 2, 2, 6, 8, 2, 1,\n",
            "        3, 6, 2, 0, 1, 2, 4, 9], device='cuda:0')\n",
            "Target: tensor([5, 3, 0, 7, 9, 1, 3, 4, 4, 5, 3, 9, 5, 6, 9, 2, 1, 1, 4, 1, 9, 4, 7, 6,\n",
            "        3, 8, 9, 0, 1, 3, 6, 3, 6, 3, 2, 0, 3, 1, 0, 5, 9, 6, 4, 8, 9, 6, 9, 6,\n",
            "        3, 0, 3, 2, 2, 7, 8, 3, 8, 2, 7, 5, 7, 2, 4, 8, 7, 4, 2, 9, 8, 8, 6, 8,\n",
            "        8, 7, 4, 3, 3, 8, 4, 9, 4, 8, 8, 1, 8, 2, 1, 3, 6, 5, 4, 2, 7, 9, 9, 4,\n",
            "        1, 4, 1, 3, 2, 7, 0, 7, 9, 7, 6, 6, 2, 5, 9, 2, 9, 1, 2, 2, 6, 8, 2, 1,\n",
            "        3, 6, 6, 0, 1, 2, 7, 0], device='cuda:0')\n",
            "Output: tensor([[-10.4336, -11.0944,  -1.0395,  ...,   0.5728, -10.4220,  -8.6035],\n",
            "        [ -1.7827,  -6.8203,  -2.5567,  ...,  -0.0684,  -2.0736,  -5.7682],\n",
            "        [ -2.7648,  -4.7000,  -6.0000,  ...,  -1.7009,  -3.2039,  -6.7983],\n",
            "        ...,\n",
            "        [ -2.2093,  -9.1959,  -0.2720,  ...,  -0.9785,  -8.9529,  -6.7826],\n",
            "        [ -2.4553,   1.0648,  -5.4201,  ...,  -2.4915,  -2.3423,   0.9212],\n",
            "        [ -3.4468,   0.5751,  -6.2515,  ...,  -6.3373,  -1.6584,   1.7480]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 4, 3, 1, 6, 4, 0, 2, 2, 6, 0, 6, 9, 1, 7, 6, 7, 0, 3, 9, 4, 8, 2, 0,\n",
            "        3, 4, 4, 2, 1, 4, 4, 4, 7, 1, 4, 7, 4, 4, 8, 4, 7, 8, 3, 5, 7, 2, 0, 8,\n",
            "        9, 5, 8, 2, 6, 2, 0, 8, 7, 3, 7, 6, 5, 0, 1, 3, 6, 2, 5, 1, 1, 2, 9, 2,\n",
            "        7, 8, 7, 2, 1, 3, 2, 0, 2, 4, 7, 9, 8, 1, 0, 7, 7, 0, 7, 8, 4, 6, 6, 5,\n",
            "        0, 1, 5, 7, 0, 1, 3, 1, 4, 2, 3, 8, 4, 2, 4, 7, 8, 6, 0, 8, 9, 0, 0, 1,\n",
            "        2, 4, 4, 6, 4, 6, 1, 9], device='cuda:0')\n",
            "Target: tensor([5, 4, 6, 1, 6, 4, 0, 2, 2, 6, 0, 5, 9, 1, 7, 6, 7, 0, 3, 9, 6, 8, 3, 0,\n",
            "        3, 4, 7, 7, 1, 4, 7, 2, 7, 1, 4, 7, 4, 4, 8, 4, 7, 7, 5, 3, 7, 2, 0, 8,\n",
            "        9, 5, 8, 3, 6, 2, 0, 8, 7, 3, 7, 6, 5, 3, 1, 3, 2, 2, 5, 4, 1, 2, 9, 2,\n",
            "        7, 0, 7, 2, 1, 3, 2, 0, 2, 4, 7, 9, 8, 9, 0, 7, 7, 0, 7, 8, 4, 6, 3, 3,\n",
            "        0, 1, 3, 7, 0, 1, 3, 1, 4, 2, 3, 8, 4, 2, 3, 7, 8, 4, 3, 0, 9, 0, 0, 1,\n",
            "        0, 4, 4, 6, 7, 6, 1, 1], device='cuda:0')\n",
            "Output: tensor([[-4.7046, -5.2165, -5.5888,  ..., -3.0349, -7.2595, -5.7539],\n",
            "        [-5.6401, -4.6551, -6.4547,  ...,  4.1893, -8.4731, -4.3059],\n",
            "        [-4.4570, -2.4934, -2.9460,  ..., -0.3873, -6.1015, -0.1450],\n",
            "        ...,\n",
            "        [-1.3689,  3.4000, -4.0968,  ..., -6.8215,  0.7686, -2.9063],\n",
            "        [ 4.7328, -5.9226, -4.2527,  ..., -2.9212,  1.0681, -5.2481],\n",
            "        [ 2.5478, -3.9999, -2.9050,  ..., -4.5651, -1.9693, -3.7900]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 7, 9, 5, 3, 6, 6, 5, 8, 7, 0, 4, 8, 8, 2, 3, 3, 4, 0, 1, 3, 8, 8, 0,\n",
            "        6, 9, 9, 9, 1, 3, 8, 5, 0, 0, 4, 2, 5, 2, 7, 2, 2, 5, 9, 8, 9, 1, 7, 2,\n",
            "        8, 3, 0, 1, 4, 8, 3, 9, 6, 9, 4, 7, 0, 3, 7, 8, 9, 1, 9, 6, 6, 6, 3, 9,\n",
            "        1, 9, 9, 4, 2, 1, 7, 0, 6, 8, 1, 1, 2, 9, 2, 4, 7, 8, 3, 1, 2, 0, 1, 3,\n",
            "        8, 7, 6, 5, 8, 1, 3, 8, 5, 8, 8, 4, 8, 1, 1, 8, 9, 6, 0, 8, 6, 1, 3, 4,\n",
            "        1, 6, 0, 5, 1, 1, 0, 0], device='cuda:0')\n",
            "Target: tensor([3, 7, 3, 5, 2, 6, 6, 5, 8, 7, 1, 6, 8, 8, 5, 3, 0, 4, 0, 1, 3, 8, 8, 0,\n",
            "        6, 9, 9, 9, 5, 5, 8, 6, 0, 0, 4, 2, 3, 2, 7, 2, 2, 5, 9, 8, 9, 1, 7, 4,\n",
            "        0, 3, 0, 1, 3, 8, 3, 9, 6, 1, 4, 7, 0, 3, 7, 8, 9, 1, 1, 6, 6, 6, 6, 9,\n",
            "        1, 9, 9, 4, 2, 1, 7, 0, 6, 8, 1, 9, 2, 9, 0, 4, 7, 8, 3, 1, 2, 0, 1, 5,\n",
            "        8, 4, 6, 3, 8, 1, 3, 8, 5, 0, 8, 4, 8, 1, 1, 8, 9, 6, 0, 8, 6, 1, 3, 4,\n",
            "        1, 6, 0, 5, 1, 1, 0, 0], device='cuda:0')\n",
            "Output: tensor([[-5.8454e+00, -7.0001e+00, -3.6140e+00,  ..., -5.6998e+00,\n",
            "         -3.6135e+00, -7.7136e+00],\n",
            "        [-2.7504e+00, -5.5473e+00, -3.6840e+00,  ..., -4.0487e-01,\n",
            "         -3.8420e+00, -2.5021e+00],\n",
            "        [ 1.8911e+00, -5.1847e+00, -2.9777e+00,  ..., -3.6423e+00,\n",
            "         -5.9698e+00, -4.1520e+00],\n",
            "        ...,\n",
            "        [-6.3217e+00, -1.0217e+01, -1.8598e+00,  ...,  3.8198e+00,\n",
            "         -7.6509e+00, -6.1172e+00],\n",
            "        [-4.9232e+00, -2.5185e+00, -6.1865e+00,  ...,  1.0530e+00,\n",
            "         -3.8556e+00, -6.4071e+00],\n",
            "        [ 5.6234e-03, -4.8164e+00,  4.1065e-01,  ..., -1.7134e+00,\n",
            "         -5.7695e+00, -4.9309e+00]], device='cuda:0')\n",
            "Prediction: tensor([3, 5, 0, 0, 4, 6, 3, 3, 1, 3, 3, 6, 0, 7, 2, 2, 2, 5, 2, 2, 8, 5, 2, 1,\n",
            "        1, 3, 7, 2, 0, 3, 1, 5, 3, 7, 6, 8, 9, 1, 3, 4, 9, 3, 1, 0, 9, 6, 3, 6,\n",
            "        0, 7, 3, 8, 0, 0, 0, 6, 6, 6, 9, 2, 5, 4, 7, 6, 3, 6, 8, 8, 3, 4, 6, 6,\n",
            "        7, 5, 1, 2, 8, 8, 8, 8, 9, 3, 9, 7, 2, 1, 2, 8, 2, 8, 9, 1, 5, 5, 7, 7,\n",
            "        5, 6, 8, 3, 2, 6, 2, 8, 4, 3, 7, 1, 2, 4, 1, 6, 9, 0, 5, 8, 6, 1, 8, 6,\n",
            "        1, 6, 2, 6, 2, 4, 7, 2], device='cuda:0')\n",
            "Target: tensor([3, 5, 0, 0, 6, 6, 3, 3, 6, 3, 6, 6, 0, 7, 2, 2, 7, 5, 5, 2, 8, 5, 2, 1,\n",
            "        1, 4, 3, 2, 0, 3, 1, 5, 3, 7, 6, 8, 9, 1, 6, 4, 9, 3, 9, 0, 9, 6, 3, 6,\n",
            "        0, 7, 3, 8, 0, 0, 0, 6, 6, 6, 9, 2, 5, 4, 4, 6, 3, 6, 0, 8, 6, 0, 6, 2,\n",
            "        7, 5, 1, 2, 7, 8, 8, 0, 9, 4, 9, 7, 2, 0, 2, 8, 3, 8, 9, 1, 5, 5, 4, 7,\n",
            "        5, 3, 8, 3, 3, 6, 2, 8, 4, 3, 7, 1, 2, 4, 1, 6, 9, 0, 5, 8, 6, 1, 8, 6,\n",
            "        1, 4, 2, 6, 2, 7, 2, 2], device='cuda:0')\n",
            "Output: tensor([[ 4.1227, -0.8441, -4.3136,  ..., -4.1176, -0.2794, -1.9908],\n",
            "        [-2.9144, -1.7906, -4.7654,  ..., -0.1738,  2.2412, -4.2987],\n",
            "        [-5.1045, -6.4448,  1.7164,  ..., -2.2378, -6.5994, -4.6541],\n",
            "        ...,\n",
            "        [-5.6177, -8.0203,  0.4829,  ..., -2.4506, -7.6624, -8.9979],\n",
            "        [-0.3474, -2.3079, -1.1113,  ..., -2.1368, -2.9625, -6.2350],\n",
            "        [-2.7899,  0.0374, -6.5658,  ..., -3.4490, -0.1238, -1.4773]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([0, 8, 6, 9, 1, 7, 1, 8, 8, 0, 7, 5, 8, 8, 3, 4, 3, 7, 7, 9, 2, 5, 1, 9,\n",
            "        1, 9, 2, 5, 3, 3, 1, 0, 6, 1, 4, 8, 0, 0, 1, 7, 6, 3, 7, 6, 2, 0, 7, 1,\n",
            "        8, 7, 2, 0, 6, 0, 1, 4, 3, 7, 3, 6, 0, 8, 5, 7, 1, 2, 8, 5, 9, 9, 1, 8,\n",
            "        7, 6, 2, 5, 4, 1, 3, 9, 1, 4, 1, 5, 7, 0, 1, 3, 2, 0, 4, 8, 4, 6, 5, 2,\n",
            "        4, 4, 7, 1, 5, 8, 2, 4, 9, 2, 1, 8, 1, 9, 8, 0, 8, 1, 0, 4, 3, 3, 1, 8,\n",
            "        7, 6, 3, 3, 5, 2, 0, 1], device='cuda:0')\n",
            "Target: tensor([0, 8, 6, 9, 1, 7, 1, 8, 8, 0, 7, 3, 8, 0, 3, 4, 3, 7, 7, 9, 2, 3, 1, 9,\n",
            "        1, 9, 6, 3, 3, 3, 1, 0, 6, 1, 4, 1, 0, 0, 1, 1, 6, 5, 4, 6, 2, 0, 7, 9,\n",
            "        8, 7, 2, 0, 6, 8, 1, 4, 3, 7, 0, 6, 1, 8, 5, 7, 8, 4, 8, 3, 9, 9, 9, 8,\n",
            "        7, 6, 6, 3, 5, 1, 5, 9, 1, 4, 1, 5, 7, 0, 1, 5, 2, 0, 8, 8, 5, 6, 7, 3,\n",
            "        2, 4, 7, 2, 5, 8, 2, 4, 9, 2, 1, 8, 1, 9, 8, 8, 8, 9, 0, 4, 3, 3, 1, 8,\n",
            "        4, 6, 3, 3, 5, 2, 2, 8], device='cuda:0')\n",
            "Output: tensor([[-3.1759, -8.8744, -2.6132,  ..., -1.6488, -2.1035, -2.0404],\n",
            "        [-2.0296, -2.4700, -2.8923,  ..., -4.5095,  2.6412, -5.7343],\n",
            "        [-1.5328, -1.7204, -4.7009,  ..., -0.1940, -3.8789,  1.6556],\n",
            "        ...,\n",
            "        [ 0.5326,  1.9332, -7.0244,  ..., -3.8446,  0.8739, -0.2022],\n",
            "        [-2.1978,  2.1841, -4.7303,  ..., -2.9236, -1.2326,  8.5969],\n",
            "        [ 2.2182, -2.1663, -3.3135,  ..., -3.7386,  2.5144, -4.1178]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 8, 9, 5, 8, 9, 8, 9, 1, 6, 5, 9, 4, 4, 8, 0, 7, 2, 9, 7, 4, 1, 4, 4,\n",
            "        0, 9, 1, 0, 5, 6, 0, 8, 6, 1, 9, 4, 5, 9, 5, 0, 7, 2, 9, 0, 3, 4, 6, 6,\n",
            "        3, 5, 2, 1, 1, 7, 3, 1, 4, 5, 6, 7, 1, 4, 7, 0, 9, 4, 5, 8, 3, 8, 4, 7,\n",
            "        4, 3, 1, 5, 6, 9, 8, 9, 7, 9, 5, 1, 4, 0, 8, 2, 3, 8, 1, 1, 1, 3, 2, 4,\n",
            "        9, 3, 1, 7, 4, 6, 2, 8, 9, 3, 3, 9, 5, 5, 6, 7, 5, 3, 5, 6, 1, 0, 7, 0,\n",
            "        5, 4, 7, 6, 1, 1, 9, 8], device='cuda:0')\n",
            "Target: tensor([3, 8, 9, 5, 8, 9, 8, 9, 1, 6, 5, 9, 4, 4, 8, 0, 7, 2, 9, 7, 4, 1, 6, 4,\n",
            "        4, 9, 1, 2, 5, 6, 0, 8, 6, 1, 9, 4, 5, 9, 5, 0, 7, 2, 0, 0, 4, 2, 6, 6,\n",
            "        5, 5, 2, 8, 1, 7, 3, 1, 4, 5, 6, 5, 1, 4, 7, 0, 9, 4, 3, 8, 2, 8, 4, 7,\n",
            "        2, 3, 1, 5, 2, 9, 8, 9, 7, 9, 5, 1, 4, 0, 8, 2, 3, 8, 9, 1, 1, 3, 2, 4,\n",
            "        9, 3, 1, 7, 4, 6, 2, 8, 9, 5, 3, 9, 5, 5, 6, 7, 2, 4, 6, 3, 1, 0, 7, 2,\n",
            "        5, 4, 7, 6, 1, 1, 9, 8], device='cuda:0')\n",
            "Output: tensor([[ -5.0628,   5.0239,  -9.2392,  ...,  -1.1212,  -2.2706,  -3.0139],\n",
            "        [  1.9011,  -2.8328,  -2.2219,  ...,  -4.1495,  -0.7857,  -3.9082],\n",
            "        [ -3.2541,   6.6096,  -6.6024,  ...,  -5.0738,  -3.5104,   2.4934],\n",
            "        ...,\n",
            "        [ -5.2121,  -5.3052,   1.2188,  ...,  -1.2490, -11.9094,  -5.0200],\n",
            "        [ -2.8448,  -5.6661,  -4.9323,  ...,  -2.3998,  -9.5975,  -5.5281],\n",
            "        [ -5.8945,  -7.4584,   0.9998,  ...,   0.8609,  -7.1165,  -9.9851]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([1, 0, 1, 3, 1, 1, 1, 7, 3, 9, 6, 8, 4, 6, 8, 7, 9, 4, 7, 9, 7, 2, 8, 4,\n",
            "        9, 7, 2, 1, 8, 1, 5, 1, 0, 4, 5, 7, 1, 3, 0, 8, 4, 6, 2, 3, 6, 5, 3, 9,\n",
            "        2, 1, 1, 8, 6, 0, 2, 0, 8, 5, 7, 1, 3, 7, 8, 0, 7, 7, 7, 9, 7, 7, 7, 3,\n",
            "        1, 2, 8, 6, 4, 0, 7, 9, 8, 6, 8, 6, 8, 1, 7, 0, 2, 8, 3, 0, 1, 1, 2, 3,\n",
            "        1, 2, 7, 2, 8, 1, 8, 1, 8, 6, 0, 2, 4, 1, 3, 6, 7, 7, 4, 4, 3, 3, 4, 5,\n",
            "        2, 4, 3, 7, 8, 2, 4, 4], device='cuda:0')\n",
            "Target: tensor([1, 0, 1, 3, 1, 1, 1, 7, 3, 9, 6, 8, 4, 6, 8, 4, 9, 4, 7, 9, 7, 6, 8, 4,\n",
            "        9, 7, 0, 1, 6, 1, 5, 9, 0, 4, 3, 4, 1, 3, 0, 8, 4, 6, 2, 2, 6, 5, 3, 6,\n",
            "        2, 1, 1, 8, 6, 0, 4, 0, 1, 9, 7, 1, 3, 7, 7, 8, 7, 7, 3, 9, 7, 7, 7, 2,\n",
            "        1, 2, 8, 6, 4, 0, 7, 9, 8, 6, 8, 4, 9, 1, 7, 2, 2, 8, 5, 8, 1, 2, 2, 4,\n",
            "        1, 2, 5, 2, 8, 1, 8, 1, 8, 6, 0, 2, 4, 1, 3, 6, 7, 7, 4, 4, 3, 3, 4, 5,\n",
            "        2, 4, 3, 7, 8, 4, 4, 4], device='cuda:0')\n",
            "Output: tensor([[-5.6807, -2.1824, -3.3643,  ..., -3.2077, -6.8367, -4.6349],\n",
            "        [-4.1470, -6.1138, -1.2805,  ...,  1.4415, -5.7736, -6.4861],\n",
            "        [-1.0925, -4.1271, -3.1858,  ..., -3.4753, -0.5307, -4.1341],\n",
            "        ...,\n",
            "        [-6.8895, -4.8836,  0.2748,  ..., -4.9701, -7.2707, -5.6374],\n",
            "        [-6.3553, -5.0882, -5.8214,  ..., -2.3515, -4.2291, -6.2844],\n",
            "        [-5.2391, -4.5557,  0.6397,  ..., -4.2975, -4.6479, -3.8905]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([5, 4, 8, 2, 8, 4, 5, 5, 4, 1, 4, 0, 4, 1, 6, 4, 4, 4, 4, 0, 8, 8, 4, 5,\n",
            "        7, 5, 6, 9, 1, 6, 7, 2, 0, 1, 4, 5, 4, 9, 9, 2, 2, 4, 4, 0, 2, 5, 9, 1,\n",
            "        7, 5, 5, 2, 5, 3, 2, 1, 4, 3, 3, 3, 0, 3, 5, 5, 7, 9, 7, 3, 0, 2, 3, 3,\n",
            "        5, 4, 3, 3, 3, 9, 1, 7, 7, 0, 7, 4, 3, 1, 4, 4, 4, 3, 9, 9, 4, 9, 9, 1,\n",
            "        8, 1, 6, 7, 5, 5, 4, 9, 7, 6, 5, 9, 6, 4, 0, 7, 8, 5, 5, 0, 0, 9, 9, 8,\n",
            "        3, 5, 0, 8, 3, 6, 3, 6], device='cuda:0')\n",
            "Target: tensor([5, 4, 3, 2, 8, 4, 5, 5, 4, 1, 4, 2, 5, 1, 6, 4, 3, 4, 4, 0, 8, 8, 4, 5,\n",
            "        7, 5, 6, 9, 1, 6, 7, 2, 0, 1, 4, 5, 6, 0, 0, 2, 7, 5, 6, 0, 6, 2, 9, 1,\n",
            "        7, 7, 5, 2, 5, 6, 4, 1, 4, 3, 3, 3, 0, 3, 5, 5, 8, 9, 7, 3, 1, 3, 3, 3,\n",
            "        4, 4, 2, 3, 3, 8, 1, 7, 7, 0, 7, 4, 5, 1, 4, 2, 4, 3, 9, 9, 4, 9, 9, 1,\n",
            "        8, 1, 6, 7, 5, 5, 4, 9, 7, 6, 5, 9, 2, 4, 0, 7, 8, 5, 5, 0, 0, 9, 9, 8,\n",
            "        2, 5, 4, 8, 3, 6, 3, 6], device='cuda:0')\n",
            "Output: tensor([[ 0.1938, -5.1674,  0.3729,  ..., -2.0780, -4.9383, -4.9553],\n",
            "        [-8.0459, -5.6020, -0.4178,  ..., -5.7212, -8.3794, -8.4651],\n",
            "        [-7.5718, -5.3849,  0.9184,  ..., -8.5345, -4.7224, -2.8985],\n",
            "        ...,\n",
            "        [-0.8381, -4.2219, -3.9990,  ..., -1.4024, -3.9319, -5.4605],\n",
            "        [-6.2158, -6.6379, -0.8896,  ..., -0.6823, -6.1061, -6.5144],\n",
            "        [-8.6226, -6.7929, -1.4820,  ..., -1.2767, -7.2088, -4.0121]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([2, 6, 6, 6, 9, 6, 2, 8, 6, 2, 4, 5, 8, 1, 2, 7, 6, 4, 7, 0, 1, 5, 0, 8,\n",
            "        6, 9, 2, 8, 9, 8, 0, 9, 0, 9, 5, 7, 5, 5, 9, 5, 3, 0, 1, 9, 7, 6, 4, 1,\n",
            "        0, 0, 7, 4, 1, 7, 0, 8, 4, 8, 6, 3, 4, 0, 0, 1, 0, 8, 4, 3, 1, 3, 9, 0,\n",
            "        5, 7, 5, 2, 1, 4, 8, 1, 0, 5, 0, 1, 0, 2, 8, 1, 5, 6, 7, 7, 2, 6, 4, 5,\n",
            "        0, 1, 4, 2, 5, 4, 3, 2, 2, 1, 7, 2, 8, 5, 5, 3, 0, 4, 8, 5, 7, 6, 3, 0,\n",
            "        1, 0, 1, 2, 3, 4, 5, 4], device='cuda:0')\n",
            "Target: tensor([0, 6, 6, 6, 9, 6, 6, 8, 6, 2, 4, 5, 8, 1, 2, 7, 6, 5, 7, 8, 1, 8, 0, 8,\n",
            "        6, 9, 2, 8, 9, 4, 0, 9, 4, 9, 5, 7, 5, 5, 9, 5, 3, 0, 1, 9, 7, 2, 4, 1,\n",
            "        0, 8, 0, 3, 1, 7, 0, 0, 4, 8, 6, 2, 4, 0, 0, 9, 0, 8, 4, 5, 9, 3, 9, 0,\n",
            "        5, 6, 5, 0, 1, 4, 8, 1, 0, 5, 2, 1, 0, 2, 8, 1, 5, 6, 7, 7, 2, 6, 2, 5,\n",
            "        0, 1, 4, 2, 5, 4, 6, 2, 2, 1, 7, 2, 8, 5, 5, 3, 0, 4, 8, 3, 7, 6, 3, 8,\n",
            "        1, 0, 1, 3, 3, 0, 7, 4], device='cuda:0')\n",
            "Output: tensor([[-1.4104,  1.2266, -5.1226,  ..., -5.9083, -2.7349,  9.2835],\n",
            "        [-6.8341, -5.3052,  0.4456,  ..., -2.1262, -6.9448, -7.5958],\n",
            "        [-4.1404, -6.9849, -2.2447,  ..., -2.2640, -6.9554, -4.5314],\n",
            "        ...,\n",
            "        [-4.8706, -5.2844, -2.6376,  ..., -5.7634,  3.4960, -8.4745],\n",
            "        [-7.5008, -3.4463,  1.2666,  ..., -5.3246, -7.7426, -6.2698],\n",
            "        [-4.2714, -5.8338, -2.3392,  ..., -2.6784, -6.2329, -6.9567]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([9, 6, 3, 6, 0, 1, 4, 4, 2, 4, 2, 5, 3, 0, 1, 5, 9, 8, 1, 1, 2, 2, 9, 9,\n",
            "        7, 4, 5, 0, 8, 4, 7, 3, 9, 2, 8, 4, 7, 1, 3, 9, 6, 8, 9, 0, 4, 8, 6, 7,\n",
            "        4, 9, 4, 8, 9, 7, 2, 5, 3, 7, 1, 0, 2, 9, 5, 5, 8, 5, 3, 2, 8, 3, 3, 5,\n",
            "        7, 7, 8, 6, 7, 8, 2, 3, 5, 6, 8, 0, 2, 3, 7, 0, 1, 9, 1, 3, 6, 5, 3, 3,\n",
            "        2, 9, 6, 8, 6, 9, 3, 8, 9, 8, 8, 7, 8, 5, 0, 0, 1, 3, 7, 1, 3, 3, 4, 4,\n",
            "        0, 0, 9, 9, 9, 8, 2, 4], device='cuda:0')\n",
            "Target: tensor([9, 5, 3, 6, 0, 1, 4, 4, 4, 4, 2, 2, 5, 8, 1, 5, 9, 8, 1, 1, 5, 3, 9, 9,\n",
            "        7, 6, 5, 0, 8, 4, 7, 0, 9, 2, 8, 4, 7, 1, 3, 9, 6, 8, 9, 0, 4, 9, 6, 7,\n",
            "        8, 9, 4, 8, 9, 7, 2, 5, 3, 7, 1, 0, 2, 9, 5, 5, 8, 5, 4, 2, 8, 3, 5, 5,\n",
            "        7, 7, 8, 6, 2, 8, 2, 3, 5, 6, 8, 0, 2, 3, 7, 0, 1, 9, 1, 3, 7, 5, 8, 3,\n",
            "        2, 9, 6, 8, 6, 9, 3, 8, 9, 8, 0, 7, 8, 5, 0, 0, 1, 3, 9, 1, 5, 3, 4, 4,\n",
            "        0, 9, 9, 9, 9, 8, 2, 4], device='cuda:0')\n",
            "Output: tensor([[-6.3535, -5.3363,  5.9608,  ..., -4.2778, -4.4375, -7.2660],\n",
            "        [-4.8097, -8.6925,  0.5850,  ..., -0.0420, -5.9309, -8.1701],\n",
            "        [-4.7823, -7.4671, -0.0904,  ..., -3.5808, -8.4566, -6.1801],\n",
            "        ...,\n",
            "        [-0.4493,  5.1361, -4.3693,  ..., -1.7873, -1.4790, -1.1775],\n",
            "        [-2.2205, -1.9550, -2.7067,  ..., -3.4528, -2.0838, -4.7650],\n",
            "        [-6.4661, -5.5863, -0.8020,  ..., -2.9406, -4.2017, -5.5075]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([2, 4, 5, 1, 4, 1, 0, 1, 4, 2, 1, 6, 0, 3, 7, 2, 3, 9, 0, 7, 3, 7, 2, 8,\n",
            "        4, 4, 8, 5, 5, 0, 5, 7, 4, 4, 4, 4, 7, 9, 6, 0, 5, 7, 6, 2, 3, 6, 7, 7,\n",
            "        0, 9, 1, 3, 6, 2, 6, 3, 5, 5, 6, 6, 9, 0, 6, 8, 7, 6, 4, 9, 5, 5, 4, 1,\n",
            "        6, 2, 8, 2, 6, 9, 8, 5, 1, 5, 4, 5, 7, 5, 7, 8, 9, 1, 4, 2, 5, 6, 8, 4,\n",
            "        4, 5, 6, 9, 9, 1, 5, 3, 6, 4, 5, 9, 7, 3, 4, 1, 2, 4, 6, 2, 3, 5, 2, 8,\n",
            "        0, 0, 1, 8, 3, 1, 3, 5], device='cuda:0')\n",
            "Target: tensor([2, 2, 5, 1, 9, 1, 0, 9, 4, 2, 1, 6, 0, 3, 7, 6, 3, 1, 8, 6, 5, 7, 2, 8,\n",
            "        4, 4, 8, 3, 5, 0, 5, 7, 4, 4, 2, 2, 7, 3, 6, 0, 2, 7, 6, 2, 3, 0, 7, 7,\n",
            "        8, 1, 1, 4, 6, 0, 6, 6, 5, 5, 6, 3, 9, 3, 6, 8, 7, 6, 4, 9, 5, 6, 4, 1,\n",
            "        6, 3, 8, 2, 3, 9, 8, 5, 1, 5, 4, 5, 7, 5, 7, 8, 9, 1, 7, 2, 5, 6, 8, 4,\n",
            "        6, 5, 3, 9, 9, 8, 5, 5, 6, 4, 5, 9, 7, 3, 4, 1, 4, 2, 3, 6, 5, 5, 2, 8,\n",
            "        0, 0, 1, 8, 3, 1, 3, 5], device='cuda:0')\n",
            "Output: tensor([[ 3.2470, -4.9479, -2.4081,  ..., -5.0526,  5.6959, -3.8470],\n",
            "        [-5.1182, -6.6665, -4.2463,  ..., -1.2438, -6.3115, -5.6391],\n",
            "        [-0.6298, -1.7279, -5.3002,  ..., -4.1655,  4.1544, -5.2734],\n",
            "        ...,\n",
            "        [-2.3766, -3.9674, -3.8926,  ..., -2.5625, -1.8231, -2.7383],\n",
            "        [-3.3871,  1.8348, -8.9773,  ..., -4.2783, -1.9817,  3.7371],\n",
            "        [-5.6146, -7.6812, -4.0001,  ...,  0.6841, -4.2384, -4.2719]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([8, 3, 8, 5, 8, 6, 3, 5, 5, 5, 0, 0, 5, 4, 4, 1, 8, 2, 4, 4, 3, 4, 2, 9,\n",
            "        6, 2, 2, 4, 3, 0, 4, 0, 1, 3, 2, 6, 7, 6, 0, 0, 7, 7, 2, 3, 1, 6, 5, 0,\n",
            "        2, 2, 0, 1, 2, 6, 0, 9, 6, 6, 5, 3, 5, 0, 0, 1, 1, 0, 5, 5, 9, 7, 8, 6,\n",
            "        3, 6, 0, 2, 0, 1, 9, 2, 6, 9, 0, 7, 2, 3, 5, 8, 7, 0, 6, 8, 9, 7, 1, 9,\n",
            "        3, 4, 2, 7, 9, 6, 9, 3, 7, 8, 5, 9, 8, 3, 9, 8, 0, 3, 8, 3, 1, 7, 0, 3,\n",
            "        5, 8, 2, 2, 7, 3, 9, 7], device='cuda:0')\n",
            "Target: tensor([8, 3, 8, 5, 8, 6, 3, 5, 5, 5, 0, 9, 5, 5, 7, 1, 8, 2, 2, 7, 3, 4, 2, 9,\n",
            "        6, 2, 2, 4, 3, 0, 2, 0, 1, 3, 2, 2, 7, 0, 1, 0, 7, 7, 2, 0, 1, 6, 5, 0,\n",
            "        2, 2, 0, 1, 2, 6, 0, 1, 6, 6, 5, 3, 4, 0, 0, 9, 1, 0, 2, 5, 9, 7, 8, 6,\n",
            "        4, 6, 0, 2, 0, 1, 9, 2, 4, 9, 0, 7, 2, 3, 4, 8, 0, 2, 6, 8, 9, 7, 1, 9,\n",
            "        3, 7, 2, 7, 9, 6, 9, 5, 7, 8, 5, 9, 8, 3, 9, 8, 0, 5, 5, 7, 1, 2, 0, 7,\n",
            "        5, 8, 2, 2, 5, 3, 9, 3], device='cuda:0')\n",
            "Output: tensor([[ -1.8262,   0.6455,  -5.4319,  ...,  -4.1375,   2.4619,  -0.4413],\n",
            "        [ -4.5379,   2.6901,  -6.5161,  ...,  -3.3869,  -7.7779,   6.2815],\n",
            "        [ -4.3717,  -2.7994,  -5.3542,  ...,  -3.6072,  -3.3026,  -2.9013],\n",
            "        ...,\n",
            "        [ -8.0054, -10.8597,   1.0487,  ...,  -3.4147,  -8.9520,  -7.3189],\n",
            "        [  1.8536,  -0.8577,  -6.1908,  ...,  -0.6241,  -4.4537,  -0.2908],\n",
            "        [ -5.5847,  -6.2319,   1.3505,  ...,  -1.7135,  -4.8948,  -7.7401]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([8, 9, 3, 1, 4, 3, 4, 4, 9, 0, 1, 3, 9, 2, 9, 4, 0, 3, 8, 4, 8, 6, 8, 2,\n",
            "        2, 5, 9, 6, 9, 3, 1, 3, 4, 4, 2, 7, 2, 6, 4, 0, 2, 8, 4, 5, 1, 2, 6, 0,\n",
            "        9, 7, 2, 8, 7, 7, 3, 3, 7, 5, 7, 2, 3, 3, 1, 7, 9, 0, 2, 3, 9, 8, 9, 5,\n",
            "        0, 8, 7, 5, 3, 3, 8, 1, 4, 0, 1, 5, 4, 3, 4, 6, 2, 8, 8, 3, 3, 3, 3, 8,\n",
            "        7, 5, 1, 6, 8, 5, 9, 9, 4, 5, 1, 4, 8, 3, 3, 2, 5, 1, 9, 5, 3, 1, 9, 8,\n",
            "        9, 7, 2, 3, 8, 4, 0, 2], device='cuda:0')\n",
            "Target: tensor([1, 9, 3, 1, 4, 3, 4, 4, 9, 0, 9, 5, 9, 2, 9, 4, 0, 2, 8, 4, 8, 6, 8, 2,\n",
            "        0, 5, 9, 6, 9, 5, 1, 3, 4, 4, 2, 7, 2, 4, 4, 0, 2, 8, 4, 5, 1, 2, 6, 8,\n",
            "        1, 7, 2, 8, 7, 4, 3, 3, 0, 3, 7, 2, 5, 6, 1, 7, 9, 0, 2, 3, 9, 8, 9, 5,\n",
            "        0, 0, 7, 6, 3, 3, 8, 1, 4, 0, 1, 5, 4, 3, 2, 6, 0, 8, 8, 6, 3, 3, 2, 9,\n",
            "        7, 5, 1, 6, 0, 5, 9, 9, 4, 5, 9, 4, 8, 3, 3, 2, 5, 1, 9, 5, 5, 8, 1, 8,\n",
            "        9, 7, 0, 6, 3, 2, 0, 2], device='cuda:0')\n",
            "Output: tensor([[-6.5489, -6.3946, -3.3897,  ..., -6.0561, -0.9939, -6.2061],\n",
            "        [-1.1250, -0.7641, -4.5153,  ..., -3.7559, -2.1211,  1.3378],\n",
            "        [-8.1176, -7.3155, -4.1007,  ..., -1.7555, -7.9180, -5.4420],\n",
            "        ...,\n",
            "        [ 3.0289, -2.6141, -0.3173,  ..., -6.1688,  4.7688, -7.3331],\n",
            "        [-4.7617, -6.2806, -6.1661,  ...,  2.3916, -4.5412, -5.2907],\n",
            "        [-1.6892, -3.4551, -3.1234,  ..., -4.4144,  2.4081, -2.3108]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([6, 9, 3, 9, 1, 6, 7, 7, 2, 6, 3, 0, 2, 5, 2, 7, 3, 5, 0, 8, 0, 7, 7, 1,\n",
            "        7, 4, 0, 4, 2, 6, 1, 5, 9, 7, 6, 2, 7, 0, 5, 6, 0, 1, 1, 8, 3, 3, 5, 1,\n",
            "        2, 4, 8, 9, 8, 1, 7, 2, 7, 4, 4, 2, 6, 7, 9, 4, 0, 1, 0, 4, 3, 0, 8, 0,\n",
            "        7, 6, 1, 8, 8, 2, 9, 2, 5, 4, 5, 9, 0, 6, 9, 2, 5, 1, 6, 7, 7, 5, 8, 8,\n",
            "        0, 3, 9, 3, 7, 6, 4, 9, 2, 4, 3, 0, 6, 6, 5, 6, 6, 7, 8, 4, 2, 8, 7, 2,\n",
            "        8, 0, 2, 5, 6, 8, 7, 8], device='cuda:0')\n",
            "Target: tensor([6, 9, 3, 9, 1, 6, 6, 7, 2, 6, 3, 2, 5, 5, 2, 7, 5, 2, 0, 8, 0, 7, 7, 1,\n",
            "        7, 4, 0, 2, 2, 6, 1, 5, 9, 7, 6, 2, 7, 0, 5, 6, 0, 1, 1, 8, 4, 5, 3, 1,\n",
            "        2, 4, 8, 9, 8, 1, 7, 2, 7, 2, 3, 2, 6, 7, 9, 4, 0, 1, 0, 4, 5, 0, 8, 0,\n",
            "        7, 6, 1, 0, 8, 5, 9, 2, 5, 4, 4, 9, 0, 6, 1, 2, 5, 1, 6, 7, 1, 5, 8, 8,\n",
            "        0, 3, 9, 4, 0, 3, 4, 9, 2, 4, 3, 0, 6, 4, 5, 6, 6, 7, 8, 4, 8, 8, 3, 2,\n",
            "        8, 0, 2, 5, 6, 8, 7, 8], device='cuda:0')\n",
            "Output: tensor([[ 0.5149, -3.2370, -4.4615,  ..., -3.1277,  0.5304,  0.2949],\n",
            "        [-6.4854,  1.7385, -5.9610,  ..., -1.8269, -4.5036,  6.4309],\n",
            "        [ 5.4653, -1.0948, -3.8784,  ..., -4.1067, -0.8105,  0.6508],\n",
            "        ...,\n",
            "        [-3.2290, -6.5210,  2.8028,  ..., -2.4657, -6.3436, -7.1189],\n",
            "        [-5.4733, -8.1266, -4.2907,  ..., -1.2661, -6.1842, -7.1496],\n",
            "        [-5.9871, -4.2349, -6.4012,  ..., -0.7727, -8.1014,  1.9093]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([8, 9, 0, 6, 6, 8, 1, 1, 0, 4, 4, 2, 5, 6, 9, 7, 2, 4, 1, 8, 2, 3, 1, 6,\n",
            "        8, 6, 8, 5, 9, 0, 1, 0, 4, 9, 2, 9, 0, 9, 7, 8, 5, 8, 6, 6, 8, 1, 6, 5,\n",
            "        8, 4, 9, 3, 1, 6, 7, 5, 6, 3, 0, 0, 8, 9, 7, 3, 9, 9, 3, 2, 4, 4, 8, 8,\n",
            "        1, 7, 3, 0, 4, 3, 7, 1, 3, 2, 9, 4, 2, 9, 7, 4, 1, 9, 1, 4, 4, 3, 5, 8,\n",
            "        9, 3, 6, 3, 0, 3, 9, 0, 5, 1, 5, 8, 6, 1, 0, 4, 2, 4, 8, 4, 0, 6, 4, 2,\n",
            "        9, 8, 4, 7, 6, 2, 4, 9], device='cuda:0')\n",
            "Target: tensor([9, 9, 0, 6, 6, 8, 1, 1, 7, 4, 4, 2, 3, 6, 9, 6, 2, 4, 1, 8, 2, 3, 1, 6,\n",
            "        8, 6, 8, 5, 9, 0, 1, 0, 2, 9, 2, 9, 0, 9, 7, 8, 5, 0, 6, 6, 8, 1, 6, 5,\n",
            "        8, 7, 9, 2, 1, 6, 7, 5, 6, 3, 0, 9, 8, 9, 5, 9, 9, 9, 3, 6, 4, 4, 8, 8,\n",
            "        1, 7, 3, 0, 6, 2, 4, 8, 2, 2, 9, 2, 2, 9, 7, 4, 1, 9, 1, 4, 3, 5, 3, 8,\n",
            "        9, 3, 6, 2, 0, 8, 9, 0, 5, 1, 5, 8, 2, 1, 2, 3, 2, 4, 8, 4, 0, 6, 4, 2,\n",
            "        9, 8, 4, 7, 6, 2, 4, 7], device='cuda:0')\n",
            "Output: tensor([[-6.1325, -3.6010, -6.7592,  ...,  1.2090, -2.8342, -4.6605],\n",
            "        [-7.2085, -3.9239, -1.7646,  ..., -1.9913, -6.3129, -7.6358],\n",
            "        [-3.2145, -6.4688, -2.6732,  ..., -2.9336, -4.5471, -3.8821],\n",
            "        ...,\n",
            "        [-5.3984, -7.5934,  2.1816,  ...,  0.6380, -5.1529, -6.5963],\n",
            "        [-3.3183, -6.5739, -4.9842,  ...,  7.2820, -7.6223, -6.9588],\n",
            "        [-2.2912,  0.1538, -5.6473,  ..., -8.0582,  7.1043, -1.6371]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([7, 6, 4, 2, 2, 3, 2, 4, 9, 0, 0, 9, 6, 5, 8, 5, 2, 4, 8, 8, 3, 4, 5, 7,\n",
            "        6, 3, 4, 0, 4, 0, 3, 8, 4, 8, 9, 2, 9, 5, 3, 3, 8, 1, 6, 0, 8, 4, 3, 1,\n",
            "        4, 5, 2, 6, 3, 6, 0, 7, 1, 3, 5, 7, 1, 8, 0, 4, 8, 2, 7, 7, 2, 6, 2, 4,\n",
            "        3, 3, 1, 4, 1, 6, 4, 6, 7, 4, 2, 3, 8, 1, 3, 1, 7, 6, 7, 5, 7, 7, 5, 4,\n",
            "        9, 7, 7, 2, 3, 3, 6, 7, 4, 1, 3, 1, 0, 6, 9, 6, 0, 2, 4, 2, 4, 7, 0, 0,\n",
            "        0, 0, 7, 5, 3, 2, 7, 8], device='cuda:0')\n",
            "Target: tensor([7, 6, 4, 2, 2, 3, 2, 4, 9, 0, 0, 9, 6, 5, 8, 5, 2, 4, 8, 8, 6, 4, 5, 7,\n",
            "        6, 3, 7, 0, 4, 0, 5, 8, 2, 8, 0, 4, 9, 2, 3, 6, 8, 1, 6, 0, 8, 4, 3, 1,\n",
            "        4, 5, 2, 4, 3, 6, 0, 7, 1, 3, 5, 7, 1, 8, 0, 4, 8, 2, 7, 7, 0, 2, 2, 4,\n",
            "        3, 3, 1, 6, 1, 6, 4, 6, 7, 4, 7, 3, 8, 1, 3, 1, 4, 6, 5, 5, 7, 4, 5, 4,\n",
            "        9, 7, 7, 2, 3, 3, 6, 7, 4, 1, 6, 1, 0, 6, 9, 6, 0, 2, 0, 2, 4, 7, 0, 0,\n",
            "        0, 0, 7, 5, 3, 2, 7, 8], device='cuda:0')\n",
            "Output: tensor([[-6.4798, -6.0126, -0.9194,  ..., -4.0365, -7.8025, -6.1802],\n",
            "        [-6.2919, -8.6436, -5.0008,  ..., -5.2802, -6.0249, -4.8585],\n",
            "        [-3.9173, -4.0233,  1.1911,  ..., -5.2080, -3.3770, -4.6725],\n",
            "        ...,\n",
            "        [ 0.0148,  5.8957, -3.1872,  ..., -3.0104, -4.1104, -0.3318],\n",
            "        [-6.1145,  0.3722, -6.4703,  ..., -2.7638, -5.7270, -0.8688],\n",
            "        [-4.3394, -4.6824, -2.9618,  ..., -3.7500, -3.5406, -2.3161]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 5, 2, 2, 8, 9, 1, 4, 2, 0, 3, 4, 9, 1, 6, 0, 0, 0, 3, 6, 4, 2, 7, 5,\n",
            "        1, 8, 9, 0, 3, 0, 5, 2, 3, 8, 5, 4, 0, 8, 5, 9, 2, 4, 6, 5, 7, 3, 7, 2,\n",
            "        0, 8, 4, 2, 0, 6, 2, 9, 9, 8, 2, 3, 1, 0, 2, 2, 3, 7, 9, 5, 0, 1, 1, 3,\n",
            "        2, 7, 4, 4, 1, 9, 4, 5, 5, 2, 0, 4, 6, 1, 2, 3, 7, 2, 4, 3, 7, 0, 0, 2,\n",
            "        1, 2, 4, 3, 0, 9, 8, 7, 0, 9, 0, 3, 7, 6, 3, 8, 4, 5, 6, 5, 5, 4, 2, 9,\n",
            "        2, 4, 9, 2, 0, 1, 1, 5], device='cuda:0')\n",
            "Target: tensor([5, 5, 2, 2, 8, 9, 1, 2, 2, 0, 3, 4, 9, 1, 6, 9, 0, 8, 3, 6, 4, 6, 7, 5,\n",
            "        1, 8, 9, 0, 5, 0, 5, 4, 3, 8, 5, 2, 0, 8, 5, 9, 2, 5, 6, 5, 7, 3, 7, 2,\n",
            "        8, 8, 4, 2, 1, 6, 2, 9, 9, 8, 2, 3, 1, 0, 5, 2, 3, 7, 9, 5, 9, 1, 1, 3,\n",
            "        2, 7, 4, 4, 1, 9, 4, 5, 5, 2, 7, 7, 6, 1, 2, 3, 7, 5, 4, 3, 7, 7, 0, 0,\n",
            "        1, 4, 4, 3, 7, 9, 8, 7, 0, 9, 0, 3, 7, 6, 3, 8, 4, 5, 6, 5, 3, 4, 2, 9,\n",
            "        2, 4, 9, 2, 0, 1, 1, 3], device='cuda:0')\n",
            "Output: tensor([[-3.9700, -1.9600, -3.1312,  ...,  1.3946, -4.9463, -3.2319],\n",
            "        [-4.5176, -0.9298, -4.4999,  ..., -5.3192, -1.9462,  6.4520],\n",
            "        [-8.3769, -6.1379, -2.6369,  ..., -4.2168, -6.0589, -6.6271],\n",
            "        ...,\n",
            "        [-1.0485, -4.4466, -3.7478,  ..., -5.5745,  2.8439, -0.8165],\n",
            "        [-6.3413, -2.5669, -2.2408,  ..., -3.3157, -3.0112, -5.2137],\n",
            "        [-1.5448,  4.2555, -5.4257,  ..., -6.2244,  0.0730,  4.6554]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([7, 9, 5, 1, 6, 0, 3, 3, 7, 6, 4, 5, 2, 3, 6, 9, 5, 0, 4, 7, 3, 8, 1, 2,\n",
            "        3, 2, 7, 1, 4, 2, 6, 0, 6, 4, 5, 4, 2, 9, 1, 3, 2, 8, 4, 5, 3, 5, 6, 6,\n",
            "        5, 6, 9, 7, 9, 4, 8, 3, 1, 9, 8, 8, 9, 8, 3, 0, 4, 4, 7, 2, 6, 5, 4, 9,\n",
            "        3, 7, 3, 7, 0, 8, 7, 7, 8, 3, 9, 9, 3, 2, 9, 2, 7, 4, 1, 9, 7, 0, 8, 1,\n",
            "        4, 0, 6, 5, 1, 4, 2, 8, 2, 8, 3, 5, 1, 4, 9, 8, 1, 1, 7, 9, 9, 4, 6, 8,\n",
            "        5, 3, 2, 3, 0, 8, 5, 9], device='cuda:0')\n",
            "Target: tensor([7, 9, 5, 1, 6, 0, 3, 3, 7, 2, 4, 5, 2, 3, 6, 3, 3, 0, 4, 4, 3, 8, 1, 2,\n",
            "        3, 2, 7, 1, 4, 5, 6, 8, 6, 6, 5, 4, 2, 9, 9, 3, 2, 0, 4, 1, 5, 5, 6, 6,\n",
            "        5, 6, 9, 7, 9, 4, 8, 3, 1, 9, 8, 1, 9, 0, 3, 0, 4, 7, 7, 2, 6, 5, 6, 9,\n",
            "        7, 7, 5, 7, 0, 8, 6, 7, 8, 3, 8, 9, 3, 2, 9, 2, 7, 4, 1, 9, 7, 0, 8, 1,\n",
            "        2, 0, 6, 5, 1, 4, 2, 8, 2, 8, 3, 3, 1, 4, 9, 8, 1, 1, 7, 9, 9, 4, 6, 8,\n",
            "        5, 3, 2, 2, 3, 8, 5, 1], device='cuda:0')\n",
            "Output: tensor([[-3.4800,  0.0343, -5.3449,  ..., -3.6448, -4.4335,  6.3003],\n",
            "        [-4.9987, -4.2252, -5.0145,  ...,  1.6931, -4.8998, -3.3342],\n",
            "        [-0.7821, -4.5079, -3.7199,  ..., -3.2176, -1.8244,  2.2333],\n",
            "        ...,\n",
            "        [-4.0837, -6.9225, -4.9199,  ..., -2.3960, -4.7756, -5.6954],\n",
            "        [ 0.2095, -2.9678, -4.1694,  ..., -5.3477, -2.0746, -5.6832],\n",
            "        [-5.7317, -8.0553, -3.6594,  ...,  0.7971, -6.7944, -4.6795]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([9, 7, 9, 1, 4, 3, 1, 0, 3, 8, 9, 2, 0, 0, 0, 9, 9, 4, 8, 0, 0, 7, 0, 7,\n",
            "        0, 5, 5, 4, 3, 0, 9, 1, 7, 2, 2, 0, 8, 2, 4, 5, 2, 2, 1, 3, 5, 5, 4, 3,\n",
            "        4, 0, 8, 8, 0, 2, 5, 6, 3, 1, 7, 2, 4, 0, 5, 6, 8, 7, 6, 3, 1, 3, 3, 6,\n",
            "        4, 5, 5, 9, 1, 8, 7, 0, 2, 2, 5, 1, 1, 1, 6, 7, 2, 1, 2, 3, 9, 8, 3, 1,\n",
            "        7, 4, 8, 4, 4, 5, 8, 6, 0, 5, 0, 8, 2, 4, 4, 9, 4, 6, 1, 7, 7, 7, 9, 0,\n",
            "        1, 4, 1, 3, 2, 4, 0, 4], device='cuda:0')\n",
            "Target: tensor([9, 7, 9, 1, 5, 3, 1, 0, 3, 8, 9, 2, 0, 8, 0, 7, 9, 4, 8, 3, 0, 7, 0, 7,\n",
            "        0, 5, 5, 4, 3, 0, 9, 9, 0, 2, 2, 0, 8, 4, 4, 5, 2, 2, 1, 1, 5, 5, 4, 3,\n",
            "        4, 0, 8, 4, 8, 2, 5, 6, 5, 1, 7, 2, 4, 0, 5, 6, 8, 7, 6, 3, 1, 3, 6, 6,\n",
            "        4, 5, 5, 5, 1, 8, 7, 3, 0, 2, 5, 1, 1, 1, 6, 7, 2, 1, 2, 3, 9, 8, 3, 1,\n",
            "        7, 4, 8, 4, 7, 5, 8, 6, 0, 3, 0, 8, 3, 4, 4, 9, 4, 6, 1, 5, 7, 7, 9, 0,\n",
            "        1, 3, 1, 3, 2, 4, 0, 7], device='cuda:0')\n",
            "Output: tensor([[-1.4068e+00, -2.5431e+00, -2.0705e+00,  ..., -2.3529e+00,\n",
            "          1.7823e-01, -4.3460e+00],\n",
            "        [-4.3010e+00, -4.5843e+00, -4.4643e+00,  ..., -2.1064e+00,\n",
            "         -4.8613e+00, -2.7289e+00],\n",
            "        [-5.5442e+00,  1.7690e+00, -2.7294e+00,  ..., -2.4502e+00,\n",
            "         -6.4700e+00,  2.0799e+00],\n",
            "        ...,\n",
            "        [-5.0792e+00, -3.7744e+00, -2.3285e+00,  ..., -3.7006e+00,\n",
            "         -6.5862e+00, -8.2796e+00],\n",
            "        [-2.2027e+00, -4.8977e+00, -2.7430e+00,  ..., -3.2936e+00,\n",
            "         -3.6514e+00, -5.2823e+00],\n",
            "        [-6.5151e+00, -3.9637e+00, -3.6532e-04,  ..., -7.4036e+00,\n",
            "         -6.6230e+00, -4.5644e+00]], device='cuda:0')\n",
            "Prediction: tensor([3, 3, 9, 6, 4, 1, 8, 8, 1, 9, 3, 1, 3, 6, 9, 0, 6, 8, 4, 1, 5, 7, 0, 4,\n",
            "        7, 7, 4, 9, 0, 7, 0, 0, 7, 6, 8, 3, 0, 0, 7, 0, 0, 2, 2, 5, 7, 0, 6, 4,\n",
            "        4, 1, 1, 8, 6, 3, 9, 6, 9, 5, 3, 7, 5, 9, 1, 5, 4, 5, 8, 2, 9, 9, 5, 3,\n",
            "        1, 9, 2, 2, 0, 1, 1, 8, 1, 1, 0, 8, 9, 9, 8, 2, 2, 9, 1, 9, 0, 1, 9, 2,\n",
            "        8, 5, 9, 9, 9, 9, 2, 1, 7, 0, 5, 6, 3, 1, 8, 3, 8, 2, 7, 7, 4, 8, 7, 1,\n",
            "        3, 2, 3, 5, 7, 6, 6, 6], device='cuda:0')\n",
            "Target: tensor([8, 3, 1, 6, 2, 1, 8, 8, 1, 9, 7, 5, 3, 2, 9, 0, 6, 8, 4, 1, 5, 7, 2, 4,\n",
            "        7, 0, 4, 9, 0, 7, 0, 6, 7, 1, 8, 5, 8, 8, 7, 0, 0, 3, 2, 5, 7, 0, 6, 4,\n",
            "        4, 9, 1, 8, 6, 3, 9, 4, 9, 3, 3, 7, 5, 9, 7, 7, 4, 5, 8, 2, 8, 9, 5, 3,\n",
            "        1, 9, 2, 2, 0, 1, 1, 8, 1, 1, 9, 8, 9, 9, 0, 2, 6, 9, 1, 9, 0, 1, 9, 2,\n",
            "        9, 9, 9, 0, 9, 9, 2, 1, 7, 0, 5, 6, 3, 9, 8, 3, 0, 2, 7, 7, 4, 8, 3, 0,\n",
            "        5, 2, 3, 5, 7, 6, 6, 6], device='cuda:0')\n",
            "Output: tensor([[-3.3938, -5.1149, -1.7433,  ..., -2.4323, -5.0799, -3.7480],\n",
            "        [-5.8165, -6.8476, -0.0674,  ..., -5.2331, -2.8063, -5.1669],\n",
            "        [-3.5228, -5.8802, -5.0453,  ..., -1.1225, -1.8396, -4.0332],\n",
            "        ...,\n",
            "        [-4.0241, -5.7186, -1.5976,  ..., -2.1922, -4.2634, -7.0085],\n",
            "        [-3.1358, -5.7063, -3.2553,  ..., -0.2892, -6.1608, -3.7866],\n",
            "        [-6.1171, -6.4516, -4.3820,  ...,  2.8306, -6.6794, -4.8278]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([5, 3, 4, 4, 3, 8, 0, 0, 9, 4, 9, 0, 6, 9, 6, 1, 9, 7, 0, 0, 3, 5, 7, 7,\n",
            "        6, 9, 8, 2, 3, 9, 3, 3, 0, 8, 3, 4, 5, 9, 6, 1, 3, 7, 4, 8, 7, 7, 0, 3,\n",
            "        0, 3, 2, 3, 4, 9, 3, 5, 4, 7, 0, 3, 7, 0, 2, 1, 5, 8, 7, 3, 5, 7, 8, 5,\n",
            "        7, 8, 1, 5, 4, 7, 0, 8, 3, 2, 9, 7, 4, 1, 6, 5, 9, 8, 5, 5, 4, 1, 0, 9,\n",
            "        4, 3, 3, 6, 4, 8, 0, 8, 2, 9, 4, 9, 7, 7, 2, 1, 9, 2, 9, 3, 7, 7, 8, 6,\n",
            "        4, 8, 3, 5, 3, 6, 4, 7], device='cuda:0')\n",
            "Target: tensor([5, 6, 5, 4, 5, 8, 8, 0, 9, 7, 9, 0, 6, 9, 6, 1, 4, 7, 9, 0, 3, 5, 4, 7,\n",
            "        6, 9, 8, 2, 3, 9, 3, 3, 7, 8, 3, 5, 5, 9, 6, 1, 4, 7, 4, 8, 7, 7, 1, 3,\n",
            "        0, 3, 2, 5, 4, 9, 3, 5, 4, 7, 0, 3, 7, 0, 2, 1, 5, 8, 7, 3, 5, 7, 8, 5,\n",
            "        7, 8, 1, 5, 4, 7, 0, 8, 3, 2, 9, 7, 4, 1, 6, 5, 9, 8, 5, 5, 4, 1, 0, 9,\n",
            "        4, 4, 3, 0, 4, 8, 0, 8, 2, 9, 5, 9, 7, 4, 6, 7, 9, 2, 9, 3, 7, 7, 8, 2,\n",
            "        2, 0, 2, 5, 3, 6, 4, 7], device='cuda:0')\n",
            "Output: tensor([[-3.2998, -6.7568, -0.2371,  ..., -5.3764, -7.6255, -6.7271],\n",
            "        [-5.3588, -5.1673, -0.8224,  ..., -4.2990, -5.2796, -6.2921],\n",
            "        [-2.9045, -3.8460, -2.9890,  ..., -0.1567, -4.0989, -2.0240],\n",
            "        ...,\n",
            "        [ 0.8198, -7.6661,  0.7909,  ..., -3.5001, -4.6957, -4.2322],\n",
            "        [-2.9787, -4.2051,  4.2164,  ..., -2.5827, -5.5885, -2.1654],\n",
            "        [-5.9972, -6.7649, -1.6002,  ..., -3.1352, -4.4179, -7.2893]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 3, 7, 8, 7, 2, 5, 8, 0, 7, 0, 9, 6, 1, 0, 3, 9, 7, 7, 9, 1, 2, 8, 1,\n",
            "        2, 3, 2, 6, 4, 8, 9, 4, 3, 4, 1, 4, 7, 9, 8, 7, 9, 3, 1, 2, 1, 6, 6, 4,\n",
            "        5, 7, 4, 3, 8, 5, 4, 8, 7, 8, 4, 3, 6, 1, 5, 3, 1, 5, 1, 9, 0, 9, 2, 0,\n",
            "        8, 2, 4, 8, 5, 7, 6, 1, 2, 9, 4, 4, 0, 3, 3, 3, 4, 7, 1, 4, 5, 0, 2, 8,\n",
            "        5, 0, 0, 5, 3, 0, 8, 4, 5, 5, 5, 6, 4, 7, 9, 7, 3, 0, 6, 2, 0, 0, 6, 4,\n",
            "        3, 1, 1, 5, 5, 0, 2, 6], device='cuda:0')\n",
            "Target: tensor([2, 3, 7, 8, 7, 2, 5, 0, 0, 7, 0, 9, 6, 1, 0, 3, 9, 7, 4, 9, 1, 6, 8, 1,\n",
            "        2, 3, 3, 5, 4, 8, 9, 7, 4, 4, 1, 2, 4, 9, 8, 7, 9, 5, 1, 2, 1, 6, 6, 4,\n",
            "        5, 7, 4, 5, 8, 5, 2, 8, 7, 8, 2, 3, 6, 1, 3, 3, 1, 5, 1, 9, 0, 9, 2, 0,\n",
            "        6, 2, 4, 8, 5, 7, 6, 1, 2, 9, 4, 5, 0, 3, 3, 7, 7, 7, 1, 4, 5, 0, 2, 8,\n",
            "        5, 0, 0, 6, 2, 0, 8, 4, 5, 4, 5, 6, 4, 7, 9, 4, 2, 0, 6, 4, 0, 0, 6, 4,\n",
            "        6, 1, 9, 5, 5, 2, 2, 6], device='cuda:0')\n",
            "Output: tensor([[-6.6656, -9.8208, -1.4340,  ..., -3.0147, -6.4394, -7.8313],\n",
            "        [-3.8690, -7.6811, -2.1459,  ..., -2.4201, -2.8300, -3.5316],\n",
            "        [-3.3915, -4.4147, -1.7246,  ..., -4.7448, -5.1629, -6.2232],\n",
            "        ...,\n",
            "        [-7.0727, -6.0753, -4.6030,  ..., -2.1406, -8.7527, -6.0503],\n",
            "        [-3.1259, -2.9497, -2.4109,  ..., -2.3138, -4.4707, -4.2701],\n",
            "        [-3.8547,  6.3274, -5.3263,  ..., -5.0359, -3.0556, -2.2211]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 4, 3, 0, 1, 7, 2, 3, 9, 6, 5, 0, 2, 8, 5, 1, 7, 2, 2, 0, 8, 6, 4, 3,\n",
            "        2, 7, 7, 0, 4, 1, 6, 5, 1, 3, 0, 3, 9, 2, 7, 2, 5, 7, 4, 0, 1, 9, 8, 4,\n",
            "        9, 4, 2, 4, 3, 5, 4, 0, 4, 3, 5, 8, 9, 1, 5, 8, 1, 8, 2, 4, 4, 6, 4, 1,\n",
            "        1, 4, 6, 9, 5, 2, 2, 5, 0, 2, 2, 3, 6, 2, 9, 6, 1, 4, 5, 9, 0, 1, 4, 0,\n",
            "        8, 1, 1, 6, 6, 3, 5, 2, 9, 5, 0, 6, 9, 1, 7, 6, 0, 6, 3, 5, 3, 2, 3, 3,\n",
            "        4, 9, 7, 1, 4, 4, 6, 1], device='cuda:0')\n",
            "Target: tensor([3, 4, 5, 9, 1, 7, 2, 3, 9, 6, 5, 0, 2, 9, 7, 1, 7, 2, 2, 0, 8, 6, 4, 3,\n",
            "        2, 7, 7, 0, 4, 1, 6, 5, 1, 3, 0, 3, 9, 0, 0, 2, 5, 0, 4, 0, 1, 9, 8, 4,\n",
            "        9, 4, 2, 4, 3, 3, 4, 0, 4, 3, 2, 8, 9, 1, 5, 8, 1, 8, 2, 4, 5, 2, 4, 1,\n",
            "        1, 6, 6, 8, 5, 2, 2, 5, 0, 8, 2, 3, 6, 2, 9, 6, 1, 4, 5, 9, 0, 1, 0, 0,\n",
            "        8, 1, 1, 6, 6, 9, 5, 4, 1, 7, 8, 6, 9, 1, 7, 6, 0, 9, 3, 5, 3, 2, 5, 3,\n",
            "        4, 9, 7, 1, 4, 4, 6, 1], device='cuda:0')\n",
            "Output: tensor([[-4.6881, -7.2053, -0.6572,  ..., -4.6540, -2.8201, -7.1705],\n",
            "        [ 2.5867, -3.2798, -2.4052,  ..., -4.2736,  6.9718, -5.5098],\n",
            "        [-0.5405, -3.7352, -2.3746,  ..., -3.3322,  3.4787, -4.3167],\n",
            "        ...,\n",
            "        [-5.0965, -2.8471, -6.0817,  ...,  1.4476, -7.0191, -1.8088],\n",
            "        [-3.6069, -7.5206,  2.0956,  ..., -2.0583, -4.7037, -7.7312],\n",
            "        [-5.7618, -5.5900, -0.8214,  ..., -5.0469, -5.1533, -4.6080]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 8, 8, 0, 6, 7, 7, 6, 7, 2, 5, 7, 4, 6, 2, 7, 2, 0, 3, 6, 2, 4, 4, 2,\n",
            "        0, 9, 3, 1, 8, 5, 5, 2, 4, 3, 4, 1, 0, 4, 3, 3, 4, 8, 9, 4, 3, 1, 8, 8,\n",
            "        1, 3, 3, 4, 3, 4, 9, 7, 2, 8, 1, 8, 9, 9, 3, 9, 7, 3, 0, 0, 2, 0, 3, 9,\n",
            "        2, 7, 4, 2, 0, 0, 6, 1, 6, 7, 5, 5, 4, 3, 9, 4, 0, 0, 3, 7, 9, 4, 1, 0,\n",
            "        0, 2, 3, 8, 9, 2, 8, 9, 5, 7, 9, 1, 4, 2, 2, 8, 6, 4, 8, 9, 3, 1, 0, 3,\n",
            "        5, 8, 4, 6, 6, 5, 2, 6], device='cuda:0')\n",
            "Target: tensor([3, 8, 8, 0, 6, 7, 7, 6, 7, 2, 3, 2, 2, 6, 2, 7, 4, 0, 3, 6, 2, 6, 3, 3,\n",
            "        0, 9, 5, 1, 1, 5, 3, 6, 4, 3, 4, 1, 0, 4, 5, 5, 2, 8, 9, 4, 3, 1, 8, 0,\n",
            "        1, 3, 3, 4, 4, 2, 9, 7, 6, 8, 1, 8, 9, 1, 3, 1, 7, 3, 0, 0, 2, 8, 3, 9,\n",
            "        2, 7, 2, 6, 0, 1, 6, 1, 6, 7, 5, 5, 2, 5, 9, 4, 0, 2, 3, 4, 9, 4, 1, 0,\n",
            "        0, 2, 3, 8, 9, 2, 8, 9, 5, 7, 9, 1, 4, 6, 2, 8, 4, 4, 8, 9, 3, 1, 1, 6,\n",
            "        5, 8, 4, 6, 4, 5, 2, 6], device='cuda:0')\n",
            "Output: tensor([[-6.7803, -5.1754, -3.0194,  ..., -4.2876, -4.7581, -6.8263],\n",
            "        [-4.2046, -8.0238,  2.2652,  ..., -2.8251, -6.1728, -8.0234],\n",
            "        [-7.3849, -2.5428, -6.0218,  ..., -2.6491, -4.4017, -3.8311],\n",
            "        ...,\n",
            "        [-4.1867,  3.2404, -4.7362,  ..., -5.5950, -7.0404, 10.2752],\n",
            "        [-3.8362,  2.6470, -0.1480,  ..., -3.5916, -6.2248, -3.7371],\n",
            "        [-7.7681, -9.4607,  1.2807,  ..., -4.1904, -6.3515, -7.5019]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([4, 2, 3, 1, 2, 6, 8, 5, 2, 4, 7, 1, 3, 2, 3, 9, 3, 8, 1, 9, 1, 0, 7, 3,\n",
            "        3, 2, 1, 7, 0, 2, 9, 3, 4, 1, 3, 9, 5, 8, 9, 7, 1, 6, 0, 0, 7, 7, 4, 4,\n",
            "        7, 0, 1, 0, 1, 8, 7, 6, 1, 9, 9, 2, 9, 5, 3, 6, 8, 7, 7, 3, 4, 2, 0, 2,\n",
            "        5, 6, 7, 4, 7, 1, 4, 2, 8, 4, 1, 7, 5, 6, 5, 3, 9, 2, 7, 8, 5, 9, 2, 8,\n",
            "        5, 4, 3, 7, 9, 8, 1, 4, 2, 8, 9, 3, 0, 3, 4, 0, 2, 0, 1, 4, 0, 8, 5, 5,\n",
            "        7, 9, 8, 7, 7, 9, 1, 5], device='cuda:0')\n",
            "Target: tensor([4, 2, 3, 1, 3, 6, 8, 5, 2, 2, 7, 1, 6, 2, 5, 9, 2, 8, 1, 6, 9, 2, 7, 5,\n",
            "        3, 2, 9, 7, 0, 2, 9, 3, 4, 1, 7, 9, 5, 8, 9, 7, 3, 4, 0, 9, 7, 4, 2, 4,\n",
            "        7, 0, 1, 8, 1, 0, 4, 6, 1, 9, 9, 2, 1, 2, 5, 6, 9, 7, 7, 3, 4, 2, 0, 2,\n",
            "        5, 6, 7, 3, 7, 9, 4, 2, 0, 6, 1, 7, 5, 6, 5, 3, 9, 2, 7, 8, 5, 9, 5, 8,\n",
            "        5, 4, 3, 7, 9, 8, 1, 2, 2, 8, 9, 3, 0, 8, 4, 0, 2, 0, 1, 4, 1, 8, 5, 5,\n",
            "        7, 9, 8, 3, 7, 9, 1, 5], device='cuda:0')\n",
            "Output: tensor([[-5.7220, -4.3905, -1.7112,  ..., -3.6334, -3.7122, -7.8761],\n",
            "        [-1.0437,  0.7368, -4.7764,  ..., -4.3351, -5.7693,  6.1848],\n",
            "        [ 0.7299, -4.5325, -4.3322,  ..., -4.2911,  4.7485, -3.0671],\n",
            "        ...,\n",
            "        [ 1.7657, -3.1445, -2.9771,  ..., -4.6437,  2.0871, -2.5734],\n",
            "        [-5.5940, -7.5413,  0.8905,  ..., -2.1524, -6.3990, -7.3536],\n",
            "        [-3.8215, -2.6770, -3.8115,  ..., -2.3254, -6.3646, -3.0676]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([6, 9, 8, 4, 2, 0, 9, 0, 8, 4, 9, 2, 2, 9, 8, 1, 1, 1, 8, 3, 7, 6, 4, 2,\n",
            "        9, 7, 0, 5, 5, 8, 8, 8, 7, 9, 6, 4, 7, 7, 5, 7, 6, 7, 1, 7, 3, 6, 8, 2,\n",
            "        6, 6, 7, 1, 5, 9, 7, 8, 7, 1, 1, 6, 0, 3, 1, 0, 1, 2, 5, 2, 2, 3, 4, 9,\n",
            "        8, 7, 4, 2, 9, 3, 6, 2, 7, 0, 2, 3, 5, 1, 9, 0, 3, 3, 1, 5, 6, 3, 2, 4,\n",
            "        6, 8, 8, 9, 6, 6, 1, 0, 7, 5, 8, 4, 9, 5, 8, 1, 4, 2, 5, 0, 3, 8, 9, 5,\n",
            "        2, 8, 7, 9, 9, 8, 4, 4], device='cuda:0')\n",
            "Target: tensor([6, 9, 8, 7, 2, 0, 9, 0, 8, 5, 9, 4, 2, 9, 8, 1, 9, 1, 8, 3, 7, 6, 4, 2,\n",
            "        3, 7, 0, 3, 5, 8, 8, 8, 7, 9, 6, 2, 7, 4, 5, 7, 6, 7, 1, 7, 3, 6, 8, 2,\n",
            "        6, 6, 7, 1, 5, 9, 7, 1, 7, 0, 1, 6, 3, 3, 9, 0, 1, 2, 3, 2, 2, 5, 4, 9,\n",
            "        8, 7, 4, 4, 9, 7, 6, 7, 7, 1, 2, 3, 5, 1, 9, 0, 3, 3, 1, 5, 6, 6, 2, 4,\n",
            "        6, 8, 8, 9, 6, 6, 1, 0, 7, 5, 8, 2, 1, 5, 8, 1, 4, 7, 5, 0, 3, 9, 9, 5,\n",
            "        2, 8, 4, 1, 9, 0, 4, 4], device='cuda:0')\n",
            "Output: tensor([[ 2.0693, -4.7068, -2.1670,  ..., -5.3671,  4.3585, -2.1145],\n",
            "        [-7.5353, -5.4670,  3.8424,  ..., -4.5898, -5.1532, -8.6785],\n",
            "        [-3.8815, -4.4780, -1.4135,  ..., -2.1966, -5.5673,  0.0134],\n",
            "        ...,\n",
            "        [-5.1838, -5.3408, -4.5767,  ..., -4.1114, -9.3243, -3.9874],\n",
            "        [-3.9087, -5.7001, -4.1047,  ...,  0.5931, -4.0635, -3.9164],\n",
            "        [-1.0712, -2.0725, -3.9033,  ..., -2.8654,  3.1770, -5.4001]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([8, 2, 9, 0, 7, 9, 8, 7, 1, 6, 2, 1, 8, 5, 9, 7, 6, 7, 7, 1, 1, 2, 4, 2,\n",
            "        0, 6, 8, 8, 6, 2, 3, 9, 8, 6, 8, 0, 0, 5, 8, 6, 8, 2, 7, 7, 7, 9, 8, 1,\n",
            "        6, 5, 9, 7, 0, 9, 6, 4, 8, 1, 9, 4, 0, 4, 1, 4, 3, 6, 2, 2, 8, 0, 0, 7,\n",
            "        0, 3, 7, 3, 6, 7, 7, 4, 5, 4, 3, 3, 5, 4, 7, 1, 0, 7, 7, 1, 6, 5, 0, 7,\n",
            "        7, 4, 2, 1, 8, 5, 9, 2, 5, 6, 5, 2, 7, 5, 5, 8, 1, 9, 6, 8, 7, 7, 8, 5,\n",
            "        7, 5, 4, 9, 7, 3, 7, 8], device='cuda:0')\n",
            "Target: tensor([8, 2, 9, 0, 7, 9, 8, 7, 1, 3, 2, 9, 9, 5, 9, 7, 6, 7, 7, 1, 1, 2, 4, 2,\n",
            "        0, 6, 8, 7, 6, 2, 2, 9, 8, 2, 4, 2, 0, 5, 8, 6, 8, 2, 7, 7, 3, 1, 8, 1,\n",
            "        6, 5, 9, 7, 8, 9, 6, 4, 8, 1, 9, 4, 0, 4, 1, 4, 3, 6, 2, 2, 7, 0, 0, 7,\n",
            "        0, 7, 4, 3, 6, 7, 7, 4, 5, 4, 3, 5, 5, 4, 7, 1, 0, 7, 7, 1, 6, 5, 0, 7,\n",
            "        7, 4, 6, 1, 8, 5, 9, 3, 5, 6, 2, 2, 7, 3, 5, 8, 1, 0, 6, 8, 7, 8, 8, 5,\n",
            "        7, 5, 4, 9, 7, 3, 3, 8], device='cuda:0')\n",
            "Output: tensor([[ 1.0580, -0.0745, -1.8827,  ..., -5.2537,  3.5048, -5.0927],\n",
            "        [-5.3082, -1.8826, -3.1253,  ..., -5.1487, -3.7956, -5.5531],\n",
            "        [-4.6847,  2.4792, -8.4782,  ..., -7.0622, -1.7563,  7.6248],\n",
            "        ...,\n",
            "        [ 1.5151, -5.6275,  0.6356,  ..., -3.2559, -2.8523, -7.6406],\n",
            "        [-4.0403, -3.4495, -6.8186,  ..., -7.9361,  2.5887, -2.2882],\n",
            "        [ 3.3468, -5.7533, -2.9123,  ..., -4.0697,  4.3665, -4.0484]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([8, 3, 9, 4, 7, 2, 0, 8, 0, 7, 3, 3, 2, 5, 2, 9, 5, 0, 6, 8, 2, 4, 8, 6,\n",
            "        4, 7, 6, 0, 8, 0, 0, 6, 1, 1, 6, 1, 4, 2, 1, 3, 2, 5, 4, 3, 7, 8, 0, 5,\n",
            "        1, 8, 2, 4, 3, 9, 6, 5, 6, 0, 7, 3, 3, 2, 7, 4, 5, 6, 0, 6, 1, 8, 1, 0,\n",
            "        9, 3, 1, 4, 5, 3, 3, 3, 3, 6, 7, 2, 0, 0, 5, 7, 4, 3, 7, 0, 6, 1, 5, 9,\n",
            "        0, 9, 0, 3, 8, 4, 0, 0, 4, 1, 0, 2, 2, 4, 1, 1, 4, 7, 9, 4, 1, 4, 7, 1,\n",
            "        0, 3, 9, 0, 4, 0, 8, 8], device='cuda:0')\n",
            "Target: tensor([8, 3, 9, 4, 7, 2, 0, 8, 0, 7, 3, 3, 2, 5, 2, 4, 4, 0, 4, 8, 2, 4, 0, 6,\n",
            "        4, 5, 6, 0, 8, 8, 0, 6, 1, 1, 6, 1, 4, 2, 1, 1, 2, 4, 4, 5, 5, 8, 8, 5,\n",
            "        1, 8, 2, 3, 3, 9, 6, 6, 5, 0, 7, 3, 3, 2, 7, 4, 5, 6, 0, 2, 1, 8, 1, 0,\n",
            "        9, 3, 1, 0, 5, 4, 2, 3, 3, 6, 7, 6, 0, 0, 5, 7, 4, 7, 7, 0, 6, 1, 3, 9,\n",
            "        0, 9, 0, 3, 8, 4, 8, 8, 4, 1, 0, 2, 2, 4, 1, 1, 2, 4, 3, 4, 1, 4, 7, 1,\n",
            "        0, 0, 9, 0, 4, 2, 8, 8], device='cuda:0')\n",
            "Output: tensor([[-6.0370, -5.8585, -0.9236,  ..., -1.4427, -5.9176, -6.8718],\n",
            "        [-3.8283, -2.5308, -3.8706,  ..., -2.6192, -1.2424, -3.8946],\n",
            "        [ 2.3218, -4.6837, -1.0136,  ..., -4.2542,  2.6879, -6.1496],\n",
            "        ...,\n",
            "        [ 1.7365, -5.0317, -2.3057,  ..., -3.9643,  0.8704, -5.8535],\n",
            "        [-4.8575, -5.9471, -3.3194,  ...,  0.3888, -5.6281, -5.7219],\n",
            "        [ 1.0765, -3.1312, -5.7413,  ..., -3.4813,  6.4045, -3.7652]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([2, 5, 8, 4, 8, 8, 2, 2, 5, 9, 2, 9, 1, 1, 5, 4, 7, 5, 8, 6, 7, 1, 5, 5,\n",
            "        3, 4, 1, 0, 9, 9, 8, 0, 8, 6, 8, 5, 8, 9, 4, 6, 2, 6, 3, 7, 7, 0, 0, 1,\n",
            "        7, 5, 1, 6, 8, 3, 1, 2, 8, 7, 3, 6, 9, 3, 0, 3, 1, 7, 2, 6, 1, 9, 0, 0,\n",
            "        8, 9, 9, 2, 8, 6, 5, 5, 6, 0, 5, 3, 0, 5, 4, 7, 2, 3, 1, 0, 8, 8, 1, 2,\n",
            "        0, 5, 4, 5, 9, 6, 7, 1, 0, 6, 9, 4, 0, 7, 3, 3, 9, 9, 9, 7, 0, 1, 3, 5,\n",
            "        7, 6, 3, 0, 8, 0, 4, 8], device='cuda:0')\n",
            "Target: tensor([5, 5, 0, 4, 0, 8, 6, 2, 5, 9, 2, 9, 1, 1, 5, 4, 7, 5, 8, 6, 2, 1, 5, 5,\n",
            "        3, 4, 1, 8, 9, 9, 8, 9, 8, 6, 8, 5, 8, 9, 4, 6, 2, 6, 3, 7, 4, 0, 0, 1,\n",
            "        7, 5, 1, 5, 9, 3, 1, 6, 8, 7, 3, 6, 9, 1, 2, 0, 1, 7, 2, 6, 1, 9, 0, 0,\n",
            "        8, 9, 9, 2, 8, 6, 2, 5, 6, 0, 3, 3, 0, 7, 4, 7, 5, 0, 1, 6, 8, 8, 1, 2,\n",
            "        1, 5, 4, 5, 9, 6, 7, 1, 0, 6, 9, 2, 7, 7, 3, 9, 9, 1, 9, 7, 0, 1, 3, 5,\n",
            "        4, 6, 3, 8, 8, 0, 4, 8], device='cuda:0')\n",
            "Output: tensor([[ -4.3361,  -2.9965,  -3.5493,  ...,  -2.1296,  -5.1399,  -2.0793],\n",
            "        [ -6.5588,  -7.9931,   0.3346,  ...,  -3.7756,  -6.9020,  -9.6109],\n",
            "        [ -2.3613,  -5.4139,  -1.0789,  ...,   2.9995,  -6.1130,  -6.8276],\n",
            "        ...,\n",
            "        [ -1.0251,  -5.4089,  -0.2737,  ...,  -4.1368,  -5.9148,  -4.0827],\n",
            "        [ -1.5577,   0.8417,  -2.4701,  ...,  -2.5410,  -2.5044,  -5.5663],\n",
            "        [ -7.5844,  -2.9523,  -5.0121,  ...,   3.2943, -12.0109,  -3.6330]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([5, 6, 7, 2, 0, 4, 3, 2, 6, 8, 4, 9, 9, 9, 3, 0, 4, 2, 8, 1, 1, 0, 0, 3,\n",
            "        4, 4, 1, 1, 9, 7, 7, 4, 6, 8, 6, 0, 2, 8, 5, 3, 5, 3, 5, 7, 9, 8, 4, 4,\n",
            "        3, 1, 4, 8, 4, 6, 5, 3, 0, 8, 9, 5, 7, 6, 2, 0, 4, 9, 9, 0, 5, 2, 7, 5,\n",
            "        1, 1, 0, 2, 8, 2, 1, 1, 7, 5, 2, 3, 4, 1, 2, 9, 2, 9, 3, 4, 8, 9, 0, 0,\n",
            "        4, 9, 0, 2, 2, 0, 6, 8, 7, 3, 3, 8, 9, 0, 2, 5, 3, 6, 1, 3, 9, 5, 9, 5,\n",
            "        4, 4, 0, 1, 1, 6, 1, 4], device='cuda:0')\n",
            "Target: tensor([3, 6, 7, 0, 0, 4, 5, 2, 6, 8, 4, 9, 9, 9, 2, 0, 4, 2, 8, 1, 1, 0, 0, 3,\n",
            "        7, 4, 1, 1, 9, 7, 7, 4, 6, 8, 6, 0, 2, 8, 5, 3, 5, 3, 5, 7, 9, 8, 4, 4,\n",
            "        3, 1, 4, 8, 3, 6, 5, 3, 0, 8, 9, 5, 7, 6, 2, 0, 4, 9, 9, 0, 5, 2, 3, 6,\n",
            "        1, 1, 0, 2, 8, 2, 1, 1, 7, 5, 2, 3, 4, 1, 2, 9, 2, 1, 3, 4, 8, 9, 0, 0,\n",
            "        4, 9, 0, 2, 2, 0, 6, 8, 7, 3, 3, 8, 9, 0, 2, 5, 3, 6, 1, 3, 9, 5, 0, 5,\n",
            "        4, 4, 0, 1, 1, 6, 1, 7], device='cuda:0')\n",
            "Output: tensor([[-4.6016,  1.3542, -6.6816,  ..., -3.0035, -4.1693,  6.1630],\n",
            "        [-2.7822,  1.4383, -4.7992,  ..., -4.4907, -0.0892,  2.3317],\n",
            "        [-5.1675, -5.4471, -1.6130,  ..., -1.2591, -6.1274, -4.4371],\n",
            "        ...,\n",
            "        [-3.4675, -8.1837,  1.2168,  ..., -2.1941, -9.2270, -7.8931],\n",
            "        [-2.7793, -7.0629, -0.6309,  ..., -0.2395, -8.6847, -9.6230],\n",
            "        [-2.7097,  4.4408, -4.7840,  ..., -4.7046, -5.4465,  2.6055]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([9, 9, 6, 2, 4, 7, 1, 3, 4, 7, 0, 4, 8, 4, 0, 5, 8, 8, 8, 3, 3, 5, 7, 7,\n",
            "        0, 4, 1, 5, 9, 7, 0, 6, 8, 3, 9, 0, 9, 8, 9, 3, 5, 9, 2, 2, 4, 0, 5, 5,\n",
            "        4, 4, 5, 1, 1, 0, 2, 2, 9, 3, 2, 8, 9, 2, 3, 1, 7, 4, 0, 2, 7, 1, 6, 5,\n",
            "        3, 3, 4, 5, 8, 0, 0, 3, 1, 3, 7, 3, 4, 7, 8, 4, 8, 9, 9, 4, 4, 4, 3, 6,\n",
            "        4, 3, 4, 6, 6, 2, 8, 7, 4, 8, 0, 5, 9, 3, 2, 0, 7, 5, 1, 9, 7, 9, 1, 0,\n",
            "        4, 2, 1, 4, 4, 2, 7, 1], device='cuda:0')\n",
            "Target: tensor([9, 9, 6, 2, 4, 3, 8, 3, 4, 7, 0, 2, 8, 4, 8, 3, 8, 8, 8, 3, 3, 5, 7, 7,\n",
            "        0, 4, 1, 5, 9, 7, 0, 6, 8, 4, 9, 0, 1, 8, 9, 6, 3, 9, 2, 4, 4, 0, 3, 3,\n",
            "        5, 4, 5, 1, 1, 8, 2, 2, 9, 3, 7, 8, 9, 2, 3, 1, 7, 3, 2, 3, 0, 1, 9, 5,\n",
            "        5, 3, 4, 5, 2, 0, 0, 3, 1, 3, 4, 7, 4, 2, 8, 4, 8, 9, 9, 4, 2, 4, 3, 6,\n",
            "        4, 6, 4, 6, 6, 3, 8, 7, 4, 8, 0, 5, 9, 3, 9, 0, 7, 5, 1, 9, 7, 9, 1, 8,\n",
            "        4, 2, 1, 6, 4, 3, 0, 1], device='cuda:0')\n",
            "Output: tensor([[-6.1578, -6.0109, -5.5867,  ..., -1.5389, -6.4478, -4.5577],\n",
            "        [-0.3554, -0.4533, -6.0638,  ..., -3.9692,  0.8528,  2.1332],\n",
            "        [-4.2467, -4.1425, -3.1021,  ..., -1.7203, -4.3256, -4.8470],\n",
            "        ...,\n",
            "        [-5.4960, -5.2304, -0.3544,  ..., -3.4611, -6.0838, -7.1088],\n",
            "        [-5.3016, -1.9142, -4.2004,  ..., -0.5647, -6.3820, -2.4802],\n",
            "        [ 0.1366, -5.2138, -4.4972,  ..., -3.6455,  0.7621, -3.4046]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([4, 9, 6, 1, 7, 9, 5, 2, 4, 3, 3, 9, 4, 8, 2, 5, 3, 4, 5, 4, 5, 7, 4, 5,\n",
            "        5, 1, 3, 7, 8, 3, 6, 0, 2, 5, 2, 5, 7, 4, 5, 1, 6, 8, 6, 8, 3, 2, 7, 7,\n",
            "        9, 7, 1, 4, 7, 3, 3, 1, 7, 3, 0, 0, 6, 6, 8, 6, 6, 0, 3, 4, 7, 4, 9, 4,\n",
            "        9, 9, 2, 4, 1, 4, 8, 3, 7, 1, 2, 6, 8, 4, 7, 8, 5, 5, 6, 5, 6, 0, 6, 4,\n",
            "        9, 4, 2, 7, 3, 8, 3, 9, 4, 1, 6, 9, 9, 3, 3, 9, 8, 1, 2, 9, 5, 9, 0, 9,\n",
            "        7, 2, 2, 4, 4, 6, 4, 8], device='cuda:0')\n",
            "Target: tensor([4, 8, 6, 1, 7, 1, 3, 5, 4, 3, 3, 9, 7, 8, 2, 5, 5, 4, 5, 4, 5, 7, 2, 5,\n",
            "        5, 1, 7, 7, 8, 3, 6, 0, 2, 5, 2, 3, 7, 4, 2, 1, 6, 8, 6, 8, 3, 2, 7, 7,\n",
            "        9, 7, 1, 4, 7, 4, 6, 1, 7, 3, 0, 0, 6, 6, 8, 6, 6, 0, 3, 4, 7, 4, 9, 4,\n",
            "        9, 9, 3, 4, 1, 4, 0, 3, 7, 1, 2, 2, 8, 4, 7, 8, 5, 5, 6, 5, 6, 0, 6, 4,\n",
            "        9, 3, 2, 7, 3, 8, 3, 9, 4, 1, 6, 9, 9, 4, 3, 9, 8, 1, 6, 9, 5, 9, 0, 9,\n",
            "        7, 2, 2, 4, 2, 6, 4, 8], device='cuda:0')\n",
            "Output: tensor([[ 0.3089,  3.7445, -4.3130,  ..., -4.9460, -2.6839, -0.5062],\n",
            "        [-2.9184, -4.2337, -3.5097,  ..., -4.7243, -0.1531, -1.1847],\n",
            "        [-5.3832, -4.7398, -6.1688,  ..., -3.6717, -5.6547, -5.6224],\n",
            "        ...,\n",
            "        [ 1.8877, -1.7665, -3.0222,  ..., -4.6505,  3.2345, -1.2022],\n",
            "        [-0.6963, -4.9370,  1.1059,  ..., -3.2889, -4.7031, -3.9178],\n",
            "        [-1.2192,  6.6467, -4.9049,  ..., -3.3354, -4.1058,  0.5644]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([1, 8, 5, 6, 3, 1, 8, 6, 0, 7, 6, 5, 0, 2, 7, 6, 1, 2, 8, 1, 6, 4, 9, 0,\n",
            "        0, 9, 1, 0, 5, 3, 1, 6, 9, 8, 9, 0, 6, 1, 3, 3, 3, 5, 6, 1, 2, 7, 2, 4,\n",
            "        8, 3, 6, 8, 6, 9, 1, 9, 0, 6, 4, 3, 9, 5, 4, 1, 9, 2, 0, 7, 3, 5, 1, 8,\n",
            "        3, 0, 5, 2, 8, 8, 3, 9, 6, 0, 9, 5, 3, 4, 1, 5, 0, 7, 3, 9, 4, 5, 3, 1,\n",
            "        2, 3, 2, 9, 1, 7, 1, 7, 0, 1, 2, 1, 5, 3, 8, 4, 1, 3, 1, 9, 9, 7, 0, 1,\n",
            "        7, 6, 0, 6, 5, 8, 2, 1], device='cuda:0')\n",
            "Target: tensor([1, 9, 5, 6, 5, 1, 8, 6, 6, 7, 6, 5, 0, 2, 7, 6, 1, 2, 3, 1, 6, 4, 9, 0,\n",
            "        3, 9, 1, 0, 5, 3, 1, 6, 9, 8, 9, 0, 6, 1, 6, 2, 3, 5, 6, 9, 0, 7, 2, 4,\n",
            "        0, 3, 6, 8, 6, 9, 1, 9, 0, 6, 4, 5, 9, 5, 4, 1, 9, 2, 0, 7, 3, 5, 1, 8,\n",
            "        3, 0, 5, 2, 8, 8, 3, 9, 6, 0, 3, 5, 3, 4, 1, 5, 0, 7, 3, 9, 4, 5, 3, 1,\n",
            "        4, 2, 4, 9, 9, 7, 1, 7, 0, 1, 2, 1, 5, 3, 8, 4, 1, 5, 1, 9, 9, 7, 0, 1,\n",
            "        7, 6, 2, 6, 5, 0, 3, 1], device='cuda:0')\n",
            "Output: tensor([[-4.6166, -5.1796, -0.9679,  ..., -1.5149, -5.5941, -4.8451],\n",
            "        [-2.7246, -2.6834, -2.3431,  ..., -4.1315, -2.5765, -0.5664],\n",
            "        [-5.8619, -1.2419, -5.6940,  ..., -2.4600, -8.1454,  5.8578],\n",
            "        ...,\n",
            "        [-7.2016, -8.5490,  1.4319,  ..., -4.3818, -5.8155, -9.7557],\n",
            "        [-3.7628, -3.0227, -2.2366,  ..., -1.8253, -6.0227, -2.7980],\n",
            "        [ 3.2630, -2.2743,  0.6520,  ..., -4.9071, -0.3349, -5.7971]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 9, 9, 8, 8, 8, 8, 5, 8, 2, 9, 1, 3, 6, 7, 6, 0, 8, 6, 2, 4, 2, 3, 5,\n",
            "        5, 4, 1, 6, 2, 1, 6, 1, 7, 9, 6, 2, 3, 0, 4, 0, 9, 7, 3, 5, 6, 8, 9, 8,\n",
            "        1, 1, 3, 5, 2, 5, 3, 8, 7, 3, 0, 7, 6, 5, 7, 1, 9, 9, 1, 1, 9, 3, 5, 5,\n",
            "        3, 1, 3, 0, 6, 6, 6, 6, 8, 1, 4, 5, 1, 1, 2, 2, 9, 0, 9, 8, 5, 4, 4, 7,\n",
            "        4, 8, 3, 3, 2, 4, 5, 7, 0, 1, 2, 9, 8, 1, 7, 3, 5, 9, 1, 9, 1, 1, 3, 6,\n",
            "        5, 4, 8, 3, 5, 6, 5, 0], device='cuda:0')\n",
            "Target: tensor([3, 8, 9, 9, 0, 8, 8, 7, 9, 2, 9, 1, 3, 6, 7, 6, 0, 8, 6, 2, 4, 2, 3, 5,\n",
            "        5, 4, 1, 6, 7, 1, 6, 1, 7, 1, 6, 2, 3, 0, 4, 2, 9, 7, 5, 5, 6, 0, 8, 8,\n",
            "        1, 1, 3, 5, 6, 5, 3, 8, 7, 5, 0, 7, 7, 5, 7, 1, 9, 9, 1, 9, 9, 3, 5, 5,\n",
            "        3, 0, 3, 0, 6, 6, 5, 6, 8, 9, 4, 5, 1, 1, 2, 7, 9, 0, 1, 2, 5, 4, 4, 7,\n",
            "        4, 8, 3, 3, 5, 3, 5, 7, 0, 1, 2, 9, 8, 1, 7, 3, 5, 9, 1, 9, 1, 1, 3, 6,\n",
            "        7, 4, 8, 3, 5, 6, 7, 0], device='cuda:0')\n",
            "Output: tensor([[-1.1760,  6.3921, -5.0614,  ..., -3.8297, -3.2310,  4.0118],\n",
            "        [ 0.1243, -8.0586, -3.3502,  ..., -1.5652, -0.2893, -2.3103],\n",
            "        [-6.9938, -9.8174, -3.3034,  ...,  7.0988, -8.8784, -7.4650],\n",
            "        ...,\n",
            "        [-7.3993, -7.6216, -1.7417,  ..., -3.8720, -6.6263, -5.3853],\n",
            "        [-5.3452, -9.6129,  4.7962,  ..., -5.5661, -4.8692, -8.1670],\n",
            "        [-3.0265, -1.7992, -1.6329,  ..., -0.2883, -6.4983, -3.7833]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([1, 0, 7, 7, 3, 3, 5, 1, 7, 6, 8, 4, 4, 5, 6, 3, 4, 7, 7, 4, 6, 0, 7, 2,\n",
            "        6, 1, 7, 1, 8, 0, 2, 6, 1, 5, 4, 0, 9, 3, 4, 0, 2, 5, 8, 5, 0, 0, 4, 6,\n",
            "        3, 9, 2, 2, 5, 7, 3, 5, 1, 7, 3, 1, 6, 7, 9, 0, 9, 2, 1, 2, 3, 1, 5, 9,\n",
            "        5, 0, 4, 1, 4, 9, 5, 1, 4, 8, 2, 4, 5, 2, 8, 1, 7, 5, 5, 9, 3, 2, 3, 9,\n",
            "        7, 6, 1, 0, 9, 0, 9, 8, 7, 7, 5, 2, 3, 5, 2, 7, 9, 7, 6, 9, 1, 9, 8, 6,\n",
            "        5, 4, 7, 5, 6, 5, 2, 7], device='cuda:0')\n",
            "Target: tensor([1, 7, 7, 2, 3, 3, 5, 1, 7, 6, 8, 4, 4, 5, 7, 3, 4, 7, 7, 3, 6, 0, 7, 2,\n",
            "        6, 7, 4, 1, 8, 0, 2, 2, 1, 5, 4, 0, 9, 3, 4, 4, 2, 3, 8, 5, 0, 0, 4, 6,\n",
            "        3, 9, 2, 0, 5, 7, 5, 5, 1, 7, 3, 1, 6, 7, 9, 0, 1, 2, 1, 4, 6, 1, 5, 9,\n",
            "        5, 0, 4, 9, 4, 9, 7, 1, 6, 8, 0, 6, 5, 0, 8, 1, 7, 5, 6, 9, 3, 2, 3, 9,\n",
            "        8, 6, 1, 0, 9, 0, 9, 8, 7, 7, 5, 2, 3, 5, 5, 7, 8, 7, 6, 9, 1, 9, 8, 6,\n",
            "        5, 6, 7, 5, 6, 7, 2, 5], device='cuda:0')\n",
            "Output: tensor([[-2.0971, -4.3048, -3.3611,  ..., -4.2753, -3.4695,  4.6469],\n",
            "        [-6.8307, -6.4388, -2.0614,  ..., -2.9700, -6.6187, -5.5520],\n",
            "        [-8.6150, -6.7657, -0.8611,  ...,  0.6717, -7.7944, -6.8657],\n",
            "        ...,\n",
            "        [-1.4874,  3.9184, -4.5559,  ..., -2.7973, -1.7194,  0.9565],\n",
            "        [-4.4406, -6.4747,  0.2012,  ...,  2.2214, -5.8997, -8.5070],\n",
            "        [-4.2248, -4.5990, -3.6486,  ..., -0.2981, -6.6122, -4.0383]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([9, 6, 6, 7, 0, 1, 0, 1, 3, 2, 2, 6, 1, 2, 6, 1, 9, 9, 8, 6, 8, 3, 8, 9,\n",
            "        0, 4, 0, 3, 3, 6, 4, 3, 9, 7, 2, 3, 5, 6, 4, 6, 0, 3, 5, 5, 0, 2, 9, 4,\n",
            "        7, 3, 9, 4, 3, 0, 6, 6, 3, 4, 3, 5, 9, 1, 1, 5, 4, 3, 5, 4, 8, 0, 0, 3,\n",
            "        9, 6, 7, 4, 2, 8, 8, 6, 2, 1, 9, 6, 3, 8, 3, 0, 4, 7, 5, 8, 0, 3, 1, 2,\n",
            "        6, 1, 3, 2, 0, 7, 5, 0, 0, 0, 1, 6, 3, 3, 8, 4, 4, 3, 7, 6, 0, 4, 0, 3,\n",
            "        1, 4, 0, 1, 2, 1, 7, 7], device='cuda:0')\n",
            "Target: tensor([9, 6, 6, 7, 0, 1, 0, 1, 3, 2, 5, 6, 1, 2, 5, 1, 9, 9, 8, 6, 8, 6, 8, 9,\n",
            "        0, 4, 0, 3, 3, 6, 4, 9, 9, 7, 2, 3, 5, 6, 4, 6, 0, 3, 7, 5, 0, 2, 9, 2,\n",
            "        7, 2, 9, 2, 4, 0, 6, 6, 3, 4, 5, 5, 9, 1, 1, 4, 4, 3, 5, 4, 8, 0, 0, 3,\n",
            "        9, 4, 7, 4, 2, 8, 3, 6, 3, 1, 9, 6, 3, 8, 3, 8, 4, 7, 5, 7, 5, 3, 1, 2,\n",
            "        6, 1, 3, 2, 0, 7, 5, 0, 0, 0, 1, 4, 3, 5, 8, 4, 3, 1, 7, 1, 0, 4, 2, 4,\n",
            "        1, 4, 0, 1, 2, 1, 7, 7], device='cuda:0')\n",
            "Output: tensor([[-3.3561, -2.0279, -3.8033,  ..., -2.6784, -2.5157,  2.8995],\n",
            "        [ 2.6856, -3.2954, -1.8486,  ..., -6.0942,  4.6612, -3.5545],\n",
            "        [-2.1340, -6.4513, -3.8539,  ...,  2.7862, -5.6753, -4.2414],\n",
            "        ...,\n",
            "        [-4.3665, -2.7353,  2.9069,  ..., -1.9506, -6.1655, -7.4766],\n",
            "        [ 1.4217, -4.6568, -4.1836,  ..., -4.0783, -0.0997, -4.3946],\n",
            "        [-1.0807,  5.8759, -4.1755,  ..., -6.6463, -0.3927, -1.5861]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([9, 8, 7, 5, 4, 0, 9, 0, 0, 8, 4, 0, 0, 6, 4, 8, 6, 2, 4, 6, 3, 5, 1, 5,\n",
            "        5, 5, 2, 4, 9, 8, 0, 0, 8, 4, 4, 4, 6, 1, 6, 7, 4, 2, 5, 9, 7, 0, 8, 8,\n",
            "        4, 6, 5, 7, 9, 7, 0, 5, 7, 7, 3, 9, 9, 3, 0, 9, 5, 3, 7, 9, 4, 4, 1, 7,\n",
            "        7, 1, 7, 1, 9, 8, 7, 5, 0, 4, 7, 2, 9, 7, 4, 9, 3, 5, 1, 0, 3, 6, 8, 3,\n",
            "        2, 4, 7, 1, 1, 3, 9, 7, 5, 1, 4, 8, 7, 0, 1, 6, 9, 0, 2, 7, 7, 8, 9, 0,\n",
            "        3, 2, 3, 7, 8, 2, 0, 1], device='cuda:0')\n",
            "Target: tensor([9, 8, 7, 5, 4, 0, 9, 0, 0, 8, 2, 0, 0, 2, 4, 8, 6, 2, 4, 6, 3, 5, 1, 5,\n",
            "        3, 7, 2, 2, 9, 8, 0, 0, 0, 3, 4, 4, 6, 1, 6, 7, 4, 4, 3, 9, 4, 0, 8, 0,\n",
            "        4, 6, 5, 7, 9, 7, 0, 5, 7, 7, 3, 1, 9, 3, 0, 9, 5, 3, 7, 9, 4, 4, 1, 7,\n",
            "        7, 1, 4, 1, 2, 8, 7, 0, 0, 4, 7, 2, 9, 7, 6, 9, 3, 5, 8, 0, 3, 6, 8, 3,\n",
            "        2, 4, 7, 1, 1, 3, 9, 7, 5, 1, 0, 8, 7, 0, 1, 6, 9, 3, 2, 7, 7, 8, 1, 0,\n",
            "        3, 4, 6, 7, 5, 2, 0, 1], device='cuda:0')\n",
            "Output: tensor([[-4.6551, -3.8787, -3.3312,  ..., -3.9697, -1.2381, -4.7626],\n",
            "        [-4.3545, -5.8577, -2.8604,  ..., -1.4611, -6.2685, -2.9531],\n",
            "        [-4.2228,  4.8224, -4.3503,  ..., -6.0247, -2.9046, -1.6309],\n",
            "        ...,\n",
            "        [-7.1505, -7.2713, -1.2943,  ..., -3.0905, -5.5193, -7.7769],\n",
            "        [-7.7501, -6.8727,  1.0917,  ..., -6.9142, -9.8018, -8.9295],\n",
            "        [-0.0180, -2.4000, -3.4550,  ..., -4.1537,  4.6874, -7.0239]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([5, 3, 1, 4, 1, 3, 0, 8, 6, 3, 1, 6, 5, 1, 1, 9, 1, 4, 1, 8, 1, 9, 1, 6,\n",
            "        9, 0, 4, 7, 2, 2, 7, 7, 9, 1, 4, 6, 6, 3, 4, 4, 1, 3, 8, 2, 6, 6, 1, 6,\n",
            "        3, 6, 3, 8, 6, 6, 7, 1, 9, 5, 6, 7, 6, 0, 7, 1, 9, 5, 2, 4, 7, 7, 2, 3,\n",
            "        9, 1, 3, 6, 0, 2, 2, 1, 1, 8, 6, 5, 0, 0, 0, 5, 7, 7, 8, 7, 5, 5, 1, 6,\n",
            "        1, 5, 1, 0, 6, 3, 3, 2, 1, 7, 5, 1, 9, 8, 3, 6, 9, 7, 5, 0, 2, 5, 9, 4,\n",
            "        6, 3, 9, 6, 8, 4, 6, 8], device='cuda:0')\n",
            "Target: tensor([5, 5, 1, 4, 1, 3, 0, 8, 6, 2, 1, 3, 6, 4, 1, 9, 0, 4, 1, 0, 1, 9, 8, 6,\n",
            "        9, 2, 4, 7, 2, 2, 7, 4, 9, 1, 3, 2, 6, 3, 4, 4, 9, 4, 8, 2, 6, 6, 1, 6,\n",
            "        3, 6, 5, 8, 4, 6, 7, 1, 9, 3, 6, 7, 6, 0, 7, 1, 9, 5, 2, 6, 7, 7, 6, 5,\n",
            "        9, 1, 5, 6, 0, 2, 0, 9, 1, 8, 3, 5, 0, 0, 0, 5, 7, 7, 8, 5, 5, 5, 1, 6,\n",
            "        1, 5, 1, 0, 6, 2, 3, 2, 1, 7, 5, 1, 9, 8, 3, 6, 9, 7, 3, 0, 2, 3, 9, 4,\n",
            "        4, 3, 9, 6, 8, 6, 6, 8], device='cuda:0')\n",
            "Output: tensor([[-3.4332, -7.7556, -3.9656,  ..., -1.6991, -6.8555, -6.6304],\n",
            "        [-7.2365, -8.3281, -0.9839,  ..., -1.9501, -6.4390, -9.3509],\n",
            "        [-6.8182, -5.6740, -2.9358,  ..., -2.4967, -3.9065, -4.3317],\n",
            "        ...,\n",
            "        [-3.0239, -5.7919, -2.7043,  ..., -2.4891, -7.5372, -6.0730],\n",
            "        [-3.8305, -6.5196, -3.3761,  ..., -3.2843, -7.5451, -4.3934],\n",
            "        [-3.6861, -6.0090,  0.2482,  ..., -1.2952, -3.8173, -5.2070]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([5, 4, 3, 6, 7, 6, 4, 6, 9, 6, 2, 4, 8, 1, 3, 6, 4, 9, 2, 6, 0, 0, 8, 7,\n",
            "        5, 9, 2, 8, 3, 0, 6, 3, 6, 3, 7, 4, 5, 7, 9, 0, 0, 6, 7, 2, 0, 5, 5, 7,\n",
            "        5, 2, 9, 4, 6, 5, 7, 3, 3, 6, 4, 3, 6, 3, 1, 6, 7, 0, 5, 0, 1, 9, 7, 3,\n",
            "        5, 2, 3, 7, 5, 6, 3, 0, 0, 0, 8, 0, 3, 2, 4, 3, 3, 7, 9, 6, 9, 8, 1, 2,\n",
            "        6, 7, 4, 4, 1, 3, 7, 0, 9, 3, 8, 3, 8, 5, 8, 5, 6, 3, 0, 0, 3, 8, 1, 9,\n",
            "        1, 7, 0, 9, 1, 3, 3, 2], device='cuda:0')\n",
            "Target: tensor([5, 4, 3, 6, 7, 7, 4, 3, 9, 6, 2, 4, 0, 1, 3, 6, 4, 9, 2, 6, 0, 3, 8, 7,\n",
            "        5, 3, 3, 8, 3, 2, 6, 3, 6, 2, 7, 4, 5, 7, 9, 0, 0, 6, 7, 2, 8, 5, 5, 7,\n",
            "        5, 5, 9, 4, 6, 4, 7, 3, 3, 6, 4, 1, 6, 3, 1, 6, 7, 0, 5, 0, 1, 9, 7, 3,\n",
            "        5, 2, 3, 9, 5, 6, 4, 0, 0, 0, 8, 0, 3, 2, 4, 5, 3, 7, 9, 6, 9, 3, 1, 2,\n",
            "        6, 7, 4, 5, 1, 3, 7, 6, 9, 5, 8, 5, 8, 5, 8, 5, 6, 5, 0, 8, 3, 8, 1, 8,\n",
            "        1, 5, 0, 9, 8, 6, 3, 6], device='cuda:0')\n",
            "Output: tensor([[-5.8020, -0.2726, -4.2493,  ..., -2.1378, -6.2536, -2.3661],\n",
            "        [-7.0685, -6.9873, -1.0249,  ...,  1.3432, -7.9119, -8.1897],\n",
            "        [-3.5832, -5.7177,  0.5445,  ..., -0.6357, -8.4531, -9.1337],\n",
            "        ...,\n",
            "        [-3.4539, -9.2133, -0.3381,  ...,  4.6953, -8.1143, -8.3662],\n",
            "        [-3.2518, -3.4546, -5.0478,  ..., -2.0822,  0.8282, -0.9059],\n",
            "        [-3.0549,  2.2907, -4.3204,  ..., -5.6718, -2.9900,  5.5088]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 4, 4, 7, 4, 7, 4, 3, 3, 4, 4, 5, 7, 3, 4, 5, 8, 0, 5, 4, 0, 5, 7, 3,\n",
            "        3, 2, 4, 2, 9, 7, 5, 8, 6, 0, 4, 1, 7, 0, 4, 3, 8, 7, 1, 7, 6, 7, 1, 0,\n",
            "        2, 3, 7, 6, 7, 1, 6, 0, 8, 3, 4, 2, 5, 4, 5, 7, 7, 1, 7, 4, 3, 5, 4, 1,\n",
            "        4, 2, 7, 0, 3, 4, 6, 6, 2, 5, 4, 6, 0, 6, 1, 2, 9, 0, 0, 1, 5, 5, 1, 6,\n",
            "        7, 8, 3, 3, 3, 3, 6, 2, 9, 9, 2, 2, 6, 1, 4, 3, 2, 5, 7, 0, 8, 5, 6, 4,\n",
            "        3, 2, 5, 4, 4, 7, 8, 9], device='cuda:0')\n",
            "Target: tensor([3, 4, 4, 7, 4, 7, 4, 3, 2, 4, 5, 5, 7, 5, 4, 5, 8, 0, 5, 4, 0, 5, 4, 3,\n",
            "        3, 2, 4, 2, 9, 4, 8, 8, 6, 2, 6, 1, 7, 0, 4, 3, 8, 5, 9, 7, 6, 7, 1, 0,\n",
            "        2, 3, 5, 6, 7, 1, 6, 2, 8, 3, 7, 2, 5, 7, 5, 7, 7, 1, 7, 4, 3, 3, 4, 0,\n",
            "        4, 2, 7, 0, 2, 2, 6, 6, 2, 5, 2, 6, 0, 6, 1, 2, 9, 0, 0, 1, 5, 5, 1, 6,\n",
            "        7, 8, 5, 6, 3, 3, 6, 0, 9, 9, 2, 2, 6, 1, 4, 6, 2, 5, 7, 8, 8, 5, 6, 4,\n",
            "        3, 2, 3, 4, 4, 4, 8, 9], device='cuda:0')\n",
            "Output: tensor([[-2.6303, -6.4701,  3.3296,  ..., -5.2051, -6.2143, -6.8873],\n",
            "        [-4.4109, -7.2454,  0.2529,  ..., -7.4617, -7.5285, -7.3081],\n",
            "        [ 2.8224, -4.9884,  0.2209,  ..., -5.6967,  0.9951, -4.5562],\n",
            "        ...,\n",
            "        [-4.0417, -8.7588, -3.1539,  ...,  0.9606, -8.5289, -6.8647],\n",
            "        [-3.6962, -8.4785,  2.4926,  ..., -2.3093, -6.3912, -8.4295],\n",
            "        [-1.2033,  3.6088, -1.2505,  ..., -6.4334, -2.9808, -3.0702]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([2, 6, 0, 9, 7, 9, 8, 7, 7, 3, 8, 2, 9, 3, 3, 7, 5, 8, 7, 7, 7, 0, 7, 1,\n",
            "        6, 2, 2, 0, 0, 2, 9, 8, 4, 8, 9, 4, 5, 2, 0, 1, 4, 9, 6, 1, 0, 1, 8, 6,\n",
            "        3, 8, 5, 7, 3, 4, 8, 5, 3, 3, 7, 7, 5, 8, 6, 5, 7, 3, 7, 0, 5, 4, 3, 6,\n",
            "        9, 3, 5, 5, 8, 2, 2, 2, 3, 4, 7, 6, 0, 5, 5, 8, 4, 5, 6, 1, 3, 5, 4, 7,\n",
            "        3, 0, 7, 2, 0, 1, 3, 4, 9, 6, 4, 5, 0, 7, 4, 5, 5, 4, 3, 9, 6, 9, 4, 8,\n",
            "        0, 5, 9, 6, 4, 4, 2, 1], device='cuda:0')\n",
            "Target: tensor([2, 6, 0, 9, 7, 9, 8, 7, 7, 3, 8, 2, 9, 3, 5, 7, 5, 8, 7, 3, 7, 0, 7, 1,\n",
            "        6, 2, 4, 0, 0, 2, 9, 8, 2, 8, 7, 7, 5, 5, 0, 1, 2, 9, 6, 1, 0, 1, 0, 6,\n",
            "        5, 0, 5, 7, 4, 4, 8, 3, 5, 3, 7, 2, 5, 8, 3, 5, 7, 3, 7, 0, 5, 4, 5, 6,\n",
            "        9, 3, 5, 3, 8, 2, 4, 2, 2, 6, 7, 6, 0, 5, 5, 8, 4, 5, 6, 1, 3, 5, 4, 7,\n",
            "        3, 0, 7, 2, 0, 1, 2, 4, 9, 6, 4, 5, 9, 7, 7, 6, 7, 4, 3, 1, 6, 9, 4, 8,\n",
            "        0, 3, 1, 6, 4, 4, 2, 1], device='cuda:0')\n",
            "Output: tensor([[-5.7689, -7.2973, -0.8089,  ..., -0.9357, -5.0653, -6.5138],\n",
            "        [-6.3712, -4.9522,  2.2691,  ..., -4.7502, -6.0727, -7.8740],\n",
            "        [-7.8352, -4.5363, -0.5884,  ...,  0.2060, -9.2448, -2.9347],\n",
            "        ...,\n",
            "        [-7.8893, -6.4393, -0.2391,  ..., -5.5012, -5.0781, -4.6709],\n",
            "        [-7.4697, -6.7640, -1.1513,  ...,  0.1919, -5.9194, -6.2783],\n",
            "        [ 3.3795, -4.1674, -6.0060,  ..., -1.4322, -0.1406, -6.3927]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([4, 6, 3, 0, 8, 9, 7, 6, 1, 4, 8, 1, 4, 5, 9, 5, 7, 1, 4, 5, 6, 7, 3, 6,\n",
            "        4, 7, 1, 3, 9, 3, 1, 4, 8, 7, 7, 0, 2, 2, 4, 6, 7, 7, 2, 4, 4, 4, 3, 6,\n",
            "        6, 1, 9, 9, 6, 9, 4, 3, 2, 9, 8, 5, 0, 6, 9, 0, 9, 3, 8, 5, 4, 4, 9, 4,\n",
            "        3, 2, 6, 3, 9, 7, 9, 9, 0, 5, 1, 1, 0, 6, 9, 9, 5, 6, 3, 5, 9, 7, 9, 4,\n",
            "        5, 4, 5, 2, 9, 8, 8, 3, 7, 8, 4, 0, 4, 0, 7, 2, 9, 0, 2, 0, 7, 4, 0, 6,\n",
            "        2, 3, 1, 7, 4, 6, 4, 0], device='cuda:0')\n",
            "Target: tensor([4, 6, 3, 0, 8, 9, 7, 6, 8, 4, 8, 1, 4, 5, 9, 4, 7, 1, 4, 5, 3, 1, 2, 6,\n",
            "        4, 7, 1, 5, 9, 3, 1, 4, 8, 7, 7, 0, 4, 3, 4, 6, 7, 7, 5, 4, 4, 4, 3, 6,\n",
            "        6, 1, 9, 9, 7, 9, 4, 3, 2, 9, 8, 5, 0, 6, 9, 0, 9, 7, 8, 5, 4, 4, 9, 4,\n",
            "        2, 6, 6, 9, 9, 7, 9, 9, 0, 5, 1, 1, 8, 6, 9, 9, 5, 6, 5, 5, 9, 7, 9, 4,\n",
            "        5, 4, 3, 3, 9, 8, 8, 3, 4, 8, 4, 0, 4, 0, 7, 2, 8, 0, 2, 0, 7, 4, 0, 6,\n",
            "        3, 3, 1, 7, 4, 6, 5, 0], device='cuda:0')\n",
            "Output: tensor([[-7.2529, -3.6378, -3.2624,  ..., -3.1713, -6.3842, -4.7267],\n",
            "        [-3.9267,  1.6865, -4.5716,  ..., -4.1135, -7.2526,  3.7182],\n",
            "        [-9.2614, -6.3250, -2.4492,  ..., -4.3340, -8.9829, -4.8759],\n",
            "        ...,\n",
            "        [ 1.7052, -2.9872,  0.4928,  ..., -6.4459,  4.6903, -5.3868],\n",
            "        [-6.5671, -5.6796, -3.4160,  ..., -1.8679, -8.1698, -4.4302],\n",
            "        [-3.0498,  0.0708, -4.8795,  ..., -4.2684, -2.7955,  1.4504]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([4, 9, 5, 6, 6, 3, 0, 5, 7, 6, 3, 3, 8, 0, 8, 5, 4, 2, 9, 3, 0, 5, 4, 8,\n",
            "        2, 7, 4, 7, 7, 8, 2, 0, 1, 8, 2, 7, 8, 4, 3, 0, 4, 0, 1, 9, 3, 0, 4, 3,\n",
            "        5, 5, 2, 8, 3, 0, 8, 7, 5, 2, 4, 3, 7, 2, 2, 3, 1, 0, 0, 2, 1, 3, 1, 4,\n",
            "        6, 7, 1, 0, 6, 2, 2, 1, 2, 5, 6, 7, 1, 1, 3, 6, 1, 0, 2, 9, 4, 2, 9, 3,\n",
            "        1, 2, 9, 8, 4, 1, 0, 1, 5, 2, 3, 6, 0, 8, 3, 2, 0, 4, 1, 3, 7, 8, 3, 3,\n",
            "        9, 2, 7, 9, 9, 8, 3, 9], device='cuda:0')\n",
            "Target: tensor([4, 9, 5, 6, 6, 3, 0, 5, 2, 6, 3, 3, 8, 0, 8, 5, 4, 2, 9, 5, 0, 5, 4, 8,\n",
            "        2, 7, 6, 5, 7, 8, 2, 0, 1, 8, 2, 4, 8, 4, 3, 0, 4, 0, 1, 9, 3, 0, 3, 6,\n",
            "        5, 5, 2, 8, 5, 0, 8, 7, 5, 2, 4, 3, 7, 0, 2, 3, 1, 0, 0, 2, 1, 2, 1, 4,\n",
            "        6, 7, 1, 0, 5, 2, 5, 9, 2, 5, 6, 7, 1, 9, 3, 6, 1, 0, 2, 9, 4, 2, 9, 3,\n",
            "        8, 2, 9, 8, 5, 1, 0, 1, 5, 2, 5, 6, 8, 8, 3, 5, 0, 4, 1, 3, 7, 0, 3, 3,\n",
            "        9, 2, 3, 9, 9, 8, 5, 9], device='cuda:0')\n",
            "Output: tensor([[-2.9766, -5.2514, -0.3501,  ...,  1.1854, -5.4836, -7.6225],\n",
            "        [-5.2734, -8.0770,  1.9811,  ..., -5.0196, -5.7470, -5.2813],\n",
            "        [-0.2788, -1.3480, -2.8145,  ..., -3.3462,  0.5427, -0.4436],\n",
            "        ...,\n",
            "        [-5.5016, -4.4825, -6.4670,  ...,  2.5738, -6.2474, -4.3993],\n",
            "        [-6.3407, -4.9083,  2.1979,  ..., -5.3772, -8.6416, -6.3228],\n",
            "        [-3.7135, -5.1713, -0.0158,  ..., -2.6707, -6.6563, -5.7183]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([7, 2, 8, 9, 4, 7, 1, 3, 6, 4, 4, 8, 9, 4, 2, 1, 2, 4, 9, 3, 7, 5, 1, 9,\n",
            "        3, 3, 6, 4, 9, 1, 5, 3, 9, 2, 7, 8, 0, 1, 1, 5, 1, 5, 9, 0, 9, 2, 3, 0,\n",
            "        6, 9, 1, 1, 3, 4, 9, 5, 3, 3, 3, 3, 7, 1, 5, 9, 8, 7, 3, 8, 7, 3, 5, 8,\n",
            "        5, 8, 9, 7, 2, 0, 0, 7, 5, 3, 5, 7, 7, 6, 2, 8, 2, 7, 7, 6, 1, 5, 8, 6,\n",
            "        2, 0, 2, 4, 8, 3, 4, 0, 2, 5, 4, 1, 2, 4, 4, 3, 9, 4, 0, 5, 8, 0, 8, 0,\n",
            "        6, 4, 7, 9, 5, 7, 6, 4], device='cuda:0')\n",
            "Target: tensor([7, 2, 0, 9, 4, 7, 1, 7, 6, 4, 4, 8, 9, 0, 7, 1, 2, 4, 0, 3, 7, 5, 1, 9,\n",
            "        3, 5, 2, 4, 1, 1, 3, 3, 9, 2, 5, 8, 0, 8, 1, 5, 1, 5, 9, 0, 9, 2, 4, 0,\n",
            "        6, 9, 1, 1, 3, 4, 9, 5, 3, 3, 3, 3, 4, 1, 5, 9, 8, 7, 4, 8, 7, 3, 5, 2,\n",
            "        5, 8, 9, 7, 2, 0, 0, 7, 5, 3, 5, 7, 4, 6, 2, 8, 2, 7, 7, 6, 1, 3, 8, 6,\n",
            "        2, 0, 4, 4, 8, 6, 4, 0, 2, 5, 4, 1, 2, 5, 4, 6, 9, 5, 0, 5, 2, 0, 8, 2,\n",
            "        6, 4, 7, 9, 5, 7, 2, 6], device='cuda:0')\n",
            "Output: tensor([[-8.2444, -4.5584,  0.6916,  ..., -6.9915, -6.2458, -7.9539],\n",
            "        [ 0.6965,  2.2109, -2.1146,  ..., -4.9463, -1.0117, -0.7746],\n",
            "        [-5.5612, -1.7737, -4.2929,  ..., -2.9264, -6.2604,  4.1335],\n",
            "        ...,\n",
            "        [-3.8702, -7.2527, -1.1727,  ..., -3.5998, -3.0008, -7.8711],\n",
            "        [-5.6693, -2.7280, -3.1902,  ..., -2.0446, -5.5131, -6.1379],\n",
            "        [ 0.0788,  1.6833, -1.2273,  ..., -5.0844, -3.1060,  0.9999]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 1, 9, 9, 7, 2, 3, 1, 9, 1, 6, 3, 0, 8, 8, 5, 6, 9, 1, 9, 6, 3, 0, 8,\n",
            "        8, 5, 3, 3, 1, 1, 0, 7, 0, 6, 0, 4, 4, 6, 5, 3, 2, 9, 6, 6, 7, 8, 7, 7,\n",
            "        0, 8, 7, 2, 7, 7, 7, 4, 0, 1, 6, 0, 5, 3, 0, 2, 4, 9, 3, 3, 3, 2, 8, 8,\n",
            "        5, 6, 8, 0, 8, 0, 0, 0, 2, 2, 7, 0, 7, 4, 7, 2, 2, 3, 4, 0, 3, 7, 8, 4,\n",
            "        7, 9, 6, 9, 1, 6, 9, 3, 8, 9, 3, 8, 7, 1, 4, 4, 4, 5, 5, 4, 3, 5, 7, 9,\n",
            "        6, 2, 0, 8, 4, 3, 4, 1], device='cuda:0')\n",
            "Target: tensor([2, 1, 9, 9, 7, 2, 5, 1, 8, 1, 6, 3, 0, 8, 8, 5, 6, 9, 9, 9, 6, 3, 0, 8,\n",
            "        8, 5, 3, 3, 1, 1, 0, 7, 0, 6, 0, 4, 4, 6, 5, 3, 2, 9, 6, 6, 7, 9, 7, 7,\n",
            "        0, 8, 4, 2, 7, 7, 7, 4, 0, 1, 6, 0, 5, 3, 0, 2, 4, 9, 7, 3, 3, 2, 8, 8,\n",
            "        5, 3, 8, 0, 8, 0, 0, 0, 2, 2, 7, 0, 7, 7, 7, 2, 2, 0, 4, 0, 3, 7, 8, 4,\n",
            "        7, 9, 6, 1, 1, 6, 9, 3, 8, 9, 5, 8, 3, 9, 4, 3, 7, 5, 5, 2, 7, 5, 7, 9,\n",
            "        6, 3, 0, 8, 4, 5, 6, 1], device='cuda:0')\n",
            "Output: tensor([[ 0.2452, -1.0091, -3.9424,  ...,  0.3132, -1.4757, -5.5902],\n",
            "        [-5.2654, -5.3997,  0.0952,  ...,  0.0361, -6.5968, -9.4601],\n",
            "        [-1.8659, -5.3489, -3.7111,  ..., -2.3282,  2.1386, -1.5541],\n",
            "        ...,\n",
            "        [-7.8732, -6.0250, -5.6113,  ..., -3.7777, -6.1496, -7.2225],\n",
            "        [-2.6010, -0.9060, -4.4852,  ..., -3.1949,  0.5787, -3.4038],\n",
            "        [-6.0230, -5.3535, -2.5323,  ..., -2.1178, -4.4278, -5.0231]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([7, 4, 8, 4, 7, 1, 5, 2, 0, 1, 1, 8, 9, 6, 0, 1, 7, 1, 0, 8, 7, 4, 3, 7,\n",
            "        8, 2, 1, 2, 2, 8, 1, 1, 8, 8, 8, 9, 0, 0, 4, 3, 3, 3, 6, 6, 3, 1, 7, 4,\n",
            "        7, 8, 0, 1, 4, 0, 4, 6, 2, 2, 6, 6, 3, 4, 0, 8, 1, 3, 2, 3, 8, 9, 1, 6,\n",
            "        1, 9, 7, 2, 0, 4, 7, 4, 0, 0, 7, 6, 8, 9, 1, 6, 4, 8, 8, 0, 1, 1, 2, 7,\n",
            "        7, 2, 2, 2, 7, 2, 3, 6, 7, 4, 5, 2, 0, 9, 4, 7, 2, 5, 5, 2, 2, 7, 6, 6,\n",
            "        6, 4, 9, 4, 8, 3, 8, 5], device='cuda:0')\n",
            "Target: tensor([0, 4, 8, 4, 7, 1, 3, 2, 0, 9, 1, 8, 8, 6, 8, 1, 7, 1, 0, 9, 7, 6, 4, 5,\n",
            "        8, 2, 1, 2, 5, 8, 1, 1, 8, 8, 0, 9, 0, 0, 4, 3, 3, 5, 6, 6, 3, 1, 4, 4,\n",
            "        7, 8, 0, 1, 4, 8, 4, 6, 2, 2, 2, 6, 3, 4, 0, 8, 1, 3, 8, 3, 8, 9, 1, 6,\n",
            "        1, 9, 8, 2, 0, 4, 7, 4, 0, 0, 7, 6, 8, 9, 8, 6, 2, 8, 8, 0, 1, 1, 0, 7,\n",
            "        7, 4, 4, 7, 7, 2, 6, 6, 7, 4, 5, 4, 0, 9, 4, 7, 2, 5, 3, 2, 2, 7, 6, 2,\n",
            "        6, 4, 9, 6, 8, 3, 8, 5], device='cuda:0')\n",
            "Output: tensor([[-4.0110, -5.2890, -3.3417,  ..., -4.6218, -6.8185, -3.9795],\n",
            "        [-2.2112, -6.8394,  0.6476,  ..., -4.7814, -6.3122, -6.3608],\n",
            "        [-5.3169, -6.3091, -1.5141,  ..., -1.8465, -6.8452, -5.4125],\n",
            "        ...,\n",
            "        [ 0.5420, -1.4178, -3.3041,  ..., -2.1171,  0.3384, -4.6576],\n",
            "        [-6.7625, -5.6756, -1.6821,  ..., -1.5638, -4.8475, -6.5382],\n",
            "        [-3.3804,  4.5810, -5.0638,  ..., -4.5632, -2.5753,  3.0510]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 2, 3, 4, 7, 2, 7, 5, 1, 5, 3, 7, 8, 1, 9, 6, 0, 8, 2, 6, 0, 0, 1, 7,\n",
            "        1, 3, 5, 1, 8, 9, 6, 2, 6, 9, 3, 6, 5, 9, 9, 4, 3, 8, 9, 5, 4, 3, 0, 3,\n",
            "        3, 1, 4, 9, 1, 1, 4, 0, 6, 1, 6, 3, 8, 2, 3, 3, 5, 3, 0, 2, 2, 1, 6, 2,\n",
            "        7, 2, 8, 9, 2, 6, 4, 7, 5, 5, 8, 8, 7, 8, 5, 7, 4, 4, 0, 0, 7, 0, 4, 0,\n",
            "        6, 4, 2, 3, 4, 0, 7, 3, 7, 0, 5, 3, 2, 9, 1, 9, 3, 0, 9, 1, 0, 2, 3, 3,\n",
            "        0, 9, 0, 6, 2, 0, 4, 1], device='cuda:0')\n",
            "Target: tensor([5, 2, 3, 4, 5, 2, 7, 5, 1, 5, 3, 7, 8, 8, 9, 6, 0, 9, 2, 6, 0, 0, 1, 4,\n",
            "        1, 3, 5, 1, 8, 9, 6, 2, 6, 9, 5, 6, 5, 9, 1, 4, 2, 8, 9, 5, 4, 7, 0, 3,\n",
            "        7, 1, 6, 9, 1, 1, 4, 0, 5, 1, 6, 3, 0, 2, 5, 5, 5, 2, 8, 2, 3, 1, 6, 2,\n",
            "        7, 0, 8, 9, 2, 6, 4, 7, 5, 5, 8, 8, 7, 8, 5, 7, 4, 4, 0, 0, 7, 0, 4, 0,\n",
            "        4, 4, 2, 5, 4, 0, 7, 7, 7, 0, 4, 6, 2, 9, 1, 1, 5, 0, 9, 1, 0, 6, 5, 3,\n",
            "        0, 9, 0, 6, 2, 0, 4, 1], device='cuda:0')\n",
            "Output: tensor([[-2.5304, -7.8678, -0.9521,  ..., -1.6486, -5.6543, -8.4085],\n",
            "        [-3.0791, -6.4722, -1.0211,  ...,  2.7733, -4.9026, -6.0998],\n",
            "        [-9.6021, -4.9515, -2.7392,  ..., -4.9211, -5.9955, -5.9555],\n",
            "        ...,\n",
            "        [-8.2736, -7.5509, -2.2420,  ..., -3.9258, -9.3791, -6.7039],\n",
            "        [-8.7084, -5.9199, -2.4710,  ..., -0.7981, -8.0497, -4.6083],\n",
            "        [-3.7917, -5.1139, -2.5081,  ..., -3.7892, -4.9377, -8.0093]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([4, 7, 3, 1, 0, 4, 6, 0, 4, 4, 2, 7, 6, 2, 3, 5, 9, 4, 5, 2, 0, 5, 7, 4,\n",
            "        3, 8, 4, 5, 8, 3, 9, 6, 1, 8, 5, 8, 6, 1, 3, 4, 0, 2, 5, 2, 7, 1, 8, 2,\n",
            "        1, 3, 3, 2, 4, 3, 3, 5, 0, 2, 1, 2, 8, 0, 8, 2, 2, 7, 3, 5, 9, 8, 3, 6,\n",
            "        1, 0, 8, 6, 3, 5, 8, 1, 4, 8, 0, 0, 2, 9, 1, 8, 3, 7, 2, 8, 6, 9, 4, 8,\n",
            "        9, 6, 8, 9, 5, 4, 8, 0, 5, 4, 4, 2, 3, 8, 8, 1, 2, 3, 1, 3, 0, 2, 6, 0,\n",
            "        8, 2, 2, 1, 7, 3, 5, 3], device='cuda:0')\n",
            "Target: tensor([6, 7, 6, 1, 0, 4, 6, 0, 4, 4, 2, 7, 6, 5, 3, 5, 9, 4, 5, 2, 0, 5, 7, 2,\n",
            "        3, 9, 4, 5, 8, 1, 9, 3, 1, 8, 5, 8, 6, 6, 3, 4, 0, 2, 3, 2, 6, 1, 8, 2,\n",
            "        1, 3, 6, 2, 2, 3, 3, 5, 0, 2, 1, 2, 8, 0, 8, 2, 2, 7, 3, 5, 9, 8, 3, 6,\n",
            "        1, 0, 8, 6, 3, 5, 8, 9, 4, 8, 0, 0, 2, 9, 1, 8, 3, 7, 2, 8, 0, 9, 4, 8,\n",
            "        9, 4, 8, 9, 5, 4, 8, 1, 5, 5, 7, 2, 5, 8, 8, 1, 2, 3, 1, 5, 0, 2, 3, 0,\n",
            "        8, 2, 2, 8, 7, 3, 5, 3], device='cuda:0')\n",
            "Output: tensor([[-4.7888, -0.3739, -8.7976,  ..., -2.7561, -2.9517,  8.2715],\n",
            "        [-5.6946, -5.2372, -3.0735,  ..., -3.4207, -6.9397, -5.5298],\n",
            "        [-5.9769, -7.4537, -1.9855,  ..., -3.5713, -8.4758, -5.9725],\n",
            "        ...,\n",
            "        [-6.3461, -7.5771,  0.3940,  ..., -0.9821, -7.1819, -8.2080],\n",
            "        [-4.9242, -7.9456, -1.1190,  ...,  4.4084, -6.2087, -5.8164],\n",
            "        [-4.8827, -2.0560, -0.2320,  ..., -4.8091, -5.9018, -5.3111]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([9, 6, 3, 5, 1, 1, 3, 7, 3, 3, 8, 9, 6, 0, 8, 0, 4, 6, 4, 6, 9, 1, 1, 7,\n",
            "        0, 2, 8, 9, 0, 6, 1, 0, 1, 1, 7, 2, 1, 0, 6, 5, 0, 3, 4, 3, 3, 8, 1, 8,\n",
            "        1, 3, 1, 5, 4, 7, 1, 0, 9, 1, 0, 1, 7, 4, 9, 8, 9, 4, 6, 0, 3, 4, 8, 4,\n",
            "        4, 2, 1, 2, 2, 0, 5, 6, 0, 6, 7, 6, 9, 2, 7, 1, 0, 3, 6, 5, 3, 0, 5, 9,\n",
            "        2, 5, 9, 7, 8, 0, 1, 4, 4, 5, 5, 9, 6, 3, 4, 2, 4, 9, 5, 7, 3, 7, 4, 9,\n",
            "        7, 2, 1, 6, 6, 4, 7, 6], device='cuda:0')\n",
            "Target: tensor([9, 6, 3, 5, 1, 1, 7, 7, 3, 3, 8, 9, 6, 0, 8, 0, 6, 6, 4, 6, 9, 1, 1, 7,\n",
            "        0, 6, 8, 9, 0, 6, 9, 0, 1, 1, 7, 2, 1, 0, 4, 5, 0, 3, 6, 3, 3, 8, 1, 8,\n",
            "        1, 3, 1, 5, 4, 5, 1, 8, 9, 1, 3, 1, 7, 2, 9, 8, 9, 4, 6, 0, 9, 4, 0, 4,\n",
            "        6, 2, 1, 3, 0, 8, 5, 6, 0, 3, 4, 6, 9, 5, 7, 9, 0, 3, 6, 5, 7, 0, 5, 1,\n",
            "        2, 2, 9, 7, 0, 0, 1, 4, 4, 5, 5, 9, 6, 3, 6, 2, 6, 9, 3, 2, 3, 2, 4, 9,\n",
            "        5, 2, 1, 6, 6, 4, 7, 6], device='cuda:0')\n",
            "Output: tensor([[-7.7385, -4.8240, -2.5260,  ..., -1.6069, -6.8630, -6.1081],\n",
            "        [-5.2093, -6.0238, -5.4601,  ..., -5.4998, -5.2090, -0.3663],\n",
            "        [-1.6547, -4.3154, -2.1241,  ..., -4.6246,  2.6563, -3.3852],\n",
            "        ...,\n",
            "        [-4.9775, -6.1749, -1.0680,  ...,  1.0904, -3.3959, -5.2805],\n",
            "        [ 4.8457, -4.1911,  1.5792,  ..., -7.8951, -2.0520, -9.6700],\n",
            "        [-0.3818, -4.3327, -4.7619,  ...,  1.8873, -9.9676, -6.4833]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 3, 8, 1, 4, 3, 4, 2, 5, 6, 2, 0, 5, 2, 7, 5, 2, 5, 2, 5, 9, 3, 6, 7,\n",
            "        9, 8, 3, 2, 1, 4, 3, 4, 5, 5, 6, 8, 6, 0, 8, 9, 2, 0, 0, 7, 6, 6, 7, 5,\n",
            "        9, 3, 4, 1, 5, 6, 5, 4, 8, 3, 2, 9, 1, 7, 9, 1, 3, 7, 4, 4, 3, 6, 1, 9,\n",
            "        1, 7, 3, 8, 4, 7, 3, 1, 4, 5, 3, 3, 0, 2, 8, 8, 8, 1, 6, 7, 7, 9, 6, 1,\n",
            "        1, 2, 0, 7, 7, 1, 3, 3, 3, 1, 0, 2, 7, 1, 2, 2, 9, 6, 3, 5, 9, 4, 8, 8,\n",
            "        4, 8, 0, 1, 3, 3, 0, 4], device='cuda:0')\n",
            "Target: tensor([3, 3, 8, 1, 4, 6, 4, 4, 5, 6, 2, 0, 5, 2, 7, 5, 2, 5, 2, 5, 9, 0, 6, 7,\n",
            "        9, 8, 3, 2, 1, 4, 3, 4, 5, 5, 6, 8, 6, 0, 8, 9, 8, 0, 7, 7, 6, 6, 7, 5,\n",
            "        9, 8, 7, 9, 5, 6, 0, 4, 8, 2, 0, 9, 1, 3, 9, 8, 3, 7, 4, 5, 0, 2, 1, 9,\n",
            "        1, 0, 5, 8, 4, 7, 3, 1, 4, 5, 3, 1, 0, 8, 8, 8, 2, 1, 3, 7, 3, 1, 6, 9,\n",
            "        1, 6, 0, 7, 7, 1, 3, 3, 3, 1, 0, 2, 7, 1, 2, 2, 9, 6, 3, 5, 9, 4, 8, 0,\n",
            "        4, 8, 0, 1, 3, 7, 0, 4], device='cuda:0')\n",
            "Output: tensor([[ 1.6928, -3.1799, -3.3204,  ..., -3.5661,  0.6177, -4.8372],\n",
            "        [ 3.6163, -3.9966, -1.5992,  ..., -3.3276,  0.1609, -1.7152],\n",
            "        [ 2.1095, -2.5225, -1.9905,  ..., -4.7036,  3.2593, -2.5750],\n",
            "        ...,\n",
            "        [ 8.3168, -6.9546,  0.2025,  ..., -2.5876, -4.9106, -6.3138],\n",
            "        [-1.6689,  6.0022, -5.7882,  ..., -5.6663, -2.8159,  3.2346],\n",
            "        [-3.3060, -8.2983, -3.3424,  ...,  4.8219, -5.3406, -6.8420]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([0, 0, 8, 3, 4, 3, 9, 8, 2, 2, 8, 3, 1, 1, 0, 0, 2, 6, 9, 4, 8, 4, 2, 1,\n",
            "        5, 2, 6, 9, 2, 0, 7, 1, 8, 9, 3, 9, 9, 0, 7, 7, 5, 4, 2, 6, 4, 5, 7, 7,\n",
            "        8, 7, 5, 6, 6, 2, 4, 4, 0, 3, 1, 5, 1, 6, 0, 0, 2, 3, 0, 2, 2, 4, 3, 5,\n",
            "        0, 9, 1, 0, 0, 6, 5, 0, 7, 9, 9, 6, 5, 5, 0, 2, 3, 1, 6, 6, 4, 1, 4, 4,\n",
            "        1, 5, 0, 0, 4, 5, 8, 0, 8, 3, 0, 5, 0, 5, 8, 1, 6, 7, 8, 9, 1, 5, 7, 4,\n",
            "        5, 5, 5, 6, 0, 0, 1, 7], device='cuda:0')\n",
            "Target: tensor([7, 4, 8, 6, 4, 3, 9, 8, 2, 2, 8, 3, 1, 1, 2, 8, 2, 6, 9, 4, 8, 4, 4, 1,\n",
            "        5, 2, 6, 9, 2, 0, 7, 1, 8, 9, 3, 9, 9, 0, 7, 7, 5, 4, 2, 6, 4, 5, 7, 7,\n",
            "        8, 7, 2, 6, 2, 2, 4, 4, 0, 7, 1, 3, 9, 6, 0, 0, 2, 3, 8, 2, 2, 4, 3, 5,\n",
            "        2, 9, 1, 0, 0, 6, 5, 5, 7, 9, 9, 6, 5, 5, 0, 5, 7, 1, 6, 6, 4, 1, 4, 4,\n",
            "        1, 5, 0, 0, 4, 5, 8, 4, 8, 3, 0, 5, 0, 5, 3, 1, 6, 7, 0, 9, 1, 5, 7, 6,\n",
            "        5, 5, 5, 6, 0, 0, 1, 7], device='cuda:0')\n",
            "Output: tensor([[-8.2678, -4.6334, -5.2811,  ...,  0.7319, -8.3142, -6.0297],\n",
            "        [-5.7369,  2.5362, -3.1978,  ..., -3.3693, -4.7160, -2.5188],\n",
            "        [-4.1250,  0.7991, -6.4569,  ..., -3.0787, -8.5376,  2.8411],\n",
            "        ...,\n",
            "        [-4.5032, -4.6950, -4.0984,  ...,  2.1331, -7.9105, -5.6156],\n",
            "        [-6.9583, -4.0038, -4.7518,  ...,  1.7994, -7.7212, -5.3414],\n",
            "        [-7.4279, -4.5221, -2.1653,  ..., -3.0677, -7.7770, -6.4039]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 1, 9, 2, 4, 1, 3, 7, 8, 2, 0, 9, 6, 6, 0, 3, 5, 8, 2, 7, 4, 0, 2, 7,\n",
            "        7, 8, 0, 7, 0, 4, 9, 1, 2, 2, 3, 5, 7, 6, 2, 3, 1, 0, 3, 3, 3, 2, 5, 1,\n",
            "        2, 3, 0, 7, 7, 3, 8, 7, 3, 1, 7, 7, 6, 3, 2, 8, 9, 5, 9, 2, 1, 7, 4, 4,\n",
            "        0, 3, 4, 1, 5, 4, 0, 8, 4, 9, 8, 0, 8, 4, 2, 3, 4, 0, 5, 4, 1, 8, 2, 5,\n",
            "        4, 3, 2, 5, 3, 4, 9, 7, 0, 4, 1, 3, 1, 4, 5, 5, 1, 7, 1, 3, 0, 1, 2, 5,\n",
            "        7, 0, 6, 3, 5, 7, 5, 5], device='cuda:0')\n",
            "Target: tensor([5, 1, 9, 2, 4, 1, 3, 7, 8, 2, 0, 9, 6, 6, 0, 6, 5, 8, 2, 7, 4, 0, 2, 7,\n",
            "        7, 8, 8, 7, 0, 4, 9, 1, 4, 4, 3, 5, 4, 6, 2, 3, 1, 0, 3, 3, 3, 6, 3, 1,\n",
            "        2, 8, 9, 7, 9, 3, 8, 7, 3, 1, 7, 7, 3, 2, 2, 8, 9, 5, 9, 2, 1, 7, 4, 4,\n",
            "        0, 5, 7, 1, 5, 4, 0, 8, 4, 9, 8, 7, 8, 4, 2, 3, 4, 0, 5, 4, 1, 8, 2, 5,\n",
            "        4, 5, 2, 5, 3, 7, 9, 7, 1, 4, 1, 3, 1, 4, 5, 5, 1, 7, 1, 3, 0, 1, 2, 5,\n",
            "        7, 0, 6, 3, 5, 7, 5, 5], device='cuda:0')\n",
            "Output: tensor([[-0.7411, -4.4465, -5.0177,  ..., -3.8289,  6.0612, -2.4406],\n",
            "        [-5.7552,  2.7756, -4.1552,  ..., -1.7096, -8.9558,  5.4918],\n",
            "        [-3.3534, -6.5513,  0.1946,  ...,  1.3686, -6.9151, -6.5286],\n",
            "        ...,\n",
            "        [-5.7831, -6.2840, -3.0716,  ..., -4.0415, -3.3898, -5.4827],\n",
            "        [-4.3642, -7.2985, -0.8241,  ..., -2.6414, -5.9447, -2.4548],\n",
            "        [-6.6585, -6.9340,  0.8764,  ..., -3.6486, -5.9084, -5.9309]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([8, 9, 4, 6, 3, 6, 6, 8, 2, 6, 4, 8, 4, 1, 6, 9, 2, 3, 3, 2, 1, 6, 4, 2,\n",
            "        8, 2, 4, 3, 1, 4, 4, 4, 4, 1, 5, 5, 7, 6, 3, 4, 5, 6, 5, 6, 2, 4, 7, 2,\n",
            "        4, 6, 5, 0, 2, 3, 9, 2, 3, 8, 0, 7, 0, 6, 8, 1, 2, 8, 9, 9, 5, 4, 2, 3,\n",
            "        5, 5, 5, 6, 5, 0, 3, 7, 1, 3, 7, 7, 7, 4, 9, 4, 3, 9, 5, 1, 4, 6, 6, 4,\n",
            "        8, 5, 1, 2, 8, 4, 3, 3, 3, 3, 0, 4, 0, 7, 4, 9, 3, 8, 3, 5, 9, 1, 8, 3,\n",
            "        1, 3, 8, 0, 4, 5, 4, 6], device='cuda:0')\n",
            "Target: tensor([8, 9, 4, 6, 3, 6, 6, 8, 2, 6, 4, 8, 4, 1, 3, 1, 2, 3, 3, 0, 1, 6, 4, 2,\n",
            "        8, 2, 4, 3, 1, 4, 4, 4, 4, 1, 5, 5, 7, 6, 5, 4, 5, 6, 5, 6, 2, 4, 7, 7,\n",
            "        4, 6, 5, 0, 2, 3, 9, 2, 3, 8, 0, 7, 0, 6, 8, 1, 2, 8, 9, 1, 5, 4, 2, 3,\n",
            "        5, 5, 3, 6, 5, 0, 3, 2, 1, 3, 3, 7, 7, 4, 9, 4, 3, 9, 5, 1, 7, 6, 6, 4,\n",
            "        8, 5, 1, 2, 8, 4, 5, 3, 3, 3, 0, 4, 8, 7, 7, 9, 4, 8, 4, 5, 9, 1, 8, 3,\n",
            "        1, 3, 8, 0, 3, 5, 4, 6], device='cuda:0')\n",
            "Output: tensor([[-4.5298, -1.1835, -4.3275,  ..., -2.5168, -3.9595,  7.0209],\n",
            "        [-4.0457, -9.5587,  6.7427,  ..., -3.6227, -3.3474, -9.5001],\n",
            "        [-5.0171, -5.7118, -5.3094,  ...,  6.6339, -9.1756, -5.6623],\n",
            "        ...,\n",
            "        [-3.0986, -2.8547, -5.4287,  ..., -2.1305, -8.2686, -1.6124],\n",
            "        [-4.1296, -6.6573, -3.4499,  ...,  0.0746, -5.1295, -7.1440],\n",
            "        [-8.3165, -4.9326, -0.1773,  ..., -0.7125, -5.4067, -7.3224]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([9, 2, 7, 6, 0, 5, 2, 9, 7, 5, 0, 4, 8, 3, 3, 3, 3, 8, 7, 6, 1, 6, 3, 8,\n",
            "        9, 0, 1, 9, 4, 9, 0, 9, 7, 7, 2, 8, 5, 9, 8, 2, 8, 8, 6, 6, 8, 6, 9, 2,\n",
            "        1, 1, 5, 5, 5, 0, 7, 6, 1, 4, 4, 1, 1, 8, 1, 1, 2, 3, 4, 1, 9, 3, 2, 7,\n",
            "        4, 3, 6, 9, 6, 9, 8, 8, 6, 7, 4, 7, 2, 0, 2, 0, 9, 0, 7, 8, 1, 2, 3, 2,\n",
            "        4, 2, 9, 0, 8, 8, 1, 6, 5, 5, 2, 4, 1, 0, 6, 1, 8, 4, 5, 6, 5, 0, 1, 9,\n",
            "        0, 5, 7, 9, 6, 5, 4, 6], device='cuda:0')\n",
            "Target: tensor([9, 2, 7, 2, 0, 5, 2, 9, 7, 5, 0, 7, 0, 3, 5, 5, 3, 8, 7, 7, 9, 6, 5, 8,\n",
            "        9, 0, 1, 9, 4, 0, 0, 1, 7, 2, 2, 8, 5, 9, 8, 2, 8, 0, 6, 6, 8, 6, 9, 6,\n",
            "        1, 1, 5, 3, 5, 0, 7, 6, 1, 4, 4, 1, 1, 8, 1, 1, 2, 3, 4, 9, 9, 5, 2, 7,\n",
            "        3, 3, 6, 9, 6, 9, 8, 8, 6, 7, 4, 7, 2, 3, 8, 0, 9, 0, 7, 8, 1, 2, 3, 2,\n",
            "        4, 2, 9, 0, 3, 8, 9, 6, 5, 5, 2, 4, 1, 0, 6, 1, 8, 4, 5, 5, 5, 0, 8, 9,\n",
            "        0, 5, 7, 9, 6, 3, 4, 6], device='cuda:0')\n",
            "Output: tensor([[-1.9117, -6.6206, -3.6237,  ...,  1.3507, -2.6527, -3.7972],\n",
            "        [-1.6257, -2.3841, -4.0509,  ..., -3.2579, -1.6843,  1.6684],\n",
            "        [-4.3133, -6.9807, -0.3499,  ..., -1.7168, -5.2463, -7.2594],\n",
            "        ...,\n",
            "        [-3.6311, -2.0335, -0.5955,  ..., -4.4852, -5.3111, -2.3542],\n",
            "        [-0.3720, -2.3763, -5.1878,  ..., -3.7405,  5.1623, -4.0871],\n",
            "        [-3.9268, -3.1741, -0.7102,  ..., -3.6453, -7.8748, -6.4563]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([4, 9, 3, 1, 3, 5, 2, 1, 0, 2, 3, 9, 8, 1, 9, 7, 7, 3, 1, 4, 1, 1, 3, 8,\n",
            "        8, 1, 9, 3, 1, 9, 3, 1, 1, 6, 2, 7, 2, 6, 4, 6, 9, 6, 0, 1, 3, 9, 3, 9,\n",
            "        8, 8, 2, 9, 1, 2, 6, 0, 5, 0, 7, 4, 7, 9, 4, 5, 0, 2, 3, 3, 3, 8, 9, 1,\n",
            "        2, 7, 1, 3, 3, 6, 9, 1, 1, 7, 1, 9, 5, 8, 8, 9, 9, 3, 9, 8, 3, 0, 2, 6,\n",
            "        9, 2, 2, 4, 0, 8, 4, 0, 1, 7, 7, 2, 4, 9, 5, 2, 2, 9, 4, 4, 2, 4, 2, 5,\n",
            "        6, 6, 8, 8, 2, 6, 8, 6], device='cuda:0')\n",
            "Target: tensor([4, 9, 7, 1, 2, 5, 2, 8, 0, 2, 2, 9, 8, 1, 9, 7, 7, 3, 1, 4, 1, 1, 3, 8,\n",
            "        8, 1, 9, 3, 8, 9, 5, 1, 1, 6, 2, 7, 2, 6, 2, 6, 9, 6, 0, 1, 3, 9, 3, 8,\n",
            "        8, 8, 2, 9, 1, 2, 6, 0, 5, 0, 7, 7, 7, 9, 4, 5, 0, 2, 3, 5, 5, 8, 9, 1,\n",
            "        2, 7, 1, 3, 4, 6, 9, 1, 0, 7, 1, 9, 5, 8, 1, 9, 9, 7, 9, 8, 3, 0, 2, 6,\n",
            "        0, 2, 2, 4, 0, 8, 4, 8, 1, 2, 7, 2, 7, 9, 5, 2, 2, 9, 2, 6, 5, 7, 2, 5,\n",
            "        6, 5, 1, 8, 2, 6, 8, 6], device='cuda:0')\n",
            "Output: tensor([[-3.4078, -5.8038, -7.9950,  ..., -1.8068, -4.0612, -2.6314],\n",
            "        [ 6.3796, -3.7888, -2.6485,  ..., -4.8447,  0.3076, -1.1347],\n",
            "        [ 0.5812, -3.6529, -2.4380,  ..., -2.5820,  1.3501,  0.5174],\n",
            "        ...,\n",
            "        [-3.8467,  1.3319, -4.4601,  ..., -3.5592, -4.5099,  4.0260],\n",
            "        [-1.1897,  2.4333, -5.7676,  ..., -2.8379, -4.1626,  5.3291],\n",
            "        [-5.6753, -5.2978, -2.7832,  ..., -4.8232, -6.9140, -4.2031]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([5, 0, 8, 0, 0, 1, 8, 6, 8, 0, 6, 6, 0, 7, 9, 8, 0, 7, 0, 9, 6, 0, 4, 7,\n",
            "        1, 7, 8, 3, 2, 3, 9, 2, 5, 3, 3, 1, 5, 1, 0, 2, 5, 2, 4, 3, 8, 4, 4, 6,\n",
            "        1, 1, 4, 1, 7, 0, 1, 0, 3, 0, 2, 9, 4, 5, 0, 3, 8, 7, 2, 3, 0, 9, 4, 7,\n",
            "        9, 9, 8, 7, 5, 6, 7, 1, 3, 3, 7, 4, 6, 8, 4, 9, 8, 4, 9, 9, 3, 6, 0, 2,\n",
            "        3, 6, 3, 5, 8, 5, 8, 5, 4, 0, 9, 5, 6, 3, 8, 6, 1, 8, 4, 5, 8, 2, 3, 9,\n",
            "        2, 4, 4, 0, 5, 9, 9, 3], device='cuda:0')\n",
            "Target: tensor([3, 0, 8, 0, 0, 9, 9, 6, 8, 0, 6, 6, 0, 7, 9, 8, 0, 5, 0, 9, 6, 0, 4, 7,\n",
            "        1, 7, 8, 8, 2, 3, 9, 6, 5, 5, 3, 1, 5, 3, 0, 2, 5, 2, 7, 3, 8, 4, 4, 6,\n",
            "        1, 9, 4, 1, 7, 0, 1, 4, 3, 0, 2, 9, 4, 5, 0, 3, 4, 7, 2, 3, 0, 1, 3, 7,\n",
            "        9, 9, 0, 7, 5, 6, 7, 1, 2, 2, 7, 4, 6, 8, 4, 9, 8, 4, 9, 2, 3, 4, 0, 2,\n",
            "        5, 6, 0, 5, 3, 5, 8, 3, 7, 4, 9, 5, 6, 3, 8, 6, 9, 8, 5, 5, 8, 3, 5, 9,\n",
            "        0, 4, 2, 0, 5, 9, 9, 3], device='cuda:0')\n",
            "Output: tensor([[-2.5663, -0.1000, -6.8770,  ..., -3.6465,  4.0785,  2.4265],\n",
            "        [ 2.7062, -4.9069, -4.6684,  ..., -5.5836, -0.6619, -2.3493],\n",
            "        [-6.7580, -7.3493, -1.3157,  ..., -1.9746, -6.7736, -5.0392],\n",
            "        ...,\n",
            "        [-4.0063, -4.1803, -2.0236,  ..., -1.8472, -5.7691, -5.1295],\n",
            "        [-1.2835, -1.7000, -3.6889,  ..., -3.4866,  5.6362, -2.4023],\n",
            "        [-4.1641, -9.8344,  0.2566,  ...,  0.5045, -8.0662, -5.5752]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([8, 0, 4, 0, 2, 5, 1, 2, 4, 6, 9, 1, 3, 2, 2, 7, 9, 3, 1, 5, 5, 2, 2, 4,\n",
            "        8, 8, 5, 0, 0, 7, 0, 4, 1, 6, 3, 0, 9, 2, 6, 7, 4, 2, 0, 0, 7, 2, 3, 2,\n",
            "        6, 4, 2, 7, 8, 7, 1, 0, 1, 3, 6, 4, 8, 0, 4, 0, 9, 1, 6, 3, 1, 0, 1, 3,\n",
            "        0, 3, 7, 9, 9, 2, 7, 3, 7, 1, 6, 6, 9, 1, 1, 4, 6, 2, 1, 7, 0, 1, 2, 1,\n",
            "        1, 4, 4, 6, 9, 9, 3, 2, 0, 0, 7, 3, 7, 6, 4, 6, 9, 4, 8, 9, 3, 0, 5, 7,\n",
            "        2, 0, 2, 9, 9, 3, 8, 7], device='cuda:0')\n",
            "Target: tensor([8, 0, 6, 0, 2, 3, 7, 0, 0, 6, 9, 1, 5, 2, 2, 7, 9, 3, 9, 3, 5, 2, 2, 2,\n",
            "        8, 8, 5, 0, 0, 7, 0, 3, 1, 6, 5, 0, 9, 2, 6, 7, 4, 2, 0, 0, 5, 3, 3, 2,\n",
            "        6, 4, 2, 7, 8, 0, 1, 0, 1, 3, 6, 4, 8, 0, 6, 0, 9, 1, 6, 2, 1, 0, 1, 3,\n",
            "        0, 3, 7, 9, 9, 3, 7, 5, 7, 1, 6, 6, 1, 1, 1, 4, 4, 2, 1, 7, 0, 1, 2, 1,\n",
            "        1, 3, 2, 6, 9, 1, 3, 2, 0, 8, 7, 3, 4, 6, 4, 6, 9, 4, 8, 9, 3, 0, 5, 7,\n",
            "        2, 0, 2, 9, 9, 3, 8, 7], device='cuda:0')\n",
            "Output: tensor([[ -2.1095,   5.7166,  -5.3689,  ...,  -5.5797,  -1.3404,  -1.0376],\n",
            "        [ -1.5084,   9.6911,  -6.4869,  ...,  -2.3090,  -2.3585,   1.4170],\n",
            "        [ -3.8534,  -8.1265,   4.5241,  ...,  -1.9064,  -9.8058,  -5.8083],\n",
            "        ...,\n",
            "        [  5.4189,  -4.8740,  -2.3546,  ...,  -3.6327,  -1.1313,  -3.6324],\n",
            "        [ -6.1266,  -9.4024,   1.5621,  ...,  -4.7199,  -4.1556, -11.7488],\n",
            "        [  0.6238,  -2.5934,  -4.5601,  ...,  -0.5407,  -1.8896,   0.7104]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([1, 1, 2, 2, 2, 4, 9, 1, 2, 6, 1, 2, 1, 1, 9, 2, 8, 1, 9, 9, 7, 4, 4, 5,\n",
            "        0, 6, 3, 7, 1, 7, 7, 9, 8, 6, 5, 5, 5, 1, 5, 2, 5, 8, 4, 3, 3, 5, 9, 2,\n",
            "        8, 5, 1, 0, 6, 5, 0, 9, 6, 0, 0, 8, 7, 1, 2, 6, 9, 1, 6, 4, 7, 1, 9, 0,\n",
            "        0, 6, 2, 8, 4, 8, 3, 8, 4, 0, 8, 3, 3, 1, 2, 3, 0, 5, 0, 4, 1, 7, 2, 0,\n",
            "        1, 7, 9, 1, 6, 1, 6, 1, 2, 8, 7, 0, 9, 5, 8, 8, 0, 1, 4, 9, 7, 1, 9, 2,\n",
            "        1, 1, 8, 3, 6, 0, 2, 9], device='cuda:0')\n",
            "Target: tensor([1, 1, 2, 2, 2, 5, 9, 1, 2, 6, 1, 2, 1, 8, 9, 2, 3, 1, 9, 9, 4, 4, 5, 5,\n",
            "        0, 3, 3, 7, 1, 7, 7, 9, 8, 6, 3, 5, 5, 1, 6, 2, 7, 8, 4, 3, 3, 5, 9, 6,\n",
            "        8, 5, 1, 0, 6, 7, 0, 9, 6, 2, 0, 8, 7, 1, 2, 6, 9, 1, 6, 4, 7, 1, 9, 0,\n",
            "        0, 5, 2, 8, 3, 8, 2, 8, 4, 0, 8, 3, 6, 1, 2, 3, 0, 5, 0, 4, 1, 7, 2, 0,\n",
            "        1, 3, 9, 1, 6, 1, 6, 1, 2, 8, 7, 0, 9, 5, 8, 0, 0, 1, 7, 9, 7, 1, 9, 2,\n",
            "        1, 9, 4, 3, 6, 0, 2, 0], device='cuda:0')\n",
            "Output: tensor([[ 1.1928, -6.3420, -0.5692,  ..., -3.3746,  0.2664, -5.0689],\n",
            "        [-2.9996, -3.6331, -3.1911,  ..., -5.1258, -3.2794,  5.4571],\n",
            "        [-3.2801,  2.5061, -4.8508,  ..., -4.0911, -5.7335,  1.9856],\n",
            "        ...,\n",
            "        [ 0.9866, -2.9170, -6.2153,  ..., -1.8888,  4.4350, -0.3839],\n",
            "        [-5.2794, -5.9280, -4.0241,  ..., -1.3749, -8.2736, -5.4913],\n",
            "        [-5.6789, -6.9011,  1.3194,  ..., -6.7563, -6.0706, -7.2854]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([0, 9, 1, 8, 9, 2, 5, 4, 3, 0, 2, 8, 7, 3, 7, 5, 4, 4, 5, 9, 5, 2, 4, 2,\n",
            "        7, 9, 3, 1, 4, 7, 1, 2, 3, 1, 1, 3, 6, 8, 3, 7, 7, 0, 8, 1, 5, 2, 3, 5,\n",
            "        6, 6, 3, 3, 9, 8, 8, 2, 8, 0, 4, 1, 7, 8, 7, 9, 4, 2, 6, 3, 8, 8, 6, 7,\n",
            "        3, 8, 1, 1, 5, 3, 4, 3, 1, 9, 8, 5, 9, 9, 6, 8, 8, 9, 8, 1, 0, 1, 8, 8,\n",
            "        6, 7, 9, 0, 6, 1, 6, 4, 6, 3, 5, 6, 5, 3, 3, 3, 6, 8, 9, 4, 4, 3, 8, 3,\n",
            "        6, 8, 8, 5, 7, 8, 4, 6], device='cuda:0')\n",
            "Target: tensor([0, 9, 9, 8, 9, 4, 5, 4, 3, 0, 7, 8, 7, 3, 7, 5, 4, 6, 5, 9, 5, 2, 4, 2,\n",
            "        7, 9, 6, 7, 4, 7, 1, 2, 3, 1, 1, 3, 6, 8, 3, 7, 7, 0, 8, 1, 5, 2, 5, 5,\n",
            "        3, 6, 3, 5, 9, 8, 4, 6, 8, 0, 4, 1, 7, 8, 4, 9, 7, 2, 6, 3, 8, 8, 6, 7,\n",
            "        3, 8, 1, 1, 5, 3, 7, 9, 9, 9, 8, 5, 9, 9, 6, 8, 8, 4, 8, 1, 0, 1, 8, 8,\n",
            "        6, 5, 9, 0, 6, 1, 2, 6, 6, 3, 5, 6, 5, 5, 3, 3, 6, 8, 9, 4, 4, 3, 8, 3,\n",
            "        6, 0, 8, 5, 7, 8, 4, 6], device='cuda:0')\n",
            "Output: tensor([[-6.4676, -5.6816, -4.6301,  ...,  0.4081, -4.7299, -5.1655],\n",
            "        [-3.1940, -5.5115, -2.6222,  ..., -3.6472, -3.9097, -5.7523],\n",
            "        [-1.6052,  2.9796, -7.1597,  ..., -2.9603, -5.7032,  2.1052],\n",
            "        ...,\n",
            "        [-5.3526, -2.8589, -5.1715,  ..., -3.5700, -4.2774, -3.1739],\n",
            "        [-5.1609, -6.7267,  2.4005,  ..., -0.7623, -2.5730, -4.1512],\n",
            "        [-5.4876, -2.2682, -5.6001,  ...,  0.0904, -7.6475, -2.4961]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([5, 3, 1, 8, 3, 0, 4, 4, 5, 3, 5, 3, 3, 2, 7, 7, 9, 6, 0, 5, 2, 6, 1, 1,\n",
            "        7, 9, 3, 3, 0, 7, 8, 8, 3, 9, 8, 7, 8, 5, 0, 3, 2, 4, 1, 7, 5, 7, 1, 8,\n",
            "        7, 6, 5, 4, 4, 3, 8, 6, 1, 1, 6, 3, 4, 2, 8, 9, 8, 3, 0, 1, 1, 1, 0, 5,\n",
            "        2, 0, 5, 0, 4, 2, 6, 3, 9, 6, 6, 3, 9, 4, 8, 3, 3, 5, 1, 1, 2, 9, 7, 8,\n",
            "        5, 5, 8, 6, 9, 0, 8, 0, 3, 1, 3, 3, 5, 4, 3, 8, 9, 4, 6, 4, 2, 4, 3, 8,\n",
            "        9, 6, 6, 4, 2, 3, 2, 7], device='cuda:0')\n",
            "Target: tensor([5, 3, 1, 8, 3, 0, 4, 4, 3, 3, 5, 3, 3, 2, 7, 7, 8, 2, 0, 5, 5, 6, 1, 1,\n",
            "        3, 9, 9, 3, 6, 7, 1, 8, 3, 9, 8, 7, 8, 8, 0, 3, 2, 4, 8, 7, 5, 7, 1, 8,\n",
            "        7, 3, 5, 4, 4, 3, 8, 6, 1, 1, 6, 3, 2, 2, 8, 9, 8, 3, 3, 8, 1, 1, 0, 5,\n",
            "        2, 0, 5, 0, 5, 8, 6, 3, 9, 6, 6, 3, 9, 4, 8, 3, 4, 3, 1, 1, 2, 9, 7, 8,\n",
            "        5, 5, 8, 6, 9, 0, 8, 3, 5, 1, 3, 3, 5, 5, 3, 8, 1, 4, 6, 4, 2, 4, 3, 8,\n",
            "        9, 6, 6, 6, 5, 5, 2, 7], device='cuda:0')\n",
            "Output: tensor([[-3.5350, -3.2790, -1.2852,  ...,  0.9103, -4.8489, -2.4369],\n",
            "        [-5.0376, -7.0587, -0.4586,  ..., -1.7431, -8.7758, -8.0518],\n",
            "        [-5.3626, -5.5301, -1.5335,  ...,  0.0453, -5.6725, -7.0150],\n",
            "        ...,\n",
            "        [ 0.9916,  5.2363, -4.3846,  ..., -4.8501, -3.1831, -3.1164],\n",
            "        [-4.8094, -6.3597, -3.3195,  ..., -3.0419, -3.1728, -7.8962],\n",
            "        [ 4.8700, -5.6306, -4.1483,  ..., -3.2888, -1.2349, -1.0775]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([7, 4, 4, 9, 5, 8, 3, 7, 7, 8, 5, 3, 9, 3, 6, 3, 3, 3, 3, 8, 5, 5, 1, 3,\n",
            "        7, 9, 6, 1, 6, 4, 9, 0, 5, 7, 4, 9, 1, 0, 0, 2, 7, 1, 3, 2, 0, 6, 1, 6,\n",
            "        6, 4, 4, 2, 1, 8, 4, 4, 7, 8, 3, 8, 9, 3, 3, 2, 2, 8, 3, 0, 1, 1, 1, 1,\n",
            "        6, 8, 6, 1, 5, 0, 4, 5, 0, 7, 0, 6, 2, 0, 9, 8, 3, 8, 5, 8, 2, 1, 6, 0,\n",
            "        4, 1, 8, 2, 3, 2, 2, 6, 5, 2, 9, 2, 4, 4, 9, 8, 5, 3, 9, 0, 5, 7, 1, 8,\n",
            "        5, 8, 6, 4, 5, 1, 3, 0], device='cuda:0')\n",
            "Target: tensor([7, 4, 4, 9, 3, 8, 3, 7, 7, 8, 5, 3, 9, 1, 6, 7, 3, 3, 3, 8, 5, 6, 1, 3,\n",
            "        7, 9, 6, 1, 6, 4, 9, 0, 5, 7, 4, 9, 1, 0, 0, 2, 7, 8, 3, 2, 7, 6, 1, 6,\n",
            "        6, 4, 4, 2, 1, 8, 4, 4, 7, 8, 5, 8, 9, 4, 3, 2, 2, 9, 3, 0, 1, 1, 1, 1,\n",
            "        6, 8, 7, 1, 3, 0, 4, 5, 0, 3, 0, 6, 2, 0, 9, 8, 6, 8, 5, 8, 2, 9, 6, 0,\n",
            "        6, 1, 8, 7, 3, 2, 2, 6, 5, 6, 9, 2, 5, 4, 9, 8, 5, 3, 9, 3, 5, 7, 1, 8,\n",
            "        5, 8, 6, 2, 5, 1, 3, 0], device='cuda:0')\n",
            "Output: tensor([[-6.0821, -9.4234,  1.0977,  ..., -1.9323, -7.8908, -9.7615],\n",
            "        [-2.3471, -0.7677, -4.6423,  ..., -1.8490, -3.1991, -2.3996],\n",
            "        [-3.9281, -2.0750, -3.4366,  ..., -3.7682, -2.6651, -1.6666],\n",
            "        ...,\n",
            "        [ 0.4343, -6.9634,  0.4306,  ..., -3.9440, -4.8403, -6.0766],\n",
            "        [-4.0389, -8.0306, -1.2479,  ...,  0.9132, -8.3367, -7.7083],\n",
            "        [-4.2718, -2.9943, -2.1380,  ..., -6.4416, -1.6297,  0.8910]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([2, 1, 3, 7, 4, 9, 2, 1, 4, 4, 5, 9, 8, 7, 2, 1, 2, 1, 9, 3, 6, 3, 8, 0,\n",
            "        2, 2, 4, 9, 3, 5, 5, 4, 6, 3, 1, 0, 2, 8, 4, 2, 7, 1, 3, 7, 0, 5, 1, 1,\n",
            "        0, 4, 1, 9, 4, 8, 1, 1, 1, 5, 9, 8, 1, 6, 7, 2, 1, 9, 3, 8, 8, 1, 9, 7,\n",
            "        0, 0, 1, 6, 6, 6, 3, 1, 3, 8, 8, 8, 3, 6, 6, 7, 1, 0, 4, 6, 4, 8, 3, 0,\n",
            "        6, 6, 8, 6, 3, 1, 1, 8, 6, 4, 0, 5, 4, 4, 2, 1, 7, 8, 2, 0, 1, 7, 4, 8,\n",
            "        7, 6, 6, 5, 0, 6, 4, 9], device='cuda:0')\n",
            "Target: tensor([4, 1, 9, 5, 4, 9, 2, 1, 4, 2, 5, 9, 8, 7, 2, 1, 6, 9, 7, 3, 6, 5, 8, 0,\n",
            "        2, 2, 4, 9, 0, 5, 5, 4, 2, 3, 1, 0, 3, 0, 4, 4, 9, 1, 3, 3, 0, 2, 1, 1,\n",
            "        0, 4, 1, 9, 4, 8, 1, 1, 1, 5, 9, 8, 1, 6, 7, 2, 9, 9, 7, 8, 8, 1, 9, 7,\n",
            "        8, 9, 1, 6, 6, 3, 3, 1, 3, 0, 8, 8, 3, 6, 6, 7, 1, 0, 7, 6, 4, 8, 2, 0,\n",
            "        6, 6, 8, 6, 3, 1, 1, 8, 6, 7, 0, 5, 4, 3, 2, 1, 7, 8, 2, 0, 1, 7, 4, 8,\n",
            "        7, 6, 6, 7, 0, 6, 4, 9], device='cuda:0')\n",
            "Output: tensor([[ -0.4856,  -6.2520,  -2.1463,  ...,  -3.9704,  -3.0952,  -7.3319],\n",
            "        [ -4.3579,  -6.6527,  -3.4253,  ...,  -2.3522,  -3.1399,  -4.7440],\n",
            "        [ -8.2039,  -5.8323,  -1.2431,  ...,  -2.3049,  -2.9601,  -5.2361],\n",
            "        ...,\n",
            "        [ -1.2614,   1.5190,  -7.2844,  ...,  -0.8212,   6.5672,  -1.8001],\n",
            "        [  3.5812,  -1.4882,  -3.1301,  ...,  -4.9949,   1.8202,  -2.4995],\n",
            "        [ -5.4646,  -5.3272,  -0.3340,  ...,  -2.0924, -10.0720,  -7.7599]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([0, 6, 5, 1, 3, 8, 6, 8, 1, 9, 6, 7, 6, 7, 7, 6, 0, 1, 3, 1, 1, 5, 2, 4,\n",
            "        3, 3, 0, 1, 3, 2, 7, 8, 8, 6, 2, 7, 4, 6, 8, 9, 3, 4, 5, 5, 3, 1, 4, 1,\n",
            "        9, 5, 3, 7, 7, 4, 5, 2, 3, 3, 3, 3, 5, 4, 0, 7, 1, 7, 6, 0, 8, 1, 2, 4,\n",
            "        1, 2, 2, 7, 9, 8, 3, 8, 7, 3, 3, 4, 7, 2, 3, 1, 5, 3, 0, 4, 3, 4, 9, 3,\n",
            "        7, 9, 1, 7, 8, 3, 1, 1, 3, 2, 3, 7, 2, 7, 9, 6, 8, 6, 8, 6, 5, 9, 0, 4,\n",
            "        8, 5, 6, 5, 3, 8, 0, 4], device='cuda:0')\n",
            "Target: tensor([2, 3, 2, 1, 3, 8, 6, 8, 1, 9, 6, 7, 6, 5, 0, 6, 0, 1, 3, 1, 1, 5, 3, 4,\n",
            "        3, 3, 8, 1, 6, 6, 1, 8, 8, 6, 2, 7, 4, 6, 8, 9, 3, 3, 5, 5, 3, 1, 4, 1,\n",
            "        9, 5, 1, 7, 7, 7, 5, 2, 3, 3, 6, 6, 6, 4, 0, 7, 1, 7, 3, 0, 8, 1, 2, 4,\n",
            "        1, 2, 2, 4, 9, 8, 2, 8, 7, 6, 3, 4, 7, 2, 3, 1, 5, 3, 0, 4, 3, 4, 9, 4,\n",
            "        7, 9, 1, 7, 8, 3, 1, 8, 3, 2, 5, 7, 2, 7, 9, 6, 8, 6, 8, 6, 5, 9, 0, 4,\n",
            "        8, 5, 6, 4, 3, 8, 0, 4], device='cuda:0')\n",
            "Output: tensor([[-1.4232, -1.8195, -3.1059,  ..., -5.0939,  1.4302, -1.7984],\n",
            "        [-5.3952, -8.3066, -2.2944,  ..., -3.3762, -6.7996, -7.2236],\n",
            "        [-5.8584, -4.0220, -3.0734,  ...,  0.2234, -7.1046, -3.9281],\n",
            "        ...,\n",
            "        [ 7.8153, -3.0532, -2.1021,  ..., -4.6663,  3.7748, -5.2689],\n",
            "        [-2.1420,  0.4689, -5.5468,  ..., -3.9551, -2.4355,  7.7956],\n",
            "        [-6.5301, -6.1235, -1.1562,  ..., -2.5081, -7.7230, -9.6997]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([8, 4, 7, 2, 7, 1, 7, 4, 1, 8, 4, 2, 2, 8, 7, 2, 2, 6, 5, 7, 3, 8, 9, 9,\n",
            "        5, 5, 2, 9, 8, 7, 2, 3, 3, 7, 4, 3, 2, 1, 9, 2, 1, 3, 2, 2, 6, 5, 9, 1,\n",
            "        5, 4, 3, 6, 2, 6, 9, 7, 0, 0, 8, 9, 1, 2, 5, 7, 2, 0, 2, 2, 1, 2, 4, 2,\n",
            "        5, 0, 0, 7, 4, 6, 5, 6, 4, 8, 3, 4, 5, 5, 0, 2, 6, 7, 5, 0, 9, 9, 6, 1,\n",
            "        0, 7, 6, 3, 6, 9, 3, 6, 6, 2, 2, 7, 0, 1, 9, 3, 4, 4, 3, 0, 2, 7, 6, 4,\n",
            "        3, 0, 8, 4, 0, 0, 9, 6], device='cuda:0')\n",
            "Target: tensor([8, 6, 7, 2, 7, 1, 7, 4, 1, 2, 4, 2, 2, 8, 7, 2, 2, 6, 7, 5, 6, 8, 9, 9,\n",
            "        6, 5, 2, 9, 8, 7, 2, 3, 3, 7, 4, 3, 2, 1, 9, 0, 1, 5, 3, 2, 6, 5, 9, 1,\n",
            "        5, 4, 3, 6, 2, 8, 9, 7, 8, 0, 8, 9, 1, 2, 5, 7, 2, 0, 6, 6, 1, 6, 3, 1,\n",
            "        5, 0, 3, 7, 4, 6, 5, 6, 4, 8, 3, 4, 5, 5, 0, 6, 6, 7, 5, 0, 9, 9, 6, 1,\n",
            "        0, 4, 6, 3, 6, 9, 3, 6, 6, 8, 2, 1, 0, 1, 9, 3, 7, 4, 3, 0, 2, 7, 6, 4,\n",
            "        3, 0, 8, 2, 0, 0, 9, 6], device='cuda:0')\n",
            "Output: tensor([[-6.7217, -4.3955, -2.5400,  ..., -2.4809, -5.7393, -6.6009],\n",
            "        [-6.3943, -5.8938, -1.2116,  ..., -2.1538, -7.4534, -4.9200],\n",
            "        [ 3.5756, -5.8283,  0.5779,  ..., -5.6913,  3.2706, -5.7597],\n",
            "        ...,\n",
            "        [-3.4591, -5.1626, -1.5922,  ...,  0.4273, -5.6322, -3.7491],\n",
            "        [-3.3747, -3.9026, -0.2612,  ..., -3.3260, -7.2458, -6.7259],\n",
            "        [-1.8259,  7.3105, -9.3752,  ..., -5.7356, -2.7312,  2.0138]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 6, 0, 7, 4, 1, 8, 1, 2, 2, 4, 8, 5, 2, 6, 5, 3, 9, 1, 0, 7, 2, 7, 4,\n",
            "        0, 7, 4, 2, 2, 4, 0, 5, 9, 4, 1, 8, 3, 5, 5, 9, 8, 5, 6, 8, 0, 9, 8, 9,\n",
            "        1, 6, 3, 3, 0, 5, 7, 4, 8, 4, 8, 9, 0, 6, 0, 0, 8, 2, 9, 4, 2, 7, 6, 0,\n",
            "        5, 8, 4, 1, 9, 0, 5, 4, 4, 8, 4, 9, 6, 0, 7, 7, 6, 8, 9, 6, 2, 0, 4, 9,\n",
            "        4, 9, 3, 9, 4, 6, 3, 0, 9, 7, 1, 3, 3, 7, 6, 7, 0, 1, 9, 4, 6, 7, 9, 8,\n",
            "        3, 9, 2, 1, 2, 7, 6, 1], device='cuda:0')\n",
            "Target: tensor([6, 6, 8, 7, 4, 1, 8, 1, 2, 2, 4, 8, 5, 2, 6, 5, 3, 9, 1, 0, 7, 2, 4, 4,\n",
            "        0, 0, 6, 2, 2, 4, 0, 5, 9, 7, 1, 8, 4, 5, 5, 9, 8, 5, 7, 8, 0, 9, 8, 9,\n",
            "        1, 6, 3, 8, 0, 3, 4, 4, 8, 4, 8, 9, 8, 6, 0, 0, 8, 2, 7, 4, 2, 5, 6, 0,\n",
            "        5, 8, 4, 1, 9, 0, 1, 4, 4, 8, 4, 9, 6, 0, 7, 7, 6, 8, 9, 6, 2, 0, 4, 9,\n",
            "        4, 9, 3, 9, 6, 6, 7, 0, 9, 7, 1, 8, 6, 0, 6, 7, 4, 1, 9, 4, 6, 7, 9, 8,\n",
            "        3, 9, 2, 1, 2, 7, 6, 1], device='cuda:0')\n",
            "Output: tensor([[ 1.9843, -2.7620,  1.1460,  ..., -4.1990, -0.5696, -4.7908],\n",
            "        [ 5.3266, -2.5103, -3.0568,  ..., -1.7065, -2.8958,  0.0520],\n",
            "        [-6.5165, -4.1278, -3.1081,  ...,  0.4043, -7.7967, -4.8800],\n",
            "        ...,\n",
            "        [ 1.6161,  0.2309, -6.0199,  ..., -4.4191,  2.1211,  3.0145],\n",
            "        [ 4.6931,  0.4632, -2.0324,  ..., -6.2009, -2.1787, -1.2462],\n",
            "        [-5.6742, -8.4181, -0.6338,  ..., -4.7370, -5.3324, -6.0177]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([0, 0, 7, 6, 0, 4, 4, 2, 1, 8, 0, 6, 0, 5, 2, 8, 7, 9, 6, 5, 9, 4, 3, 4,\n",
            "        6, 0, 6, 2, 2, 4, 1, 0, 5, 9, 8, 9, 3, 5, 9, 3, 8, 1, 6, 3, 7, 5, 6, 2,\n",
            "        0, 2, 8, 2, 0, 5, 7, 8, 1, 0, 8, 9, 7, 0, 3, 8, 2, 5, 9, 5, 8, 9, 2, 0,\n",
            "        0, 2, 2, 7, 7, 9, 2, 6, 2, 3, 1, 5, 2, 0, 0, 4, 0, 6, 5, 8, 0, 5, 8, 6,\n",
            "        4, 8, 5, 2, 9, 7, 9, 7, 1, 0, 1, 9, 6, 9, 2, 7, 9, 4, 4, 0, 6, 3, 4, 1,\n",
            "        5, 7, 3, 8, 5, 9, 0, 3], device='cuda:0')\n",
            "Target: tensor([0, 0, 5, 6, 0, 4, 3, 2, 8, 8, 0, 6, 9, 5, 2, 8, 7, 0, 6, 5, 9, 7, 2, 3,\n",
            "        6, 9, 6, 2, 2, 4, 1, 0, 5, 0, 8, 9, 3, 5, 9, 3, 8, 1, 6, 3, 7, 5, 6, 2,\n",
            "        0, 2, 8, 2, 8, 7, 7, 8, 1, 0, 8, 9, 7, 0, 3, 8, 0, 5, 9, 5, 8, 4, 2, 0,\n",
            "        9, 2, 2, 4, 4, 9, 2, 2, 2, 5, 1, 3, 2, 0, 0, 4, 0, 6, 5, 8, 0, 5, 8, 6,\n",
            "        4, 8, 5, 2, 9, 7, 9, 7, 1, 0, 1, 9, 6, 9, 2, 7, 9, 4, 4, 0, 6, 2, 4, 1,\n",
            "        3, 7, 2, 8, 5, 9, 0, 3], device='cuda:0')\n",
            "Output: tensor([[-8.1992, -6.3545,  1.6796,  ..., -6.7425, -6.6933, -8.3165],\n",
            "        [-4.2461, -6.4948, -3.3927,  ..., -4.9804, -4.6794, -8.8745],\n",
            "        [-6.9137, -5.9252,  3.2794,  ..., -1.8100, -9.0133, -6.7596],\n",
            "        ...,\n",
            "        [-7.0385, -3.9078, -0.5240,  ..., -0.7134, -5.4639, -3.1731],\n",
            "        [-4.0366, -4.2495, -2.9187,  ..., -1.6432, -6.7878,  2.6795],\n",
            "        [-7.8717, -3.3626, -3.1152,  ..., -4.4381, -6.6992, -4.6846]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([2, 3, 2, 7, 6, 3, 2, 5, 9, 2, 5, 1, 9, 8, 7, 7, 4, 0, 6, 9, 2, 3, 1, 0,\n",
            "        4, 1, 8, 8, 4, 9, 2, 4, 3, 3, 9, 2, 0, 1, 1, 8, 4, 4, 8, 5, 3, 9, 5, 7,\n",
            "        2, 2, 5, 4, 7, 3, 3, 1, 0, 9, 5, 9, 3, 7, 6, 0, 4, 2, 2, 5, 6, 3, 8, 9,\n",
            "        3, 6, 1, 4, 5, 4, 6, 7, 2, 0, 0, 2, 0, 4, 9, 8, 4, 9, 1, 1, 5, 0, 0, 8,\n",
            "        4, 1, 7, 4, 5, 4, 3, 3, 2, 0, 6, 6, 0, 1, 7, 9, 8, 3, 7, 8, 9, 4, 8, 9,\n",
            "        0, 1, 7, 1, 6, 3, 9, 5], device='cuda:0')\n",
            "Target: tensor([2, 3, 2, 7, 6, 3, 2, 5, 9, 0, 5, 9, 9, 8, 7, 7, 4, 8, 6, 5, 2, 3, 1, 0,\n",
            "        4, 1, 8, 8, 4, 9, 4, 4, 3, 3, 9, 2, 0, 1, 1, 8, 4, 4, 8, 3, 2, 9, 5, 7,\n",
            "        6, 2, 5, 4, 7, 3, 3, 9, 0, 1, 5, 9, 3, 7, 6, 0, 4, 2, 2, 5, 6, 3, 8, 9,\n",
            "        5, 6, 1, 4, 5, 4, 6, 7, 2, 1, 0, 2, 0, 4, 9, 8, 8, 9, 1, 1, 5, 0, 0, 8,\n",
            "        7, 1, 7, 4, 5, 4, 3, 3, 2, 0, 6, 6, 0, 1, 3, 9, 8, 3, 7, 8, 9, 4, 8, 9,\n",
            "        0, 9, 7, 1, 6, 2, 9, 5], device='cuda:0')\n",
            "Output: tensor([[ -8.7622,  -6.0526,  -3.2199,  ...,  -5.1887,  -5.9573,  -7.3149],\n",
            "        [  2.3621,   1.3082,  -3.7031,  ..., -11.0138,  -0.2989,   2.9995],\n",
            "        [ -5.0928,  -5.6777,  -2.9236,  ...,  -2.3211,  -7.9885,  -5.7294],\n",
            "        ...,\n",
            "        [ -2.1694,  -5.2575,  -1.6444,  ...,  -1.9114,  -5.9026,  -5.5709],\n",
            "        [ -1.1734,  -3.0128,  -3.3156,  ...,  -2.7316,  -3.1131,  -4.6285],\n",
            "        [  0.0963,   0.9337,  -3.7594,  ...,  -4.3075,   1.9242,   0.4428]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([6, 9, 3, 9, 8, 7, 7, 1, 6, 3, 3, 9, 5, 1, 2, 4, 1, 8, 2, 8, 9, 2, 9, 8,\n",
            "        8, 6, 7, 3, 7, 1, 3, 9, 9, 9, 3, 6, 7, 2, 2, 3, 0, 3, 1, 7, 3, 5, 3, 6,\n",
            "        3, 1, 8, 2, 7, 3, 9, 0, 0, 2, 8, 0, 5, 3, 7, 8, 9, 3, 3, 9, 7, 1, 7, 9,\n",
            "        8, 3, 4, 0, 3, 7, 4, 5, 8, 2, 1, 9, 6, 5, 3, 9, 5, 7, 8, 7, 3, 5, 3, 5,\n",
            "        9, 7, 6, 7, 4, 6, 4, 3, 9, 2, 0, 1, 9, 6, 0, 3, 3, 4, 4, 7, 9, 0, 7, 4,\n",
            "        5, 5, 3, 1, 1, 3, 6, 8], device='cuda:0')\n",
            "Target: tensor([6, 9, 3, 9, 8, 7, 7, 1, 6, 5, 3, 1, 3, 1, 2, 7, 1, 8, 2, 0, 9, 7, 9, 8,\n",
            "        8, 6, 7, 3, 7, 1, 3, 9, 0, 9, 3, 6, 7, 2, 7, 3, 0, 5, 9, 7, 5, 5, 0, 6,\n",
            "        5, 1, 8, 2, 7, 5, 9, 0, 0, 0, 8, 8, 7, 3, 7, 8, 9, 3, 7, 9, 7, 8, 7, 9,\n",
            "        8, 5, 4, 8, 3, 7, 6, 3, 8, 2, 1, 9, 5, 7, 3, 9, 5, 5, 8, 7, 3, 5, 3, 5,\n",
            "        9, 7, 6, 7, 3, 6, 4, 3, 9, 4, 2, 1, 9, 6, 0, 2, 6, 7, 4, 7, 9, 0, 7, 4,\n",
            "        3, 5, 3, 1, 1, 2, 6, 8], device='cuda:0')\n",
            "Output: tensor([[ -5.3334, -10.8606,   3.6593,  ...,  -0.0867,  -5.8800,  -8.7267],\n",
            "        [ -3.0010,   3.1353,  -6.4587,  ...,  -2.6226,  -2.1551,   2.7485],\n",
            "        [ -6.3628,  -7.3987,   0.2134,  ...,   0.2509,  -8.2403,  -5.6491],\n",
            "        ...,\n",
            "        [  8.0679,  -4.0407,  -0.5501,  ...,  -1.1080,  -2.8431,  -3.7713],\n",
            "        [ -4.2422,  -0.6162,  -4.5795,  ...,  -4.5433,  -3.9177,   7.2105],\n",
            "        [ -4.0756,  -6.1382,  -1.0289,  ...,  -0.6465,  -8.0117,  -6.1763]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([2, 1, 7, 8, 5, 0, 6, 1, 9, 5, 0, 6, 0, 9, 2, 8, 4, 8, 9, 3, 5, 6, 2, 9,\n",
            "        1, 4, 1, 8, 7, 1, 7, 4, 5, 4, 9, 7, 3, 2, 9, 9, 4, 2, 6, 1, 3, 8, 7, 9,\n",
            "        0, 4, 5, 7, 5, 2, 1, 7, 6, 9, 6, 7, 3, 8, 3, 6, 6, 9, 5, 7, 8, 0, 3, 1,\n",
            "        7, 4, 8, 2, 5, 1, 5, 4, 2, 6, 4, 1, 3, 2, 4, 5, 1, 3, 3, 5, 6, 3, 7, 0,\n",
            "        8, 4, 4, 5, 7, 9, 5, 4, 3, 9, 4, 8, 4, 3, 3, 1, 6, 9, 7, 0, 3, 4, 2, 9,\n",
            "        4, 7, 8, 4, 7, 0, 9, 4], device='cuda:0')\n",
            "Target: tensor([2, 1, 7, 8, 5, 9, 6, 1, 1, 5, 0, 6, 0, 9, 2, 6, 5, 8, 9, 5, 5, 6, 2, 9,\n",
            "        1, 5, 8, 8, 7, 1, 7, 3, 5, 4, 9, 7, 5, 2, 9, 9, 4, 7, 4, 1, 3, 8, 7, 9,\n",
            "        0, 4, 5, 7, 5, 2, 8, 7, 6, 9, 6, 9, 3, 8, 5, 6, 6, 9, 5, 7, 8, 0, 5, 0,\n",
            "        7, 4, 8, 2, 5, 1, 3, 2, 2, 6, 2, 1, 7, 4, 6, 3, 1, 3, 7, 2, 1, 3, 7, 0,\n",
            "        8, 4, 4, 5, 7, 9, 5, 4, 3, 9, 6, 8, 2, 3, 3, 1, 6, 1, 7, 0, 3, 4, 2, 9,\n",
            "        4, 5, 8, 2, 7, 0, 9, 6], device='cuda:0')\n",
            "Output: tensor([[ -1.4524,  -2.6385,  -4.9144,  ...,  -3.7885,   5.5831,  -0.9268],\n",
            "        [  5.3005,  -7.6952,   0.5586,  ...,  -3.7304,  -1.3913,  -4.6466],\n",
            "        [  1.5328,  -3.6172,  -2.5065,  ...,  -4.9724,   4.3919,  -5.0019],\n",
            "        ...,\n",
            "        [ -4.9965,  -6.2387,  -0.8251,  ...,   0.3512, -10.1905,  -8.1513],\n",
            "        [ -4.4425,   2.0031,  -6.8574,  ...,  -3.4998,  -2.3418,   5.0211],\n",
            "        [ -2.2615,  -3.1882,  -4.5343,  ...,   3.1850,  -3.8020,  -4.9775]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([8, 0, 8, 2, 8, 5, 7, 7, 2, 2, 2, 8, 0, 7, 4, 1, 6, 6, 8, 8, 9, 0, 9, 0,\n",
            "        1, 6, 6, 0, 9, 6, 6, 2, 6, 3, 4, 8, 8, 4, 1, 4, 0, 6, 5, 0, 9, 7, 1, 9,\n",
            "        1, 2, 5, 5, 4, 2, 9, 6, 8, 9, 6, 6, 8, 0, 6, 1, 3, 1, 8, 0, 5, 4, 1, 2,\n",
            "        0, 9, 6, 4, 2, 4, 6, 5, 9, 7, 7, 6, 6, 5, 0, 1, 9, 0, 3, 1, 9, 0, 9, 7,\n",
            "        8, 6, 7, 6, 8, 2, 4, 5, 3, 0, 3, 2, 1, 7, 5, 9, 3, 4, 5, 5, 1, 5, 4, 1,\n",
            "        1, 9, 9, 7, 5, 4, 9, 7], device='cuda:0')\n",
            "Target: tensor([8, 0, 8, 2, 8, 5, 7, 7, 2, 2, 0, 0, 0, 7, 4, 1, 6, 6, 8, 8, 9, 0, 9, 0,\n",
            "        1, 3, 3, 0, 9, 6, 6, 2, 6, 3, 4, 0, 8, 4, 1, 4, 0, 6, 5, 0, 9, 9, 9, 9,\n",
            "        1, 2, 3, 5, 4, 2, 9, 6, 0, 9, 6, 6, 8, 0, 6, 1, 4, 6, 8, 0, 5, 4, 1, 2,\n",
            "        0, 9, 6, 4, 2, 4, 6, 5, 9, 7, 7, 4, 6, 5, 0, 1, 9, 0, 3, 1, 9, 0, 9, 7,\n",
            "        8, 6, 7, 6, 8, 2, 4, 5, 3, 0, 3, 2, 1, 7, 5, 9, 3, 4, 5, 7, 1, 5, 0, 1,\n",
            "        1, 1, 9, 7, 5, 4, 9, 7], device='cuda:0')\n",
            "Output: tensor([[ 2.1828, -4.2143, -1.6659,  ..., -4.8927,  4.3001, -3.2855],\n",
            "        [-4.1054, -2.0054, -3.8677,  ..., -2.7143, -3.2590, -3.7848],\n",
            "        [ 4.0779, -2.1734, -4.3044,  ..., -3.7176, -1.5610, -3.6808],\n",
            "        ...,\n",
            "        [-0.8848,  0.3630, -3.2452,  ..., -3.7735, -0.3185, -1.5094],\n",
            "        [-3.4142,  1.9027, -5.5502,  ..., -4.7358, -3.0330,  6.4869],\n",
            "        [-2.0630,  0.8105, -2.7611,  ..., -6.1254, -2.3921, -2.1776]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([8, 3, 0, 4, 7, 5, 6, 7, 0, 9, 4, 8, 2, 4, 6, 6, 2, 8, 1, 8, 4, 6, 5, 9,\n",
            "        2, 2, 4, 4, 9, 1, 6, 7, 2, 0, 1, 4, 6, 5, 2, 2, 5, 6, 0, 0, 0, 1, 7, 3,\n",
            "        3, 5, 8, 7, 5, 6, 5, 8, 0, 5, 9, 4, 6, 5, 1, 1, 0, 5, 5, 9, 7, 8, 1, 7,\n",
            "        7, 9, 9, 4, 3, 6, 3, 2, 8, 3, 7, 6, 7, 0, 2, 1, 2, 9, 4, 2, 9, 6, 9, 0,\n",
            "        1, 8, 7, 0, 0, 4, 7, 4, 4, 6, 9, 7, 1, 0, 7, 4, 5, 8, 1, 6, 7, 9, 9, 8,\n",
            "        8, 6, 3, 7, 0, 1, 9, 1], device='cuda:0')\n",
            "Target: tensor([8, 1, 0, 2, 8, 5, 6, 7, 0, 1, 4, 8, 4, 4, 6, 6, 5, 8, 1, 8, 4, 6, 5, 9,\n",
            "        2, 2, 1, 4, 9, 1, 6, 7, 2, 0, 1, 7, 6, 5, 2, 2, 5, 6, 0, 9, 0, 1, 5, 3,\n",
            "        3, 5, 8, 7, 5, 6, 5, 8, 0, 5, 9, 4, 6, 5, 1, 1, 0, 3, 3, 9, 4, 8, 1, 7,\n",
            "        7, 9, 9, 4, 3, 6, 3, 2, 8, 2, 7, 6, 7, 0, 2, 1, 2, 9, 4, 6, 9, 6, 1, 0,\n",
            "        1, 8, 7, 0, 0, 4, 7, 4, 2, 6, 9, 5, 9, 0, 7, 4, 5, 8, 1, 4, 7, 9, 9, 8,\n",
            "        8, 6, 3, 7, 0, 8, 9, 6], device='cuda:0')\n",
            "Output: tensor([[-3.4048e+00, -3.0225e+00, -6.1426e-01,  ..., -6.1285e-01,\n",
            "         -2.2511e+00, -3.3544e+00],\n",
            "        [-6.7031e+00, -9.5389e+00,  1.0389e+00,  ..., -4.5077e+00,\n",
            "         -9.2976e+00, -9.0456e+00],\n",
            "        [-4.6612e+00, -8.0104e+00, -4.2983e-03,  ..., -4.9401e+00,\n",
            "         -8.1549e+00, -7.3554e+00],\n",
            "        ...,\n",
            "        [-5.6642e+00, -5.1208e+00, -3.5092e+00,  ..., -4.8120e+00,\n",
            "         -3.1246e+00, -5.8652e+00],\n",
            "        [-1.6876e+00, -2.1589e+00, -6.7858e+00,  ..., -3.2363e+00,\n",
            "         -5.3414e+00,  5.0629e+00],\n",
            "        [-5.1963e+00, -7.6167e+00, -2.9731e+00,  ...,  6.3933e+00,\n",
            "         -7.0171e+00, -5.4371e+00]], device='cuda:0')\n",
            "Prediction: tensor([7, 4, 6, 2, 9, 7, 4, 6, 8, 5, 6, 1, 2, 5, 2, 9, 1, 3, 7, 0, 3, 0, 2, 0,\n",
            "        7, 2, 9, 6, 5, 3, 6, 5, 3, 5, 6, 1, 2, 4, 0, 2, 7, 8, 4, 4, 3, 9, 5, 9,\n",
            "        4, 9, 3, 4, 4, 3, 2, 2, 8, 8, 6, 0, 1, 6, 8, 9, 2, 2, 1, 4, 9, 1, 5, 7,\n",
            "        1, 8, 0, 1, 5, 2, 8, 1, 1, 3, 4, 1, 3, 1, 8, 4, 1, 1, 0, 3, 0, 4, 0, 5,\n",
            "        1, 2, 0, 4, 9, 5, 1, 2, 2, 7, 8, 7, 2, 4, 5, 6, 7, 0, 6, 1, 5, 9, 8, 0,\n",
            "        2, 0, 4, 6, 0, 5, 9, 7], device='cuda:0')\n",
            "Target: tensor([2, 4, 6, 2, 9, 7, 4, 6, 8, 5, 6, 1, 3, 5, 9, 9, 1, 3, 2, 0, 3, 0, 2, 0,\n",
            "        7, 3, 9, 3, 5, 7, 6, 5, 9, 5, 6, 1, 2, 4, 0, 2, 7, 8, 4, 4, 0, 9, 5, 9,\n",
            "        2, 9, 3, 2, 4, 3, 2, 2, 8, 8, 6, 8, 1, 6, 8, 9, 8, 2, 1, 4, 9, 1, 5, 7,\n",
            "        1, 6, 0, 1, 5, 2, 8, 1, 1, 3, 8, 1, 3, 1, 8, 5, 1, 9, 0, 3, 0, 4, 0, 5,\n",
            "        1, 2, 0, 4, 9, 5, 1, 2, 2, 7, 8, 7, 2, 4, 5, 3, 4, 0, 6, 1, 5, 9, 8, 0,\n",
            "        2, 0, 7, 6, 0, 5, 9, 7], device='cuda:0')\n",
            "Output: tensor([[-5.4229, -3.3224, -3.5966,  ..., -1.6187, -4.0311, -2.8856],\n",
            "        [ 0.0626,  1.3719, -1.3211,  ..., -5.2336, -1.1859, -1.6935],\n",
            "        [-8.2654, -5.8767, -0.5230,  ..., -1.4771, -7.7013, -5.9829],\n",
            "        ...,\n",
            "        [-5.2391, -7.4942, -1.2846,  ...,  0.9656, -6.8700, -8.3497],\n",
            "        [-8.0522, -4.3011, -1.6057,  ..., -5.1417, -7.0013, -5.0981],\n",
            "        [ 1.8690, -0.2899, -2.6220,  ..., -4.4795, -3.0633, -5.9193]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([5, 1, 4, 6, 2, 4, 1, 6, 8, 1, 1, 0, 9, 7, 2, 1, 0, 2, 3, 9, 0, 6, 4, 7,\n",
            "        8, 1, 1, 7, 9, 8, 4, 7, 0, 9, 0, 3, 5, 2, 1, 4, 4, 8, 9, 9, 8, 3, 1, 4,\n",
            "        0, 1, 1, 1, 8, 1, 7, 4, 0, 7, 8, 2, 1, 2, 5, 4, 6, 0, 5, 7, 4, 4, 6, 9,\n",
            "        3, 8, 8, 0, 0, 7, 4, 1, 8, 4, 9, 5, 4, 1, 7, 7, 7, 7, 0, 3, 8, 3, 3, 0,\n",
            "        5, 4, 0, 8, 0, 0, 9, 3, 2, 3, 4, 8, 6, 2, 6, 3, 3, 6, 2, 9, 4, 0, 9, 7,\n",
            "        5, 5, 7, 3, 0, 4, 6, 0], device='cuda:0')\n",
            "Target: tensor([5, 0, 4, 6, 0, 6, 3, 6, 8, 1, 1, 8, 9, 7, 2, 9, 0, 2, 2, 9, 0, 6, 5, 7,\n",
            "        7, 9, 1, 7, 9, 8, 4, 5, 0, 8, 0, 2, 5, 2, 1, 4, 4, 8, 9, 7, 8, 3, 6, 6,\n",
            "        0, 1, 1, 1, 8, 1, 4, 4, 0, 7, 8, 2, 1, 2, 5, 4, 6, 0, 5, 7, 4, 4, 3, 9,\n",
            "        5, 8, 8, 0, 8, 7, 4, 1, 8, 4, 9, 5, 4, 1, 7, 7, 7, 7, 0, 3, 8, 3, 3, 0,\n",
            "        5, 7, 0, 8, 0, 0, 9, 2, 2, 3, 4, 8, 2, 2, 6, 3, 3, 6, 2, 9, 4, 0, 1, 7,\n",
            "        5, 5, 7, 3, 0, 4, 2, 0], device='cuda:0')\n",
            "Output: tensor([[ -6.1308,  -8.3390,  -5.4125,  -3.6443,   0.4701,   1.7840,  -7.7957,\n",
            "           6.4229,  -7.3485,  -7.2370],\n",
            "        [ -5.8288,  -6.2940,  -0.3760,   0.6285,  -4.9401,  -0.6888,  -4.4277,\n",
            "          -2.7051,  -7.9451,  -2.4886],\n",
            "        [  1.1711,  -3.6066,  -3.3851,  -2.3354,  -3.6643,  -3.5166,  -4.8057,\n",
            "          -3.9600,   2.8357,  -6.0737],\n",
            "        [  2.7476,  -2.8961,  -2.2055,  -4.0697,  -3.7999,  -3.4017,  -6.6052,\n",
            "          -5.4856,  -1.4654,  -3.2456],\n",
            "        [ -0.8689,  -2.4118,  -5.1460,  -4.0904,  -2.2473,  -4.0527,  -6.3741,\n",
            "          -2.6329,   3.3019,  -3.0366],\n",
            "        [ -4.2961,  -6.3883,   2.8321,  -3.4522,   4.8676,  -2.1746,  -6.1517,\n",
            "          -1.5884,  -7.7896,  -7.0716],\n",
            "        [ -4.3268,  -4.9827,  -2.0704,  -2.3281,  -2.0395,  -4.6508,  -7.9250,\n",
            "           2.5817,  -3.2126,  -5.7537],\n",
            "        [ -0.4397,  -0.6089,  -1.4063,  -1.9957,  -4.6412,  -5.7670,  -6.7565,\n",
            "          -3.8070,  -3.3503,  -2.3898],\n",
            "        [ -5.0877,  -2.4098,  -3.7246,   0.7382,  -3.5637,  -2.1470,  -3.2093,\n",
            "          -2.7573,  -6.1134,  -4.3987],\n",
            "        [ -3.4536,  -3.2745,  -4.9751,  -1.3940,  -3.5801,   0.5665,  -7.2735,\n",
            "          -1.2710,  -6.4388,  -2.3075],\n",
            "        [ -4.1587,  -3.1269,  -3.0461,   2.0300,  -2.3811,  -2.5477,  -5.4296,\n",
            "          -2.7036,  -5.9556,  -5.3530],\n",
            "        [ -0.8529,  -3.2375,  -5.3181,  -3.1732,  -6.0231,  -2.9100,  -3.8370,\n",
            "          -3.0123,   0.1698,  -6.4042],\n",
            "        [ -7.6304,  -8.8104,  -0.7068,   2.2136,  -0.6887,  -0.9317,   0.6911,\n",
            "          -2.6323,  -8.7267,  -7.2421],\n",
            "        [ -3.9392,  -5.5928,  -1.1130,  -0.1326,  -2.8514,   2.6647,  -3.4071,\n",
            "          -3.0449,  -4.9365,  -4.5663],\n",
            "        [ -1.4236,   0.1320,  -1.6663,  -5.5825,  -3.5371,  -6.9459,  -4.6304,\n",
            "          -2.7437,  -4.4546,  -4.9333],\n",
            "        [ -4.5803,  -4.4745,  -5.2437,  -3.8714,   0.0779,  -0.6595, -10.1137,\n",
            "           7.0337,  -9.1826,  -6.8857]], device='cuda:0')\n",
            "Prediction: tensor([7, 3, 8, 0, 8, 4, 7, 0, 3, 5, 3, 8, 3, 5, 1, 7], device='cuda:0')\n",
            "Target: tensor([7, 5, 8, 0, 8, 2, 7, 0, 3, 5, 3, 8, 3, 5, 1, 7], device='cuda:0')\n",
            "Test Loss: 0.5692, Test Accuracy: 0.8064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further Exploring the CIFAR10 Dataset with different dimension"
      ],
      "metadata": {
        "id": "Omf9arG1_Vll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "JLe7VCkg9wVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Parameters\n",
        "batch_size = 128\n",
        "num_classes = 10      #CIFAR10 has 10 class\n",
        "epochs = 5\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "I3XMlzsdBOta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Transformation Pipeline for CIFAR-10\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),    #Resnet expect 224 images\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])   #Normalization for pre-trained models\n",
        "])"
      ],
      "metadata": {
        "id": "P12dsdJGKoiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading CIFAR-10 Datasets\n",
        "train_dataset = datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "test_dataset = datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "#DataLoader for batch processing\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRCmqVJgLHAv",
        "outputId": "c538e6fd-bda3-439b-d7f4-4009195c6947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load a pre-trained resnet18 model\n",
        "model = models.resnet18(pretrained=True)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8As3osiMNkp",
        "outputId": "49a30f2f-8a18-4314-9e28-ad6e75c6a4b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Freeze All Layers\n",
        "for param in model.parameters():\n",
        "  print(param)\n",
        "  param.requires_grad=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6i4JogLM8S5",
        "outputId": "78da515e-9215-4a2e-c894-27be0840a91c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[[[-1.0419e-02, -6.1356e-03, -1.8098e-03,  ...,  5.6615e-02,\n",
            "            1.7083e-02, -1.2694e-02],\n",
            "          [ 1.1083e-02,  9.5276e-03, -1.0993e-01,  ..., -2.7124e-01,\n",
            "           -1.2907e-01,  3.7424e-03],\n",
            "          [-6.9434e-03,  5.9089e-02,  2.9548e-01,  ...,  5.1972e-01,\n",
            "            2.5632e-01,  6.3573e-02],\n",
            "          ...,\n",
            "          [-2.7535e-02,  1.6045e-02,  7.2595e-02,  ..., -3.3285e-01,\n",
            "           -4.2058e-01, -2.5781e-01],\n",
            "          [ 3.0613e-02,  4.0960e-02,  6.2850e-02,  ...,  4.1384e-01,\n",
            "            3.9359e-01,  1.6606e-01],\n",
            "          [-1.3736e-02, -3.6746e-03, -2.4084e-02,  ..., -1.5070e-01,\n",
            "           -8.2230e-02, -5.7828e-03]],\n",
            "\n",
            "         [[-1.1397e-02, -2.6619e-02, -3.4641e-02,  ...,  3.2521e-02,\n",
            "            6.6221e-04, -2.5743e-02],\n",
            "          [ 4.5687e-02,  3.3603e-02, -1.0453e-01,  ..., -3.1253e-01,\n",
            "           -1.6051e-01, -1.2826e-03],\n",
            "          [-8.3730e-04,  9.8420e-02,  4.0210e-01,  ...,  7.0789e-01,\n",
            "            3.6887e-01,  1.2455e-01],\n",
            "          ...,\n",
            "          [-5.5926e-02, -5.2239e-03,  2.7081e-02,  ..., -4.6178e-01,\n",
            "           -5.7080e-01, -3.6552e-01],\n",
            "          [ 3.2860e-02,  5.5574e-02,  9.9670e-02,  ...,  5.4636e-01,\n",
            "            4.8276e-01,  1.9867e-01],\n",
            "          [ 5.3051e-03,  6.6938e-03, -1.7254e-02,  ..., -1.4822e-01,\n",
            "           -7.7248e-02,  7.2183e-04]],\n",
            "\n",
            "         [[-2.0315e-03, -9.1617e-03,  2.1209e-02,  ...,  8.9177e-02,\n",
            "            3.3655e-02, -2.0102e-02],\n",
            "          [ 1.5398e-02, -1.8648e-02, -1.2591e-01,  ..., -2.5342e-01,\n",
            "           -1.2980e-01, -2.7975e-02],\n",
            "          [ 9.8454e-03,  4.9047e-02,  2.1699e-01,  ...,  3.4872e-01,\n",
            "            1.0433e-01,  1.8413e-02],\n",
            "          ...,\n",
            "          [-2.8356e-02,  1.8404e-02,  9.8647e-02,  ..., -1.1740e-01,\n",
            "           -2.5760e-01, -1.5451e-01],\n",
            "          [ 2.0766e-02, -2.6286e-03, -3.7825e-02,  ...,  2.4141e-01,\n",
            "            2.4345e-01,  1.1796e-01],\n",
            "          [ 7.4684e-04,  7.7677e-04, -1.0050e-02,  ..., -1.4865e-01,\n",
            "           -1.1754e-01, -3.8350e-02]]],\n",
            "\n",
            "\n",
            "        [[[-4.4154e-03, -4.0645e-03,  3.1589e-03,  ..., -3.7026e-02,\n",
            "           -2.5158e-02, -4.7945e-02],\n",
            "          [ 5.1310e-02,  5.3402e-02,  8.0436e-02,  ...,  1.4480e-01,\n",
            "            1.4287e-01,  1.2312e-01],\n",
            "          [-7.3337e-03,  2.1755e-03,  3.7580e-02,  ...,  6.1517e-02,\n",
            "            8.0324e-02,  1.1715e-01],\n",
            "          ...,\n",
            "          [-2.6754e-02, -1.2297e-01, -1.3653e-01,  ..., -1.4068e-01,\n",
            "           -1.1155e-01, -4.9556e-02],\n",
            "          [ 2.3524e-02, -1.7288e-02, -1.1122e-02,  ..., -1.8826e-02,\n",
            "           -2.3320e-02, -2.9474e-02],\n",
            "          [ 2.8689e-02,  2.1659e-02,  4.7888e-02,  ...,  2.5498e-02,\n",
            "            3.5346e-02,  1.1280e-02]],\n",
            "\n",
            "         [[ 4.6919e-04,  1.2153e-02,  4.2035e-02,  ...,  4.6403e-02,\n",
            "            4.0423e-02, -1.4439e-02],\n",
            "          [ 4.3463e-02,  6.8779e-02,  1.3268e-01,  ...,  2.8606e-01,\n",
            "            2.6905e-01,  2.0935e-01],\n",
            "          [-5.7621e-02, -2.2642e-02,  3.0547e-02,  ...,  1.3763e-01,\n",
            "            1.6538e-01,  1.7946e-01],\n",
            "          ...,\n",
            "          [-1.0816e-01, -2.5227e-01, -2.9742e-01,  ..., -2.8503e-01,\n",
            "           -2.1493e-01, -1.0320e-01],\n",
            "          [ 4.0709e-02, -3.2771e-02, -6.3450e-02,  ..., -9.2360e-02,\n",
            "           -6.9876e-02, -4.9841e-02],\n",
            "          [ 8.2942e-02,  8.7580e-02,  1.0111e-01,  ...,  5.2714e-02,\n",
            "            6.0968e-02,  4.1198e-02]],\n",
            "\n",
            "         [[-1.6391e-02, -1.3870e-02,  5.2810e-03,  ...,  4.3698e-02,\n",
            "            2.2707e-02, -4.5983e-02],\n",
            "          [ 3.3202e-02,  4.2014e-02,  9.3500e-02,  ...,  2.6162e-01,\n",
            "            2.2970e-01,  1.6694e-01],\n",
            "          [-4.5987e-02, -1.6365e-02,  2.6811e-02,  ...,  1.4951e-01,\n",
            "            1.3216e-01,  1.3579e-01],\n",
            "          ...,\n",
            "          [-7.2129e-02, -1.8902e-01, -2.3389e-01,  ..., -1.9038e-01,\n",
            "           -1.5609e-01, -7.5974e-02],\n",
            "          [ 5.1161e-02, -2.5815e-02, -6.9357e-02,  ..., -5.8999e-02,\n",
            "           -6.1550e-02, -4.4555e-02],\n",
            "          [ 1.1174e-01,  7.8979e-02,  6.5849e-02,  ...,  3.1617e-02,\n",
            "            2.5221e-02,  7.4257e-03]]],\n",
            "\n",
            "\n",
            "        [[[-7.0826e-08, -6.4306e-08, -7.3806e-08,  ..., -9.8000e-08,\n",
            "           -1.0905e-07, -8.3421e-08],\n",
            "          [-6.1125e-09,  2.0613e-09, -8.0922e-09,  ..., -4.9840e-08,\n",
            "           -4.3836e-08, -3.0538e-09],\n",
            "          [ 7.1953e-08,  7.5616e-08,  5.9282e-08,  ..., -9.7509e-09,\n",
            "           -1.0951e-09,  4.2442e-08],\n",
            "          ...,\n",
            "          [ 9.5889e-08,  1.0039e-07,  7.9817e-08,  ..., -1.7491e-08,\n",
            "           -4.7666e-08, -1.3265e-08],\n",
            "          [ 1.2904e-07,  1.4762e-07,  1.7477e-07,  ...,  1.3233e-07,\n",
            "            1.0628e-07,  9.3316e-08],\n",
            "          [ 1.2558e-07,  1.3644e-07,  1.8431e-07,  ...,  2.1399e-07,\n",
            "            1.7710e-07,  1.7166e-07]],\n",
            "\n",
            "         [[-1.2690e-07, -9.6139e-08, -1.0372e-07,  ..., -1.1808e-07,\n",
            "           -1.3309e-07, -1.0820e-07],\n",
            "          [-5.7412e-08, -2.5055e-08, -3.0115e-08,  ..., -7.2922e-08,\n",
            "           -6.7022e-08, -2.2574e-08],\n",
            "          [ 2.1813e-08,  4.8608e-08,  3.1222e-08,  ..., -1.8694e-08,\n",
            "           -7.9591e-09,  3.9750e-08],\n",
            "          ...,\n",
            "          [ 5.6013e-08,  7.5526e-08,  4.4496e-08,  ..., -4.4128e-08,\n",
            "           -5.9930e-08, -1.8247e-08],\n",
            "          [ 7.7614e-08,  9.8348e-08,  1.0455e-07,  ...,  6.3272e-08,\n",
            "            4.1781e-08,  4.5901e-08],\n",
            "          [ 5.9834e-08,  7.1006e-08,  9.0437e-08,  ...,  1.1654e-07,\n",
            "            8.7550e-08,  9.8837e-08]],\n",
            "\n",
            "         [[-4.3810e-08,  1.3270e-08,  7.8275e-09,  ..., -5.8804e-09,\n",
            "           -2.6217e-08, -1.5649e-08],\n",
            "          [ 4.1700e-08,  1.0778e-07,  1.0946e-07,  ...,  7.6403e-08,\n",
            "            7.1450e-08,  9.7615e-08],\n",
            "          [ 1.0436e-07,  1.6586e-07,  1.5933e-07,  ...,  1.3517e-07,\n",
            "            1.3487e-07,  1.6449e-07],\n",
            "          ...,\n",
            "          [ 9.8763e-08,  1.5072e-07,  1.2547e-07,  ...,  6.8316e-08,\n",
            "            6.8382e-08,  1.1367e-07],\n",
            "          [ 9.1435e-08,  1.3576e-07,  1.3793e-07,  ...,  1.1678e-07,\n",
            "            1.1723e-07,  1.4394e-07],\n",
            "          [ 6.2183e-08,  8.8184e-08,  1.0456e-07,  ...,  1.3941e-07,\n",
            "            1.3333e-07,  1.5844e-07]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-6.1896e-02, -3.0206e-02,  1.9225e-02,  ...,  4.3665e-02,\n",
            "           -2.2114e-02, -4.2214e-02],\n",
            "          [-3.8061e-02,  6.0774e-03,  4.5797e-02,  ...,  9.6029e-02,\n",
            "            5.9254e-02,  2.9958e-02],\n",
            "          [-2.9672e-02,  2.7766e-03,  2.0457e-02,  ...,  5.9828e-02,\n",
            "            4.1422e-02,  2.3134e-02],\n",
            "          ...,\n",
            "          [ 1.1916e-02,  4.5701e-02,  4.4892e-02,  ...,  4.7419e-02,\n",
            "            2.2274e-02, -5.4993e-03],\n",
            "          [-3.2468e-02, -1.2210e-02,  2.2023e-02,  ...,  5.8061e-02,\n",
            "           -7.5033e-03, -5.9736e-02],\n",
            "          [-4.3314e-02, -2.8162e-02, -5.9126e-03,  ...,  8.8460e-02,\n",
            "            8.4406e-03, -5.0019e-02]],\n",
            "\n",
            "         [[-6.1292e-02, -1.4004e-02,  1.7229e-02,  ...,  1.8349e-02,\n",
            "           -3.2708e-02, -4.1060e-02],\n",
            "          [-3.1506e-02,  2.4460e-02,  4.5516e-02,  ...,  6.6806e-02,\n",
            "            4.6687e-02,  3.3248e-02],\n",
            "          [-3.2216e-02,  2.0718e-02,  2.3343e-02,  ...,  3.5265e-02,\n",
            "            3.6478e-02,  3.1291e-02],\n",
            "          ...,\n",
            "          [ 1.7739e-02,  6.1040e-02,  4.8247e-02,  ...,  3.7785e-02,\n",
            "            2.8894e-02,  1.3984e-02],\n",
            "          [-1.0890e-02,  2.2079e-02,  4.2737e-02,  ...,  6.0247e-02,\n",
            "            1.6197e-02, -1.2493e-02],\n",
            "          [-2.2284e-02,  1.3220e-02,  3.0897e-02,  ...,  1.0403e-01,\n",
            "            4.0119e-02, -5.3310e-03]],\n",
            "\n",
            "         [[-8.5322e-02, -4.2603e-02,  6.8145e-03,  ...,  3.0751e-02,\n",
            "           -3.4818e-02, -4.9945e-02],\n",
            "          [-2.9215e-02,  1.8165e-02,  5.1092e-02,  ...,  9.0200e-02,\n",
            "            5.3438e-02,  4.0169e-02],\n",
            "          [-3.9932e-02, -1.1100e-03,  9.6176e-03,  ...,  2.4114e-02,\n",
            "            2.6298e-02,  2.5489e-02],\n",
            "          ...,\n",
            "          [-3.1890e-03,  3.0454e-02,  1.6316e-02,  ...,  5.5054e-03,\n",
            "           -6.2689e-03, -8.4638e-03],\n",
            "          [-2.2995e-02, -2.8211e-03,  2.3203e-02,  ...,  3.5888e-02,\n",
            "           -1.4296e-02, -3.2419e-02],\n",
            "          [-9.8894e-03,  7.0542e-03,  1.0659e-02,  ...,  7.0495e-02,\n",
            "            1.2996e-02, -8.3417e-03]]],\n",
            "\n",
            "\n",
            "        [[[-7.8699e-03,  1.9911e-02,  3.4208e-02,  ...,  2.8694e-02,\n",
            "            1.2820e-02,  1.8142e-02],\n",
            "          [ 8.7942e-03, -3.2875e-02, -3.5713e-02,  ...,  7.2533e-02,\n",
            "            4.5889e-02,  5.2383e-02],\n",
            "          [-3.6122e-02, -1.1878e-01, -1.3767e-01,  ...,  3.3811e-02,\n",
            "            3.7806e-02,  2.6944e-02],\n",
            "          ...,\n",
            "          [ 1.7322e-02,  3.9589e-03, -8.2269e-03,  ...,  2.7543e-03,\n",
            "            1.8313e-02,  1.6057e-02],\n",
            "          [-9.5007e-04,  1.6428e-02,  1.7156e-02,  ...,  3.3672e-03,\n",
            "            2.2857e-02,  6.5783e-04],\n",
            "          [ 6.1727e-03,  2.7145e-02,  1.4340e-02,  ...,  7.5867e-03,\n",
            "            1.8770e-02,  1.5624e-02]],\n",
            "\n",
            "         [[-1.3423e-02, -5.0696e-04,  8.0959e-03,  ..., -6.0963e-03,\n",
            "            9.2341e-03,  1.5751e-02],\n",
            "          [-1.8343e-02, -6.7982e-02, -7.0685e-02,  ...,  2.9855e-02,\n",
            "            2.6264e-02,  2.3773e-02],\n",
            "          [-5.4359e-02, -1.4663e-01, -1.6211e-01,  ...,  1.1781e-02,\n",
            "            3.2477e-02,  1.1980e-02],\n",
            "          ...,\n",
            "          [ 8.3686e-04, -1.7564e-02, -1.9535e-02,  ..., -4.1382e-03,\n",
            "            2.4658e-02,  1.2893e-02],\n",
            "          [-6.3183e-04,  1.1788e-02,  2.4810e-02,  ...,  6.1105e-03,\n",
            "            3.9210e-02,  9.6696e-03],\n",
            "          [-7.1831e-03,  6.6918e-03,  5.2723e-03,  ..., -7.6077e-03,\n",
            "            2.7253e-02,  1.7735e-02]],\n",
            "\n",
            "         [[-2.3753e-04, -4.9343e-03,  2.2991e-03,  ..., -4.7958e-02,\n",
            "           -2.6154e-02, -2.3525e-02],\n",
            "          [-3.3053e-04, -5.1502e-02, -5.9977e-02,  ..., -1.7369e-02,\n",
            "           -2.3337e-02, -3.7312e-02],\n",
            "          [-2.2674e-02, -9.9412e-02, -1.1176e-01,  ..., -1.1725e-02,\n",
            "           -8.3744e-03, -4.0615e-02],\n",
            "          ...,\n",
            "          [ 1.1437e-02, -8.0313e-03, -1.4955e-03,  ..., -3.4133e-02,\n",
            "           -8.7267e-03, -2.3526e-02],\n",
            "          [ 2.9522e-03,  6.7770e-04,  1.9933e-02,  ..., -2.2002e-02,\n",
            "            1.4814e-02, -1.4487e-02],\n",
            "          [-1.9085e-02, -2.9430e-02, -2.3284e-02,  ..., -4.8587e-02,\n",
            "           -1.3049e-02, -2.4368e-02]]],\n",
            "\n",
            "\n",
            "        [[[-3.6296e-02,  7.1996e-03,  1.9100e-02,  ...,  1.9602e-02,\n",
            "            1.4870e-02, -1.7298e-02],\n",
            "          [-1.1061e-02,  8.5665e-02,  1.2667e-01,  ...,  1.3744e-02,\n",
            "           -5.5036e-05, -3.0162e-02],\n",
            "          [ 1.1322e-01,  1.8634e-01,  5.0658e-02,  ..., -1.7333e-01,\n",
            "           -7.2041e-02, -6.2474e-02],\n",
            "          ...,\n",
            "          [-5.3062e-02, -2.5781e-01, -2.6747e-01,  ...,  2.6781e-01,\n",
            "            1.4344e-01,  5.5145e-02],\n",
            "          [-2.1009e-02, -2.9969e-02,  1.0245e-01,  ...,  2.0843e-01,\n",
            "           -4.1518e-03, -3.8118e-02],\n",
            "          [-2.2155e-02,  1.2380e-02,  8.4302e-02,  ..., -4.4992e-02,\n",
            "           -1.4687e-01, -9.0890e-02]],\n",
            "\n",
            "         [[-5.3969e-03,  3.2799e-02,  1.5486e-02,  ..., -7.7451e-03,\n",
            "            3.0229e-03,  1.1216e-03],\n",
            "          [ 6.1723e-02,  1.4899e-01,  1.4645e-01,  ..., -2.8897e-02,\n",
            "           -2.0227e-02, -9.1878e-03],\n",
            "          [ 1.6146e-01,  2.0886e-01, -2.5589e-02,  ..., -2.7278e-01,\n",
            "           -1.0735e-01, -6.2971e-02],\n",
            "          ...,\n",
            "          [-1.3723e-01, -4.0863e-01, -3.8551e-01,  ...,  4.0846e-01,\n",
            "            2.6202e-01,  1.3491e-01],\n",
            "          [-5.9388e-02, -6.1187e-02,  1.4197e-01,  ...,  3.5780e-01,\n",
            "            9.0893e-02, -1.7392e-03],\n",
            "          [ 7.8613e-03,  5.8403e-02,  1.5339e-01,  ...,  4.7045e-02,\n",
            "           -1.0095e-01, -9.7920e-02]],\n",
            "\n",
            "         [[-5.6799e-03,  1.3425e-02, -2.6461e-02,  ...,  4.4881e-03,\n",
            "            2.0666e-03,  1.3902e-02],\n",
            "          [ 6.5943e-03,  4.5181e-02,  6.0260e-02,  ...,  1.4368e-02,\n",
            "           -5.0725e-03,  4.0505e-03],\n",
            "          [ 5.5257e-02,  1.2397e-01,  4.3193e-02,  ..., -1.4486e-01,\n",
            "           -7.4489e-02, -5.7533e-02],\n",
            "          ...,\n",
            "          [-3.1513e-02, -1.6334e-01, -1.5795e-01,  ...,  2.2904e-01,\n",
            "            1.2017e-01,  7.1998e-02],\n",
            "          [-1.0456e-02, -1.1248e-03,  8.4582e-02,  ...,  1.5748e-01,\n",
            "            2.2142e-02, -1.0083e-02],\n",
            "          [-4.8639e-03, -5.0065e-03,  3.6341e-02,  ..., -2.4361e-02,\n",
            "           -7.1195e-02, -6.6788e-02]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 2.3487e-01,  2.6626e-01, -5.1096e-08,  5.1870e-01,  3.4404e-09,\n",
            "         2.2239e-01,  4.2289e-01,  1.3153e-07,  2.5093e-01,  1.5152e-06,\n",
            "         3.1687e-01,  2.5049e-01,  3.7893e-01,  1.0862e-05,  2.7526e-01,\n",
            "         2.3674e-01,  2.4202e-01,  3.9531e-01,  4.6935e-01,  2.9090e-01,\n",
            "         2.7268e-01,  2.7803e-01,  2.9069e-01,  2.0693e-01,  2.5899e-01,\n",
            "         2.7871e-01,  2.9115e-01,  3.1601e-01,  3.8889e-01,  3.0411e-01,\n",
            "         2.6776e-01,  2.1093e-01,  2.8708e-01,  3.3243e-01,  4.2673e-01,\n",
            "         3.7326e-01,  7.4804e-08,  1.9068e-01,  1.4740e-08,  2.2303e-01,\n",
            "         1.7908e-01,  2.4860e-01,  2.7400e-01,  2.5923e-01,  2.9420e-01,\n",
            "         2.9924e-01,  2.2369e-01,  2.6280e-01,  2.2001e-08,  2.6610e-01,\n",
            "         2.2089e-01,  2.8429e-01,  3.3072e-01,  2.2681e-01,  3.6538e-01,\n",
            "         2.1230e-01,  2.3965e-01,  2.4950e-01,  5.2583e-01,  2.4825e-01,\n",
            "         2.9565e-01,  2.5878e-01,  4.8326e-01,  2.6670e-01],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 2.3072e-01,  2.5382e-01, -1.0543e-06, -6.6439e-01, -1.6571e-08,\n",
            "         1.6152e-01,  4.5450e-01, -4.3020e-07,  3.0051e-01, -8.0052e-06,\n",
            "         3.4942e-01,  3.1148e-01, -2.4953e-01, -3.4749e-05,  1.0773e-01,\n",
            "         2.1897e-01,  3.8141e-01, -5.2988e-01, -6.2864e-01,  5.7140e-01,\n",
            "         2.9985e-01,  5.8430e-01,  4.8202e-01,  3.2853e-01,  1.9672e-01,\n",
            "         1.9496e-01,  1.5215e-01,  8.5522e-02,  5.1314e-01,  1.5237e-02,\n",
            "         1.6644e-01,  3.3239e-01,  2.4921e-01,  4.4337e-01, -2.8017e-01,\n",
            "        -2.0385e-02, -2.4507e-07,  3.2134e-01, -4.9152e-08,  2.3777e-01,\n",
            "         2.3291e-01,  3.1527e-01,  4.2776e-01,  2.9313e-01,  2.6379e-01,\n",
            "         6.7598e-01,  4.2910e-01,  3.4566e-01, -8.6909e-08,  2.4729e-01,\n",
            "         3.0316e-01,  6.1577e-01,  3.9835e-01,  3.3207e-01, -4.1219e-01,\n",
            "         3.7807e-01,  1.7895e-01,  2.5748e-01, -4.4908e-01,  2.1306e-01,\n",
            "         5.6934e-01,  5.7274e-01, -4.0238e-01,  2.3406e-01],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 5.7593e-02, -9.5114e-02, -2.0272e-02],\n",
            "          [-7.4556e-02, -7.9931e-01, -2.1284e-01],\n",
            "          [ 6.5571e-02, -9.6534e-02, -1.2111e-02]],\n",
            "\n",
            "         [[-6.9944e-03,  1.4266e-02,  5.5824e-04],\n",
            "          [ 4.1238e-02, -1.6125e-01, -2.3208e-02],\n",
            "          [ 3.2887e-03,  7.1779e-03,  7.1686e-02]],\n",
            "\n",
            "         [[-2.3627e-09, -3.9270e-08, -3.2971e-08],\n",
            "          [ 2.1737e-08,  8.3299e-09,  1.2543e-08],\n",
            "          [ 1.1382e-08,  8.8096e-09,  1.5506e-08]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.6921e-02,  1.8294e-02, -2.9358e-02],\n",
            "          [-9.8615e-02, -4.3645e-02, -5.2717e-02],\n",
            "          [-7.9635e-02,  2.9396e-02,  4.1479e-03]],\n",
            "\n",
            "         [[ 1.6948e-02,  1.3978e-02,  9.6727e-03],\n",
            "          [ 1.4297e-02, -6.6985e-04, -2.2077e-02],\n",
            "          [ 1.2398e-02,  3.5454e-02, -2.2320e-02]],\n",
            "\n",
            "         [[-2.2600e-02, -2.5331e-02, -2.3548e-02],\n",
            "          [ 6.0860e-02, -9.6779e-02,  2.4057e-02],\n",
            "          [-1.2750e-02,  9.2237e-02,  4.0152e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 2.2160e-02,  4.2177e-02, -1.6428e-02],\n",
            "          [-2.9667e-02,  5.6865e-02,  2.5486e-02],\n",
            "          [ 4.3847e-03,  5.1188e-02,  1.0436e-02]],\n",
            "\n",
            "         [[ 2.5342e-02,  5.4374e-02,  5.3888e-02],\n",
            "          [-2.8334e-02, -2.0139e-01, -5.6358e-02],\n",
            "          [ 5.6774e-02,  7.4188e-02,  2.1585e-02]],\n",
            "\n",
            "         [[-3.1458e-08,  3.5335e-08,  5.3791e-08],\n",
            "          [-2.6896e-08,  5.1530e-08,  5.4480e-08],\n",
            "          [-3.8487e-08, -1.1234e-08, -7.5787e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.2754e-01,  4.3552e-02, -6.5607e-02],\n",
            "          [-6.0462e-02,  1.5989e-01, -7.7070e-03],\n",
            "          [-9.4202e-02,  5.0750e-02, -7.8154e-02]],\n",
            "\n",
            "         [[-3.3309e-02,  1.6631e-03, -8.8497e-03],\n",
            "          [ 1.5553e-02, -5.8277e-02, -2.7437e-02],\n",
            "          [ 1.3126e-02, -3.0268e-02, -2.1661e-03]],\n",
            "\n",
            "         [[-4.2313e-03,  3.4517e-02,  3.8193e-03],\n",
            "          [ 5.4317e-02, -1.2457e-02,  3.2900e-02],\n",
            "          [ 2.2000e-04,  1.6040e-02,  1.2764e-01]]],\n",
            "\n",
            "\n",
            "        [[[-3.5247e-02,  8.0748e-03,  2.0353e-02],\n",
            "          [ 1.7344e-02, -2.4320e-02, -1.5511e-04],\n",
            "          [-2.7634e-04,  2.8024e-02, -2.3777e-03]],\n",
            "\n",
            "         [[-2.3741e-02, -3.2057e-03, -5.7059e-03],\n",
            "          [-1.1582e-02,  1.7200e-03,  2.1067e-02],\n",
            "          [ 4.3606e-03, -4.6459e-02, -7.2954e-02]],\n",
            "\n",
            "         [[ 3.1002e-08,  5.3568e-08,  3.1873e-08],\n",
            "          [-1.6063e-08, -1.8072e-08, -1.9508e-09],\n",
            "          [-5.8339e-08, -4.5366e-08, -1.2395e-08]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.9689e-03, -2.6809e-02, -4.3760e-02],\n",
            "          [ 2.4518e-02, -2.8396e-02, -3.5896e-02],\n",
            "          [-1.7883e-04, -2.4661e-02, -2.0085e-02]],\n",
            "\n",
            "         [[ 2.1551e-02,  2.2789e-03, -2.5823e-02],\n",
            "          [ 2.3272e-02, -7.9333e-03, -2.0814e-03],\n",
            "          [-5.7062e-03, -2.6934e-02, -1.4421e-02]],\n",
            "\n",
            "         [[-1.9674e-02,  2.7914e-02, -2.0025e-02],\n",
            "          [ 6.3222e-02, -3.9077e-02, -3.3220e-03],\n",
            "          [-2.7434e-02,  1.1390e-02, -3.1608e-03]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 4.3440e-03, -7.6970e-03, -6.4950e-02],\n",
            "          [ 1.3846e-02, -2.2803e-02, -4.6478e-02],\n",
            "          [ 2.7776e-02,  1.6080e-02, -1.3363e-02]],\n",
            "\n",
            "         [[ 4.7379e-02, -2.4982e-02, -2.7605e-02],\n",
            "          [ 7.0091e-02,  4.2084e-03, -1.0805e-01],\n",
            "          [ 1.7526e-02,  4.5647e-02,  7.8810e-03]],\n",
            "\n",
            "         [[ 2.6680e-09,  2.7671e-08,  2.4702e-08],\n",
            "          [ 6.3905e-09,  4.1020e-08,  3.3631e-08],\n",
            "          [ 5.8335e-09,  1.3334e-08,  9.6604e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.5900e-03,  4.7084e-02, -8.6949e-03],\n",
            "          [-6.3011e-03,  5.9585e-02,  5.8667e-03],\n",
            "          [-2.0255e-02,  4.3285e-02,  4.5094e-03]],\n",
            "\n",
            "         [[ 1.1253e-03, -5.7461e-03, -6.8411e-03],\n",
            "          [ 6.0616e-03,  7.3295e-03, -1.1784e-02],\n",
            "          [-1.1455e-03,  5.1868e-03, -1.9867e-02]],\n",
            "\n",
            "         [[ 1.7529e-02,  4.4606e-02, -2.6595e-02],\n",
            "          [ 2.2102e-02,  4.5857e-02,  2.3347e-02],\n",
            "          [ 1.8052e-02,  5.9689e-02,  1.7129e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.9112e-02,  3.4242e-03, -1.7523e-02],\n",
            "          [-2.3682e-02,  2.2716e-02, -3.8301e-02],\n",
            "          [-1.0308e-02, -4.3802e-03, -2.3582e-02]],\n",
            "\n",
            "         [[-4.9607e-02, -3.2724e-03, -1.5345e-02],\n",
            "          [-1.3524e-02,  5.4842e-02,  1.1187e-02],\n",
            "          [-2.3549e-02, -2.8495e-02, -6.6371e-02]],\n",
            "\n",
            "         [[-4.9804e-08, -2.8211e-08, -2.0583e-08],\n",
            "          [-5.2389e-08, -2.8522e-08, -3.5099e-08],\n",
            "          [-3.2171e-08, -3.4110e-08, -4.3153e-08]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.4487e-03,  2.6532e-02, -1.1202e-02],\n",
            "          [ 7.0925e-03,  3.7903e-02, -3.2481e-02],\n",
            "          [ 4.1381e-02,  3.2329e-02,  2.8309e-03]],\n",
            "\n",
            "         [[-6.5955e-03,  1.6476e-02,  2.1810e-02],\n",
            "          [-1.2293e-02,  2.2310e-02,  1.2645e-02],\n",
            "          [-8.9897e-03,  1.1948e-03, -5.2390e-03]],\n",
            "\n",
            "         [[-2.5295e-03,  7.2689e-02, -7.8046e-03],\n",
            "          [-4.2221e-02,  7.9756e-02, -2.7738e-02],\n",
            "          [ 4.6716e-03, -5.6596e-02, -8.2261e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 5.2235e-02,  3.5231e-03, -3.3131e-02],\n",
            "          [ 3.1048e-02,  1.6193e-02,  1.7283e-02],\n",
            "          [ 1.4446e-02,  2.4302e-02, -1.9689e-03]],\n",
            "\n",
            "         [[-2.4717e-02,  8.3009e-03, -6.1336e-02],\n",
            "          [-1.6134e-02,  5.5323e-02, -6.5029e-02],\n",
            "          [-2.4715e-02,  1.0030e-03,  3.2437e-02]],\n",
            "\n",
            "         [[ 1.8496e-08,  5.2798e-09,  4.1820e-08],\n",
            "          [ 3.7489e-08,  2.5450e-08,  3.0419e-08],\n",
            "          [ 1.1246e-08, -5.6956e-09, -2.0008e-08]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.1194e-03, -4.1052e-02, -1.0002e-02],\n",
            "          [ 2.5924e-02, -6.3819e-02,  1.3366e-02],\n",
            "          [ 2.9751e-02, -7.9476e-03,  1.4007e-02]],\n",
            "\n",
            "         [[-2.5166e-03,  2.2051e-02, -1.9967e-02],\n",
            "          [-5.9436e-02,  4.3872e-02,  2.6832e-02],\n",
            "          [-1.7509e-02,  2.4625e-02,  2.4822e-02]],\n",
            "\n",
            "         [[ 3.5832e-02, -7.0357e-02,  3.9452e-03],\n",
            "          [-2.9835e-02,  9.2727e-02,  1.9336e-02],\n",
            "          [-2.9145e-02, -9.7087e-03, -7.3388e-02]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.3090, 0.2147, 0.2366, 0.4259, 0.5137, 0.2181, 0.2204, 0.2300, 0.2640,\n",
            "        0.2695, 0.2138, 0.4602, 0.2661, 0.2319, 0.3900, 0.2389, 0.2660, 0.3634,\n",
            "        0.3474, 0.2477, 0.3285, 0.5349, 0.6440, 0.2275, 0.4482, 0.3078, 0.2604,\n",
            "        0.4651, 0.2179, 0.2858, 0.3426, 0.4420, 0.4450, 0.4500, 0.5516, 0.5092,\n",
            "        0.2564, 0.2634, 0.5664, 0.6410, 0.2228, 0.1986, 0.2460, 0.2242, 0.2143,\n",
            "        0.1982, 0.6368, 0.3106, 0.5049, 0.2403, 0.3065, 0.3760, 0.3794, 0.4281,\n",
            "        0.2991, 0.3326, 0.2596, 0.3345, 0.2006, 0.4351, 0.1683, 0.5149, 0.2629,\n",
            "        0.3254], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.1657,  0.2420,  0.1780, -0.0431, -0.2053,  0.1598,  0.2929,  0.0912,\n",
            "         0.1116,  0.0884,  0.1104, -0.2035,  0.1539,  0.0857, -0.1094,  0.0654,\n",
            "         0.0766, -0.2067, -0.0212,  0.1396,  0.0401, -0.2827, -0.3257, -0.0035,\n",
            "        -0.4373, -0.1248,  0.1282, -0.0874,  0.1199, -0.0829, -0.5315, -0.0780,\n",
            "        -0.3876, -0.0547, -0.1816, -0.1888,  0.1320,  0.0031, -0.2697, -0.2984,\n",
            "         0.1394,  0.2597,  0.1372,  0.0053,  0.0132,  0.3295, -0.2715, -0.0187,\n",
            "        -0.2467,  0.1579,  0.0165, -0.0890, -0.1903, -0.0787,  0.1700, -0.4832,\n",
            "         0.0619, -0.0677,  0.3125, -0.5064,  0.3138, -0.2617, -0.1545,  0.0063],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 2.5947e-02, -1.0458e-01, -4.7712e-03],\n",
            "          [-8.6223e-02, -3.3021e-01, -1.0275e-01],\n",
            "          [-5.7426e-02, -1.9074e-01, -5.4646e-02]],\n",
            "\n",
            "         [[-1.6951e-02,  2.1384e-02, -2.1074e-03],\n",
            "          [-3.2983e-03,  4.5014e-02, -1.1510e-02],\n",
            "          [-5.9602e-02,  6.4942e-03,  2.9080e-03]],\n",
            "\n",
            "         [[-4.4903e-03,  1.9637e-02,  1.3167e-02],\n",
            "          [ 1.3050e-02, -7.7471e-03,  1.1931e-02],\n",
            "          [ 1.3454e-02,  1.1103e-02,  5.5145e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.2706e-03, -7.7438e-03,  2.0753e-02],\n",
            "          [-4.0024e-02, -4.0383e-02, -3.4821e-02],\n",
            "          [-2.0251e-02, -9.5164e-03,  1.3954e-02]],\n",
            "\n",
            "         [[-2.3430e-03,  3.2303e-02, -4.3342e-03],\n",
            "          [ 8.6194e-03,  1.0553e-02,  1.8074e-03],\n",
            "          [-1.2760e-02, -1.0232e-02,  4.5711e-03]],\n",
            "\n",
            "         [[ 1.5302e-02,  2.1361e-02, -7.0908e-03],\n",
            "          [-1.4221e-02,  4.5979e-02,  2.1369e-02],\n",
            "          [ 3.1312e-02,  6.6428e-02,  2.1465e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 5.3422e-02,  4.0515e-02,  9.6680e-03],\n",
            "          [ 3.2884e-02, -2.3474e-02,  3.4642e-02],\n",
            "          [-1.2861e-02,  5.0066e-02,  5.4579e-02]],\n",
            "\n",
            "         [[ 2.8764e-02,  4.3431e-02,  2.8258e-02],\n",
            "          [ 2.8734e-02, -3.5459e-02, -5.2788e-02],\n",
            "          [-5.5119e-02, -7.1813e-02, -8.2970e-02]],\n",
            "\n",
            "         [[ 9.5293e-02,  1.2549e-01, -6.4001e-02],\n",
            "          [-4.1166e-02, -9.0480e-04,  5.1387e-02],\n",
            "          [-1.1311e-01, -7.9823e-02,  1.4373e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-7.6924e-03,  2.0647e-02,  1.9521e-02],\n",
            "          [-6.7352e-03,  1.2601e-04,  4.8309e-03],\n",
            "          [-6.2405e-03, -9.2119e-03, -2.5806e-04]],\n",
            "\n",
            "         [[-2.6153e-02, -2.4641e-02,  4.0970e-02],\n",
            "          [-1.9164e-02, -1.0160e-02,  3.3163e-02],\n",
            "          [ 5.4200e-03,  9.0485e-04,  6.7799e-04]],\n",
            "\n",
            "         [[ 7.7762e-03,  2.6447e-02,  6.3650e-02],\n",
            "          [-3.0608e-02,  2.4959e-02,  1.2951e-02],\n",
            "          [-2.0938e-02, -7.7342e-03, -3.8790e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.0893e-02, -1.4409e-02,  1.5730e-02],\n",
            "          [ 1.6655e-02,  4.4535e-02,  6.3212e-02],\n",
            "          [ 3.4121e-02,  7.3135e-02,  5.9203e-02]],\n",
            "\n",
            "         [[ 2.3195e-03,  7.7598e-03,  2.0308e-02],\n",
            "          [ 2.0457e-02,  4.0029e-02,  3.4744e-02],\n",
            "          [-4.7356e-02, -3.7286e-02,  1.4542e-02]],\n",
            "\n",
            "         [[-2.2742e-02, -1.9000e-02, -8.4317e-03],\n",
            "          [-9.8759e-04,  2.1510e-02,  6.3959e-03],\n",
            "          [-9.4558e-03,  2.6833e-03, -3.1136e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-7.5787e-03, -1.6056e-02, -6.4204e-04],\n",
            "          [-5.5104e-03,  1.4252e-02,  4.5000e-02],\n",
            "          [-9.2800e-03,  2.2351e-02,  4.1728e-02]],\n",
            "\n",
            "         [[ 2.5705e-02,  4.8207e-02,  7.9145e-02],\n",
            "          [-4.4350e-03,  3.8872e-03,  4.1694e-02],\n",
            "          [ 8.0536e-04, -1.0601e-02,  9.2706e-03]],\n",
            "\n",
            "         [[-3.3892e-02,  9.3543e-03,  4.1746e-02],\n",
            "          [-1.6470e-02,  3.9542e-03,  6.2438e-02],\n",
            "          [-3.1055e-02, -3.6302e-03,  7.0817e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-7.1044e-05, -9.0020e-03, -2.6998e-03],\n",
            "          [ 3.0072e-03,  1.1579e-02,  1.5214e-02],\n",
            "          [ 3.4832e-03,  1.1353e-05,  1.6320e-02]],\n",
            "\n",
            "         [[-2.6334e-02,  2.1967e-02, -6.0039e-02],\n",
            "          [ 4.4519e-02,  1.3203e-01, -9.1163e-03],\n",
            "          [ 5.4242e-02,  1.3726e-01,  2.7454e-02]],\n",
            "\n",
            "         [[ 1.7122e-02,  3.7646e-03,  1.4872e-02],\n",
            "          [ 1.2092e-02,  1.1319e-02,  3.4667e-02],\n",
            "          [ 8.1790e-03, -2.0805e-02,  2.7143e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.0111e-02, -1.0526e-02,  2.8394e-02],\n",
            "          [-2.5112e-02, -2.2196e-02,  3.7229e-02],\n",
            "          [-3.8220e-02, -4.6644e-02,  1.5660e-02]],\n",
            "\n",
            "         [[-2.5913e-03, -2.4307e-02,  1.0611e-02],\n",
            "          [-2.1730e-02, -4.3938e-02, -7.1536e-03],\n",
            "          [-2.5171e-02, -5.9467e-02, -2.5577e-02]],\n",
            "\n",
            "         [[ 2.8652e-02,  2.5850e-04,  1.1416e-03],\n",
            "          [ 3.7812e-02, -1.1271e-03,  9.6027e-03],\n",
            "          [ 3.9350e-02,  1.0134e-02,  1.0449e-02]]],\n",
            "\n",
            "\n",
            "        [[[-7.9305e-03,  7.0872e-03,  2.1412e-02],\n",
            "          [-6.0065e-02,  1.4147e-03,  9.7281e-02],\n",
            "          [-6.0130e-02, -2.1725e-02,  3.6863e-02]],\n",
            "\n",
            "         [[ 2.8024e-02,  2.6183e-02, -2.3027e-02],\n",
            "          [ 5.1900e-02, -2.0588e-03, -1.0940e-01],\n",
            "          [-3.2729e-02, -6.2752e-03,  8.0630e-03]],\n",
            "\n",
            "         [[-1.8062e-02, -1.9510e-02,  4.3163e-02],\n",
            "          [ 4.6080e-02,  2.9494e-02,  4.0844e-02],\n",
            "          [ 5.9607e-03, -6.5891e-03, -6.4623e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.2193e-02,  8.4653e-03,  3.6764e-03],\n",
            "          [ 1.7549e-02,  2.1971e-02, -4.5108e-03],\n",
            "          [ 2.1124e-02,  3.4591e-02, -1.6310e-02]],\n",
            "\n",
            "         [[ 3.8144e-02,  4.8395e-02, -9.5556e-02],\n",
            "          [ 1.8923e-02,  1.1341e-02, -7.6311e-02],\n",
            "          [ 4.7358e-03,  3.2138e-02, -7.4777e-02]],\n",
            "\n",
            "         [[-1.9031e-02, -3.2568e-02, -3.8251e-02],\n",
            "          [ 1.0705e-02,  2.3121e-03, -7.5078e-02],\n",
            "          [ 3.3316e-02,  3.5515e-02, -2.1023e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.3330e-01,  7.4683e-02, -3.8624e-03],\n",
            "          [ 9.1377e-02,  8.2415e-02,  3.9469e-02],\n",
            "          [-1.8265e-02, -5.9943e-02,  8.9354e-02]],\n",
            "\n",
            "         [[ 1.5566e-02, -4.1716e-02,  1.0633e-02],\n",
            "          [ 7.2644e-03,  3.1934e-02,  1.2732e-03],\n",
            "          [-2.0851e-02, -3.7593e-03, -7.0170e-02]],\n",
            "\n",
            "         [[-6.6139e-02,  1.0627e-01,  1.9590e-02],\n",
            "          [ 5.4987e-02, -1.5552e-01, -1.8819e-02],\n",
            "          [-4.2554e-03,  4.4964e-02, -2.4632e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-6.1691e-02, -4.5531e-02, -9.1721e-03],\n",
            "          [ 4.3995e-02,  4.5703e-02, -7.0108e-02],\n",
            "          [ 1.1388e-02,  4.4678e-02, -4.5953e-02]],\n",
            "\n",
            "         [[ 4.3432e-03,  2.3194e-02, -2.1895e-02],\n",
            "          [-8.0216e-02, -5.7606e-02, -9.8455e-03],\n",
            "          [-3.3285e-02, -1.1468e-01, -2.3779e-02]],\n",
            "\n",
            "         [[-6.3785e-02, -2.4485e-02, -4.9061e-02],\n",
            "          [-6.1594e-02,  1.0328e-01,  5.9685e-03],\n",
            "          [ 8.1863e-02, -3.0314e-02, -4.6373e-03]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.2496, 0.2198, 0.2756, 0.6073, 0.2654, 0.2942, 0.1136, 0.4425, 0.2868,\n",
            "        0.2974, 0.2506, 0.4103, 0.4855, 0.3383, 0.4670, 0.1772, 0.2171, 0.5025,\n",
            "        0.2263, 0.3667, 0.4867, 0.4586, 0.4652, 0.2200, 0.1510, 0.2761, 0.3813,\n",
            "        0.2803, 0.2382, 0.3953, 0.3032, 0.3163, 0.2025, 0.2323, 0.2003, 0.1661,\n",
            "        0.4690, 0.3476, 0.3414, 0.2274, 0.2485, 0.2356, 0.2726, 0.4657, 0.3429,\n",
            "        0.2465, 0.4674, 0.2812, 0.6241, 0.4152, 0.3403, 0.4218, 0.1152, 0.2985,\n",
            "        0.5802, 0.2795, 0.4706, 0.4517, 0.4303, 0.2749, 0.3427, 0.1137, 0.5069,\n",
            "        0.4370], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 2.2752e-01,  8.6747e-03, -6.7346e-02, -6.8779e-02,  3.5977e-01,\n",
            "        -2.0167e-01, -4.8431e-05,  2.3735e-02,  3.9549e-01,  3.7079e-02,\n",
            "         6.8793e-03,  2.7578e-01, -7.0272e-02, -2.3970e-01, -8.1753e-02,\n",
            "        -9.4132e-02, -1.4544e-01,  3.7301e-02, -3.6174e-01, -3.9561e-01,\n",
            "        -4.0789e-01,  3.5559e-03, -2.7878e-01, -3.5299e-02, -7.0281e-02,\n",
            "         2.1005e-01, -4.6362e-03, -1.9665e-01, -2.8066e-01, -1.6540e-02,\n",
            "         2.6452e-01, -8.9359e-02, -2.1046e-01, -1.3026e-01,  1.7215e-01,\n",
            "         5.3403e-02, -2.2295e-01, -4.8033e-02,  2.4572e-01,  2.0950e-01,\n",
            "         1.6220e-01,  1.1370e-01,  1.1457e-01, -1.4870e-01, -3.2150e-02,\n",
            "        -3.0549e-01,  4.9125e-01,  1.0873e-01,  1.2779e-02,  1.0044e-01,\n",
            "         4.1553e-01, -1.4710e-02,  2.3922e-02,  9.9812e-02, -1.7273e-01,\n",
            "         1.0078e-01, -1.4564e-01, -2.2735e-01,  1.3637e-01,  2.0127e-01,\n",
            "        -5.7430e-02,  2.3530e-01, -1.1299e-01,  3.0933e-01],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 1.9712e-02, -5.2562e-03, -3.7619e-03],\n",
            "          [-1.9635e-02, -1.2336e-02, -3.5196e-02],\n",
            "          [ 5.0761e-02,  7.5668e-02,  4.3344e-02]],\n",
            "\n",
            "         [[ 1.4160e-02, -8.6094e-03, -1.0541e-02],\n",
            "          [-4.2586e-02, -2.3814e-02, -5.4694e-02],\n",
            "          [-1.4018e-03,  4.6720e-02,  5.0898e-02]],\n",
            "\n",
            "         [[ 2.1559e-02,  4.1633e-03, -9.7118e-03],\n",
            "          [-9.3201e-03, -2.5432e-02, -2.8274e-02],\n",
            "          [-3.0107e-02, -4.8230e-02, -2.6001e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.4300e-03,  9.1875e-02,  3.1938e-03],\n",
            "          [-1.7945e-02,  5.7266e-02, -8.4098e-03],\n",
            "          [-3.4961e-02, -2.3296e-02, -3.5089e-02]],\n",
            "\n",
            "         [[ 2.5603e-02, -3.1689e-02, -5.4160e-02],\n",
            "          [ 6.9736e-02, -1.0716e-02, -6.8034e-02],\n",
            "          [ 3.5578e-02,  3.4749e-02, -1.9334e-02]],\n",
            "\n",
            "         [[-6.5420e-02, -4.6427e-03, -2.3362e-02],\n",
            "          [ 7.5833e-02,  9.1174e-03, -4.9701e-02],\n",
            "          [ 6.2944e-02, -9.8735e-02,  3.3158e-02]]],\n",
            "\n",
            "\n",
            "        [[[-9.0557e-03, -3.0753e-02,  1.1953e-02],\n",
            "          [-3.2539e-02, -6.2846e-03, -2.0235e-02],\n",
            "          [ 4.7996e-03, -2.1462e-02, -4.1557e-03]],\n",
            "\n",
            "         [[ 1.7163e-02, -2.3303e-03,  7.3972e-02],\n",
            "          [-3.2105e-02, -7.7536e-02, -1.2648e-02],\n",
            "          [ 3.8985e-02, -4.3170e-02,  1.0904e-02]],\n",
            "\n",
            "         [[-2.9643e-02, -5.8534e-02, -5.9736e-02],\n",
            "          [-2.9437e-02, -3.6441e-02, -1.2380e-02],\n",
            "          [-2.2775e-02, -2.4485e-03, -1.6124e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.6830e-02,  1.4267e-02,  6.2658e-02],\n",
            "          [ 3.0585e-04, -5.3241e-03,  3.2786e-03],\n",
            "          [ 2.1097e-02, -2.3189e-02,  1.2102e-02]],\n",
            "\n",
            "         [[-6.1182e-02, -2.9227e-02,  2.0036e-02],\n",
            "          [-7.6089e-02, -7.7057e-02,  8.6544e-02],\n",
            "          [-3.9228e-02, -3.2361e-02, -8.8970e-02]],\n",
            "\n",
            "         [[-1.3372e-01,  8.8362e-02,  8.3836e-02],\n",
            "          [-1.1688e-02,  4.3156e-01, -3.3629e-03],\n",
            "          [-2.3925e-02, -1.0092e-01, -1.0184e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 8.0165e-02,  4.3042e-02,  2.7325e-03],\n",
            "          [ 3.5269e-02, -1.5504e-02, -3.5011e-02],\n",
            "          [-1.7164e-02, -2.6827e-02, -3.3946e-02]],\n",
            "\n",
            "         [[ 4.5439e-02,  5.1585e-02,  1.8321e-02],\n",
            "          [-3.9647e-02,  2.3956e-02, -2.6609e-02],\n",
            "          [-3.0358e-02, -6.4729e-02,  2.5834e-02]],\n",
            "\n",
            "         [[ 3.8105e-02,  4.0986e-02,  4.1005e-02],\n",
            "          [ 1.7584e-02, -1.6494e-02, -3.2716e-02],\n",
            "          [ 5.5886e-03, -1.7068e-02, -3.0605e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.3694e-01, -1.4074e-01,  5.1423e-02],\n",
            "          [-1.2521e-01, -1.3128e-01,  7.5733e-02],\n",
            "          [-4.5032e-02, -1.7081e-02,  7.1252e-02]],\n",
            "\n",
            "         [[ 6.3381e-02,  1.5874e-02, -2.7322e-02],\n",
            "          [ 8.0356e-02,  3.6104e-02, -2.8506e-02],\n",
            "          [ 2.6638e-02,  2.2021e-02,  3.2345e-02]],\n",
            "\n",
            "         [[-1.2068e-03, -4.6179e-02, -1.5351e-02],\n",
            "          [-1.1276e-02,  1.9200e-02,  3.4336e-02],\n",
            "          [ 1.6540e-02, -7.8592e-03, -2.5392e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 3.3384e-02,  6.9963e-02,  1.0745e-02],\n",
            "          [-1.7518e-02, -5.3524e-02, -6.4960e-02],\n",
            "          [ 3.4248e-04, -4.5557e-02, -4.7336e-02]],\n",
            "\n",
            "         [[-5.1031e-03,  7.9784e-03, -8.6553e-04],\n",
            "          [-1.6557e-03,  1.4661e-02,  5.3365e-03],\n",
            "          [-3.1784e-02, -6.6940e-02, -4.6889e-02]],\n",
            "\n",
            "         [[-1.1775e-02,  7.2759e-03,  7.6622e-03],\n",
            "          [-6.1288e-02, -5.2078e-02, -4.5152e-02],\n",
            "          [-8.6584e-02, -9.7381e-02, -1.0405e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.1243e-02,  6.2456e-02,  2.5188e-02],\n",
            "          [-2.2911e-02, -2.1100e-03, -2.7573e-02],\n",
            "          [ 4.6557e-02,  6.4980e-02,  3.1879e-02]],\n",
            "\n",
            "         [[ 6.2867e-03,  2.4255e-02,  8.9674e-02],\n",
            "          [-7.7718e-03, -5.4311e-02, -4.6843e-02],\n",
            "          [-6.7499e-03, -6.6857e-02, -4.9842e-02]],\n",
            "\n",
            "         [[ 4.7326e-03, -3.9533e-02,  1.1500e-03],\n",
            "          [-2.7957e-02, -1.3466e-01, -6.0753e-02],\n",
            "          [-3.2010e-03,  7.2213e-02,  1.1009e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 2.3763e-02, -1.7876e-02, -7.4843e-03],\n",
            "          [ 1.6239e-02,  5.4479e-04, -3.3735e-02],\n",
            "          [-2.2854e-02, -1.4316e-03,  1.1010e-02]],\n",
            "\n",
            "         [[ 5.2277e-03, -2.5941e-03,  5.9594e-03],\n",
            "          [-2.9058e-03, -7.3409e-03,  3.0652e-02],\n",
            "          [ 7.5540e-02,  6.6445e-03,  2.5518e-03]],\n",
            "\n",
            "         [[-6.5970e-02, -4.1286e-02, -3.0278e-02],\n",
            "          [-3.5108e-02, -3.9099e-02, -1.6818e-02],\n",
            "          [-1.0224e-02, -8.6995e-03, -5.9939e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.1233e-02, -2.4559e-02, -7.4436e-03],\n",
            "          [-4.3734e-03, -3.2864e-02, -3.3453e-02],\n",
            "          [ 8.9269e-03, -1.7646e-02,  3.8375e-04]],\n",
            "\n",
            "         [[-7.8930e-02, -7.2940e-02, -6.7911e-02],\n",
            "          [-8.4146e-02, -8.3657e-02,  5.3666e-02],\n",
            "          [-3.5577e-02, -3.6835e-02,  5.8987e-03]],\n",
            "\n",
            "         [[ 8.3767e-02,  8.0476e-05,  7.2164e-02],\n",
            "          [-6.4219e-02, -1.2661e-01,  4.6026e-02],\n",
            "          [ 9.3033e-02, -4.7521e-02,  3.6777e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 4.1012e-02,  1.3361e-03, -5.8616e-02],\n",
            "          [ 4.2461e-02,  2.9437e-03, -2.0445e-02],\n",
            "          [ 7.6097e-02,  5.2504e-02, -5.5636e-03]],\n",
            "\n",
            "         [[ 2.2046e-02,  4.0888e-03,  1.4645e-02],\n",
            "          [-7.7532e-02, -1.1912e-01, -7.0892e-02],\n",
            "          [-1.0618e-02, -3.2121e-02, -2.3969e-02]],\n",
            "\n",
            "         [[-2.1612e-02, -2.6110e-03, -3.1664e-02],\n",
            "          [-3.2892e-02, -3.9771e-02, -5.1463e-02],\n",
            "          [-2.6150e-02, -3.6554e-02, -2.3315e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.4600e-03,  8.4181e-02,  2.3199e-02],\n",
            "          [ 5.7595e-02,  1.3036e-01,  3.2172e-02],\n",
            "          [-2.2774e-03,  4.2065e-02, -4.8619e-02]],\n",
            "\n",
            "         [[ 3.1533e-02, -4.3655e-02,  2.0361e-02],\n",
            "          [ 3.9973e-03, -5.1430e-02, -6.3839e-02],\n",
            "          [ 6.4002e-03,  4.5347e-02,  4.7346e-02]],\n",
            "\n",
            "         [[-9.1818e-02,  1.0264e-02,  9.6565e-02],\n",
            "          [-2.1635e-03, -2.3452e-02, -5.9038e-02],\n",
            "          [ 1.9402e-02,  2.8854e-02, -9.6113e-02]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.3910, 0.4375, 0.3746, 0.3990, 0.3404, 0.3503, 0.2618, 0.2707, 0.2865,\n",
            "        0.4308, 0.1895, 0.3041, 0.3837, 0.2944, 0.2105, 0.3304, 0.2943, 0.2887,\n",
            "        0.2060, 0.4627, 0.2335, 0.1831, 0.4489, 0.2830, 0.3389, 0.2997, 0.3503,\n",
            "        0.2735, 0.3908, 0.2817, 0.2636, 0.4462, 0.3282, 0.3776, 0.4471, 0.3878,\n",
            "        0.2516, 0.3172, 0.3661, 0.3166, 0.3818, 0.3128, 0.2274, 0.3627, 0.2902,\n",
            "        0.2381, 0.2988, 0.2469, 0.3840, 0.2886, 0.3197, 0.2879, 0.3218, 0.4559,\n",
            "        0.3500, 0.2420, 0.3396, 0.3519, 0.3839, 0.3806, 0.4039, 0.2826, 0.4594,\n",
            "        0.3342], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0997, -0.4755, -0.0474, -0.2698, -0.0834, -0.0072,  0.0474,  0.1022,\n",
            "        -0.0170, -0.1471,  0.2307,  0.1447, -0.1775,  0.0273,  0.1559, -0.1836,\n",
            "         0.1238, -0.1522,  0.0554, -0.2881, -0.2606,  0.2316, -0.3242, -0.0219,\n",
            "        -0.2645,  0.0576, -0.2465,  0.0481, -0.3530,  0.0950, -0.1862, -0.1707,\n",
            "        -0.0161, -0.2604, -0.3145, -0.1083,  0.0659, -0.1427, -0.0570, -0.0076,\n",
            "        -0.3006, -0.0744, -0.0683, -0.1104,  0.0253,  0.0489, -0.2515,  0.1150,\n",
            "        -0.3783,  0.0846, -0.0368,  0.1439, -0.0468, -0.3087, -0.0240,  0.1397,\n",
            "        -0.0908, -0.1795, -0.1129, -0.0793, -0.1491,  0.0594, -0.4433, -0.0138],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-2.1574e-02, -4.5688e-03,  4.5483e-03],\n",
            "          [-8.1870e-03,  4.1740e-02,  2.3010e-02],\n",
            "          [-8.9283e-03,  5.7352e-02,  2.9818e-02]],\n",
            "\n",
            "         [[ 5.8627e-02,  4.2864e-02,  4.4912e-02],\n",
            "          [ 2.2281e-02, -1.2969e-02,  7.6099e-03],\n",
            "          [ 4.5373e-02,  3.0712e-02,  3.7700e-02]],\n",
            "\n",
            "         [[-1.5456e-02, -3.8692e-02, -4.6010e-02],\n",
            "          [-2.3123e-02,  2.8293e-02,  4.7790e-03],\n",
            "          [-2.0328e-02,  1.3756e-02,  2.5883e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.1302e-02,  4.2291e-02,  5.7833e-02],\n",
            "          [ 4.5210e-02,  5.5850e-02,  1.4318e-02],\n",
            "          [ 1.4241e-02,  1.7968e-02,  1.4344e-02]],\n",
            "\n",
            "         [[ 4.6012e-03,  1.2566e-02,  4.8931e-02],\n",
            "          [-6.5754e-03, -2.6431e-02,  1.5855e-02],\n",
            "          [ 1.3192e-02,  1.9011e-02,  1.3842e-02]],\n",
            "\n",
            "         [[ 6.1983e-02,  6.9919e-02,  6.1035e-02],\n",
            "          [ 6.1253e-02,  9.9557e-02,  5.9060e-02],\n",
            "          [ 5.8298e-02,  8.1652e-02,  8.1499e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.0088e-02, -1.2959e-02,  9.7798e-03],\n",
            "          [ 5.5408e-02,  4.3501e-02,  5.6983e-02],\n",
            "          [ 5.3427e-02,  3.5118e-02,  3.6782e-02]],\n",
            "\n",
            "         [[ 2.4442e-03, -3.0207e-02, -1.0377e-02],\n",
            "          [-4.5297e-02, -4.5318e-02,  5.4623e-03],\n",
            "          [-4.4762e-02, -1.5508e-02,  6.9745e-03]],\n",
            "\n",
            "         [[ 3.9658e-02,  3.6838e-02,  5.8796e-03],\n",
            "          [ 2.3207e-02,  3.9240e-03, -2.0887e-02],\n",
            "          [-1.4829e-02,  5.3606e-03,  1.7404e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.2160e-02,  5.9042e-02,  4.8433e-02],\n",
            "          [-2.6464e-02, -8.0667e-03, -1.0359e-02],\n",
            "          [-2.6699e-02, -9.5411e-03, -2.8902e-02]],\n",
            "\n",
            "         [[-2.9235e-02, -3.9078e-02, -4.4955e-02],\n",
            "          [-2.0346e-02, -4.4891e-02, -3.7477e-02],\n",
            "          [ 1.9653e-02, -1.5562e-03, -5.8245e-03]],\n",
            "\n",
            "         [[-5.0696e-02, -4.8902e-02,  9.1631e-03],\n",
            "          [ 5.1668e-03,  2.0509e-02,  6.6874e-02],\n",
            "          [ 2.8934e-02,  4.6717e-02,  2.1371e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 2.1744e-02, -2.8354e-02, -3.2557e-02],\n",
            "          [ 3.0519e-02,  1.8536e-02,  1.5244e-02],\n",
            "          [ 1.3832e-03,  1.7051e-02,  3.2020e-02]],\n",
            "\n",
            "         [[-3.6293e-02,  1.0914e-02,  4.5371e-02],\n",
            "          [ 1.3399e-02,  6.4272e-02,  8.8210e-02],\n",
            "          [ 4.6697e-02,  9.9653e-02,  8.7606e-02]],\n",
            "\n",
            "         [[-2.4336e-02, -2.9627e-02,  1.9537e-02],\n",
            "          [-3.3412e-02, -2.2290e-02, -2.8879e-02],\n",
            "          [ 1.4765e-02,  1.7234e-02, -1.8185e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.9859e-02, -7.1075e-02, -5.8546e-02],\n",
            "          [ 2.2902e-02,  1.1184e-02, -2.3654e-02],\n",
            "          [ 8.1897e-02,  1.1996e-01,  9.3242e-02]],\n",
            "\n",
            "         [[ 3.1984e-02,  7.4931e-02,  6.6020e-02],\n",
            "          [ 2.8490e-02,  1.1931e-01,  1.2100e-01],\n",
            "          [ 7.9259e-04,  4.3812e-02,  4.4648e-02]],\n",
            "\n",
            "         [[ 3.2748e-02,  4.1444e-02, -8.1932e-03],\n",
            "          [ 4.5541e-02,  2.9426e-02, -8.5440e-03],\n",
            "          [ 1.1634e-04,  1.8045e-03,  1.4826e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-4.4144e-02, -8.3106e-02, -5.3073e-02],\n",
            "          [ 3.2124e-02,  1.0286e-02,  2.4409e-02],\n",
            "          [ 6.1606e-03, -1.9455e-02,  4.0534e-02]],\n",
            "\n",
            "         [[ 5.6026e-04,  9.6961e-03,  2.5010e-03],\n",
            "          [ 7.1679e-03, -1.7535e-02, -2.3857e-02],\n",
            "          [-9.8745e-03, -1.8550e-02,  1.7301e-03]],\n",
            "\n",
            "         [[ 4.3882e-03,  4.2049e-02,  7.5950e-02],\n",
            "          [-6.5610e-02, -3.6130e-02, -1.9404e-02],\n",
            "          [-3.8091e-02, -2.6749e-02, -1.3865e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.5593e-02, -4.6050e-02, -2.2809e-02],\n",
            "          [-9.7648e-03,  2.4910e-03,  2.4503e-02],\n",
            "          [ 2.0381e-02,  5.2393e-02,  6.9019e-02]],\n",
            "\n",
            "         [[ 9.3306e-04,  1.2483e-02, -1.1817e-02],\n",
            "          [-1.2627e-02, -1.8756e-02, -1.4144e-03],\n",
            "          [-5.2490e-03, -4.6126e-03, -1.3224e-02]],\n",
            "\n",
            "         [[ 7.4689e-04, -1.0135e-02, -7.8264e-03],\n",
            "          [ 1.2491e-02, -2.5865e-02,  4.0514e-02],\n",
            "          [ 5.8855e-03,  4.5990e-02,  1.0651e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.2262e-02, -1.5378e-02,  1.3862e-03],\n",
            "          [ 4.1166e-02, -2.4944e-02, -2.6686e-02],\n",
            "          [-1.7423e-02,  5.2690e-03, -2.1861e-02]],\n",
            "\n",
            "         [[-3.1207e-02, -3.3025e-02,  2.2114e-02],\n",
            "          [-2.4009e-02,  1.2988e-02,  2.2430e-02],\n",
            "          [ 1.0332e-02,  4.3601e-03,  4.7321e-03]],\n",
            "\n",
            "         [[ 2.0182e-02,  6.1569e-02, -2.8771e-02],\n",
            "          [ 5.8231e-02,  4.6767e-02, -2.8417e-05],\n",
            "          [ 3.7545e-02, -4.5886e-02,  1.5849e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.0431e-03, -3.6082e-03,  7.1986e-03],\n",
            "          [ 2.4895e-02,  6.1671e-03, -3.2427e-02],\n",
            "          [ 7.2338e-03,  2.2406e-03, -5.3330e-02]],\n",
            "\n",
            "         [[ 2.8072e-02, -1.0571e-02, -1.3854e-02],\n",
            "          [-1.0879e-02,  6.1929e-03, -5.6713e-03],\n",
            "          [-2.6083e-02,  8.1861e-03, -3.2873e-02]],\n",
            "\n",
            "         [[-3.1032e-02, -6.0485e-02, -2.5583e-02],\n",
            "          [-4.6239e-02, -2.2805e-02, -7.7678e-03],\n",
            "          [-9.4698e-03,  4.0247e-03, -4.8637e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 2.3128e-02, -5.6038e-02, -3.4572e-02],\n",
            "          [ 1.0638e-03,  5.7929e-02, -7.6970e-03],\n",
            "          [-3.0103e-02,  3.5573e-02, -1.8143e-02]],\n",
            "\n",
            "         [[ 9.6840e-02, -1.1186e-01, -7.8766e-02],\n",
            "          [-1.0444e-01, -1.0851e-01, -1.9553e-01],\n",
            "          [-1.1986e-01, -7.1474e-02,  3.6750e-02]],\n",
            "\n",
            "         [[-2.2194e-02,  6.0298e-03,  5.6914e-02],\n",
            "          [-4.8342e-02,  7.8893e-02, -5.1026e-02],\n",
            "          [-5.1294e-02, -5.7434e-02, -1.9178e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.4896e-02, -8.1267e-02,  5.1794e-02],\n",
            "          [-8.3985e-02, -5.7778e-02,  6.7891e-02],\n",
            "          [ 2.3837e-02,  3.8954e-02,  4.1141e-02]],\n",
            "\n",
            "         [[ 4.6446e-03,  2.7367e-02, -2.3154e-02],\n",
            "          [ 2.0675e-02,  2.3429e-02,  6.4380e-04],\n",
            "          [-5.2222e-02, -1.4854e-02, -2.5150e-02]],\n",
            "\n",
            "         [[ 2.1291e-02,  1.2736e-02,  8.4553e-03],\n",
            "          [-8.2932e-02,  7.2067e-02,  1.3107e-01],\n",
            "          [ 8.5491e-03,  1.3677e-01,  3.9867e-02]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.2560, 0.5690, 0.4042, 0.5130, 0.2178, 0.4940, 0.3315, 0.5510, 0.4354,\n",
            "        0.5291, 0.2081, 0.4735, 0.5945, 0.5645, 0.2761, 0.2571, 0.4853, 0.6240,\n",
            "        0.4370, 0.2308, 0.4970, 0.3157, 0.5706, 0.2162, 0.1932, 0.1448, 0.2218,\n",
            "        0.2389, 0.5871, 0.3501, 0.4109, 0.3199, 0.5808, 0.3281, 0.2723, 0.1971,\n",
            "        0.6139, 0.4075, 0.6304, 0.3874, 0.7605, 0.2111, 0.3071, 0.4603, 0.3099,\n",
            "        0.1914, 0.4431, 0.2537, 0.5745, 0.6459, 0.3914, 0.3090, 0.6782, 0.1937,\n",
            "        0.5814, 0.2570, 0.3514, 0.2124, 0.5794, 0.3415, 0.2051, 0.0715, 0.4090,\n",
            "        0.4416], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1778, -0.1287,  0.0349, -0.1452,  0.1864, -0.1413, -0.4201, -0.1334,\n",
            "         0.2183, -0.1912,  0.0311, -0.0235, -0.1724, -0.0274, -0.0295, -0.1031,\n",
            "         0.0047,  0.0828, -0.1521,  0.0183, -0.2418, -0.0831, -0.0491, -0.0688,\n",
            "        -0.2560,  0.1381, -0.0165,  0.2092, -0.0028, -0.0265, -0.0225,  0.0286,\n",
            "        -0.1065, -0.3698,  0.2862, -0.1036,  0.3080, -0.0894,  0.2772,  0.1136,\n",
            "        -0.3157,  0.0423,  0.0567,  0.2369, -0.0727,  0.0465, -0.0536,  0.1309,\n",
            "         0.0282, -0.1371,  0.1464, -0.0717, -0.3237, -0.1583, -0.0424, -0.1278,\n",
            "        -0.1703,  0.0413,  0.0891,  0.0770, -0.0730,  0.0683, -0.0391,  0.0476],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-7.1555e-02, -1.1031e-01, -1.3711e-01],\n",
            "          [ 7.0593e-02, -1.4782e-02, -1.0053e-01],\n",
            "          [ 1.1938e-01,  8.7330e-02, -8.2206e-03]],\n",
            "\n",
            "         [[-2.3999e-02, -6.3682e-03,  2.4303e-03],\n",
            "          [ 6.1831e-03,  1.8781e-02,  2.5324e-02],\n",
            "          [ 2.3656e-03, -4.0037e-03, -1.1949e-02]],\n",
            "\n",
            "         [[ 6.0344e-03,  6.3784e-03, -1.2247e-02],\n",
            "          [ 7.8854e-03, -1.3464e-02, -4.2702e-02],\n",
            "          [ 1.7380e-02, -1.3862e-02, -4.7145e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.4324e-02,  3.2257e-02,  2.5819e-02],\n",
            "          [ 8.4676e-03, -4.5413e-04, -1.0832e-02],\n",
            "          [-6.7166e-03, -1.5052e-02, -2.6939e-02]],\n",
            "\n",
            "         [[-1.2089e-02, -2.3588e-02, -2.2689e-02],\n",
            "          [ 1.0135e-02,  1.8285e-02, -1.5695e-02],\n",
            "          [ 2.1352e-02,  5.8568e-02,  4.2873e-02]],\n",
            "\n",
            "         [[ 1.4421e-02, -2.8298e-02, -7.0770e-03],\n",
            "          [ 3.0260e-02, -6.6294e-03, -1.6901e-02],\n",
            "          [ 3.9085e-02,  1.4222e-02,  2.2294e-02]]],\n",
            "\n",
            "\n",
            "        [[[-7.7911e-02, -7.3929e-02, -3.6671e-02],\n",
            "          [-3.4903e-02, -6.2355e-02, -3.7793e-02],\n",
            "          [-2.8379e-02, -5.4291e-02, -4.9411e-02]],\n",
            "\n",
            "         [[-1.2970e-02, -2.1825e-02, -2.8767e-04],\n",
            "          [ 7.6444e-03,  1.7653e-02,  1.6660e-02],\n",
            "          [ 3.8337e-02,  2.3006e-02, -1.6620e-03]],\n",
            "\n",
            "         [[-8.7592e-02, -8.4735e-02, -5.5818e-02],\n",
            "          [-7.7731e-02, -8.0311e-02, -3.2554e-02],\n",
            "          [-5.6313e-02, -4.2047e-02,  1.5247e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.2377e-02, -4.0018e-02, -2.9523e-02],\n",
            "          [-1.5294e-02, -1.4165e-02,  2.7086e-03],\n",
            "          [ 1.1652e-02,  2.3886e-02,  2.4413e-02]],\n",
            "\n",
            "         [[ 2.0891e-03, -3.0475e-02, -3.3818e-02],\n",
            "          [ 6.7829e-03,  3.8681e-04, -1.4540e-02],\n",
            "          [-3.1306e-03,  6.7689e-03,  8.4524e-03]],\n",
            "\n",
            "         [[ 3.0586e-02,  4.6281e-02,  3.8359e-04],\n",
            "          [ 5.3079e-02,  6.7488e-02,  3.0547e-02],\n",
            "          [ 2.3374e-02,  4.3993e-02, -3.8713e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 1.3878e-02,  3.2724e-02,  4.6584e-02],\n",
            "          [-8.0647e-03,  1.6209e-03,  1.5153e-02],\n",
            "          [-7.0342e-02, -5.3299e-02, -4.5920e-02]],\n",
            "\n",
            "         [[ 4.6035e-02,  3.5400e-02,  3.4941e-02],\n",
            "          [ 5.8351e-02,  5.4640e-02,  2.7162e-02],\n",
            "          [ 2.6799e-02,  4.5056e-02,  6.6886e-03]],\n",
            "\n",
            "         [[-3.3766e-02, -3.8605e-02, -2.4172e-02],\n",
            "          [-1.8285e-03,  1.0888e-02,  1.1425e-02],\n",
            "          [ 2.2282e-02,  1.4024e-02,  3.6332e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.6330e-02, -6.9552e-02, -8.9737e-02],\n",
            "          [ 3.9766e-02,  1.5501e-02, -2.2695e-02],\n",
            "          [ 1.0290e-01,  1.2294e-01,  6.3867e-02]],\n",
            "\n",
            "         [[-4.2318e-03,  4.9511e-02, -7.6289e-03],\n",
            "          [-2.7720e-02,  7.0398e-03, -9.4052e-03],\n",
            "          [-6.7008e-02, -6.0542e-02, -2.5967e-02]],\n",
            "\n",
            "         [[-5.8560e-03, -1.7573e-02, -3.8016e-02],\n",
            "          [ 2.8579e-03, -4.1603e-03,  1.0113e-02],\n",
            "          [ 2.6243e-02,  3.5200e-02,  3.1143e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-4.4193e-02, -6.5322e-02, -1.7594e-02],\n",
            "          [-9.3970e-02, -5.8291e-02,  1.2093e-02],\n",
            "          [-2.2998e-02,  3.2463e-02,  7.1731e-02]],\n",
            "\n",
            "         [[-4.7220e-03, -3.0125e-03, -1.8075e-02],\n",
            "          [ 1.2667e-02, -8.0509e-03, -1.4605e-02],\n",
            "          [ 7.8220e-03, -1.0720e-02, -2.6515e-02]],\n",
            "\n",
            "         [[-2.5299e-02, -4.9383e-02, -1.2720e-02],\n",
            "          [-5.2206e-02, -4.7233e-02, -4.2470e-03],\n",
            "          [-4.8697e-02, -2.5320e-02,  8.6178e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.7617e-03,  7.8398e-03, -5.9525e-03],\n",
            "          [ 4.0277e-03,  7.3575e-03, -1.1667e-02],\n",
            "          [-3.9997e-02, -3.8038e-02, -5.0469e-02]],\n",
            "\n",
            "         [[-3.8949e-03, -6.8965e-03,  3.4102e-02],\n",
            "          [-6.9814e-03, -4.9762e-02,  5.8711e-02],\n",
            "          [ 1.8361e-02,  2.5874e-02,  8.0028e-02]],\n",
            "\n",
            "         [[-3.3014e-02, -2.1510e-02, -2.1509e-03],\n",
            "          [-4.3894e-02, -3.2009e-02, -1.6265e-02],\n",
            "          [-1.1037e-02,  2.8872e-04,  3.0937e-02]]],\n",
            "\n",
            "\n",
            "        [[[-4.9907e-02, -5.0222e-02, -5.0985e-02],\n",
            "          [ 2.2644e-02, -1.4098e-02, -2.4426e-02],\n",
            "          [ 1.9960e-02,  9.6426e-02,  1.0580e-01]],\n",
            "\n",
            "         [[-3.6873e-02,  2.1413e-03,  8.3469e-03],\n",
            "          [-4.0796e-02, -3.3767e-02, -3.4955e-02],\n",
            "          [ 3.9466e-02,  7.0508e-02,  8.6065e-02]],\n",
            "\n",
            "         [[ 1.4842e-02,  6.6914e-03,  1.4324e-02],\n",
            "          [-3.2621e-02, -4.4027e-02, -2.2269e-02],\n",
            "          [ 7.1982e-03, -1.9187e-02, -4.9348e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.9938e-03,  1.6018e-02,  1.1242e-02],\n",
            "          [-4.7668e-03,  2.1921e-02,  2.2660e-02],\n",
            "          [-2.6753e-02,  2.6917e-04, -5.6827e-03]],\n",
            "\n",
            "         [[-8.7725e-03,  1.0761e-02,  7.3603e-03],\n",
            "          [-1.8010e-05, -1.7926e-02,  4.8229e-03],\n",
            "          [ 4.2431e-02, -1.5764e-02,  2.3554e-02]],\n",
            "\n",
            "         [[-1.3830e-02, -3.0793e-03, -4.0854e-03],\n",
            "          [ 3.3363e-02,  4.2952e-02,  3.5867e-02],\n",
            "          [-3.9653e-02, -3.0855e-02, -4.3189e-02]]],\n",
            "\n",
            "\n",
            "        [[[-3.8617e-02, -3.1549e-03,  2.5739e-03],\n",
            "          [-1.1592e-02,  9.8761e-03,  7.5235e-03],\n",
            "          [-1.9339e-02, -9.8779e-03,  2.1755e-03]],\n",
            "\n",
            "         [[ 1.6889e-04,  1.8302e-03, -8.9537e-03],\n",
            "          [ 5.8343e-03,  1.7360e-02, -1.9029e-02],\n",
            "          [ 5.8642e-03, -7.4307e-04,  1.4667e-03]],\n",
            "\n",
            "         [[-1.6506e-02, -2.8401e-02,  1.3986e-02],\n",
            "          [-2.2922e-02, -4.3484e-02,  1.0471e-02],\n",
            "          [-2.5801e-03, -4.5258e-02,  7.9791e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.5260e-03, -7.6469e-03,  1.3597e-02],\n",
            "          [ 5.5301e-04, -2.9176e-03,  2.2147e-02],\n",
            "          [ 3.2763e-03, -1.0775e-05,  1.3163e-02]],\n",
            "\n",
            "         [[ 5.1756e-03,  1.8495e-02, -8.0268e-03],\n",
            "          [-3.5030e-02,  2.6403e-02, -7.1220e-03],\n",
            "          [-5.2325e-02, -1.1185e-02,  1.9146e-02]],\n",
            "\n",
            "         [[-6.8805e-02,  5.1618e-02,  1.9787e-02],\n",
            "          [ 2.5533e-02, -6.1926e-02,  4.9924e-02],\n",
            "          [ 1.0532e-01, -4.4136e-02,  4.9907e-02]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.3248, 0.3613, 0.2960, 0.2913, 0.3407, 0.3435, 0.3049, 0.3308, 0.3447,\n",
            "        0.3860, 0.3196, 0.2622, 0.2994, 0.2189, 0.2397, 0.3744, 0.3555, 0.1948,\n",
            "        0.3349, 0.2159, 0.3349, 0.3454, 0.3094, 0.3769, 0.3546, 0.3267, 0.3178,\n",
            "        0.3272, 0.3832, 0.2585, 0.2973, 0.3481, 0.2827, 0.2995, 0.3451, 0.3471,\n",
            "        0.3440, 0.3344, 0.3211, 0.3180, 0.2940, 0.3353, 0.3253, 0.3733, 0.3198,\n",
            "        0.2987, 0.1620, 0.3262, 0.3271, 0.3410, 0.3693, 0.3320, 0.3357, 0.2951,\n",
            "        0.3115, 0.3185, 0.3139, 0.2633, 0.3089, 0.3601, 0.2734, 0.3433, 0.3335,\n",
            "        0.3288, 0.2706, 0.2879, 0.3318, 0.3310, 0.3170, 0.2977, 0.3300, 0.3216,\n",
            "        0.3205, 0.3231, 0.3481, 0.3130, 0.2826, 0.2856, 0.3279, 0.3666, 0.3288,\n",
            "        0.3575, 0.3377, 0.2904, 0.3273, 0.3214, 0.3332, 0.3452, 0.1842, 0.3916,\n",
            "        0.3337, 0.2325, 0.3285, 0.3358, 0.2885, 0.3149, 0.3288, 0.2236, 0.3159,\n",
            "        0.2993, 0.3403, 0.3220, 0.3171, 0.2950, 0.2847, 0.3224, 0.3119, 0.2613,\n",
            "        0.3374, 0.3333, 0.3330, 0.2959, 0.4087, 0.2192, 0.2982, 0.4006, 0.3081,\n",
            "        0.3171, 0.2862, 0.2952, 0.3070, 0.3583, 0.3232, 0.3345, 0.3453, 0.3043,\n",
            "        0.3327, 0.3337], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0589, -0.1686, -0.0206,  0.0027, -0.0955, -0.1048,  0.0349, -0.0885,\n",
            "        -0.2053, -0.1764, -0.1224, -0.0364, -0.0785,  0.2088, -0.0403, -0.1820,\n",
            "        -0.1076,  0.2989, -0.0570,  0.2064, -0.0921, -0.1376, -0.1304, -0.1193,\n",
            "        -0.1006, -0.0380, -0.1108, -0.0477, -0.1087,  0.1581, -0.1123, -0.1584,\n",
            "         0.0976, -0.0430, -0.1349, -0.1189, -0.0986, -0.0479, -0.0837, -0.0720,\n",
            "        -0.0836, -0.2442, -0.3376, -0.2124, -0.0693, -0.0651,  0.4979, -0.0811,\n",
            "        -0.1021, -0.0788, -0.1802, -0.1011, -0.1090, -0.0617, -0.0856, -0.0495,\n",
            "        -0.0370,  0.0023, -0.0508, -0.2430,  0.0009, -0.1525, -0.0963, -0.0516,\n",
            "        -0.0473,  0.0884, -0.1028, -0.0907, -0.1086, -0.0379, -0.1030, -0.1609,\n",
            "        -0.0903, -0.0898, -0.1282, -0.0830, -0.0186, -0.0232, -0.0045, -0.2131,\n",
            "        -0.1431, -0.1391, -0.1303, -0.0568, -0.1862, -0.1209, -0.0340, -0.1181,\n",
            "         0.2298, -0.2085, -0.1335,  0.1418, -0.0891, -0.1273,  0.0107, -0.1029,\n",
            "        -0.1025,  0.1562, -0.0937, -0.0657, -0.1245, -0.0451, -0.0707, -0.0447,\n",
            "         0.0715, -0.0484, -0.0312, -0.0437, -0.0927, -0.1465, -0.1151, -0.0183,\n",
            "        -0.1927,  0.2491,  0.0300, -0.1310, -0.0468, -0.0851, -0.0421, -0.0413,\n",
            "        -0.0457, -0.1433, -0.0981, -0.1046, -0.1315, -0.1249, -0.0982, -0.0961],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-0.0074, -0.0098,  0.0028],\n",
            "          [-0.0108,  0.0258,  0.0455],\n",
            "          [-0.0272,  0.0053,  0.0132]],\n",
            "\n",
            "         [[ 0.0354,  0.0251,  0.0078],\n",
            "          [ 0.0040,  0.0199,  0.0274],\n",
            "          [ 0.0353,  0.0355,  0.0133]],\n",
            "\n",
            "         [[ 0.0193, -0.0213, -0.0362],\n",
            "          [-0.0196, -0.0189, -0.0595],\n",
            "          [-0.0218, -0.0077,  0.0039]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0068,  0.0108, -0.0037],\n",
            "          [ 0.0135,  0.0114, -0.0013],\n",
            "          [ 0.0081,  0.0002,  0.0006]],\n",
            "\n",
            "         [[ 0.0077,  0.0077,  0.0044],\n",
            "          [-0.0102, -0.0117, -0.0096],\n",
            "          [-0.0039,  0.0181,  0.0133]],\n",
            "\n",
            "         [[ 0.0124, -0.0269, -0.0120],\n",
            "          [ 0.0268,  0.0264, -0.0215],\n",
            "          [ 0.0129,  0.0028, -0.0054]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0128,  0.0185, -0.0027],\n",
            "          [-0.0168, -0.0123,  0.0355],\n",
            "          [-0.0009,  0.0245,  0.0182]],\n",
            "\n",
            "         [[-0.0067, -0.0207, -0.0144],\n",
            "          [-0.0073,  0.0426,  0.0074],\n",
            "          [ 0.0276,  0.0160,  0.0159]],\n",
            "\n",
            "         [[-0.0229, -0.0206,  0.0236],\n",
            "          [-0.0275, -0.0561, -0.0699],\n",
            "          [ 0.0205,  0.0513,  0.0220]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0272, -0.0390, -0.0395],\n",
            "          [-0.0621, -0.0749, -0.0956],\n",
            "          [-0.0605, -0.0777, -0.0746]],\n",
            "\n",
            "         [[ 0.0287,  0.0293,  0.0287],\n",
            "          [ 0.0079,  0.0471,  0.0146],\n",
            "          [-0.0018,  0.0220,  0.0074]],\n",
            "\n",
            "         [[ 0.0016, -0.0168, -0.0046],\n",
            "          [-0.0081, -0.0255, -0.0524],\n",
            "          [-0.0093, -0.0010, -0.0378]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0047,  0.0061, -0.0071],\n",
            "          [ 0.0236, -0.0931, -0.0793],\n",
            "          [-0.0079, -0.0505, -0.0105]],\n",
            "\n",
            "         [[ 0.0148,  0.0162, -0.0515],\n",
            "          [ 0.0086,  0.0081, -0.0429],\n",
            "          [ 0.0908,  0.0654,  0.0435]],\n",
            "\n",
            "         [[-0.0138, -0.0064,  0.0085],\n",
            "          [ 0.0138, -0.0124,  0.0054],\n",
            "          [ 0.0202, -0.0035,  0.0080]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0009,  0.0185, -0.0306],\n",
            "          [-0.0170,  0.0057, -0.0491],\n",
            "          [-0.0328, -0.0374, -0.0459]],\n",
            "\n",
            "         [[-0.0046,  0.0069, -0.0011],\n",
            "          [-0.0079, -0.0499,  0.0421],\n",
            "          [-0.0752, -0.0048, -0.0058]],\n",
            "\n",
            "         [[ 0.0115, -0.0146,  0.0379],\n",
            "          [ 0.0141,  0.0486,  0.0232],\n",
            "          [ 0.0215, -0.0101,  0.0338]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0264,  0.0040,  0.0075],\n",
            "          [ 0.0636, -0.0320, -0.0018],\n",
            "          [ 0.0262,  0.0076,  0.0495]],\n",
            "\n",
            "         [[-0.0287, -0.0227, -0.0513],\n",
            "          [-0.0260, -0.0487, -0.0140],\n",
            "          [-0.0173, -0.0416, -0.0117]],\n",
            "\n",
            "         [[-0.0350,  0.0356,  0.0347],\n",
            "          [ 0.0183,  0.0436, -0.0263],\n",
            "          [ 0.0178,  0.0356,  0.0113]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0097, -0.0173, -0.0002],\n",
            "          [-0.0199,  0.0820,  0.0129],\n",
            "          [-0.0238, -0.0048,  0.0486]],\n",
            "\n",
            "         [[-0.0244, -0.0258, -0.0353],\n",
            "          [-0.0296, -0.0966, -0.0535],\n",
            "          [-0.0150,  0.0059, -0.0197]],\n",
            "\n",
            "         [[ 0.0068, -0.0368, -0.0255],\n",
            "          [-0.0116, -0.0236, -0.0078],\n",
            "          [ 0.0086,  0.0079, -0.0189]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0329,  0.0297,  0.0111],\n",
            "          [ 0.0390, -0.0090,  0.0226],\n",
            "          [ 0.0078, -0.0280, -0.0230]],\n",
            "\n",
            "         [[ 0.0137,  0.0229, -0.0190],\n",
            "          [ 0.0027,  0.0112,  0.0074],\n",
            "          [ 0.0211,  0.0436,  0.0108]],\n",
            "\n",
            "         [[-0.0237,  0.0221,  0.0004],\n",
            "          [-0.0309,  0.0609,  0.0167],\n",
            "          [-0.0885, -0.0834, -0.0342]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0073, -0.0197,  0.0018],\n",
            "          [ 0.0073, -0.0343, -0.0243],\n",
            "          [-0.0115, -0.0605, -0.0551]],\n",
            "\n",
            "         [[ 0.0030,  0.0026,  0.0171],\n",
            "          [-0.0134, -0.0086,  0.0090],\n",
            "          [ 0.0195,  0.0094,  0.0045]],\n",
            "\n",
            "         [[ 0.0014,  0.0008,  0.0119],\n",
            "          [-0.0024, -0.0107,  0.0126],\n",
            "          [-0.0051, -0.0058, -0.0146]]],\n",
            "\n",
            "\n",
            "        [[[-0.0074,  0.0118, -0.0427],\n",
            "          [ 0.0016, -0.0457, -0.1398],\n",
            "          [-0.0065, -0.0021, -0.0484]],\n",
            "\n",
            "         [[ 0.0075,  0.0527,  0.0388],\n",
            "          [-0.0125,  0.0847,  0.0062],\n",
            "          [ 0.0013, -0.0197, -0.0822]],\n",
            "\n",
            "         [[-0.0249,  0.0166,  0.0169],\n",
            "          [ 0.0087,  0.0214,  0.0117],\n",
            "          [-0.0009,  0.0306,  0.0136]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0078,  0.0220, -0.0175],\n",
            "          [-0.0076, -0.0211, -0.0037],\n",
            "          [ 0.0126, -0.0207, -0.0054]],\n",
            "\n",
            "         [[ 0.0302, -0.0082, -0.0649],\n",
            "          [-0.0238, -0.0954, -0.0530],\n",
            "          [-0.0168, -0.0111,  0.0010]],\n",
            "\n",
            "         [[-0.0245, -0.0847,  0.0251],\n",
            "          [ 0.0106,  0.0387,  0.1400],\n",
            "          [ 0.0155, -0.0095,  0.0041]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.1454, 0.3270, 0.3113, 0.2538, 0.4086, 0.3937, 0.4400, 0.3108, 0.3406,\n",
            "        0.2168, 0.2170, 0.3857, 0.1971, 0.2692, 0.1663, 0.2454, 0.3232, 0.3686,\n",
            "        0.3893, 0.3264, 0.3875, 0.4707, 0.1958, 0.4717, 0.1673, 0.3938, 0.3044,\n",
            "        0.1929, 0.2175, 0.2119, 0.4230, 0.3683, 0.2455, 0.2229, 0.3370, 0.3229,\n",
            "        0.2688, 0.3557, 0.2581, 0.4031, 0.4492, 0.3642, 0.2599, 0.1881, 0.1359,\n",
            "        0.2958, 0.1913, 0.3065, 0.3981, 0.4102, 0.1874, 0.4516, 0.3340, 0.1628,\n",
            "        0.3599, 0.1624, 0.2886, 0.1358, 0.4491, 0.2694, 0.4823, 0.3393, 0.4764,\n",
            "        0.3155, 0.6005, 0.4654, 0.5264, 0.2991, 0.2992, 0.4621, 0.2614, 0.4247,\n",
            "        0.4662, 0.4249, 0.3345, 0.2655, 0.4048, 0.3605, 0.1782, 0.3833, 0.2823,\n",
            "        0.3843, 0.3307, 0.2151, 0.3317, 0.1458, 0.2771, 0.4917, 0.3199, 0.4222,\n",
            "        0.1559, 0.4884, 0.3267, 0.3440, 0.1608, 0.4855, 0.2677, 0.1616, 0.3221,\n",
            "        0.4243, 0.3661, 0.1893, 0.3400, 0.3648, 0.1779, 0.3544, 0.2852, 0.2437,\n",
            "        0.4472, 0.3011, 0.3997, 0.6173, 0.2794, 0.4867, 0.1502, 0.6021, 0.3604,\n",
            "        0.4696, 0.3711, 0.2388, 0.5347, 0.1509, 0.3213, 0.4394, 0.3229, 0.4329,\n",
            "        0.1489, 0.3702], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0246,  0.0593,  0.1347, -0.1089, -0.0470, -0.1359, -0.0550,  0.0509,\n",
            "        -0.0613,  0.0916,  0.0031, -0.0274, -0.0539,  0.0177,  0.0432,  0.0074,\n",
            "         0.0548, -0.0321, -0.0224,  0.0142, -0.2150, -0.1160,  0.0486, -0.1141,\n",
            "         0.1066,  0.0355,  0.0140,  0.0177,  0.0781,  0.1331,  0.0139,  0.0447,\n",
            "         0.1063,  0.0528, -0.0539, -0.1160,  0.1055, -0.1591,  0.0100,  0.1197,\n",
            "         0.0170,  0.0929, -0.0675,  0.0987,  0.1034,  0.0501,  0.0297,  0.0281,\n",
            "        -0.0075, -0.0577, -0.0144, -0.1640,  0.1255,  0.0817,  0.0635,  0.0936,\n",
            "         0.0213,  0.0486, -0.1174,  0.0237, -0.2177,  0.0099, -0.1883,  0.0467,\n",
            "        -0.0829,  0.0585, -0.0306,  0.0509,  0.0541, -0.1671,  0.0115, -0.0302,\n",
            "        -0.1393,  0.0115,  0.0428,  0.1189, -0.1289,  0.0479,  0.0474, -0.0625,\n",
            "         0.0009, -0.0144,  0.0909,  0.1342, -0.0338,  0.0560,  0.0848, -0.0467,\n",
            "         0.0228, -0.0097,  0.1360, -0.2625,  0.0088, -0.0553,  0.0383, -0.0720,\n",
            "         0.0907,  0.1612, -0.1076,  0.1011, -0.0519,  0.0838, -0.0704, -0.0806,\n",
            "        -0.0243,  0.0533,  0.1277,  0.1403, -0.0593, -0.0639, -0.0766, -0.1163,\n",
            "         0.0661, -0.1644,  0.0422, -0.2786, -0.1006, -0.0696, -0.0761,  0.0371,\n",
            "        -0.0247,  0.0916, -0.0200, -0.0176,  0.0298, -0.0373,  0.0466, -0.1371],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 0.0159]],\n",
            "\n",
            "         [[-0.3109]],\n",
            "\n",
            "         [[ 0.0126]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1672]],\n",
            "\n",
            "         [[ 0.0127]],\n",
            "\n",
            "         [[ 0.0132]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0036]],\n",
            "\n",
            "         [[-0.0011]],\n",
            "\n",
            "         [[-0.0083]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0234]],\n",
            "\n",
            "         [[-0.0756]],\n",
            "\n",
            "         [[-0.0126]]],\n",
            "\n",
            "\n",
            "        [[[-0.0419]],\n",
            "\n",
            "         [[ 0.0079]],\n",
            "\n",
            "         [[-0.1662]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0319]],\n",
            "\n",
            "         [[-0.0188]],\n",
            "\n",
            "         [[ 0.0645]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.0287]],\n",
            "\n",
            "         [[ 0.0470]],\n",
            "\n",
            "         [[-0.0523]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0474]],\n",
            "\n",
            "         [[ 0.0586]],\n",
            "\n",
            "         [[ 0.0588]]],\n",
            "\n",
            "\n",
            "        [[[-0.0078]],\n",
            "\n",
            "         [[-0.0203]],\n",
            "\n",
            "         [[ 0.0564]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.7802]],\n",
            "\n",
            "         [[-0.0023]],\n",
            "\n",
            "         [[-0.0259]]],\n",
            "\n",
            "\n",
            "        [[[-0.0283]],\n",
            "\n",
            "         [[-0.0132]],\n",
            "\n",
            "         [[-0.0514]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0236]],\n",
            "\n",
            "         [[-0.0677]],\n",
            "\n",
            "         [[ 0.0268]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.3334,  0.0581,  0.0715,  0.3442,  0.1756,  0.1509,  0.1568,  0.3100,\n",
            "         0.1927,  0.1516,  0.3044,  0.2238,  0.3706,  0.1739,  0.3051,  0.2610,\n",
            "         0.1575,  0.2015,  0.2933,  0.1010,  0.5871,  0.0676,  0.2499,  0.0929,\n",
            "         0.2443,  0.0495,  0.2449,  0.2750,  0.3071,  0.3025,  0.1818,  0.0688,\n",
            "         0.2223,  0.3766,  0.4661,  0.3284,  0.1035,  0.3400,  0.2325,  0.1514,\n",
            "         0.1753,  0.2269,  0.2606,  0.1831,  0.2894,  0.2590,  0.2208,  0.1399,\n",
            "         0.0643,  0.2833,  0.3451,  0.2017,  0.0696,  0.2722,  0.1127,  0.2917,\n",
            "         0.2358,  0.2703,  0.0911,  0.2591,  0.1302,  0.2261,  0.1967,  0.0539,\n",
            "         0.0697,  0.0524,  0.1050,  0.0861,  0.1173,  0.0957,  0.1862,  0.1642,\n",
            "         0.1336,  0.1065,  0.1312,  0.0888,  0.0793,  0.0475,  0.3049,  0.2325,\n",
            "         0.2908,  0.1292,  0.0778,  0.2263,  0.2379,  0.3405,  0.0914,  0.1936,\n",
            "         0.1223,  0.1400,  0.2953,  0.2360,  0.1681,  0.1338,  0.2666,  0.1495,\n",
            "         0.0761,  0.1674,  0.1784,  0.1720,  0.2318,  0.3753,  0.2103,  0.1922,\n",
            "         0.4002,  0.1718,  0.0593,  0.0742,  0.0686,  0.1931,  0.1386,  0.1111,\n",
            "         0.3055,  0.1205,  0.3443,  0.1633,  0.3673,  0.1534,  0.0742,  0.2088,\n",
            "         0.0394,  0.2594,  0.1385, -0.0051,  0.1905,  0.1275,  0.3071,  0.1682],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0246,  0.0593,  0.1347, -0.1089, -0.0470, -0.1359, -0.0550,  0.0509,\n",
            "        -0.0613,  0.0916,  0.0031, -0.0274, -0.0539,  0.0177,  0.0432,  0.0074,\n",
            "         0.0548, -0.0321, -0.0224,  0.0142, -0.2150, -0.1160,  0.0486, -0.1141,\n",
            "         0.1066,  0.0355,  0.0140,  0.0177,  0.0781,  0.1331,  0.0139,  0.0447,\n",
            "         0.1063,  0.0528, -0.0539, -0.1160,  0.1055, -0.1591,  0.0100,  0.1197,\n",
            "         0.0170,  0.0929, -0.0675,  0.0987,  0.1034,  0.0501,  0.0297,  0.0281,\n",
            "        -0.0075, -0.0577, -0.0144, -0.1640,  0.1255,  0.0817,  0.0635,  0.0936,\n",
            "         0.0213,  0.0486, -0.1174,  0.0237, -0.2177,  0.0099, -0.1883,  0.0467,\n",
            "        -0.0829,  0.0585, -0.0306,  0.0509,  0.0541, -0.1671,  0.0115, -0.0302,\n",
            "        -0.1393,  0.0115,  0.0428,  0.1189, -0.1289,  0.0479,  0.0474, -0.0625,\n",
            "         0.0009, -0.0144,  0.0909,  0.1342, -0.0338,  0.0560,  0.0848, -0.0467,\n",
            "         0.0228, -0.0097,  0.1360, -0.2625,  0.0088, -0.0553,  0.0383, -0.0720,\n",
            "         0.0907,  0.1612, -0.1076,  0.1011, -0.0519,  0.0838, -0.0704, -0.0806,\n",
            "        -0.0243,  0.0533,  0.1277,  0.1403, -0.0593, -0.0639, -0.0766, -0.1163,\n",
            "         0.0661, -0.1644,  0.0422, -0.2786, -0.1006, -0.0696, -0.0761,  0.0371,\n",
            "        -0.0247,  0.0916, -0.0200, -0.0176,  0.0298, -0.0373,  0.0466, -0.1371],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-9.9023e-04, -7.7429e-03, -7.9740e-03],\n",
            "          [ 2.4844e-02,  1.8642e-03,  5.8352e-03],\n",
            "          [ 9.5089e-03, -1.6476e-02,  3.9157e-03]],\n",
            "\n",
            "         [[-2.1488e-02, -1.2330e-03, -1.4281e-02],\n",
            "          [-1.7044e-02,  9.5922e-03,  7.0445e-03],\n",
            "          [ 1.0790e-02, -7.2350e-03, -1.1357e-02]],\n",
            "\n",
            "         [[-1.1126e-03,  3.0388e-02,  2.2247e-02],\n",
            "          [-6.1184e-02, -2.3797e-02,  2.3747e-03],\n",
            "          [ 4.0678e-02, -1.0356e-01, -6.0011e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-8.5833e-03,  1.1438e-02,  2.0800e-02],\n",
            "          [-1.6565e-02, -3.9587e-02,  1.2594e-02],\n",
            "          [-1.4314e-03, -5.4257e-03,  3.6794e-02]],\n",
            "\n",
            "         [[-1.3687e-02, -2.9514e-02, -1.4745e-02],\n",
            "          [ 2.8299e-02,  2.2096e-02,  3.4839e-03],\n",
            "          [-4.3521e-03, -2.6706e-03,  1.2258e-04]],\n",
            "\n",
            "         [[ 7.6403e-03,  2.0666e-02,  3.7429e-02],\n",
            "          [ 6.9478e-03,  4.3983e-02,  1.7538e-02],\n",
            "          [-9.7797e-03, -2.4789e-02, -1.1349e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 8.4439e-02,  8.4827e-02, -5.1478e-02],\n",
            "          [ 3.5253e-02, -1.1375e-03, -1.0331e-01],\n",
            "          [-6.4078e-02, -1.2660e-01, -1.2952e-01]],\n",
            "\n",
            "         [[ 1.0628e-03, -1.4083e-02,  4.7109e-03],\n",
            "          [-2.1059e-02, -2.8778e-02,  9.9708e-03],\n",
            "          [ 1.4074e-02,  1.8691e-02,  5.8192e-02]],\n",
            "\n",
            "         [[ 2.2139e-02,  8.9027e-03,  1.4790e-02],\n",
            "          [-1.7497e-02, -5.3924e-03,  2.7834e-02],\n",
            "          [-1.3855e-02, -1.3346e-02,  1.7668e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.8032e-02, -2.3097e-02, -7.1775e-03],\n",
            "          [-3.5089e-02,  1.0861e-02,  1.3640e-02],\n",
            "          [ 6.3449e-04,  9.7476e-03,  7.3670e-03]],\n",
            "\n",
            "         [[-4.4184e-02, -1.6190e-02,  1.2243e-02],\n",
            "          [-4.0349e-02, -1.7894e-02,  2.8911e-02],\n",
            "          [-6.5176e-03, -1.0490e-02,  9.1658e-03]],\n",
            "\n",
            "         [[ 4.3621e-03,  1.3119e-02,  1.8442e-03],\n",
            "          [ 1.1555e-02, -1.3031e-02, -9.5657e-03],\n",
            "          [-2.3314e-02,  1.1609e-03,  2.6771e-03]]],\n",
            "\n",
            "\n",
            "        [[[-2.1180e-02, -6.2213e-03,  1.7609e-03],\n",
            "          [-4.7424e-03,  1.1101e-02,  1.1296e-02],\n",
            "          [-1.4529e-02,  2.9843e-02,  2.4383e-03]],\n",
            "\n",
            "         [[ 6.9183e-03,  9.2937e-03,  3.0078e-02],\n",
            "          [-4.2612e-03,  4.9560e-03, -4.7338e-03],\n",
            "          [ 3.1360e-02,  1.9035e-03, -4.7242e-03]],\n",
            "\n",
            "         [[-3.6726e-02,  5.7285e-03,  1.3919e-01],\n",
            "          [-4.2992e-02,  9.4023e-04,  7.7141e-02],\n",
            "          [-5.0050e-02, -4.9479e-03,  2.4693e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.7203e-02,  7.4712e-03, -4.2659e-02],\n",
            "          [-8.1729e-03, -9.2536e-02, -5.4934e-03],\n",
            "          [-2.5927e-02,  8.3993e-04,  7.4632e-02]],\n",
            "\n",
            "         [[ 1.8076e-02,  4.5272e-03, -1.3757e-02],\n",
            "          [-1.8939e-02, -3.2739e-02, -2.9666e-02],\n",
            "          [-2.0608e-02, -4.6167e-03,  1.3080e-03]],\n",
            "\n",
            "         [[-1.2078e-02, -2.0285e-03, -1.6998e-02],\n",
            "          [-3.4805e-02, -4.9195e-02, -3.1973e-02],\n",
            "          [-2.1021e-02, -5.1164e-03, -4.8522e-03]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 3.1791e-02,  2.2948e-02,  1.0390e-02],\n",
            "          [-1.2628e-02, -2.9320e-03,  4.2645e-03],\n",
            "          [-2.1707e-02, -1.0856e-02,  1.6094e-02]],\n",
            "\n",
            "         [[-1.4525e-03, -1.0131e-02, -4.6862e-04],\n",
            "          [ 2.2130e-02,  2.2736e-02,  5.0183e-03],\n",
            "          [-6.0125e-02, -4.3150e-02, -4.4480e-02]],\n",
            "\n",
            "         [[ 3.0761e-03,  3.4396e-03,  6.0877e-03],\n",
            "          [-1.3683e-02,  4.0576e-03, -2.6544e-02],\n",
            "          [ 6.8231e-02,  6.3474e-02, -9.3660e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.8752e-02,  1.9400e-02,  4.1691e-02],\n",
            "          [ 8.7770e-03,  8.2394e-04,  1.8619e-02],\n",
            "          [ 1.8796e-02,  6.2238e-02, -2.3801e-02]],\n",
            "\n",
            "         [[-2.9788e-02, -3.4598e-02, -2.5225e-02],\n",
            "          [ 8.4234e-03, -2.3222e-02, -9.4612e-03],\n",
            "          [ 6.9035e-03,  6.9737e-02, -1.3359e-02]],\n",
            "\n",
            "         [[ 2.6981e-03, -4.3182e-02, -1.6731e-02],\n",
            "          [ 2.5812e-02, -7.2025e-02, -6.5399e-02],\n",
            "          [ 4.6257e-02,  2.9469e-02, -1.5811e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.1079e-02,  3.8220e-02,  8.3305e-03],\n",
            "          [-5.9912e-03,  3.5584e-02, -1.7534e-03],\n",
            "          [ 1.8735e-02,  7.0859e-03, -3.5151e-03]],\n",
            "\n",
            "         [[-4.5937e-02, -7.4695e-02, -5.3608e-02],\n",
            "          [-8.6266e-03,  9.0894e-03, -3.0345e-02],\n",
            "          [-2.8158e-02, -2.1204e-02, -8.4730e-03]],\n",
            "\n",
            "         [[-7.1772e-02, -6.8582e-02,  2.5544e-02],\n",
            "          [ 5.0363e-02,  2.5269e-02,  5.6668e-02],\n",
            "          [ 2.6238e-03,  1.3871e-03, -8.4692e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.9644e-02,  1.0896e-02, -3.0402e-02],\n",
            "          [ 1.5095e-03,  5.0455e-02,  1.5597e-02],\n",
            "          [-2.1015e-02, -1.0757e-02, -3.4942e-02]],\n",
            "\n",
            "         [[-2.7573e-02,  2.9707e-02, -2.9490e-02],\n",
            "          [ 2.3301e-03, -3.9011e-02,  6.8010e-03],\n",
            "          [ 4.4006e-02,  3.5397e-02,  7.9087e-02]],\n",
            "\n",
            "         [[-2.7480e-02,  5.0337e-02,  1.4290e-02],\n",
            "          [-5.2482e-02, -4.7748e-03,  1.2988e-02],\n",
            "          [-1.8935e-02, -3.0808e-02, -1.7583e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.2280e-02,  4.7408e-02,  3.4054e-02],\n",
            "          [ 2.1445e-02,  3.8987e-03,  4.6985e-04],\n",
            "          [ 1.5159e-02,  8.2067e-03,  3.2426e-02]],\n",
            "\n",
            "         [[ 9.2653e-03,  2.3661e-02,  4.2089e-02],\n",
            "          [ 2.1976e-02,  4.6128e-02,  1.1402e-02],\n",
            "          [ 7.2843e-03,  5.2285e-02,  8.6340e-03]],\n",
            "\n",
            "         [[ 1.4022e-02,  1.2800e-02,  3.5398e-02],\n",
            "          [-4.4398e-02,  1.7399e-02, -1.5838e-02],\n",
            "          [ 3.1712e-02,  5.8679e-02, -9.3244e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.8399e-03,  7.8628e-03, -5.6169e-04],\n",
            "          [ 8.0402e-03,  1.7392e-02,  7.8734e-03],\n",
            "          [-1.7713e-02, -4.5957e-02, -9.8762e-03]],\n",
            "\n",
            "         [[-9.7569e-03, -7.5795e-03, -2.4627e-02],\n",
            "          [-8.2454e-03,  6.3065e-02, -3.2954e-03],\n",
            "          [-7.7549e-03, -1.3404e-04, -8.1337e-03]],\n",
            "\n",
            "         [[ 1.7664e-02,  1.0114e-02,  4.2687e-03],\n",
            "          [-3.7950e-03,  2.6715e-02,  2.0121e-02],\n",
            "          [ 1.6868e-02, -6.6515e-03, -1.1107e-02]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.3323, 0.2908, 0.3246, 0.3435, 0.3011, 0.3054, 0.3041, 0.3539, 0.2862,\n",
            "        0.3601, 0.2970, 0.3381, 0.2565, 0.3276, 0.3030, 0.4085, 0.3519, 0.4218,\n",
            "        0.3055, 0.2551, 0.3425, 0.3215, 0.3366, 0.2700, 0.2849, 0.3954, 0.3166,\n",
            "        0.3286, 0.3515, 0.3953, 0.2768, 0.3625, 0.1988, 0.2717, 0.3355, 0.2797,\n",
            "        0.2510, 0.3832, 0.3266, 0.3263, 0.3681, 0.3401, 0.3651, 0.3391, 0.3071,\n",
            "        0.3231, 0.3691, 0.2410, 0.3536, 0.3189, 0.3238, 0.3611, 0.3086, 0.3309,\n",
            "        0.3886, 0.4362, 0.4550, 0.2962, 0.3071, 0.3386, 0.3317, 0.3228, 0.2393,\n",
            "        0.3147, 0.2738, 0.3218, 0.3198, 0.3411, 0.3611, 0.2833, 0.3035, 0.3183,\n",
            "        0.3146, 0.3890, 0.2607, 0.3479, 0.3236, 0.3709, 0.2592, 0.3742, 0.2555,\n",
            "        0.2966, 0.3505, 0.3165, 0.2808, 0.2660, 0.2817, 0.4795, 0.3372, 0.2723,\n",
            "        0.2955, 0.3225, 0.2470, 0.3160, 0.3515, 0.3131, 0.3372, 0.2837, 0.3540,\n",
            "        0.2897, 0.2490, 0.3019, 0.3114, 0.3510, 0.3022, 0.3617, 0.2859, 0.2831,\n",
            "        0.3243, 0.2769, 0.3314, 0.2394, 0.2932, 0.2788, 0.2686, 0.3194, 0.3542,\n",
            "        0.2683, 0.2955, 0.2924, 0.3538, 0.4256, 0.3603, 0.3013, 0.2763, 0.4354,\n",
            "        0.3991, 0.2694], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1735, -0.2337, -0.3383, -0.0806, -0.1920, -0.0621, -0.1885, -0.2830,\n",
            "        -0.1680, -0.1796, -0.2645, -0.1983, -0.1183, -0.2432, -0.1706, -0.3090,\n",
            "        -0.2661, -0.4040, -0.1949, -0.1392, -0.2449, -0.1242, -0.2012, -0.1901,\n",
            "        -0.1014, -0.3468, -0.2245, -0.3272, -0.3057, -0.3289, -0.1532, -0.1967,\n",
            "        -0.0667, -0.3281, -0.1418, -0.1527, -0.0987, -0.3243, -0.2252, -0.3462,\n",
            "        -0.2284, -0.2263, -0.1810, -0.1564, -0.1730, -0.1507, -0.2913, -0.1643,\n",
            "        -0.1998, -0.1532, -0.2211, -0.2247, -0.0913, -0.1563, -0.2453, -0.4854,\n",
            "        -0.4428, -0.1021, -0.1615, -0.2125, -0.2239, -0.1952, -0.0447, -0.1733,\n",
            "        -0.1178, -0.4775, -0.2110, -0.2305, -0.1795, -0.1582, -0.2008, -0.2041,\n",
            "        -0.1974, -0.2750, -0.0395, -0.2161, -0.2786, -0.2626, -0.0997, -0.2953,\n",
            "        -0.1431, -0.1448, -0.1894, -0.1283, -0.1807, -0.1144, -0.1308, -0.4154,\n",
            "        -0.2324, -0.1376, -0.1154, -0.2099, -0.0966, -0.1669, -0.3835, -0.2545,\n",
            "        -0.1603, -0.1904, -0.2420, -0.1658, -0.1133, -0.1498, -0.1213, -0.2318,\n",
            "        -0.2017, -0.3827, -0.1491, -0.1174, -0.1261, -0.2031, -0.1832, -0.2274,\n",
            "        -0.1281, -0.2557, -0.1400, -0.0723, -0.2212, -0.1486, -0.2914, -0.1116,\n",
            "        -0.2194, -0.4898, -0.3693, -0.1437, -0.1232, -0.3723, -0.6794, -0.1536],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-1.6153e-02,  5.0134e-03, -9.0186e-04],\n",
            "          [-8.8386e-03, -1.9390e-02, -2.4174e-02],\n",
            "          [ 6.3052e-03,  1.0245e-02, -1.3816e-02]],\n",
            "\n",
            "         [[-1.0979e-02,  2.6164e-03,  2.3656e-02],\n",
            "          [-1.7687e-02,  1.9861e-02,  6.4150e-02],\n",
            "          [ 6.0224e-03,  7.6342e-02,  1.0215e-01]],\n",
            "\n",
            "         [[-8.1113e-03,  6.8414e-03,  2.5436e-02],\n",
            "          [-8.0696e-03,  9.2929e-03,  8.2899e-03],\n",
            "          [ 7.7306e-03,  1.2159e-02,  7.1625e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.5175e-02,  6.2196e-03,  2.1798e-02],\n",
            "          [-1.5199e-02, -8.5439e-02, -2.4713e-02],\n",
            "          [-1.8460e-02, -4.9767e-02, -1.6818e-03]],\n",
            "\n",
            "         [[ 3.0728e-02,  3.9962e-02,  3.1253e-02],\n",
            "          [-1.8738e-02, -6.7510e-02, -2.7649e-02],\n",
            "          [ 2.8429e-02,  3.1854e-02,  1.0543e-02]],\n",
            "\n",
            "         [[-1.8320e-02, -1.5854e-02, -1.0685e-02],\n",
            "          [-2.7442e-02, -3.0616e-02, -1.0485e-02],\n",
            "          [-1.5122e-02, -1.0595e-02, -2.5322e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.6868e-03,  3.0996e-02,  4.2763e-02],\n",
            "          [ 4.6537e-02,  4.8606e-02,  2.3800e-03],\n",
            "          [ 1.6654e-02,  1.2900e-02, -1.8230e-02]],\n",
            "\n",
            "         [[-1.0441e-02, -1.5934e-03, -1.6128e-02],\n",
            "          [-1.2799e-02,  4.9570e-03, -1.4585e-02],\n",
            "          [-2.3553e-02, -3.7023e-03, -1.4399e-02]],\n",
            "\n",
            "         [[ 1.0338e-02, -1.7560e-02, -3.3046e-02],\n",
            "          [-3.2090e-02, -5.9258e-03,  2.0201e-03],\n",
            "          [-4.1428e-02,  4.9121e-03,  1.6906e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.9525e-02, -4.6498e-02, -5.9916e-02],\n",
            "          [-2.6670e-02, -1.9079e-02, -2.9419e-02],\n",
            "          [-3.9683e-03,  1.9405e-02,  7.3317e-03]],\n",
            "\n",
            "         [[ 1.4293e-02,  1.5643e-02,  5.8117e-04],\n",
            "          [ 5.1493e-03,  7.4332e-03, -3.6928e-03],\n",
            "          [-1.3522e-02, -8.5536e-03, -2.1259e-03]],\n",
            "\n",
            "         [[-3.0908e-02, -1.9839e-02, -1.9375e-02],\n",
            "          [-1.0368e-02, -2.4294e-02,  2.4103e-04],\n",
            "          [-1.9275e-02, -2.9707e-02, -1.5623e-02]]],\n",
            "\n",
            "\n",
            "        [[[-4.9212e-02, -2.9588e-02,  8.8023e-02],\n",
            "          [ 4.7453e-03,  4.3564e-02,  9.3115e-02],\n",
            "          [ 7.4083e-02,  4.2868e-02, -5.1033e-02]],\n",
            "\n",
            "         [[ 6.6992e-03,  2.1676e-02, -5.4254e-04],\n",
            "          [ 1.9286e-02,  1.0920e-02, -4.5440e-03],\n",
            "          [ 3.1075e-02, -1.7168e-03, -2.7603e-02]],\n",
            "\n",
            "         [[ 6.0096e-02, -2.9359e-02, -5.8911e-02],\n",
            "          [-1.9133e-02, -8.1624e-02, -2.2553e-02],\n",
            "          [ 1.1597e-02,  2.5092e-02,  1.2130e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.4307e-03, -2.3130e-02,  9.6233e-03],\n",
            "          [-4.3785e-02, -2.6735e-02,  2.1993e-02],\n",
            "          [-3.5919e-02, -4.1009e-02, -2.1860e-02]],\n",
            "\n",
            "         [[ 3.3705e-02,  6.2938e-02,  4.3502e-02],\n",
            "          [ 1.1111e-03,  1.9243e-02, -1.9707e-03],\n",
            "          [-1.1493e-02, -5.3445e-02, -9.6676e-03]],\n",
            "\n",
            "         [[-2.6664e-03, -2.6954e-02, -1.7667e-02],\n",
            "          [-8.3382e-03,  8.9920e-03,  8.1260e-04],\n",
            "          [-2.6832e-02, -3.5991e-02, -4.2495e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1.8876e-03, -2.2728e-02, -4.2991e-03],\n",
            "          [-9.2231e-03, -3.4333e-02, -1.3392e-02],\n",
            "          [-1.2774e-02, -1.1435e-02,  1.5617e-02]],\n",
            "\n",
            "         [[ 1.0703e-02,  1.2792e-02,  2.2662e-02],\n",
            "          [ 7.3185e-03, -1.7847e-02,  1.0674e-02],\n",
            "          [-1.5936e-02, -1.9318e-02,  2.1768e-02]],\n",
            "\n",
            "         [[-7.3009e-03,  3.0234e-02, -1.1899e-02],\n",
            "          [-2.6099e-02,  3.7452e-03,  3.2776e-02],\n",
            "          [-3.3101e-02, -7.1923e-03,  1.6559e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.2818e-02, -1.0021e-01, -4.7012e-02],\n",
            "          [ 2.8293e-03,  4.1410e-02, -1.1391e-02],\n",
            "          [-1.1152e-02, -5.5861e-03,  1.9968e-02]],\n",
            "\n",
            "         [[-2.3932e-02, -3.0687e-02, -1.1756e-03],\n",
            "          [ 1.5311e-03, -3.5002e-02, -2.4414e-02],\n",
            "          [-8.7575e-03, -7.7842e-02, -3.8842e-02]],\n",
            "\n",
            "         [[ 2.6107e-02,  1.5406e-02,  1.7569e-02],\n",
            "          [-1.5130e-02, -4.8687e-03,  3.0773e-03],\n",
            "          [-1.3470e-02, -9.3201e-03, -4.8982e-03]]],\n",
            "\n",
            "\n",
            "        [[[-2.0228e-02, -3.0006e-02, -9.8419e-03],\n",
            "          [-3.8676e-02, -3.3481e-02, -7.4265e-03],\n",
            "          [-2.8935e-02, -3.2037e-02,  2.9245e-03]],\n",
            "\n",
            "         [[-1.2900e-02,  3.8046e-03,  1.5940e-02],\n",
            "          [-2.4030e-02,  2.0666e-03,  5.7250e-03],\n",
            "          [ 6.9989e-03,  1.2192e-02,  1.5406e-02]],\n",
            "\n",
            "         [[-1.5018e-02, -9.0988e-03,  2.4450e-02],\n",
            "          [ 1.0039e-02,  1.2561e-02,  2.6997e-02],\n",
            "          [ 2.9556e-02,  1.9463e-02, -2.6584e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.8481e-02,  3.9417e-04,  9.9768e-03],\n",
            "          [-4.5447e-03,  1.2307e-02,  3.5507e-02],\n",
            "          [-1.1873e-03, -2.6185e-03,  1.1547e-02]],\n",
            "\n",
            "         [[ 4.6292e-03, -1.3690e-02, -1.0171e-02],\n",
            "          [ 1.2104e-02,  1.6793e-02,  1.3003e-02],\n",
            "          [ 1.3328e-03,  3.4701e-03,  1.7323e-02]],\n",
            "\n",
            "         [[-8.7332e-05,  5.8646e-03, -3.5117e-03],\n",
            "          [ 3.8112e-03, -7.1828e-03, -1.1407e-02],\n",
            "          [ 1.9705e-02,  2.0556e-02,  5.7084e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 3.6998e-02,  3.2616e-02, -9.4535e-04],\n",
            "          [-2.9484e-02, -2.3441e-02, -2.8085e-02],\n",
            "          [-2.5451e-02,  3.9048e-02,  3.6686e-02]],\n",
            "\n",
            "         [[-1.8732e-02, -1.5352e-02,  1.1149e-02],\n",
            "          [-2.1324e-03, -2.3177e-02,  1.7628e-02],\n",
            "          [-4.0012e-03,  1.5463e-02,  9.2496e-03]],\n",
            "\n",
            "         [[-2.9346e-02,  7.7071e-03, -5.6520e-03],\n",
            "          [-2.3611e-02, -1.9390e-03,  2.0221e-02],\n",
            "          [ 8.0955e-03, -2.3268e-02, -2.8827e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.3532e-02, -2.9092e-02, -4.0045e-02],\n",
            "          [ 2.6530e-03, -2.0568e-02,  1.3075e-02],\n",
            "          [ 1.6061e-02, -5.5725e-02, -4.9167e-02]],\n",
            "\n",
            "         [[-7.9132e-03,  2.1466e-02,  2.0913e-02],\n",
            "          [-1.7259e-02, -2.5851e-02,  2.7177e-03],\n",
            "          [-4.6532e-02, -2.4846e-02, -1.9911e-02]],\n",
            "\n",
            "         [[-5.0350e-02, -2.5574e-02,  1.7763e-02],\n",
            "          [-3.4474e-02,  5.5247e-03, -2.7754e-02],\n",
            "          [-2.0743e-02, -2.2332e-02, -4.3512e-02]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.1194, 0.1625, 0.3084, 0.2931, 0.2957, 0.5263, 0.4038, 0.2024, 0.3401,\n",
            "        0.1982, 0.2559, 0.2311, 0.1630, 0.2891, 0.2248, 0.2311, 0.2417, 0.2187,\n",
            "        0.1922, 0.3103, 0.2015, 0.4802, 0.2481, 0.3898, 0.3204, 0.4035, 0.2617,\n",
            "        0.1551, 0.2256, 0.2117, 0.2708, 0.3537, 0.2505, 0.1843, 0.2465, 0.6501,\n",
            "        0.3898, 0.4289, 0.1799, 0.1604, 0.1775, 0.3600, 0.2694, 0.1283, 0.1662,\n",
            "        0.1716, 0.1837, 0.1710, 0.4178, 0.3249, 0.1759, 0.4717, 0.4115, 0.1995,\n",
            "        0.2025, 0.1492, 0.2860, 0.1072, 0.3649, 0.1906, 0.5369, 0.2400, 0.4411,\n",
            "        0.1702, 0.1993, 0.2045, 0.1972, 0.4041, 0.3034, 0.6168, 0.2284, 0.3228,\n",
            "        0.4547, 0.4370, 0.1570, 0.4057, 0.5791, 0.2338, 0.1586, 0.3130, 0.2201,\n",
            "        0.3195, 0.1166, 0.2517, 0.2184, 0.0989, 0.3116, 0.2613, 0.3277, 0.1778,\n",
            "        0.2718, 0.4174, 0.5140, 0.2136, 0.1905, 0.2898, 0.2472, 0.1341, 0.6212,\n",
            "        0.1810, 0.2394, 0.1417, 0.1759, 0.2827, 0.1987, 0.3775, 0.3749, 0.1274,\n",
            "        0.3656, 0.4305, 0.4212, 0.2673, 0.2016, 0.5098, 0.1449, 0.4408, 0.3583,\n",
            "        0.2503, 0.5682, 0.2518, 0.1392, 0.0617, 0.3406, 0.1313, 0.4586, 0.2914,\n",
            "        0.1326, 0.3915], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1403, -0.0889, -0.4147, -0.2264, -0.0737, -0.3534, -0.3379, -0.0752,\n",
            "        -0.1791,  0.0448, -0.2842, -0.1765, -0.1591, -0.0675, -0.1543, -0.1061,\n",
            "        -0.2334, -0.0981, -0.0908, -0.0567, -0.1908, -0.2055, -0.2704, -0.1883,\n",
            "        -0.3570, -0.1125, -0.1632, -0.0211, -0.1687, -0.2124, -0.1713, -0.0872,\n",
            "        -0.2194, -0.1888, -0.2954, -0.4570, -0.0226, -0.0527,  0.0406, -0.0609,\n",
            "        -0.0456, -0.1176, -0.0145,  0.0318, -0.2046, -0.0953, -0.0496, -0.1051,\n",
            "        -0.0793, -0.1933, -0.1467, -0.3215, -0.3257, -0.2287, -0.0356, -0.1869,\n",
            "        -0.1932, -0.0771,  0.2768, -0.0656, -0.0895, -0.2548, -0.2365,  0.0021,\n",
            "        -0.0987, -0.3178,  0.1613,  0.0006, -0.2347, -0.4150, -0.1310, -0.3142,\n",
            "        -0.2582, -0.5400,  0.0772, -0.2546, -0.4454, -0.0262, -0.0937, -0.2201,\n",
            "        -0.2044, -0.0155, -0.0893, -0.2167,  0.1112, -0.0619, -0.1217, -0.1593,\n",
            "        -0.1317, -0.1717, -0.3729, -0.3354, -0.3414,  0.0358, -0.2067, -0.1087,\n",
            "         0.0141, -0.0338, -0.2129, -0.1122, -0.1627, -0.2000,  0.0908, -0.0041,\n",
            "        -0.1313, -0.2942,  0.0160, -0.1065, -0.1289, -0.1699, -0.1721, -0.1809,\n",
            "        -0.2295, -0.3611, -0.1746, -0.3540, -0.1554, -0.2709, -0.2607,  0.0084,\n",
            "        -0.0311, -0.0022, -0.0831,  0.0380, -0.4893, -0.2749,  0.1245, -0.1272],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-0.0159, -0.0166, -0.0159],\n",
            "          [-0.0053,  0.0151,  0.0099],\n",
            "          [-0.0149,  0.0004, -0.0114]],\n",
            "\n",
            "         [[-0.0095, -0.0186, -0.0061],\n",
            "          [ 0.0098, -0.0123, -0.0053],\n",
            "          [ 0.0071, -0.0161, -0.0071]],\n",
            "\n",
            "         [[-0.0227, -0.0377, -0.0337],\n",
            "          [-0.0316, -0.0580, -0.0391],\n",
            "          [-0.0346, -0.0388, -0.0157]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0049,  0.0080,  0.0144],\n",
            "          [-0.0015,  0.0242,  0.0056],\n",
            "          [-0.0044,  0.0062,  0.0069]],\n",
            "\n",
            "         [[ 0.0160, -0.0120, -0.0013],\n",
            "          [ 0.0096,  0.0057,  0.0016],\n",
            "          [-0.0099, -0.0136, -0.0064]],\n",
            "\n",
            "         [[ 0.0534,  0.0464,  0.0248],\n",
            "          [ 0.0341, -0.0029, -0.0041],\n",
            "          [-0.0140, -0.0046, -0.0142]]],\n",
            "\n",
            "\n",
            "        [[[-0.0012, -0.0186, -0.0345],\n",
            "          [ 0.0050, -0.0117, -0.0333],\n",
            "          [ 0.0060, -0.0162, -0.0175]],\n",
            "\n",
            "         [[ 0.0107,  0.0140, -0.0192],\n",
            "          [ 0.0029,  0.0126,  0.0073],\n",
            "          [-0.0168, -0.0187, -0.0014]],\n",
            "\n",
            "         [[-0.0219,  0.0063,  0.0159],\n",
            "          [-0.0035,  0.0057,  0.0311],\n",
            "          [ 0.0023,  0.0032,  0.0175]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0162, -0.0078,  0.0077],\n",
            "          [-0.0015, -0.0122, -0.0093],\n",
            "          [ 0.0009, -0.0022,  0.0074]],\n",
            "\n",
            "         [[ 0.0029, -0.0236,  0.0058],\n",
            "          [ 0.0143, -0.0169, -0.0062],\n",
            "          [-0.0077, -0.0323, -0.0337]],\n",
            "\n",
            "         [[ 0.0087,  0.0140,  0.0081],\n",
            "          [-0.0034,  0.0105,  0.0150],\n",
            "          [ 0.0189,  0.0309,  0.0256]]],\n",
            "\n",
            "\n",
            "        [[[-0.0358, -0.0226, -0.0144],\n",
            "          [-0.0075, -0.0221,  0.0111],\n",
            "          [-0.0036, -0.0148, -0.0164]],\n",
            "\n",
            "         [[-0.0146, -0.0307, -0.0204],\n",
            "          [-0.0285, -0.0453, -0.0579],\n",
            "          [ 0.0288, -0.0152, -0.0245]],\n",
            "\n",
            "         [[ 0.0174,  0.0199, -0.0046],\n",
            "          [ 0.0178,  0.0236,  0.0136],\n",
            "          [ 0.0293,  0.0434,  0.0186]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0079, -0.0122, -0.0158],\n",
            "          [-0.0107, -0.0314, -0.0075],\n",
            "          [-0.0070, -0.0173, -0.0308]],\n",
            "\n",
            "         [[-0.0103, -0.0118, -0.0171],\n",
            "          [-0.0264, -0.0015,  0.0280],\n",
            "          [-0.0089,  0.0054,  0.0097]],\n",
            "\n",
            "         [[ 0.0145, -0.0315, -0.0197],\n",
            "          [-0.0149, -0.0177,  0.0077],\n",
            "          [ 0.0125,  0.0071, -0.0062]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0053,  0.0255,  0.0071],\n",
            "          [ 0.0197,  0.0270,  0.0420],\n",
            "          [ 0.0412,  0.0223,  0.0350]],\n",
            "\n",
            "         [[ 0.0026,  0.0041,  0.0106],\n",
            "          [ 0.0046,  0.0203,  0.0081],\n",
            "          [ 0.0145, -0.0030, -0.0197]],\n",
            "\n",
            "         [[-0.0040,  0.0255,  0.0023],\n",
            "          [-0.0152,  0.0260,  0.0083],\n",
            "          [-0.0021,  0.0269,  0.0032]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0055, -0.0049,  0.0183],\n",
            "          [ 0.0085,  0.0023,  0.0077],\n",
            "          [ 0.0082,  0.0096,  0.0215]],\n",
            "\n",
            "         [[-0.0177,  0.0099, -0.0129],\n",
            "          [-0.0127,  0.0096, -0.0124],\n",
            "          [ 0.0090,  0.0493,  0.0362]],\n",
            "\n",
            "         [[ 0.0123,  0.0184, -0.0178],\n",
            "          [ 0.0058, -0.0058, -0.0117],\n",
            "          [ 0.0034, -0.0103, -0.0425]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0316,  0.0765,  0.0443],\n",
            "          [ 0.0940,  0.1480,  0.1510],\n",
            "          [ 0.0665,  0.1386,  0.1132]],\n",
            "\n",
            "         [[-0.0282, -0.0041, -0.0200],\n",
            "          [-0.0193, -0.0012,  0.0107],\n",
            "          [-0.0165, -0.0028,  0.0008]],\n",
            "\n",
            "         [[-0.0122, -0.0322, -0.0153],\n",
            "          [-0.0009, -0.0123, -0.0039],\n",
            "          [-0.0193, -0.0107, -0.0211]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0030,  0.0066, -0.0003],\n",
            "          [ 0.0146, -0.0167,  0.0142],\n",
            "          [ 0.0227,  0.0120,  0.0072]],\n",
            "\n",
            "         [[ 0.0039, -0.0197,  0.0111],\n",
            "          [-0.0251, -0.0384, -0.0447],\n",
            "          [-0.0359, -0.0981, -0.0684]],\n",
            "\n",
            "         [[-0.0085,  0.0023,  0.0031],\n",
            "          [ 0.0038,  0.0182,  0.0069],\n",
            "          [ 0.0089,  0.0080,  0.0126]]],\n",
            "\n",
            "\n",
            "        [[[-0.0130, -0.0090,  0.0011],\n",
            "          [-0.0260, -0.0195, -0.0094],\n",
            "          [ 0.0048,  0.0024,  0.0106]],\n",
            "\n",
            "         [[-0.0025, -0.0140, -0.0286],\n",
            "          [-0.0024,  0.0011, -0.0233],\n",
            "          [ 0.0123,  0.0008,  0.0014]],\n",
            "\n",
            "         [[-0.0490, -0.0439, -0.0579],\n",
            "          [-0.0359, -0.0365, -0.0386],\n",
            "          [-0.0410, -0.0333, -0.0137]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0118, -0.0081, -0.0158],\n",
            "          [-0.0272, -0.0285,  0.0075],\n",
            "          [-0.0244,  0.0139,  0.0061]],\n",
            "\n",
            "         [[-0.0273,  0.0197,  0.0222],\n",
            "          [-0.0376,  0.0208,  0.0187],\n",
            "          [-0.0446, -0.0044, -0.0168]],\n",
            "\n",
            "         [[ 0.0199,  0.0268,  0.0120],\n",
            "          [ 0.0203,  0.0215,  0.0025],\n",
            "          [ 0.0002, -0.0076, -0.0196]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.2856, 0.2425, 0.3032, 0.3168, 0.3011, 0.3475, 0.3076, 0.3105, 0.3646,\n",
            "        0.3255, 0.2195, 0.3167, 0.2674, 0.3104, 0.3026, 0.3443, 0.2915, 0.3379,\n",
            "        0.2887, 0.2996, 0.3588, 0.3164, 0.2882, 0.2917, 0.3492, 0.3749, 0.3587,\n",
            "        0.3166, 0.2756, 0.2978, 0.3364, 0.2893, 0.3106, 0.2506, 0.3460, 0.3621,\n",
            "        0.2570, 0.3695, 0.2935, 0.3286, 0.3243, 0.3188, 0.3093, 0.3314, 0.3550,\n",
            "        0.2978, 0.2737, 0.3023, 0.3179, 0.2831, 0.3065, 0.3390, 0.3053, 0.3099,\n",
            "        0.3017, 0.3472, 0.3034, 0.2935, 0.3352, 0.3676, 0.3163, 0.3404, 0.3078,\n",
            "        0.2819, 0.3794, 0.3083, 0.2778, 0.3363, 0.2284, 0.3259, 0.2790, 0.3072,\n",
            "        0.2975, 0.3847, 0.3372, 0.2253, 0.2827, 0.3737, 0.2796, 0.3485, 0.3879,\n",
            "        0.3288, 0.3340, 0.3335, 0.2756, 0.3500, 0.2897, 0.2798, 0.2907, 0.3220,\n",
            "        0.3824, 0.3522, 0.3278, 0.3689, 0.3147, 0.3600, 0.3123, 0.2519, 0.2355,\n",
            "        0.3211, 0.3203, 0.3345, 0.2768, 0.3341, 0.3153, 0.3175, 0.2224, 0.2956,\n",
            "        0.3206, 0.2658, 0.3662, 0.2715, 0.3655, 0.3427, 0.2820, 0.2754, 0.4669,\n",
            "        0.3090, 0.3468, 0.3144, 0.3220, 0.2765, 0.3301, 0.3219, 0.3152, 0.2813,\n",
            "        0.2497, 0.3514, 0.3264, 0.3014, 0.2734, 0.3522, 0.3831, 0.3028, 0.2940,\n",
            "        0.2825, 0.3099, 0.2373, 0.2705, 0.4189, 0.2985, 0.3841, 0.2754, 0.3091,\n",
            "        0.3169, 0.2824, 0.2749, 0.3493, 0.4018, 0.3108, 0.2176, 0.2821, 0.3199,\n",
            "        0.3358, 0.2468, 0.3332, 0.2876, 0.2964, 0.2385, 0.3451, 0.3081, 0.2760,\n",
            "        0.2533, 0.2576, 0.3092, 0.2950, 0.3089, 0.3113, 0.3475, 0.3172, 0.2474,\n",
            "        0.3371, 0.3450, 0.3189, 0.3150, 0.3008, 0.2694, 0.3730, 0.3235, 0.2988,\n",
            "        0.2812, 0.3245, 0.3630, 0.2843, 0.3533, 0.3451, 0.3244, 0.3524, 0.3118,\n",
            "        0.3429, 0.3215, 0.2748, 0.3287, 0.3656, 0.2901, 0.2523, 0.3284, 0.2523,\n",
            "        0.3426, 0.2851, 0.2918, 0.2497, 0.5159, 0.3026, 0.2743, 0.2379, 0.3524,\n",
            "        0.3394, 0.2264, 0.2652, 0.3759, 0.3777, 0.2459, 0.3046, 0.3067, 0.3775,\n",
            "        0.2976, 0.3552, 0.2696, 0.2649, 0.2872, 0.2985, 0.2867, 0.3676, 0.3494,\n",
            "        0.3823, 0.3246, 0.3567, 0.2662, 0.3357, 0.2935, 0.2987, 0.2664, 0.3019,\n",
            "        0.3175, 0.2436, 0.3274, 0.2764, 0.2466, 0.2876, 0.3060, 0.3157, 0.3329,\n",
            "        0.2984, 0.2961, 0.3309, 0.3729, 0.3238, 0.3491, 0.3342, 0.3037, 0.3578,\n",
            "        0.2849, 0.2827, 0.2809, 0.3249], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0915,  0.0189, -0.1235, -0.0613, -0.1003, -0.1306, -0.1473, -0.1079,\n",
            "        -0.2438, -0.1113,  0.1361, -0.1477,  0.0387, -0.0907,  0.0352, -0.1851,\n",
            "        -0.1319, -0.1746, -0.0815, -0.1004, -0.3394, -0.1712, -0.0807, -0.1228,\n",
            "        -0.2263, -0.1503, -0.2314, -0.2327, -0.0854, -0.0802, -0.0716, -0.0839,\n",
            "        -0.0592,  0.0358, -0.0322, -0.2197,  0.0027, -0.1471, -0.0264, -0.1886,\n",
            "        -0.2417, -0.1494, -0.1904, -0.1089, -0.2657, -0.1362, -0.0487, -0.1340,\n",
            "        -0.0930, -0.0064, -0.1721, -0.1476, -0.1714,  0.0336, -0.1011, -0.1761,\n",
            "        -0.1184, -0.0482, -0.3260, -0.1555, -0.0169, -0.2373, -0.1015, -0.1051,\n",
            "        -0.2738, -0.1917, -0.0503, -0.1098,  0.1484, -0.2282, -0.0700, -0.1427,\n",
            "        -0.1417, -0.3096, -0.2043,  0.0269, -0.0779, -0.0842, -0.0464, -0.1429,\n",
            "        -0.3917,  0.0257, -0.1779, -0.0993, -0.0507, -0.2222, -0.0951, -0.0861,\n",
            "        -0.0743, -0.1666, -0.2054, -0.1782, -0.1150, -0.2525, -0.0694, -0.0536,\n",
            "        -0.0499, -0.0311,  0.1212, -0.0988, -0.1570, -0.3093, -0.0797, -0.0994,\n",
            "        -0.1774, -0.0505,  0.0766, -0.0480, -0.1278, -0.0651, -0.1737,  0.0303,\n",
            "        -0.1334, -0.2435, -0.0746, -0.0365, -0.1843, -0.0887, -0.1924, -0.1110,\n",
            "        -0.1458, -0.0895, -0.0956, -0.2042, -0.1338, -0.0637, -0.0699, -0.1656,\n",
            "        -0.1521, -0.1317, -0.0826, -0.2470, -0.1174, -0.1475, -0.0840, -0.0681,\n",
            "        -0.1789,  0.0288, -0.0362, -0.3005, -0.1441, -0.0812, -0.0492, -0.0657,\n",
            "        -0.1249, -0.1104,  0.0187, -0.1351, -0.1944, -0.0909,  0.2067, -0.1081,\n",
            "        -0.2499, -0.0999,  0.0507, -0.1899, -0.0369, -0.1432,  0.1279, -0.1782,\n",
            "        -0.1172, -0.0099,  0.0785, -0.0681, -0.0365, -0.1596, -0.1606, -0.0922,\n",
            "        -0.1773, -0.1788,  0.0306, -0.1101, -0.1355, -0.2244, -0.0860, -0.1232,\n",
            "        -0.0927, -0.1666, -0.1393, -0.0898, -0.0614, -0.1740, -0.2503, -0.0593,\n",
            "        -0.1272, -0.1422, -0.0743, -0.2208, -0.2207, -0.2742, -0.1302, -0.0916,\n",
            "        -0.1696, -0.2481, -0.1524,  0.0410, -0.1077,  0.0408, -0.1915, -0.0697,\n",
            "        -0.1049, -0.0110, -0.3257, -0.1336, -0.1021,  0.0128, -0.2717, -0.1245,\n",
            "         0.0288, -0.1025, -0.2405, -0.1476,  0.1008, -0.0220, -0.0983, -0.4417,\n",
            "        -0.0774, -0.3207, -0.0272, -0.0726, -0.0608, -0.0430, -0.0872, -0.1280,\n",
            "        -0.1608, -0.1529, -0.1745, -0.1702, -0.0486, -0.1459, -0.0552, -0.0808,\n",
            "        -0.0264, -0.0952, -0.1126, -0.0452, -0.0837, -0.0331,  0.0127, -0.0865,\n",
            "        -0.1446, -0.0732, -0.2160, -0.0952, -0.1297, -0.2008, -0.2135, -0.2204,\n",
            "        -0.2381, -0.1787, -0.1386, -0.1901, -0.0981, -0.0850, -0.0761, -0.0586],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-0.0093, -0.0339, -0.0119],\n",
            "          [-0.0246, -0.0798, -0.0487],\n",
            "          [-0.0435, -0.0801, -0.0653]],\n",
            "\n",
            "         [[-0.0289,  0.0002, -0.0286],\n",
            "          [ 0.0099,  0.0103, -0.0177],\n",
            "          [-0.0107,  0.0028, -0.0125]],\n",
            "\n",
            "         [[-0.0147,  0.0226,  0.0044],\n",
            "          [ 0.0155,  0.0109, -0.0040],\n",
            "          [-0.0208, -0.0180, -0.0173]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0003, -0.0041, -0.0008],\n",
            "          [-0.0217, -0.0223, -0.0299],\n",
            "          [ 0.0105,  0.0035, -0.0114]],\n",
            "\n",
            "         [[ 0.0097,  0.0184,  0.0370],\n",
            "          [ 0.0037,  0.0104,  0.0152],\n",
            "          [ 0.0084,  0.0183,  0.0302]],\n",
            "\n",
            "         [[ 0.0014,  0.0084,  0.0097],\n",
            "          [ 0.0265,  0.0415,  0.0553],\n",
            "          [ 0.0169,  0.0610,  0.0563]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0117,  0.0165,  0.0051],\n",
            "          [ 0.0294,  0.0204,  0.0216],\n",
            "          [ 0.0079,  0.0133,  0.0117]],\n",
            "\n",
            "         [[-0.0153, -0.0213, -0.0090],\n",
            "          [-0.0292, -0.0516, -0.0436],\n",
            "          [-0.0045, -0.0372, -0.0420]],\n",
            "\n",
            "         [[ 0.0003,  0.0398, -0.0001],\n",
            "          [ 0.0129,  0.0332,  0.0163],\n",
            "          [-0.0096, -0.0057, -0.0170]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0447,  0.0082,  0.0169],\n",
            "          [-0.0066, -0.0371, -0.0059],\n",
            "          [-0.0239, -0.0607, -0.0289]],\n",
            "\n",
            "         [[-0.0057, -0.0423, -0.0219],\n",
            "          [-0.0228, -0.0314, -0.0583],\n",
            "          [-0.0196, -0.0530, -0.0485]],\n",
            "\n",
            "         [[ 0.0065,  0.0033,  0.0093],\n",
            "          [ 0.0010, -0.0049, -0.0110],\n",
            "          [-0.0160, -0.0101, -0.0146]]],\n",
            "\n",
            "\n",
            "        [[[-0.0061, -0.0067, -0.0069],\n",
            "          [-0.0052, -0.0089, -0.0143],\n",
            "          [-0.0115, -0.0171, -0.0237]],\n",
            "\n",
            "         [[ 0.0399,  0.0167,  0.0210],\n",
            "          [ 0.0165, -0.0262, -0.0116],\n",
            "          [ 0.0059, -0.0206, -0.0153]],\n",
            "\n",
            "         [[ 0.0060,  0.0242,  0.0207],\n",
            "          [ 0.0050, -0.0062,  0.0148],\n",
            "          [ 0.0099, -0.0279, -0.0054]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0097, -0.0017, -0.0101],\n",
            "          [-0.0006,  0.0434,  0.0381],\n",
            "          [ 0.0038,  0.0450,  0.0392]],\n",
            "\n",
            "         [[ 0.0134,  0.0069,  0.0196],\n",
            "          [ 0.0068,  0.0250,  0.0152],\n",
            "          [ 0.0018, -0.0044,  0.0037]],\n",
            "\n",
            "         [[-0.0174, -0.0163, -0.0240],\n",
            "          [-0.0197, -0.0174, -0.0178],\n",
            "          [-0.0300, -0.0137, -0.0211]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0066,  0.0061,  0.0284],\n",
            "          [-0.0130, -0.0509, -0.0204],\n",
            "          [-0.0154, -0.0149, -0.0181]],\n",
            "\n",
            "         [[ 0.0123,  0.0325,  0.0227],\n",
            "          [-0.0042, -0.0181,  0.0022],\n",
            "          [ 0.0029, -0.0174,  0.0032]],\n",
            "\n",
            "         [[-0.0110,  0.0111, -0.0142],\n",
            "          [ 0.0082,  0.0301,  0.0416],\n",
            "          [ 0.0006,  0.0002,  0.0213]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0008,  0.0266,  0.0118],\n",
            "          [-0.0159, -0.0074, -0.0051],\n",
            "          [ 0.0042, -0.0069, -0.0073]],\n",
            "\n",
            "         [[-0.0070,  0.0010,  0.0019],\n",
            "          [ 0.0061,  0.0510, -0.0035],\n",
            "          [-0.0081, -0.0303, -0.0174]],\n",
            "\n",
            "         [[-0.0100,  0.0003, -0.0011],\n",
            "          [-0.0015, -0.0297, -0.0195],\n",
            "          [-0.0010,  0.0049,  0.0151]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0161,  0.0126,  0.0055],\n",
            "          [-0.0132, -0.0196, -0.0208],\n",
            "          [-0.0077, -0.0230, -0.0202]],\n",
            "\n",
            "         [[-0.0019,  0.0530,  0.0134],\n",
            "          [ 0.0027,  0.0249,  0.0167],\n",
            "          [-0.0192, -0.0188, -0.0200]],\n",
            "\n",
            "         [[ 0.0081,  0.0300,  0.0006],\n",
            "          [ 0.0036,  0.0254, -0.0104],\n",
            "          [-0.0010, -0.0193, -0.0121]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0163,  0.0251,  0.0290],\n",
            "          [-0.0073, -0.0014,  0.0124],\n",
            "          [-0.0211, -0.0347, -0.0195]],\n",
            "\n",
            "         [[ 0.0165,  0.0519,  0.0494],\n",
            "          [ 0.0058,  0.0199,  0.0363],\n",
            "          [-0.0049, -0.0165, -0.0130]],\n",
            "\n",
            "         [[-0.0102, -0.0308, -0.0340],\n",
            "          [ 0.0055, -0.0109,  0.0005],\n",
            "          [ 0.0382,  0.0209,  0.0326]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0184, -0.0031,  0.0103],\n",
            "          [-0.0072, -0.0013, -0.0070],\n",
            "          [ 0.0217,  0.0011, -0.0033]],\n",
            "\n",
            "         [[-0.0166,  0.0002, -0.0092],\n",
            "          [ 0.0150,  0.0584,  0.0218],\n",
            "          [-0.0136,  0.0181,  0.0164]],\n",
            "\n",
            "         [[ 0.0219,  0.0316,  0.0183],\n",
            "          [-0.0021, -0.0156,  0.0203],\n",
            "          [ 0.0053,  0.0005,  0.0157]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0045, -0.0025,  0.0017],\n",
            "          [-0.0052, -0.0088,  0.0137],\n",
            "          [-0.0107,  0.0010,  0.0170]],\n",
            "\n",
            "         [[-0.0060, -0.0368,  0.0031],\n",
            "          [ 0.0102,  0.0291, -0.0007],\n",
            "          [ 0.0099,  0.0600,  0.0272]],\n",
            "\n",
            "         [[-0.0092, -0.0183,  0.0062],\n",
            "          [-0.0319, -0.0294, -0.0149],\n",
            "          [-0.0148, -0.0123, -0.0236]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.3212, 0.2124, 0.2661, 0.3594, 0.2785, 0.2582, 0.3108, 0.3096, 0.3348,\n",
            "        0.2992, 0.2545, 0.2458, 0.3133, 0.4159, 0.2997, 0.3070, 0.3135, 0.4418,\n",
            "        0.3743, 0.2570, 0.2943, 0.3078, 0.2738, 0.3948, 0.2928, 0.3572, 0.3435,\n",
            "        0.5379, 0.4243, 0.3908, 0.2745, 0.2798, 0.3217, 0.1956, 0.2751, 0.3187,\n",
            "        0.3507, 0.2751, 0.1919, 0.3307, 0.2850, 0.3038, 0.2179, 0.2652, 0.2944,\n",
            "        0.2138, 0.2184, 0.2948, 0.3262, 0.3759, 0.2557, 0.3796, 0.2950, 0.3386,\n",
            "        0.3243, 0.3070, 0.3331, 0.2302, 0.3036, 0.3377, 0.2922, 0.2204, 0.3267,\n",
            "        0.3198, 0.4023, 0.2987, 0.4860, 0.2854, 0.2716, 0.4341, 0.2834, 0.2296,\n",
            "        0.2507, 0.3120, 0.3673, 0.3244, 0.3380, 0.3272, 0.2868, 0.2877, 0.3210,\n",
            "        0.2332, 0.3379, 0.2767, 0.2942, 0.2672, 0.4401, 0.2908, 0.3771, 0.2789,\n",
            "        0.3056, 0.3276, 0.3871, 0.2453, 0.2559, 0.2783, 0.3168, 0.3410, 0.2318,\n",
            "        0.3577, 0.5036, 0.3557, 0.2475, 0.1852, 0.2273, 0.3602, 0.2919, 0.3928,\n",
            "        0.4423, 0.2052, 0.2524, 0.2189, 0.4113, 0.3611, 0.4284, 0.2333, 0.3504,\n",
            "        0.7001, 0.3754, 0.2874, 0.3702, 0.3174, 0.3640, 0.2889, 0.4155, 0.2479,\n",
            "        0.2898, 0.3740, 0.4926, 0.2808, 0.2388, 0.3473, 0.1868, 0.2837, 0.3090,\n",
            "        0.3614, 0.2797, 0.6871, 0.2854, 0.2937, 0.3128, 0.4863, 0.2193, 0.2871,\n",
            "        0.2554, 0.4175, 0.3044, 0.3230, 0.3343, 0.4947, 0.3924, 0.2264, 0.2657,\n",
            "        0.4193, 0.3483, 0.3551, 0.2877, 0.2559, 0.2459, 0.2775, 0.3842, 0.2949,\n",
            "        0.3510, 0.1926, 0.3101, 0.3417, 0.3931, 0.3918, 0.3239, 0.2851, 0.4583,\n",
            "        0.2669, 0.2663, 0.4433, 0.3221, 0.3655, 0.3336, 0.4393, 0.3970, 0.3727,\n",
            "        0.3523, 0.3586, 0.3286, 0.4181, 0.2955, 0.3050, 0.2988, 0.4320, 0.2309,\n",
            "        0.3826, 0.2270, 0.2228, 0.3206, 0.3273, 0.2627, 0.3087, 0.2920, 0.2328,\n",
            "        0.4144, 0.4075, 0.3264, 0.3583, 0.3014, 0.3150, 0.4438, 0.4042, 0.2028,\n",
            "        0.3855, 0.2570, 0.2361, 0.2343, 0.3312, 0.2303, 0.3744, 0.4727, 0.3601,\n",
            "        0.2754, 0.1987, 0.3027, 0.3427, 0.2994, 0.2533, 0.2639, 0.3460, 0.3847,\n",
            "        0.4368, 0.3786, 0.3123, 0.2591, 0.3979, 0.2577, 0.3131, 0.2934, 0.3027,\n",
            "        0.2942, 0.2266, 0.2806, 0.2977, 0.1858, 0.2788, 0.2504, 0.3948, 0.3496,\n",
            "        0.2429, 0.2155, 0.2683, 0.4100, 0.3495, 0.4243, 0.2627, 0.3329, 0.2849,\n",
            "        0.3924, 0.3728, 0.2655, 0.3338], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-2.6385e-02,  9.9545e-02, -6.8362e-03, -8.7678e-02,  7.8322e-03,\n",
            "         4.0653e-02, -3.0738e-02,  5.9561e-03,  1.6938e-03,  4.7769e-02,\n",
            "         6.3004e-02,  3.5760e-02, -5.0405e-02,  2.1372e-02, -9.0327e-03,\n",
            "        -3.3702e-02, -4.5529e-02, -1.9238e-01, -6.7608e-02,  7.7464e-02,\n",
            "        -3.3957e-02, -7.9947e-02,  1.3138e-01, -1.2730e-01, -6.2825e-02,\n",
            "        -5.4973e-03, -9.1520e-02, -1.7570e-01, -8.2958e-03, -9.4468e-02,\n",
            "         2.4915e-03, -3.1894e-02, -1.5755e-02,  1.4372e-01, -3.4770e-03,\n",
            "         1.0756e-02, -5.1074e-02,  3.5848e-02,  8.7819e-02, -4.5240e-02,\n",
            "        -4.5785e-02,  1.4690e-02,  6.8685e-02,  1.6806e-02, -4.7715e-02,\n",
            "         5.6814e-02,  4.5985e-02, -5.0668e-02,  5.8914e-03, -1.0335e-01,\n",
            "         1.0281e-02, -1.0520e-01, -1.6557e-02, -1.9213e-02, -3.4522e-02,\n",
            "         2.0138e-02, -1.3621e-01,  3.9602e-02, -8.8277e-03, -1.0837e-02,\n",
            "        -2.9803e-02,  7.2119e-02, -6.6893e-02, -9.4287e-03, -3.0956e-02,\n",
            "        -2.6700e-02, -1.4184e-01,  1.1900e-01,  6.6929e-02, -2.1375e-01,\n",
            "         4.2720e-02,  4.7782e-02,  3.3905e-02,  1.3857e-04, -1.4822e-01,\n",
            "        -2.3684e-02, -7.4337e-02, -6.8427e-02, -2.0076e-02,  1.4697e-02,\n",
            "        -3.9620e-02,  1.9393e-02, -6.9640e-02, -5.5779e-02,  7.9664e-03,\n",
            "         2.3639e-02, -2.5781e-01,  6.3564e-03, -1.0041e-01,  2.8047e-02,\n",
            "         1.5223e-02, -4.8413e-02, -1.5355e-01,  1.0493e-01,  4.9892e-02,\n",
            "         6.5689e-02, -5.4139e-02,  7.7265e-03,  9.4090e-02, -1.9978e-02,\n",
            "        -2.3561e-01, -6.2284e-02,  3.3363e-02,  1.1017e-01,  7.6999e-02,\n",
            "        -3.2478e-02,  4.8089e-02, -1.4995e-01, -1.6503e-01,  1.2304e-01,\n",
            "         7.1157e-02,  5.8895e-02, -4.8155e-02, -9.7220e-02, -1.8604e-01,\n",
            "         8.5326e-02, -5.1602e-02, -3.0799e-01, -6.0410e-02, -7.7131e-02,\n",
            "        -2.7282e-01,  2.8950e-02, -1.3280e-01,  1.7264e-02, -3.9184e-02,\n",
            "         5.4153e-02, -3.7219e-02, -1.5283e-01, -1.7663e-01,  8.3937e-02,\n",
            "         6.9329e-02, -8.2645e-02,  1.1178e-01, -5.0772e-02, -4.4772e-02,\n",
            "        -3.7455e-02,  3.0361e-02, -3.7824e-01,  1.4857e-02,  6.8245e-03,\n",
            "        -5.2090e-02, -2.9501e-01,  8.9860e-02,  2.9646e-02,  1.9930e-02,\n",
            "        -8.3483e-02, -9.6416e-02, -2.3830e-02,  3.4911e-02, -2.6626e-01,\n",
            "        -1.6184e-01,  7.3552e-02,  2.7568e-02, -1.1087e-01, -1.0322e-02,\n",
            "        -9.7499e-02,  1.3995e-02,  1.0802e-02,  7.8444e-02,  1.3057e-02,\n",
            "        -3.9476e-02,  2.4805e-02, -7.7445e-02, -2.8419e-02,  1.0412e-02,\n",
            "        -4.2330e-02, -1.6634e-01, -9.4908e-02, -3.4328e-02,  4.5544e-02,\n",
            "        -3.0001e-01, -6.9376e-03,  1.4103e-02, -2.6155e-01, -7.3598e-02,\n",
            "        -1.0627e-01, -1.0544e-02, -7.1160e-02, -1.0339e-01, -2.9787e-02,\n",
            "        -1.4277e-01, -5.1710e-02, -5.7126e-02, -5.4392e-02, -4.2260e-02,\n",
            "        -8.5120e-03,  1.5887e-02, -6.5358e-02, -6.1346e-02, -1.4502e-01,\n",
            "         3.9939e-02,  8.1640e-02, -7.7597e-03, -3.4062e-02,  3.1969e-02,\n",
            "        -4.4756e-02, -7.0306e-02,  1.0213e-01, -1.7993e-01, -2.1173e-01,\n",
            "        -5.9797e-02, -1.1596e-01,  3.9271e-02, -4.5443e-02, -1.8446e-01,\n",
            "        -1.0848e-01,  5.5781e-02, -6.3649e-02,  1.6825e-02,  1.6623e-04,\n",
            "         7.9866e-02, -6.7240e-02,  7.9827e-02, -3.9905e-03, -1.9016e-01,\n",
            "         2.0026e-02,  7.3181e-02,  1.0323e-01, -2.6431e-02,  2.3963e-02,\n",
            "        -4.4247e-02,  2.2896e-02,  2.3444e-02, -2.3481e-02,  1.0516e-02,\n",
            "        -2.1494e-01, -1.2810e-01, -1.8338e-02, -6.0221e-04, -5.1624e-02,\n",
            "         5.6575e-02, -5.4297e-02,  1.4146e-02, -4.9852e-02,  6.7255e-02,\n",
            "         5.1713e-02, -4.0266e-03,  3.5057e-02,  8.2781e-02,  1.0034e-02,\n",
            "         5.9230e-02, -2.0429e-01, -7.6169e-02,  4.1432e-02,  7.7517e-02,\n",
            "         7.6005e-02, -1.5919e-01, -8.3613e-02, -1.6625e-01,  2.2968e-03,\n",
            "        -6.8489e-02,  3.8119e-02, -9.8731e-02, -2.0290e-02,  1.5394e-02,\n",
            "        -1.0548e-01], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 0.0081]],\n",
            "\n",
            "         [[-0.0192]],\n",
            "\n",
            "         [[-0.0173]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0128]],\n",
            "\n",
            "         [[ 0.0025]],\n",
            "\n",
            "         [[ 0.0054]]],\n",
            "\n",
            "\n",
            "        [[[-0.0143]],\n",
            "\n",
            "         [[-0.0554]],\n",
            "\n",
            "         [[-0.0346]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0275]],\n",
            "\n",
            "         [[ 0.0360]],\n",
            "\n",
            "         [[ 0.0240]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0076]],\n",
            "\n",
            "         [[ 0.0207]],\n",
            "\n",
            "         [[-0.0101]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0278]],\n",
            "\n",
            "         [[ 0.0064]],\n",
            "\n",
            "         [[-0.0022]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0336]],\n",
            "\n",
            "         [[-0.0424]],\n",
            "\n",
            "         [[ 0.0226]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0330]],\n",
            "\n",
            "         [[-0.0009]],\n",
            "\n",
            "         [[-0.0177]]],\n",
            "\n",
            "\n",
            "        [[[-0.0114]],\n",
            "\n",
            "         [[-0.0183]],\n",
            "\n",
            "         [[ 0.0076]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0151]],\n",
            "\n",
            "         [[ 0.0332]],\n",
            "\n",
            "         [[ 0.0002]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0063]],\n",
            "\n",
            "         [[-0.0200]],\n",
            "\n",
            "         [[ 0.0010]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0191]],\n",
            "\n",
            "         [[ 0.0455]],\n",
            "\n",
            "         [[ 0.0078]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.0674, 0.0514, 0.0385, 0.1692, 0.0604, 0.0460, 0.1209, 0.1110, 0.0418,\n",
            "        0.0387, 0.0442, 0.0707, 0.0790, 0.1094, 0.0959, 0.0544, 0.1032, 0.2190,\n",
            "        0.0459, 0.0372, 0.1410, 0.0587, 0.0360, 0.0955, 0.1657, 0.1024, 0.1417,\n",
            "        0.0580, 0.0536, 0.0716, 0.0865, 0.1110, 0.0511, 0.0515, 0.0809, 0.1154,\n",
            "        0.0777, 0.0449, 0.0490, 0.1056, 0.1457, 0.0744, 0.0530, 0.0600, 0.1026,\n",
            "        0.0486, 0.0408, 0.1312, 0.0639, 0.1062, 0.0915, 0.1476, 0.0900, 0.0742,\n",
            "        0.1069, 0.0776, 0.1423, 0.0495, 0.0974, 0.0661, 0.1292, 0.0548, 0.1145,\n",
            "        0.0950, 0.0921, 0.1579, 0.0496, 0.0236, 0.0398, 0.0935, 0.0291, 0.0653,\n",
            "        0.0885, 0.1190, 0.1692, 0.0692, 0.1316, 0.0606, 0.0480, 0.0654, 0.1082,\n",
            "        0.0624, 0.1103, 0.1106, 0.1076, 0.0400, 0.0723, 0.0947, 0.0662, 0.0464,\n",
            "        0.0444, 0.1727, 0.0921, 0.0345, 0.0451, 0.0374, 0.0940, 0.0818, 0.0397,\n",
            "        0.0452, 0.0985, 0.1095, 0.1072, 0.0506, 0.0444, 0.0755, 0.0420, 0.1046,\n",
            "        0.1172, 0.0447, 0.0459, 0.0409, 0.0539, 0.1036, 0.0741, 0.0311, 0.1086,\n",
            "        0.1746, 0.0777, 0.0689, 0.1100, 0.0489, 0.1048, 0.1097, 0.1025, 0.0448,\n",
            "        0.0675, 0.0707, 0.1364, 0.0438, 0.0346, 0.1769, 0.0667, 0.1155, 0.0628,\n",
            "        0.0873, 0.0406, 0.2890, 0.0703, 0.0428, 0.1173, 0.1049, 0.0611, 0.0469,\n",
            "        0.0400, 0.0744, 0.1003, 0.1012, 0.0599, 0.1078, 0.1512, 0.0322, 0.0430,\n",
            "        0.0977, 0.0951, 0.0838, 0.0958, 0.0448, 0.0263, 0.0425, 0.1154, 0.0771,\n",
            "        0.1781, 0.0300, 0.0699, 0.0724, 0.1600, 0.0893, 0.1130, 0.0534, 0.1359,\n",
            "        0.0375, 0.0809, 0.1145, 0.1232, 0.0942, 0.0880, 0.0346, 0.0996, 0.0461,\n",
            "        0.0694, 0.0630, 0.1590, 0.0509, 0.1254, 0.0590, 0.0744, 0.1084, 0.0514,\n",
            "        0.0931, 0.0848, 0.0240, 0.0279, 0.0993, 0.0612, 0.0599, 0.1095, 0.0508,\n",
            "        0.0658, 0.1162, 0.0833, 0.1651, 0.0505, 0.1231, 0.1228, 0.1038, 0.0369,\n",
            "        0.0756, 0.0415, 0.1192, 0.0292, 0.0839, 0.0577, 0.0951, 0.0944, 0.0309,\n",
            "        0.0390, 0.0604, 0.0672, 0.0501, 0.0383, 0.0946, 0.0958, 0.0501, 0.0243,\n",
            "        0.1074, 0.1908, 0.0693, 0.1376, 0.1151, 0.0329, 0.0647, 0.0616, 0.1106,\n",
            "        0.0358, 0.0721, 0.0851, 0.0375, 0.0368, 0.0947, 0.0464, 0.1666, 0.1049,\n",
            "        0.0755, 0.0398, 0.0249, 0.1528, 0.1167, 0.0886, 0.0540, 0.0726, 0.0736,\n",
            "        0.0797, 0.0854, 0.0609, 0.1263], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-2.6385e-02,  9.9545e-02, -6.8362e-03, -8.7678e-02,  7.8322e-03,\n",
            "         4.0653e-02, -3.0738e-02,  5.9561e-03,  1.6938e-03,  4.7769e-02,\n",
            "         6.3004e-02,  3.5760e-02, -5.0405e-02,  2.1372e-02, -9.0327e-03,\n",
            "        -3.3702e-02, -4.5529e-02, -1.9238e-01, -6.7608e-02,  7.7464e-02,\n",
            "        -3.3957e-02, -7.9947e-02,  1.3138e-01, -1.2730e-01, -6.2825e-02,\n",
            "        -5.4973e-03, -9.1520e-02, -1.7570e-01, -8.2958e-03, -9.4468e-02,\n",
            "         2.4915e-03, -3.1894e-02, -1.5755e-02,  1.4372e-01, -3.4770e-03,\n",
            "         1.0756e-02, -5.1074e-02,  3.5848e-02,  8.7819e-02, -4.5240e-02,\n",
            "        -4.5785e-02,  1.4690e-02,  6.8685e-02,  1.6806e-02, -4.7715e-02,\n",
            "         5.6814e-02,  4.5985e-02, -5.0668e-02,  5.8914e-03, -1.0335e-01,\n",
            "         1.0281e-02, -1.0520e-01, -1.6557e-02, -1.9213e-02, -3.4522e-02,\n",
            "         2.0138e-02, -1.3621e-01,  3.9602e-02, -8.8277e-03, -1.0837e-02,\n",
            "        -2.9803e-02,  7.2119e-02, -6.6893e-02, -9.4287e-03, -3.0956e-02,\n",
            "        -2.6700e-02, -1.4184e-01,  1.1900e-01,  6.6929e-02, -2.1375e-01,\n",
            "         4.2720e-02,  4.7782e-02,  3.3905e-02,  1.3857e-04, -1.4822e-01,\n",
            "        -2.3684e-02, -7.4337e-02, -6.8427e-02, -2.0076e-02,  1.4697e-02,\n",
            "        -3.9620e-02,  1.9393e-02, -6.9640e-02, -5.5779e-02,  7.9664e-03,\n",
            "         2.3639e-02, -2.5781e-01,  6.3564e-03, -1.0041e-01,  2.8047e-02,\n",
            "         1.5223e-02, -4.8413e-02, -1.5355e-01,  1.0493e-01,  4.9892e-02,\n",
            "         6.5689e-02, -5.4139e-02,  7.7265e-03,  9.4090e-02, -1.9978e-02,\n",
            "        -2.3561e-01, -6.2284e-02,  3.3363e-02,  1.1017e-01,  7.6999e-02,\n",
            "        -3.2478e-02,  4.8089e-02, -1.4995e-01, -1.6503e-01,  1.2304e-01,\n",
            "         7.1157e-02,  5.8895e-02, -4.8155e-02, -9.7220e-02, -1.8604e-01,\n",
            "         8.5326e-02, -5.1602e-02, -3.0799e-01, -6.0410e-02, -7.7131e-02,\n",
            "        -2.7282e-01,  2.8950e-02, -1.3280e-01,  1.7264e-02, -3.9184e-02,\n",
            "         5.4153e-02, -3.7219e-02, -1.5283e-01, -1.7663e-01,  8.3937e-02,\n",
            "         6.9329e-02, -8.2645e-02,  1.1178e-01, -5.0772e-02, -4.4772e-02,\n",
            "        -3.7455e-02,  3.0361e-02, -3.7824e-01,  1.4857e-02,  6.8245e-03,\n",
            "        -5.2090e-02, -2.9501e-01,  8.9860e-02,  2.9646e-02,  1.9930e-02,\n",
            "        -8.3483e-02, -9.6416e-02, -2.3830e-02,  3.4911e-02, -2.6626e-01,\n",
            "        -1.6184e-01,  7.3552e-02,  2.7568e-02, -1.1087e-01, -1.0322e-02,\n",
            "        -9.7499e-02,  1.3995e-02,  1.0802e-02,  7.8444e-02,  1.3057e-02,\n",
            "        -3.9476e-02,  2.4805e-02, -7.7445e-02, -2.8419e-02,  1.0412e-02,\n",
            "        -4.2330e-02, -1.6634e-01, -9.4908e-02, -3.4328e-02,  4.5544e-02,\n",
            "        -3.0001e-01, -6.9376e-03,  1.4103e-02, -2.6155e-01, -7.3598e-02,\n",
            "        -1.0627e-01, -1.0544e-02, -7.1160e-02, -1.0339e-01, -2.9787e-02,\n",
            "        -1.4277e-01, -5.1710e-02, -5.7126e-02, -5.4392e-02, -4.2260e-02,\n",
            "        -8.5120e-03,  1.5887e-02, -6.5358e-02, -6.1346e-02, -1.4502e-01,\n",
            "         3.9939e-02,  8.1640e-02, -7.7597e-03, -3.4062e-02,  3.1969e-02,\n",
            "        -4.4756e-02, -7.0306e-02,  1.0213e-01, -1.7993e-01, -2.1173e-01,\n",
            "        -5.9797e-02, -1.1596e-01,  3.9271e-02, -4.5443e-02, -1.8446e-01,\n",
            "        -1.0848e-01,  5.5781e-02, -6.3649e-02,  1.6825e-02,  1.6623e-04,\n",
            "         7.9866e-02, -6.7240e-02,  7.9827e-02, -3.9905e-03, -1.9016e-01,\n",
            "         2.0026e-02,  7.3181e-02,  1.0323e-01, -2.6431e-02,  2.3963e-02,\n",
            "        -4.4247e-02,  2.2896e-02,  2.3444e-02, -2.3481e-02,  1.0516e-02,\n",
            "        -2.1494e-01, -1.2810e-01, -1.8338e-02, -6.0221e-04, -5.1624e-02,\n",
            "         5.6575e-02, -5.4297e-02,  1.4146e-02, -4.9852e-02,  6.7255e-02,\n",
            "         5.1713e-02, -4.0266e-03,  3.5057e-02,  8.2781e-02,  1.0034e-02,\n",
            "         5.9230e-02, -2.0429e-01, -7.6169e-02,  4.1432e-02,  7.7517e-02,\n",
            "         7.6005e-02, -1.5919e-01, -8.3613e-02, -1.6625e-01,  2.2968e-03,\n",
            "        -6.8489e-02,  3.8119e-02, -9.8731e-02, -2.0290e-02,  1.5394e-02,\n",
            "        -1.0548e-01], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 4.8367e-02,  4.8045e-02,  3.8471e-02],\n",
            "          [ 4.9888e-02,  5.5208e-02,  5.6701e-02],\n",
            "          [ 2.4192e-02,  1.3436e-02,  2.4655e-02]],\n",
            "\n",
            "         [[-3.6542e-03, -3.1100e-03,  4.9227e-03],\n",
            "          [-1.2114e-03,  3.4020e-03,  1.9846e-02],\n",
            "          [-2.1704e-02, -2.1158e-02, -2.8686e-03]],\n",
            "\n",
            "         [[-1.2536e-02, -2.0486e-02, -2.3154e-02],\n",
            "          [-1.3515e-02, -2.3781e-02, -2.5515e-02],\n",
            "          [ 1.0584e-02,  7.2999e-03, -5.2329e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.3596e-02, -1.8328e-02, -5.0577e-02],\n",
            "          [ 1.6590e-02,  5.0719e-02,  2.1919e-02],\n",
            "          [-1.9203e-02, -8.8315e-03, -2.0335e-02]],\n",
            "\n",
            "         [[-7.6949e-03, -1.5848e-02,  1.5841e-03],\n",
            "          [-6.2470e-03, -1.3135e-02,  6.9092e-03],\n",
            "          [-3.3791e-03,  1.7889e-03,  3.7373e-03]],\n",
            "\n",
            "         [[-6.6310e-03,  5.8503e-03, -5.8571e-04],\n",
            "          [-2.4600e-02, -8.9747e-03, -7.2466e-03],\n",
            "          [-1.7566e-02, -8.5829e-03, -7.5220e-03]]],\n",
            "\n",
            "\n",
            "        [[[-2.3679e-02, -9.4399e-03, -1.1688e-02],\n",
            "          [-2.4777e-02, -1.7326e-02, -3.1489e-02],\n",
            "          [-3.3683e-03,  9.7571e-03, -5.1527e-03]],\n",
            "\n",
            "         [[-3.0809e-02, -4.0685e-02, -2.2731e-02],\n",
            "          [-5.1065e-03, -1.6457e-02, -1.8804e-02],\n",
            "          [ 5.0382e-02,  5.2054e-02,  3.9185e-02]],\n",
            "\n",
            "         [[-3.7790e-02, -4.2234e-02, -2.9703e-02],\n",
            "          [-6.4766e-03,  2.6967e-03, -8.1736e-03],\n",
            "          [ 3.7747e-02,  5.5416e-02,  2.5806e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.7275e-02, -4.5364e-02, -3.9567e-02],\n",
            "          [ 8.9827e-03,  1.6150e-02,  1.1675e-02],\n",
            "          [-9.7209e-03, -3.6449e-02, -1.6842e-02]],\n",
            "\n",
            "         [[ 1.7824e-02,  1.5013e-02,  1.0225e-02],\n",
            "          [ 5.4044e-03,  1.1664e-02,  6.4623e-03],\n",
            "          [ 2.1803e-02,  4.1795e-02,  1.9234e-02]],\n",
            "\n",
            "         [[-2.6730e-04,  1.5218e-03, -5.0352e-03],\n",
            "          [ 2.5761e-02,  2.7110e-02, -9.3395e-04],\n",
            "          [-1.1949e-02, -7.5204e-03, -3.9370e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.7447e-02, -1.8358e-02, -2.6020e-02],\n",
            "          [-1.4074e-02, -1.1302e-02, -1.4814e-02],\n",
            "          [-3.1460e-03, -1.8674e-02, -9.3350e-03]],\n",
            "\n",
            "         [[-5.1125e-03, -4.8036e-03,  1.8139e-02],\n",
            "          [-1.0524e-02, -1.5152e-02,  2.3904e-03],\n",
            "          [ 8.7093e-03,  9.3810e-03,  2.4203e-03]],\n",
            "\n",
            "         [[-7.6392e-03, -8.1496e-03, -1.5331e-02],\n",
            "          [-8.0622e-03, -1.3383e-02, -1.3938e-02],\n",
            "          [-1.6904e-02, -3.0059e-02, -1.8659e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.8390e-02, -2.6080e-03,  9.3782e-03],\n",
            "          [-6.4662e-04, -1.3146e-02,  1.0045e-02],\n",
            "          [-2.2293e-03, -1.4097e-02,  1.7385e-02]],\n",
            "\n",
            "         [[ 3.0293e-04,  2.9622e-03,  1.0030e-02],\n",
            "          [-5.7588e-03, -1.6943e-03,  6.9988e-03],\n",
            "          [ 9.8134e-03,  1.4197e-02,  5.9742e-03]],\n",
            "\n",
            "         [[ 2.8753e-03, -1.7814e-03,  1.0873e-02],\n",
            "          [ 1.5230e-02,  4.5867e-03,  1.6860e-02],\n",
            "          [ 1.9536e-03,  1.9503e-02,  1.2168e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 1.3983e-02,  2.4598e-03, -7.4604e-03],\n",
            "          [-2.2250e-02, -1.2757e-02, -2.8846e-03],\n",
            "          [-1.0911e-02,  7.5499e-03,  8.6910e-03]],\n",
            "\n",
            "         [[-4.8463e-03, -8.3250e-03,  1.3420e-02],\n",
            "          [-6.2502e-03, -7.3982e-03,  1.1153e-02],\n",
            "          [ 4.0391e-03, -9.0354e-03, -7.5441e-03]],\n",
            "\n",
            "         [[-5.1627e-03, -8.9529e-03, -1.2414e-02],\n",
            "          [-4.9261e-03, -3.5488e-03,  2.1501e-03],\n",
            "          [-1.1709e-02, -1.4984e-02, -1.9216e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.5428e-02, -7.6036e-04, -1.3522e-03],\n",
            "          [-3.4856e-02, -7.4478e-04, -6.5064e-03],\n",
            "          [-9.1655e-03, -2.8467e-02, -4.8924e-02]],\n",
            "\n",
            "         [[ 1.2207e-02,  1.0519e-02, -8.4421e-03],\n",
            "          [-2.5495e-02,  2.8140e-03,  1.6165e-03],\n",
            "          [-1.8831e-02,  1.2268e-02,  1.5439e-02]],\n",
            "\n",
            "         [[-1.3684e-02, -4.1732e-03,  1.2609e-02],\n",
            "          [-6.8834e-04,  5.9757e-03, -1.0183e-02],\n",
            "          [ 2.1559e-04, -1.3462e-02, -3.0114e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.6186e-02, -6.4926e-02, -4.3146e-02],\n",
            "          [-2.1790e-02, -4.9106e-02, -3.4568e-02],\n",
            "          [ 4.0506e-02,  4.2449e-02,  6.1562e-02]],\n",
            "\n",
            "         [[ 3.5715e-03, -1.0916e-02, -2.2922e-02],\n",
            "          [-2.4831e-03,  6.4555e-03, -1.1316e-02],\n",
            "          [ 1.6662e-03, -1.9145e-02, -2.3007e-02]],\n",
            "\n",
            "         [[-7.1243e-03, -4.2783e-05,  4.9363e-03],\n",
            "          [-1.5832e-02,  4.0474e-03,  4.5135e-04],\n",
            "          [-4.7967e-03, -7.2164e-04, -1.7230e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.1589e-02,  7.7814e-04,  6.3205e-03],\n",
            "          [ 1.1360e-02, -6.2076e-03, -2.7689e-02],\n",
            "          [ 2.6392e-02,  2.3775e-03, -1.4937e-02]],\n",
            "\n",
            "         [[-1.1237e-02, -2.6285e-03,  9.1537e-03],\n",
            "          [-8.2120e-03, -2.2236e-02,  3.2917e-04],\n",
            "          [ 5.5909e-03, -1.3858e-03,  6.8947e-03]],\n",
            "\n",
            "         [[-1.4783e-02, -1.0367e-02, -2.7472e-02],\n",
            "          [-4.1090e-02, -3.8532e-02, -3.9202e-02],\n",
            "          [-2.1614e-02, -3.4340e-02, -1.8542e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.9492e-02, -1.6098e-02, -3.1792e-02],\n",
            "          [ 2.5374e-02,  4.6815e-02,  2.7513e-02],\n",
            "          [ 3.5903e-02,  3.1892e-02,  2.6156e-02]],\n",
            "\n",
            "         [[ 1.6856e-02,  1.5645e-02,  1.4189e-02],\n",
            "          [ 2.2550e-02,  3.0456e-02,  1.6739e-02],\n",
            "          [-2.3615e-04, -7.9501e-03, -1.9666e-03]],\n",
            "\n",
            "         [[-7.9060e-03, -4.7390e-03,  1.6030e-03],\n",
            "          [ 1.3802e-03, -8.5837e-03,  6.9451e-03],\n",
            "          [ 1.1407e-02, -5.9877e-03,  1.3759e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.0124e-03,  2.9951e-02,  1.1915e-02],\n",
            "          [-4.3412e-02, -3.1776e-03, -2.7705e-02],\n",
            "          [-1.6183e-02, -1.1247e-02, -3.5084e-02]],\n",
            "\n",
            "         [[ 2.9837e-02,  5.9935e-02,  2.4631e-02],\n",
            "          [-1.9571e-03,  2.2415e-02, -1.5499e-02],\n",
            "          [ 1.6075e-02,  1.7850e-02, -1.8412e-02]],\n",
            "\n",
            "         [[-4.3712e-03, -4.9032e-02, -2.1335e-02],\n",
            "          [-5.2598e-03, -2.8579e-02, -2.2090e-02],\n",
            "          [ 8.5126e-03,  2.0862e-03,  2.3301e-02]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.2480, 0.1972, 0.2279, 0.2709, 0.3296, 0.2640, 0.2710, 0.3475, 0.2388,\n",
            "        0.2904, 0.2769, 0.3045, 0.2268, 0.2634, 0.2999, 0.2397, 0.2724, 0.2723,\n",
            "        0.2133, 0.3806, 0.2767, 0.2403, 0.2406, 0.2917, 0.2675, 0.2305, 0.2394,\n",
            "        0.3123, 0.2984, 0.3353, 0.2234, 0.1919, 0.3168, 0.2626, 0.2901, 0.2918,\n",
            "        0.3455, 0.2561, 0.2434, 0.2298, 0.3318, 0.3481, 0.2032, 0.2478, 0.2478,\n",
            "        0.2483, 0.3252, 0.2567, 0.2685, 0.1977, 0.2541, 0.4079, 0.2480, 0.2076,\n",
            "        0.2276, 0.2683, 0.2098, 0.2056, 0.2010, 0.3560, 0.2384, 0.3284, 0.1952,\n",
            "        0.2445, 0.2848, 0.3742, 0.2746, 0.2117, 0.3859, 0.4785, 0.3005, 0.2848,\n",
            "        0.3762, 0.2903, 0.2126, 0.1776, 0.2778, 0.3878, 0.3123, 0.1974, 0.2679,\n",
            "        0.2300, 0.2474, 0.2320, 0.2635, 0.2819, 0.2296, 0.3194, 0.3814, 0.2503,\n",
            "        0.2269, 0.2676, 0.3431, 0.3799, 0.3787, 0.2968, 0.3021, 0.2575, 0.3007,\n",
            "        0.1939, 0.1950, 0.3217, 0.3623, 0.2171, 0.2486, 0.2266, 0.2133, 0.2851,\n",
            "        0.2715, 0.2720, 0.3107, 0.2174, 0.2675, 0.2387, 0.3434, 0.2761, 0.2084,\n",
            "        0.2975, 0.3178, 0.2818, 0.2858, 0.3498, 0.2675, 0.2638, 0.3159, 0.2879,\n",
            "        0.1873, 0.2986, 0.3584, 0.2570, 0.1815, 0.2758, 0.2640, 0.2486, 0.2567,\n",
            "        0.2252, 0.3420, 0.2910, 0.2898, 0.2902, 0.2404, 0.2381, 0.3633, 0.2690,\n",
            "        0.3810, 0.2947, 0.2743, 0.4644, 0.3133, 0.2444, 0.3477, 0.3001, 0.1977,\n",
            "        0.2301, 0.2513, 0.2660, 0.3271, 0.1622, 0.2274, 0.2225, 0.3596, 0.3215,\n",
            "        0.1997, 0.2215, 0.2706, 0.2831, 0.2621, 0.3710, 0.2730, 0.2903, 0.1893,\n",
            "        0.2140, 0.2460, 0.3141, 0.2424, 0.3699, 0.2364, 0.2420, 0.2948, 0.2497,\n",
            "        0.2760, 0.2686, 0.2895, 0.3857, 0.1398, 0.2832, 0.3362, 0.2522, 0.2823,\n",
            "        0.2381, 0.2311, 0.3274, 0.4078, 0.2648, 0.2525, 0.3388, 0.3251, 0.2420,\n",
            "        0.2856, 0.3605, 0.2603, 0.2294, 0.2483, 0.2171, 0.2353, 0.4117, 0.2588,\n",
            "        0.2888, 0.1972, 0.2408, 0.2755, 0.3031, 0.2457, 0.2744, 0.3564, 0.2546,\n",
            "        0.3673, 0.2883, 0.2590, 0.3021, 0.2890, 0.3505, 0.2092, 0.2953, 0.3222,\n",
            "        0.2925, 0.2574, 0.3012, 0.3893, 0.2211, 0.2226, 0.3258, 0.3205, 0.2975,\n",
            "        0.2323, 0.3323, 0.2812, 0.2702, 0.2300, 0.2846, 0.3318, 0.2292, 0.3498,\n",
            "        0.2622, 0.3581, 0.4003, 0.2924, 0.3049, 0.3478, 0.2845, 0.2742, 0.2019,\n",
            "        0.2466, 0.2988, 0.2044, 0.2691], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1332, -0.0644, -0.3239, -0.2390, -0.3262, -0.1796, -0.2087, -0.3208,\n",
            "        -0.1874, -0.2988, -0.2099, -0.2283, -0.2141, -0.2460, -0.2768, -0.1351,\n",
            "        -0.2498, -0.2393, -0.1223, -0.4590, -0.2172, -0.1220, -0.2101, -0.1779,\n",
            "        -0.2426, -0.1546, -0.1549, -0.3716, -0.2817, -0.3886, -0.1545, -0.0687,\n",
            "        -0.3412, -0.2261, -0.1961, -0.2242, -0.2984, -0.1381, -0.2251, -0.1658,\n",
            "        -0.4534, -0.3226, -0.0977, -0.1349, -0.2619, -0.1428, -0.3960, -0.1633,\n",
            "        -0.2101, -0.1161, -0.1448, -0.5502, -0.2179, -0.1246,  0.0502, -0.1902,\n",
            "        -0.1047, -0.1000, -0.1411, -0.3124, -0.2190, -0.3062, -0.1247, -0.1557,\n",
            "        -0.2973, -0.3825, -0.1951, -0.1381, -0.5761, -0.3879, -0.2808, -0.2542,\n",
            "        -0.3470, -0.2460, -0.1091, -0.0562, -0.1833, -0.4956, -0.3059, -0.0988,\n",
            "        -0.2255, -0.1958, -0.1320, -0.1738, -0.2287, -0.1926, -0.0924, -0.3427,\n",
            "        -0.5489, -0.2431, -0.1935, -0.1641, -0.2503, -0.3274, -0.4008, -0.2824,\n",
            "        -0.2694, -0.1939, -0.2413, -0.0309, -0.0880, -0.3421, -0.3104, -0.1102,\n",
            "        -0.1539, -0.1233, -0.1780, -0.2715, -0.2005, -0.1846, -0.2843, -0.1117,\n",
            "        -0.1816, -0.2119, -0.3304, -0.2267, -0.1413, -0.3376, -0.2674, -0.2524,\n",
            "        -0.2554, -0.4735, -0.2342, -0.2130, -0.3282, -0.1966, -0.1063, -0.2615,\n",
            "        -0.4234, -0.1374, -0.0811, -0.3069, -0.1538, -0.1453, -0.1612, -0.1631,\n",
            "        -0.3759, -0.2608, -0.2382, -0.2499, -0.1485, -0.1487, -0.4328, -0.1377,\n",
            "        -0.2781, -0.2259, -0.2072, -0.4165, -0.3582, -0.1382, -0.3598, -0.2672,\n",
            "        -0.2090, -0.0177, -0.1279, -0.2812, -0.3621,  0.0476, -0.2232, -0.1272,\n",
            "        -0.3237, -0.3008, -0.1119, -0.0839, -0.2426, -0.2000, -0.1873, -0.4685,\n",
            "        -0.2000, -0.3462, -0.0706, -0.1973, -0.3548, -0.1975, -0.3537, -0.3546,\n",
            "        -0.1433, -0.2052, -0.2722, -0.1528, -0.2798, -0.1945, -0.2474, -0.4910,\n",
            "         0.1322, -0.2378, -0.5166, -0.3959, -0.2354, -0.1266, -0.0810, -0.4132,\n",
            "        -0.5576, -0.2238, -0.1563, -0.3950, -0.3283, -0.0846, -0.3103, -0.3130,\n",
            "        -0.1498, -0.1396, -0.0972, -0.1620, -0.1631, -0.6364, -0.1350, -0.2345,\n",
            "        -0.1049, -0.1625, -0.2878, -0.2450, -0.1468, -0.2035, -0.5358, -0.1683,\n",
            "        -0.5524, -0.2511, -0.1230, -0.2305, -0.1925, -0.3759, -0.1014, -0.1697,\n",
            "        -0.4002, -0.2980, -0.3035, -0.1563, -0.4660, -0.1155, -0.1665, -0.3382,\n",
            "        -0.2935, -0.3122, -0.3015, -0.3261, -0.2542, -0.2037, -0.0955, -0.2070,\n",
            "        -0.4370, -0.2051, -0.4205, -0.3125, -0.4845, -0.3528, -0.2624, -0.2894,\n",
            "        -0.3976, -0.2107, -0.1791, -0.1075, -0.1213, -0.3022,  0.0516, -0.1928],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-4.2568e-02, -2.6148e-02, -2.2019e-02],\n",
            "          [-1.7334e-02, -7.5950e-03, -7.2384e-03],\n",
            "          [-1.7876e-03,  2.3800e-02,  1.4873e-02]],\n",
            "\n",
            "         [[-2.8277e-03, -5.0644e-03, -4.9442e-03],\n",
            "          [ 1.2117e-03,  1.4908e-02,  1.6013e-02],\n",
            "          [ 1.4391e-02,  3.3109e-02,  5.0061e-02]],\n",
            "\n",
            "         [[-3.4891e-03, -4.4437e-03,  2.6589e-03],\n",
            "          [ 1.5105e-02,  2.6303e-02,  2.6802e-02],\n",
            "          [ 3.9232e-02,  5.0057e-02,  4.6637e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.2877e-02,  1.5454e-02, -2.4483e-02],\n",
            "          [ 3.1145e-02,  3.4944e-02,  1.3296e-02],\n",
            "          [-1.7674e-04,  7.3297e-03, -5.7174e-03]],\n",
            "\n",
            "         [[-2.1781e-02, -3.7379e-02, -1.3382e-02],\n",
            "          [ 1.8976e-02,  1.4155e-02, -6.5395e-03],\n",
            "          [ 2.6831e-02,  3.6354e-02,  1.1450e-02]],\n",
            "\n",
            "         [[ 3.1603e-02,  3.3933e-02,  3.1575e-02],\n",
            "          [-1.0098e-02, -1.2657e-02,  1.1674e-02],\n",
            "          [ 1.0325e-02,  7.9424e-05,  1.5911e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 2.5937e-02,  6.2590e-03,  6.0798e-03],\n",
            "          [-4.5745e-03, -3.5188e-02, -2.9249e-02],\n",
            "          [ 2.1366e-02,  2.0480e-03,  6.2699e-03]],\n",
            "\n",
            "         [[-2.9549e-03, -1.3679e-03, -8.6876e-03],\n",
            "          [ 7.9988e-03,  1.2888e-03, -5.9629e-03],\n",
            "          [-9.1481e-03, -2.1914e-02, -4.1572e-02]],\n",
            "\n",
            "         [[ 7.6390e-03,  3.0253e-03,  2.7817e-04],\n",
            "          [ 7.0329e-03,  1.1914e-02, -2.4419e-03],\n",
            "          [-8.2131e-03, -9.7848e-05, -1.9223e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.4498e-03,  5.1611e-03,  3.7416e-03],\n",
            "          [ 3.2110e-04,  8.3762e-03,  3.6612e-03],\n",
            "          [ 9.3343e-03,  8.1829e-03,  1.1234e-03]],\n",
            "\n",
            "         [[-6.6849e-02, -5.9871e-02, -3.3931e-02],\n",
            "          [ 2.2337e-02,  3.1932e-02,  3.7244e-02],\n",
            "          [ 9.3296e-03,  3.7222e-02,  1.4052e-02]],\n",
            "\n",
            "         [[-2.0643e-03,  1.2408e-02, -3.1072e-03],\n",
            "          [-8.2882e-03,  1.3917e-02, -2.0680e-02],\n",
            "          [-1.9329e-02,  1.1953e-02, -2.3436e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.7788e-03, -3.5982e-03, -1.2592e-03],\n",
            "          [-1.5320e-02, -1.0690e-02, -2.0311e-02],\n",
            "          [-3.4649e-04, -2.2188e-03, -1.5021e-02]],\n",
            "\n",
            "         [[-2.8952e-02, -3.3958e-02, -2.5437e-02],\n",
            "          [-1.5919e-04,  1.5204e-02,  3.4554e-02],\n",
            "          [ 3.6892e-02,  7.0144e-02,  7.3610e-02]],\n",
            "\n",
            "         [[ 1.0721e-02,  2.1531e-03, -5.6155e-03],\n",
            "          [ 1.1754e-02, -4.8546e-03, -5.5013e-03],\n",
            "          [-3.7388e-04, -9.7639e-03, -1.5029e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.5622e-02,  9.8976e-03,  3.4725e-03],\n",
            "          [ 1.4711e-02,  7.0707e-03, -9.1826e-03],\n",
            "          [ 7.0986e-03,  6.3087e-03, -3.5893e-03]],\n",
            "\n",
            "         [[-6.4518e-03, -6.7673e-03,  1.1635e-02],\n",
            "          [ 1.4707e-02,  2.3831e-02,  4.9396e-02],\n",
            "          [ 1.8897e-02,  3.4981e-02,  4.5488e-02]],\n",
            "\n",
            "         [[ 1.5900e-02,  3.3369e-02,  2.6194e-02],\n",
            "          [ 1.0616e-02,  1.8515e-02,  3.0190e-03],\n",
            "          [ 1.1004e-02,  2.5503e-02,  1.3654e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-2.1231e-02, -1.2804e-02, -1.5498e-02],\n",
            "          [ 7.6750e-03,  1.2120e-02,  1.5099e-02],\n",
            "          [ 1.8536e-02,  2.5110e-02,  2.5283e-02]],\n",
            "\n",
            "         [[ 7.4059e-03, -3.0540e-03, -1.5475e-03],\n",
            "          [-8.4415e-03, -2.2002e-02, -3.4099e-03],\n",
            "          [ 9.1918e-03,  2.2617e-03, -1.4260e-02]],\n",
            "\n",
            "         [[-5.2568e-03, -5.3507e-03, -3.2230e-03],\n",
            "          [-1.5805e-02,  6.0508e-03, -1.5917e-03],\n",
            "          [-8.9323e-03,  2.6483e-03,  5.0508e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.9826e-02, -2.1209e-03,  1.4889e-02],\n",
            "          [ 5.7275e-02,  3.5549e-02,  6.0175e-03],\n",
            "          [ 2.3347e-02, -2.2153e-02, -2.5497e-02]],\n",
            "\n",
            "         [[-1.3985e-02, -6.4766e-02, -1.7286e-02],\n",
            "          [ 1.1704e-02,  1.0714e-02,  4.6278e-02],\n",
            "          [-1.0038e-02, -3.5707e-03,  2.2691e-02]],\n",
            "\n",
            "         [[-8.3342e-03, -1.3070e-03, -1.0049e-02],\n",
            "          [ 3.2605e-02,  5.3259e-02,  2.2172e-02],\n",
            "          [ 3.7339e-02,  6.1155e-02,  4.4555e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.6584e-02, -1.3850e-02, -1.4604e-02],\n",
            "          [-1.7604e-02, -2.1268e-02, -1.6734e-02],\n",
            "          [-6.0039e-04,  3.8569e-03,  1.2837e-02]],\n",
            "\n",
            "         [[ 1.7623e-02,  2.3706e-02,  2.7633e-02],\n",
            "          [-2.2841e-02, -1.9576e-02, -1.6551e-02],\n",
            "          [-8.0822e-03,  4.3779e-03, -5.3622e-03]],\n",
            "\n",
            "         [[ 1.5582e-02,  3.7879e-02,  2.3555e-02],\n",
            "          [-6.4632e-03,  9.8620e-03,  1.2121e-02],\n",
            "          [-1.3743e-02, -6.1246e-03, -2.7332e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.5037e-03, -1.2064e-02, -9.0989e-03],\n",
            "          [-4.7911e-04, -2.8339e-03,  2.1365e-03],\n",
            "          [-6.2077e-03, -2.6615e-03,  1.1215e-02]],\n",
            "\n",
            "         [[-8.1794e-03, -2.2417e-02, -3.4012e-02],\n",
            "          [-2.8553e-02, -2.9546e-02, -4.4372e-02],\n",
            "          [-5.0348e-02, -3.4973e-02, -5.2028e-02]],\n",
            "\n",
            "         [[ 1.4728e-02,  3.2834e-02,  2.6312e-02],\n",
            "          [ 1.3449e-02,  2.6407e-02,  2.6924e-02],\n",
            "          [ 2.5572e-02,  3.4316e-02,  2.6184e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 7.2026e-03, -2.3931e-03,  2.2182e-03],\n",
            "          [ 4.2555e-03, -6.4084e-03,  7.8548e-03],\n",
            "          [ 2.0510e-02,  1.8644e-02,  2.3280e-02]],\n",
            "\n",
            "         [[-1.2471e-02,  1.3008e-02,  1.0010e-02],\n",
            "          [-1.7496e-03,  6.1331e-03,  4.3366e-03],\n",
            "          [ 5.2269e-03,  1.5111e-02, -8.1881e-03]],\n",
            "\n",
            "         [[-3.7337e-02,  1.9923e-02, -2.4149e-02],\n",
            "          [-4.9487e-02, -1.0510e-02, -4.2107e-02],\n",
            "          [-5.7684e-03, -4.8632e-03, -1.8332e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.2013e-03, -1.5208e-02, -1.6507e-02],\n",
            "          [-8.8276e-03, -1.8698e-02, -1.6637e-03],\n",
            "          [-1.2015e-02,  2.9667e-03,  6.2300e-03]],\n",
            "\n",
            "         [[-1.8341e-02, -9.0521e-03,  2.6030e-02],\n",
            "          [ 3.5930e-02,  5.3049e-02,  5.8487e-02],\n",
            "          [-1.3661e-02, -3.6888e-03, -7.1606e-03]],\n",
            "\n",
            "         [[-1.2594e-02, -4.0898e-02,  1.7162e-03],\n",
            "          [-1.7420e-02, -4.3435e-02, -1.3183e-02],\n",
            "          [-3.7506e-02, -5.5707e-02, -3.0051e-02]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.1971, 0.1771, 0.1303, 0.1995, 0.1839, 0.0934, 0.2333, 0.2236, 0.1654,\n",
            "        0.1280, 0.0842, 0.1085, 0.3168, 0.2032, 0.3246, 0.2184, 0.3208, 0.2824,\n",
            "        0.3408, 0.3339, 0.3307, 0.5571, 0.2821, 0.3081, 0.2114, 0.2971, 0.2361,\n",
            "        0.5500, 0.1221, 0.3381, 0.1528, 0.1544, 0.1982, 0.0582, 0.1812, 0.2489,\n",
            "        0.1954, 0.0705, 0.0918, 0.1328, 0.2616, 0.2013, 0.0720, 0.1573, 0.1919,\n",
            "        0.0813, 0.1170, 0.2504, 0.2863, 0.3032, 0.1476, 0.3696, 0.1870, 0.2097,\n",
            "        0.1907, 0.2364, 0.1642, 0.1079, 0.2531, 0.1703, 0.1266, 0.0814, 0.2407,\n",
            "        0.2609, 0.2705, 0.2128, 0.5007, 0.2375, 0.0802, 0.2896, 0.1776, 0.0887,\n",
            "        0.1094, 0.1834, 0.2812, 0.1971, 0.2021, 0.3443, 0.1411, 0.1362, 0.2676,\n",
            "        0.1618, 0.2723, 0.2727, 0.2528, 0.0982, 0.4707, 0.2239, 0.3649, 0.1987,\n",
            "        0.0815, 0.2543, 0.3322, 0.1561, 0.2336, 0.1294, 0.2570, 0.1700, 0.1374,\n",
            "        0.2215, 0.5015, 0.3132, 0.1487, 0.1174, 0.0916, 0.2130, 0.1393, 0.3057,\n",
            "        0.5634, 0.1018, 0.0994, 0.0492, 0.4427, 0.3142, 0.4002, 0.1334, 0.2174,\n",
            "        0.5522, 0.2806, 0.2784, 0.4333, 0.2602, 0.3788, 0.1827, 0.2664, 0.1077,\n",
            "        0.3001, 0.2428, 0.5130, 0.0829, 0.1254, 0.1996, 0.1451, 0.2253, 0.1467,\n",
            "        0.3712, 0.0794, 0.5425, 0.2058, 0.2103, 0.1288, 0.4993, 0.1815, 0.1845,\n",
            "        0.4154, 0.3817, 0.2054, 0.2205, 0.1471, 0.4964, 0.4202, 0.0801, 0.0623,\n",
            "        0.3536, 0.2760, 0.3840, 0.1632, 0.1402, 0.2674, 0.0844, 0.2305, 0.2259,\n",
            "        0.2146, 0.4181, 0.2821, 0.2926, 0.3416, 0.4640, 0.3025, 0.3732, 0.5871,\n",
            "        0.0616, 0.2797, 0.3042, 0.2173, 0.3550, 0.2096, 0.2449, 0.3428, 0.2868,\n",
            "        0.3543, 0.4667, 0.3220, 0.3805, 0.2632, 0.2160, 0.1924, 0.4074, 0.4966,\n",
            "        0.3623, 0.1670, 0.1321, 0.2374, 0.2118, 0.1522, 0.1668, 0.3836, 0.0983,\n",
            "        0.3729, 0.3943, 0.4353, 0.2270, 0.1508, 0.3133, 0.3850, 0.5774, 0.1892,\n",
            "        0.2822, 0.0907, 0.2364, 0.0964, 0.2360, 0.0699, 0.2938, 0.5100, 0.3348,\n",
            "        0.2339, 0.1145, 0.2155, 0.2266, 0.2829, 0.2341, 0.1891, 0.2906, 0.2681,\n",
            "        0.3876, 0.3915, 0.1844, 0.1889, 0.4405, 0.1405, 0.3460, 0.2724, 0.2567,\n",
            "        0.2785, 0.1148, 0.1607, 0.1754, 0.0883, 0.1649, 0.1268, 0.2356, 0.2811,\n",
            "        0.0766, 0.1424, 0.1683, 0.3979, 0.2685, 0.6383, 0.1087, 0.3180, 0.1760,\n",
            "        0.3634, 0.2615, 0.1999, 0.2541], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0162, -0.2033,  0.0294, -0.1697, -0.1840, -0.0309, -0.2039, -0.1426,\n",
            "        -0.0443, -0.0886, -0.0647, -0.0968, -0.0380, -0.2073, -0.3061,  0.1443,\n",
            "        -0.3079, -0.1232, -0.1627, -0.0980, -0.2471, -0.2837, -0.1201, -0.2893,\n",
            "        -0.2303, -0.3562, -0.0825, -0.3483,  0.0707, -0.1321, -0.1074, -0.1451,\n",
            "         0.0235,  0.0225, -0.1885, -0.2507, -0.2461,  0.0631, -0.0023, -0.1209,\n",
            "        -0.2581, -0.1640, -0.0172, -0.1143, -0.2096, -0.0158,  0.0128, -0.1332,\n",
            "        -0.3139, -0.2294, -0.1527, -0.3503,  0.2086,  0.0785, -0.1597, -0.1990,\n",
            "         0.0346,  0.0388, -0.1269,  0.1019,  0.0981, -0.0390, -0.2537, -0.1356,\n",
            "        -0.1796, -0.2422, -0.4517, -0.3124, -0.0177, -0.2615, -0.1567,  0.0212,\n",
            "        -0.0753, -0.1426, -0.2788,  0.0062, -0.1895, -0.2327, -0.1298, -0.1200,\n",
            "        -0.1917, -0.0987, -0.1916, -0.1666, -0.2729,  0.1287, -0.4620, -0.2259,\n",
            "        -0.2270,  0.1939,  0.0230, -0.3303, -0.3202, -0.1292, -0.0716,  0.0048,\n",
            "        -0.2579, -0.0116, -0.0557, -0.1229, -0.4804, -0.2351, -0.1367, -0.0578,\n",
            "        -0.0537, -0.2743, -0.0827, -0.1922, -0.3481, -0.0358, -0.1094,  0.0138,\n",
            "        -0.1888, -0.2592, -0.3293, -0.0820, -0.1839, -0.1636, -0.3163, -0.0246,\n",
            "        -0.1667, -0.1653, -0.3076, -0.2229, -0.1834, -0.0536, -0.0621, -0.1752,\n",
            "        -0.5243, -0.1933, -0.1119, -0.2283, -0.0437, -0.1777, -0.1300, -0.2519,\n",
            "        -0.0456, -0.6305, -0.1364, -0.2138,  0.0406, -0.5287, -0.2014, -0.1442,\n",
            "        -0.1930, -0.3033,  0.1030, -0.1499, -0.2297, -0.5301, -0.2543, -0.0417,\n",
            "         0.0429, -0.3218, -0.1611, -0.2562, -0.1187, -0.1001,  0.0225,  0.0996,\n",
            "        -0.2138, -0.2019,  0.0808, -0.0121, -0.2364, -0.3247, -0.1482, -0.4846,\n",
            "        -0.3449, -0.1365, -0.6664,  0.0418, -0.2807, -0.0961, -0.2378, -0.1834,\n",
            "        -0.1890, -0.0377, -0.3056, -0.1843, -0.1357, -0.3038, -0.2680, -0.4143,\n",
            "        -0.2633, -0.1750, -0.1856, -0.2405, -0.1082, -0.2250, -0.1268, -0.1094,\n",
            "         0.0594, -0.1419, -0.1178, -0.1602, -0.0328, -0.0194, -0.1985,  0.0470,\n",
            "        -0.1887, -0.2776, -0.0930, -0.4092, -0.3378, -0.7252,  0.0260, -0.1829,\n",
            "         0.0561, -0.2227, -0.0026, -0.3218, -0.0093, -0.2843, -0.5121, -0.2337,\n",
            "        -0.0836, -0.0818, -0.1296, -0.2090,  0.0169, -0.1899, -0.1892, -0.3075,\n",
            "        -0.3108, -0.2986, -0.4712, -0.1823, -0.1893, -0.3131, -0.0876, -0.1166,\n",
            "        -0.2995, -0.0831, -0.3427, -0.0772, -0.1460, -0.1611,  0.0203, -0.0627,\n",
            "        -0.0610, -0.2574, -0.1383,  0.0470, -0.0302, -0.1638, -0.3323, -0.1741,\n",
            "        -0.6307, -0.0772, -0.2123, -0.1559, -0.0459, -0.2416, -0.0143, -0.2079],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-1.1645e-02, -1.9010e-02, -2.1876e-02],\n",
            "          [ 2.0482e-02,  2.3962e-02,  2.9161e-02],\n",
            "          [ 4.3672e-02,  3.3278e-02,  4.9908e-02]],\n",
            "\n",
            "         [[-7.4040e-03,  2.8083e-03, -4.7339e-03],\n",
            "          [ 6.9030e-03,  1.4271e-02, -3.6954e-03],\n",
            "          [-3.1341e-03,  1.3736e-02,  1.6127e-03]],\n",
            "\n",
            "         [[ 1.8676e-02, -1.0553e-02, -1.4233e-02],\n",
            "          [ 8.9944e-03, -2.5068e-03, -1.2145e-02],\n",
            "          [-4.9455e-03, -2.9206e-02, -9.6385e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.2655e-02,  1.7691e-02,  9.8264e-04],\n",
            "          [ 7.4271e-03,  7.6115e-03,  1.1135e-02],\n",
            "          [ 2.3242e-02,  1.1058e-02,  4.0498e-03]],\n",
            "\n",
            "         [[ 1.8557e-02,  1.2472e-02,  1.7220e-02],\n",
            "          [-4.8544e-03,  8.3627e-03,  2.2811e-02],\n",
            "          [-5.1675e-03,  2.3264e-02,  3.4068e-02]],\n",
            "\n",
            "         [[ 2.4934e-02,  2.2373e-02,  4.2614e-02],\n",
            "          [ 1.3486e-02,  1.6760e-03,  1.3019e-02],\n",
            "          [-6.2821e-03, -1.5112e-03, -8.9229e-03]]],\n",
            "\n",
            "\n",
            "        [[[-9.8089e-04, -6.3011e-03,  5.9932e-03],\n",
            "          [ 1.5936e-02,  1.3394e-02,  2.9934e-02],\n",
            "          [ 2.3149e-02,  2.0709e-02,  2.5485e-02]],\n",
            "\n",
            "         [[-2.0015e-02, -3.3349e-02, -8.0396e-03],\n",
            "          [-7.2800e-03, -1.2187e-02, -2.0389e-04],\n",
            "          [-1.3138e-02, -2.0427e-02, -1.6286e-02]],\n",
            "\n",
            "         [[-6.7681e-03,  5.0045e-03, -2.6683e-03],\n",
            "          [-2.1073e-02,  2.8275e-04, -1.8205e-02],\n",
            "          [-1.7382e-02, -5.0244e-03, -3.0386e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.1035e-02, -2.2964e-02, -1.1028e-02],\n",
            "          [-6.3256e-03, -4.1667e-03, -1.7323e-02],\n",
            "          [-1.3611e-02, -2.3468e-02, -1.6436e-02]],\n",
            "\n",
            "         [[ 7.3663e-03,  6.6219e-03,  5.2776e-03],\n",
            "          [-3.5464e-03,  3.2750e-03, -9.1126e-03],\n",
            "          [ 3.5593e-04, -1.0151e-02, -1.9123e-02]],\n",
            "\n",
            "         [[ 1.8193e-03,  8.8087e-03,  5.1361e-03],\n",
            "          [ 3.1915e-03,  2.5287e-02,  2.4939e-02],\n",
            "          [ 1.3968e-02,  1.9613e-02,  2.2382e-02]]],\n",
            "\n",
            "\n",
            "        [[[-4.1548e-03,  8.8964e-03,  2.0143e-03],\n",
            "          [ 1.1327e-02,  1.3251e-02,  1.4014e-02],\n",
            "          [ 7.2196e-03,  1.3045e-02,  2.4827e-02]],\n",
            "\n",
            "         [[-1.5025e-02,  5.0530e-03,  7.4766e-03],\n",
            "          [-2.4685e-02, -1.6732e-02, -1.0888e-02],\n",
            "          [-2.8064e-02, -1.1875e-02, -3.4120e-03]],\n",
            "\n",
            "         [[ 2.8449e-02,  1.4594e-02,  6.9441e-03],\n",
            "          [ 2.4799e-02,  1.9453e-02,  1.1294e-02],\n",
            "          [-1.0787e-02, -2.1006e-02, -1.0372e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.4967e-02,  8.2449e-03,  2.0244e-03],\n",
            "          [ 1.4287e-02, -6.3867e-03, -8.0757e-03],\n",
            "          [ 2.7547e-02,  1.0791e-02,  1.6567e-02]],\n",
            "\n",
            "         [[ 3.6191e-02,  3.8918e-02,  3.9028e-02],\n",
            "          [-8.3489e-04,  1.3273e-02,  2.0172e-02],\n",
            "          [-2.0652e-02, -5.4010e-03,  1.7147e-03]],\n",
            "\n",
            "         [[ 2.0373e-04,  3.5919e-03,  8.5592e-03],\n",
            "          [ 6.2363e-03, -9.3086e-05,  1.2940e-02],\n",
            "          [ 1.3152e-02,  1.0732e-02,  1.9896e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1.7400e-02, -6.7019e-03, -9.1787e-03],\n",
            "          [-9.9672e-03,  2.6298e-04,  3.3439e-03],\n",
            "          [ 1.5721e-02,  1.4216e-02,  2.0509e-02]],\n",
            "\n",
            "         [[ 2.1410e-02,  3.6914e-02,  2.8239e-02],\n",
            "          [ 3.8158e-02,  4.8944e-02,  3.4652e-02],\n",
            "          [ 3.1723e-02,  4.4208e-02,  4.0035e-02]],\n",
            "\n",
            "         [[-3.3437e-03, -1.0482e-02, -5.3990e-03],\n",
            "          [-5.3186e-03,  1.1394e-02,  1.7593e-03],\n",
            "          [-5.6652e-03, -6.6373e-03, -1.3492e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.7099e-02, -1.8145e-03, -1.3040e-02],\n",
            "          [-2.2750e-02, -3.6062e-03, -8.0294e-03],\n",
            "          [-1.6087e-02, -1.0175e-02, -1.3529e-02]],\n",
            "\n",
            "         [[ 4.1701e-04, -5.1785e-03, -2.1884e-02],\n",
            "          [ 2.6919e-03,  8.9139e-03, -1.4217e-04],\n",
            "          [-7.3746e-03, -6.6853e-03, -2.3725e-02]],\n",
            "\n",
            "         [[ 1.9425e-02,  1.3175e-02,  1.7511e-02],\n",
            "          [ 1.8235e-02,  4.4286e-02,  2.3767e-02],\n",
            "          [ 2.6504e-02,  3.3104e-02,  1.9696e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.0177e-02, -1.0701e-02, -2.0428e-02],\n",
            "          [-1.7986e-02,  5.9928e-03, -1.0584e-03],\n",
            "          [-1.8794e-02, -1.8773e-03, -6.9449e-03]],\n",
            "\n",
            "         [[-2.8498e-03,  1.6427e-03,  1.4575e-04],\n",
            "          [-5.4403e-03,  8.3667e-03, -9.4164e-03],\n",
            "          [-4.4999e-03,  5.4902e-03,  2.4863e-03]],\n",
            "\n",
            "         [[-1.3356e-02, -2.1525e-02,  5.3421e-04],\n",
            "          [-1.9160e-02, -2.4645e-02, -1.3791e-02],\n",
            "          [-6.1991e-03, -1.3174e-02, -3.6783e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.3993e-03, -2.7823e-03,  7.6715e-03],\n",
            "          [-2.0649e-02, -1.2731e-02, -9.4138e-03],\n",
            "          [-1.3678e-03, -3.4410e-02, -2.6984e-02]],\n",
            "\n",
            "         [[-3.5651e-04,  2.0102e-03,  1.4130e-02],\n",
            "          [-1.3073e-02, -1.6616e-02, -1.2690e-02],\n",
            "          [-3.5934e-02, -4.1700e-02, -3.3968e-02]],\n",
            "\n",
            "         [[ 2.0470e-02,  8.0159e-04, -1.1607e-03],\n",
            "          [ 9.5101e-03,  3.0336e-02,  2.7362e-02],\n",
            "          [ 1.5588e-02,  3.2851e-02,  1.3015e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.5574e-02, -3.2971e-02, -3.1939e-02],\n",
            "          [-2.2502e-02, -5.7187e-03, -5.6729e-03],\n",
            "          [-2.7309e-02, -1.6981e-02,  1.2832e-04]],\n",
            "\n",
            "         [[-1.1925e-02, -2.9479e-02, -2.0437e-02],\n",
            "          [-2.4408e-02, -2.2069e-02, -1.9965e-03],\n",
            "          [-2.3279e-02, -5.5140e-03,  2.5630e-02]],\n",
            "\n",
            "         [[-1.6100e-02, -8.2417e-03,  1.5266e-04],\n",
            "          [-2.6195e-03, -8.2754e-03, -2.9435e-02],\n",
            "          [-2.7493e-03, -2.4889e-02, -2.3583e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.7985e-02,  1.8594e-02,  8.9198e-04],\n",
            "          [-1.7319e-02,  7.8735e-03, -2.8659e-03],\n",
            "          [ 3.8596e-03,  2.9061e-02,  2.4188e-02]],\n",
            "\n",
            "         [[-2.6735e-02, -1.4391e-02, -4.0148e-02],\n",
            "          [-2.6728e-02, -2.4455e-02, -6.9176e-03],\n",
            "          [-5.7244e-02, -2.1995e-04,  5.5438e-02]],\n",
            "\n",
            "         [[ 2.3487e-02,  2.7157e-03, -8.4719e-04],\n",
            "          [ 1.7886e-02,  5.4860e-03,  2.8059e-02],\n",
            "          [ 4.6468e-03,  1.8598e-02,  1.3761e-03]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.2427, 0.2232, 0.2511, 0.2288, 0.2074, 0.2905, 0.2482, 0.3102, 0.2749,\n",
            "        0.2892, 0.2448, 0.1759, 0.2426, 0.2780, 0.2315, 0.2631, 0.3383, 0.2785,\n",
            "        0.2536, 0.2989, 0.2335, 0.2812, 0.3486, 0.2778, 0.2280, 0.2547, 0.3032,\n",
            "        0.2468, 0.2512, 0.2973, 0.2577, 0.3200, 0.2385, 0.2714, 0.2532, 0.2625,\n",
            "        0.3344, 0.2626, 0.1838, 0.2839, 0.2187, 0.2666, 0.2858, 0.2471, 0.2915,\n",
            "        0.2332, 0.2637, 0.2691, 0.2432, 0.2384, 0.2356, 0.2525, 0.2564, 0.2451,\n",
            "        0.2529, 0.2522, 0.2800, 0.3165, 0.2340, 0.2634, 0.2569, 0.1942, 0.2621,\n",
            "        0.2205, 0.2301, 0.2323, 0.2811, 0.1897, 0.2280, 0.3472, 0.2717, 0.3191,\n",
            "        0.2440, 0.2719, 0.2781, 0.2262, 0.3444, 0.2648, 0.2725, 0.2851, 0.2039,\n",
            "        0.2935, 0.2742, 0.2774, 0.2654, 0.2430, 0.2721, 0.2708, 0.3085, 0.2895,\n",
            "        0.2596, 0.2147, 0.3119, 0.3449, 0.2262, 0.2814, 0.2326, 0.2712, 0.2637,\n",
            "        0.2323, 0.3333, 0.2714, 0.2991, 0.2747, 0.2515, 0.2394, 0.2709, 0.2836,\n",
            "        0.2866, 0.2408, 0.2560, 0.2048, 0.2394, 0.2813, 0.3267, 0.2761, 0.2123,\n",
            "        0.2715, 0.2540, 0.2771, 0.3209, 0.1905, 0.3989, 0.2676, 0.2357, 0.2169,\n",
            "        0.3216, 0.3596, 0.2838, 0.2648, 0.2702, 0.2469, 0.2442, 0.2553, 0.2599,\n",
            "        0.2693, 0.2399, 0.2700, 0.2063, 0.2711, 0.2834, 0.2781, 0.2529, 0.2013,\n",
            "        0.2343, 0.2082, 0.3063, 0.1635, 0.2673, 0.2197, 0.2787, 0.2724, 0.2744,\n",
            "        0.2287, 0.2969, 0.2662, 0.2982, 0.2396, 0.3039, 0.2319, 0.2773, 0.2661,\n",
            "        0.2898, 0.2489, 0.3060, 0.2612, 0.2937, 0.3045, 0.2999, 0.2580, 0.2093,\n",
            "        0.2714, 0.2993, 0.2679, 0.2963, 0.2754, 0.2580, 0.2566, 0.2634, 0.2325,\n",
            "        0.2442, 0.2934, 0.2398, 0.2631, 0.2851, 0.2870, 0.2239, 0.2410, 0.2676,\n",
            "        0.2681, 0.2638, 0.2732, 0.2812, 0.2203, 0.2670, 0.2764, 0.2550, 0.3160,\n",
            "        0.2888, 0.2615, 0.2178, 0.2485, 0.2414, 0.2798, 0.2872, 0.2767, 0.2551,\n",
            "        0.2429, 0.2459, 0.3288, 0.3024, 0.2912, 0.2625, 0.3019, 0.2643, 0.2721,\n",
            "        0.2108, 0.2368, 0.2269, 0.1988, 0.2830, 0.2569, 0.2349, 0.2755, 0.2442,\n",
            "        0.2717, 0.2747, 0.2785, 0.2516, 0.2227, 0.2783, 0.2465, 0.2652, 0.2641,\n",
            "        0.2960, 0.2671, 0.2679, 0.2537, 0.2847, 0.2507, 0.2525, 0.2024, 0.2311,\n",
            "        0.2618, 0.2764, 0.3031, 0.2452, 0.2716, 0.2273, 0.2295, 0.2611, 0.2329,\n",
            "        0.2690, 0.2753, 0.2737, 0.2590, 0.2421, 0.2685, 0.3392, 0.3073, 0.1371,\n",
            "        0.3650, 0.2980, 0.2460, 0.2487, 0.2912, 0.2704, 0.2560, 0.2213, 0.2569,\n",
            "        0.2661, 0.2367, 0.2742, 0.2847, 0.3055, 0.2671, 0.2819, 0.2791, 0.2401,\n",
            "        0.2549, 0.2210, 0.3507, 0.2852, 0.2162, 0.2821, 0.2369, 0.2905, 0.2826,\n",
            "        0.2300, 0.2745, 0.2437, 0.2522, 0.2489, 0.2395, 0.2851, 0.2887, 0.2621,\n",
            "        0.2500, 0.2689, 0.2427, 0.3010, 0.3067, 0.2861, 0.2387, 0.2462, 0.2859,\n",
            "        0.2550, 0.2630, 0.2442, 0.2145, 0.2898, 0.2282, 0.2327, 0.2242, 0.2738,\n",
            "        0.2485, 0.2379, 0.3058, 0.2798, 0.2761, 0.2252, 0.2866, 0.2660, 0.3250,\n",
            "        0.2612, 0.2767, 0.3205, 0.2932, 0.3183, 0.2939, 0.3103, 0.2553, 0.2981,\n",
            "        0.3667, 0.3086, 0.2254, 0.2352, 0.2348, 0.2555, 0.2597, 0.2369, 0.3017,\n",
            "        0.2776, 0.2728, 0.3174, 0.2785, 0.2721, 0.2637, 0.2702, 0.3633, 0.2869,\n",
            "        0.2675, 0.3405, 0.2587, 0.2732, 0.2747, 0.2821, 0.2750, 0.2630, 0.2018,\n",
            "        0.2358, 0.3034, 0.3155, 0.3013, 0.2775, 0.2511, 0.2945, 0.1605, 0.2825,\n",
            "        0.2964, 0.2194, 0.2061, 0.2332, 0.2348, 0.2663, 0.2543, 0.2927, 0.2215,\n",
            "        0.2521, 0.2827, 0.1993, 0.2453, 0.2597, 0.2654, 0.2757, 0.2650, 0.2444,\n",
            "        0.2949, 0.2308, 0.3071, 0.1904, 0.3024, 0.2786, 0.3659, 0.2966, 0.2746,\n",
            "        0.2449, 0.2201, 0.2564, 0.2853, 0.2392, 0.2457, 0.2467, 0.2374, 0.2664,\n",
            "        0.2460, 0.3182, 0.1793, 0.2379, 0.2596, 0.2847, 0.2452, 0.1974, 0.2388,\n",
            "        0.2949, 0.2879, 0.2786, 0.2765, 0.3296, 0.2530, 0.2690, 0.2547, 0.2333,\n",
            "        0.2348, 0.2690, 0.2718, 0.2679, 0.2516, 0.2710, 0.2366, 0.2601, 0.2764,\n",
            "        0.2880, 0.2008, 0.2637, 0.2263, 0.2511, 0.2604, 0.2805, 0.2989, 0.2965,\n",
            "        0.2597, 0.2767, 0.2553, 0.2959, 0.2512, 0.2925, 0.3008, 0.2423, 0.2394,\n",
            "        0.2708, 0.3704, 0.2879, 0.2532, 0.2248, 0.2023, 0.2279, 0.2366, 0.3082,\n",
            "        0.2980, 0.2909, 0.2777, 0.4293, 0.2658, 0.2940, 0.2418, 0.2816, 0.3247,\n",
            "        0.2647, 0.2216, 0.2758, 0.2421, 0.2078, 0.2332, 0.2271, 0.2611, 0.3650,\n",
            "        0.2017, 0.2598, 0.2160, 0.2641, 0.1408, 0.2664, 0.2502, 0.2553, 0.2227,\n",
            "        0.2417, 0.2696, 0.2388, 0.2833, 0.2333, 0.2667, 0.2224, 0.2691, 0.2710,\n",
            "        0.2459, 0.2674, 0.2430, 0.2593, 0.1851, 0.2950, 0.3664, 0.2212, 0.3026,\n",
            "        0.1840, 0.3443, 0.2140, 0.3717, 0.2360, 0.3081, 0.2638, 0.2233],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1986, -0.1593, -0.2054, -0.1598, -0.1268, -0.3226, -0.1597, -0.3477,\n",
            "        -0.2497, -0.2730, -0.2319, -0.0286, -0.1899, -0.2813, -0.1733, -0.2412,\n",
            "        -0.3712, -0.2747, -0.2053, -0.2585, -0.1535, -0.2748, -0.3241, -0.2525,\n",
            "        -0.1906, -0.2252, -0.3436, -0.2202, -0.1664, -0.2716, -0.1920, -0.3399,\n",
            "        -0.2026, -0.2972, -0.2616, -0.2238, -0.2486, -0.2606, -0.0893, -0.3572,\n",
            "        -0.1283, -0.2583, -0.2450, -0.1523, -0.3165, -0.1445, -0.2522, -0.1963,\n",
            "        -0.1794, -0.1071, -0.1662, -0.2053, -0.2530, -0.1447, -0.2517, -0.2062,\n",
            "        -0.2817, -0.3376, -0.1382, -0.2389, -0.2557, -0.0156, -0.2169, -0.1763,\n",
            "        -0.1486, -0.2122, -0.2002, -0.0716, -0.2089, -0.3580, -0.2588, -0.3599,\n",
            "        -0.1528, -0.2107, -0.2925, -0.1855, -0.3970, -0.1257, -0.2574, -0.2412,\n",
            "        -0.0863, -0.3065, -0.2701, -0.3380, -0.2485, -0.1935, -0.2987, -0.2279,\n",
            "        -0.3600, -0.2764, -0.2480, -0.1208, -0.3378, -0.2661, -0.1677, -0.2470,\n",
            "        -0.2152, -0.2591, -0.1936, -0.1543, -0.4117, -0.1570, -0.2372, -0.2997,\n",
            "        -0.2124, -0.2034, -0.1848, -0.3070, -0.3438, -0.1839, -0.1937, -0.0916,\n",
            "        -0.2338, -0.3558, -0.1967, -0.3303, -0.1398, -0.2177, -0.1665, -0.1857,\n",
            "        -0.3115, -0.1049, -0.4229, -0.2408, -0.1320, -0.1631, -0.3378, -0.3300,\n",
            "        -0.3183, -0.2268, -0.2787, -0.1950, -0.1950, -0.1463, -0.2437, -0.2297,\n",
            "        -0.1282, -0.2164, -0.1179, -0.2437, -0.2611, -0.2656, -0.1948, -0.1208,\n",
            "        -0.1668, -0.1351, -0.2713, -0.0560, -0.2243, -0.1318, -0.2356, -0.2720,\n",
            "        -0.2051, -0.1736, -0.2891, -0.2627, -0.3358, -0.1779, -0.2309, -0.1477,\n",
            "        -0.2685, -0.1882, -0.2629, -0.1983, -0.3522, -0.1905, -0.2778, -0.3395,\n",
            "        -0.2895, -0.2240, -0.1150, -0.2462, -0.2426, -0.2581, -0.3133, -0.2315,\n",
            "        -0.2271, -0.2077, -0.2109, -0.1371, -0.1323, -0.2529, -0.1716, -0.2532,\n",
            "        -0.2277, -0.2084, -0.1803, -0.1868, -0.2404, -0.2166, -0.2197, -0.2870,\n",
            "        -0.3062, -0.1507, -0.1054, -0.2199, -0.2415, -0.3310, -0.2700, -0.1568,\n",
            "        -0.1449, -0.2610, -0.1828, -0.2648, -0.3134, -0.2937, -0.2687, -0.2115,\n",
            "        -0.2164, -0.4522, -0.2999, -0.3032, -0.2292, -0.3099, -0.2642, -0.2695,\n",
            "        -0.1441, -0.1671, -0.1570, -0.1415, -0.2222, -0.1736, -0.1481, -0.2573,\n",
            "        -0.2060, -0.1703, -0.2360, -0.1770, -0.2132, -0.2016, -0.3001, -0.1518,\n",
            "        -0.2086, -0.2805, -0.2698, -0.2292, -0.1293, -0.2514, -0.2600, -0.2454,\n",
            "        -0.1744, -0.1029, -0.1679, -0.2353, -0.2007, -0.3363, -0.1640, -0.2430,\n",
            "        -0.1699, -0.1697, -0.1837, -0.1625, -0.2415, -0.2687, -0.2305, -0.2029,\n",
            "        -0.2209, -0.2240, -0.2675, -0.3233,  0.1462, -0.4777, -0.2376, -0.1489,\n",
            "        -0.1462, -0.3055, -0.2234, -0.1697, -0.1952, -0.2131, -0.2340, -0.2039,\n",
            "        -0.3054, -0.2596, -0.3470, -0.2176, -0.2706, -0.2897, -0.1729, -0.2300,\n",
            "        -0.1066, -0.3556, -0.2912, -0.1777, -0.2007, -0.1699, -0.3009, -0.3046,\n",
            "        -0.1693, -0.2602, -0.2053, -0.1810, -0.1808, -0.1730, -0.3757, -0.1808,\n",
            "        -0.1805, -0.1895, -0.2643, -0.2075, -0.2365, -0.1975, -0.3064, -0.1984,\n",
            "        -0.1811, -0.3676, -0.1198, -0.1485, -0.1770, -0.0781, -0.2052, -0.1360,\n",
            "        -0.1417, -0.1691, -0.2395, -0.1785, -0.1747, -0.2484, -0.2717, -0.3096,\n",
            "        -0.1465, -0.2239, -0.2584, -0.3572, -0.2311, -0.2878, -0.3841, -0.3475,\n",
            "        -0.3896, -0.1891, -0.2861, -0.2431, -0.2837, -0.4365, -0.3353, -0.1802,\n",
            "        -0.1976, -0.1529, -0.1978, -0.2535, -0.1954, -0.2667, -0.2813, -0.2487,\n",
            "        -0.3070, -0.2339, -0.2212, -0.1925, -0.2224, -0.4178, -0.3151, -0.2663,\n",
            "        -0.3581, -0.1935, -0.2385, -0.2424, -0.1850, -0.2265, -0.1803, -0.0777,\n",
            "        -0.1492, -0.3361, -0.4133, -0.3123, -0.2745, -0.1247, -0.3102,  0.0041,\n",
            "        -0.1981, -0.3301, -0.2047, -0.1053, -0.1653, -0.1634, -0.1116, -0.2314,\n",
            "        -0.3191, -0.1818, -0.2657, -0.2220, -0.1029, -0.1999, -0.2702, -0.2139,\n",
            "        -0.2256, -0.2653, -0.1630, -0.3322, -0.1617, -0.3446,  0.0288, -0.2456,\n",
            "        -0.3171, -0.3580, -0.2857, -0.2520, -0.2031, -0.1522, -0.2203, -0.3490,\n",
            "        -0.1685, -0.1424, -0.1602, -0.1553, -0.3057, -0.2420, -0.3536, -0.0551,\n",
            "        -0.0987, -0.2272, -0.2619, -0.2035, -0.0906, -0.1976, -0.3040, -0.2732,\n",
            "        -0.3161, -0.2102, -0.3384, -0.1740, -0.1475, -0.1842, -0.1823, -0.1151,\n",
            "        -0.2183, -0.2010, -0.2659, -0.2205, -0.2567, -0.1633, -0.2213, -0.2658,\n",
            "        -0.2938, -0.1069, -0.2522, -0.1103, -0.2216, -0.2244, -0.2908, -0.2176,\n",
            "        -0.3605, -0.2374, -0.2391, -0.2251, -0.2256, -0.1339, -0.1970, -0.2970,\n",
            "        -0.2206, -0.2051, -0.2229, -0.3602, -0.2923, -0.2498, -0.1466, -0.0979,\n",
            "        -0.1686, -0.2158, -0.2881, -0.3002, -0.2760, -0.2496, -0.3536, -0.2868,\n",
            "        -0.3251, -0.1847, -0.3062, -0.3861, -0.2650, -0.1339, -0.1846, -0.1630,\n",
            "        -0.0630, -0.1717, -0.1415, -0.1906, -0.4611, -0.1391, -0.1920, -0.1369,\n",
            "        -0.1647, -0.0055, -0.2598, -0.2653, -0.2319, -0.1780, -0.1913, -0.2055,\n",
            "        -0.1891, -0.2625, -0.1633, -0.2497, -0.1696, -0.1907, -0.2431, -0.1825,\n",
            "        -0.2607, -0.1943, -0.2361, -0.0581, -0.2758, -0.2593, -0.1466, -0.3589,\n",
            "        -0.0439, -0.3440, -0.1089, -0.4219, -0.1503, -0.2792, -0.3035, -0.1156],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 1.6218e-04, -1.4720e-02, -1.7000e-02],\n",
            "          [-1.2850e-02, -3.3085e-02, -3.6656e-02],\n",
            "          [ 2.7812e-02,  1.7691e-02, -1.8369e-02]],\n",
            "\n",
            "         [[ 1.0528e-02,  3.1379e-02,  2.4801e-02],\n",
            "          [-1.2698e-02, -2.9453e-02, -1.1834e-02],\n",
            "          [-9.4094e-03, -8.9462e-03, -3.1349e-02]],\n",
            "\n",
            "         [[-7.8447e-03, -2.9256e-02,  5.3590e-03],\n",
            "          [-1.3791e-02, -1.1116e-02,  5.0388e-03],\n",
            "          [-2.4919e-03,  7.3514e-03,  5.4013e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.0276e-03, -1.0275e-02, -2.9986e-02],\n",
            "          [-3.8465e-03,  1.9549e-03, -1.6291e-02],\n",
            "          [-1.8100e-03,  8.3778e-03, -8.5481e-03]],\n",
            "\n",
            "         [[-1.8196e-02, -1.3533e-02, -1.7457e-02],\n",
            "          [ 2.2457e-02,  5.7402e-02,  1.9325e-02],\n",
            "          [-2.4977e-02, -3.2113e-02, -8.1780e-03]],\n",
            "\n",
            "         [[ 3.6550e-03,  4.9358e-03, -5.7597e-03],\n",
            "          [-1.6875e-02,  1.3999e-04,  3.7629e-04],\n",
            "          [-2.6272e-03,  1.0947e-03,  1.1145e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 1.4018e-02,  3.9198e-03, -1.7189e-03],\n",
            "          [-1.3175e-03,  4.3503e-04, -1.1798e-02],\n",
            "          [-9.8003e-03, -1.7693e-02, -1.9910e-02]],\n",
            "\n",
            "         [[-1.4957e-02, -1.9796e-02, -2.8724e-02],\n",
            "          [ 5.8908e-03, -1.5228e-02, -5.6715e-03],\n",
            "          [ 2.9284e-03, -1.8028e-02, -7.1433e-03]],\n",
            "\n",
            "         [[-1.1625e-02, -3.3804e-02, -1.0025e-02],\n",
            "          [-1.6606e-02, -5.5716e-02, -2.3204e-02],\n",
            "          [-2.5758e-02, -4.3135e-02, -2.5901e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.5007e-02, -1.4333e-02, -2.5937e-03],\n",
            "          [-2.3078e-02, -1.5820e-02, -2.2818e-03],\n",
            "          [-4.1318e-03, -8.0353e-03, -2.3236e-03]],\n",
            "\n",
            "         [[-1.8531e-02, -1.8004e-02, -2.8084e-02],\n",
            "          [-3.6680e-02, -6.8641e-02, -5.2469e-02],\n",
            "          [-1.1712e-02, -2.4334e-02, -1.6733e-02]],\n",
            "\n",
            "         [[-2.2078e-02, -2.9163e-02, -3.8717e-03],\n",
            "          [-7.0301e-03,  1.6718e-02,  5.4339e-03],\n",
            "          [-1.3131e-02,  1.1999e-02, -1.7480e-02]]],\n",
            "\n",
            "\n",
            "        [[[-5.2378e-03, -3.4890e-03, -2.0851e-03],\n",
            "          [ 1.5306e-02, -2.1752e-02, -8.7682e-03],\n",
            "          [ 2.2460e-02,  9.9175e-03, -3.3635e-03]],\n",
            "\n",
            "         [[ 7.4677e-03, -9.1762e-03, -9.2569e-05],\n",
            "          [ 1.9441e-04,  1.2344e-03, -8.9978e-03],\n",
            "          [-5.1243e-04,  2.1850e-04, -4.8828e-03]],\n",
            "\n",
            "         [[ 1.7078e-02,  3.3955e-03,  9.3503e-03],\n",
            "          [ 2.0334e-02, -1.0621e-04, -8.2017e-05],\n",
            "          [ 1.0706e-02, -1.8414e-03,  1.0828e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.2008e-02,  2.3494e-02,  2.5386e-02],\n",
            "          [ 1.9307e-02,  2.3924e-02,  2.8972e-02],\n",
            "          [ 9.9003e-03,  2.0158e-02,  2.2655e-02]],\n",
            "\n",
            "         [[-9.8395e-03, -1.1114e-02, -3.7696e-03],\n",
            "          [-2.9508e-02, -3.6956e-02, -1.8228e-02],\n",
            "          [-1.3663e-03, -2.5845e-03,  1.0352e-02]],\n",
            "\n",
            "         [[-7.3867e-03, -2.5413e-02, -2.1942e-02],\n",
            "          [-1.6699e-02, -1.5133e-02, -1.3030e-02],\n",
            "          [-2.0090e-02,  3.7970e-03, -1.0341e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1.6157e-02, -1.6883e-02, -2.8328e-04],\n",
            "          [-7.7759e-03, -2.4465e-03, -1.4641e-02],\n",
            "          [ 2.4639e-02,  3.9862e-02,  2.1048e-02]],\n",
            "\n",
            "         [[ 2.4491e-03, -9.3885e-03, -1.1786e-02],\n",
            "          [ 2.5301e-02,  2.5625e-04,  7.1335e-03],\n",
            "          [ 2.2342e-02,  1.9042e-02,  7.2526e-03]],\n",
            "\n",
            "         [[-1.4652e-02, -2.7802e-02, -4.3564e-03],\n",
            "          [-1.7961e-02, -4.3846e-02,  2.7409e-03],\n",
            "          [-4.7968e-03, -8.4231e-03,  1.2070e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.0171e-02, -3.3546e-02, -1.6728e-02],\n",
            "          [-1.7847e-02, -5.1713e-02, -2.6780e-02],\n",
            "          [-1.3145e-03, -4.3181e-03, -9.6373e-03]],\n",
            "\n",
            "         [[-5.3917e-03, -2.0410e-04,  2.7798e-03],\n",
            "          [-9.6882e-04, -2.5141e-02,  1.4804e-02],\n",
            "          [ 2.8748e-02,  9.0832e-03,  4.2548e-02]],\n",
            "\n",
            "         [[-1.5698e-02, -1.9303e-02, -9.1469e-03],\n",
            "          [-2.0025e-02, -1.1131e-02, -3.3902e-02],\n",
            "          [-5.7436e-03, -7.3640e-03, -1.0044e-02]]],\n",
            "\n",
            "\n",
            "        [[[-8.8612e-03, -4.5370e-03, -1.2354e-02],\n",
            "          [-5.9245e-03, -1.7058e-02, -2.8041e-02],\n",
            "          [-1.0435e-02,  7.6695e-04, -1.0578e-02]],\n",
            "\n",
            "         [[ 9.5200e-03, -5.1975e-03,  1.2947e-02],\n",
            "          [ 4.4305e-03, -2.3992e-02, -8.4569e-04],\n",
            "          [ 4.6608e-03,  9.6787e-03,  8.2174e-03]],\n",
            "\n",
            "         [[ 5.1559e-03,  4.4635e-04, -7.9934e-03],\n",
            "          [ 3.3069e-03,  1.4450e-02,  8.9234e-03],\n",
            "          [ 6.3402e-03,  1.9043e-02,  1.9021e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.6964e-03, -1.3777e-02,  6.0539e-03],\n",
            "          [-1.5745e-03, -2.3391e-02, -1.0052e-02],\n",
            "          [ 9.5183e-03, -1.2251e-02,  2.2436e-03]],\n",
            "\n",
            "         [[ 1.0375e-02,  3.5875e-03, -5.7940e-04],\n",
            "          [ 7.0412e-03, -1.0673e-02, -4.9120e-03],\n",
            "          [-2.6034e-03,  1.1306e-02,  7.0696e-03]],\n",
            "\n",
            "         [[-1.7509e-02, -2.3182e-02, -1.7897e-02],\n",
            "          [-1.7769e-03,  1.9672e-03, -7.3220e-03],\n",
            "          [-6.6833e-03,  9.8286e-03,  2.0653e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 2.8375e-02, -8.1936e-03,  1.8009e-02],\n",
            "          [ 1.5829e-02, -1.3571e-02, -1.9335e-02],\n",
            "          [ 4.0766e-03, -1.5722e-02, -5.0620e-02]],\n",
            "\n",
            "         [[-5.5310e-03, -1.8996e-02, -7.9436e-03],\n",
            "          [ 1.3825e-03, -4.9608e-02,  1.7256e-03],\n",
            "          [ 7.6629e-03, -7.6101e-03,  1.2541e-02]],\n",
            "\n",
            "         [[ 1.8052e-02,  3.1718e-02,  4.2556e-03],\n",
            "          [-3.6760e-03,  3.0490e-03, -1.2264e-02],\n",
            "          [-8.9404e-03, -1.6604e-02,  1.6348e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.3192e-03,  1.8204e-02,  1.8114e-02],\n",
            "          [-6.1202e-03,  1.5905e-03,  2.0264e-02],\n",
            "          [-1.1471e-02, -1.5697e-02,  9.0871e-03]],\n",
            "\n",
            "         [[ 3.7707e-03,  8.0599e-03,  1.8290e-02],\n",
            "          [ 1.7257e-02,  6.9638e-03,  1.8746e-02],\n",
            "          [ 1.0751e-02,  1.3663e-02, -1.0081e-03]],\n",
            "\n",
            "         [[ 1.9711e-02, -1.4569e-02, -2.4663e-02],\n",
            "          [ 2.5966e-03, -2.4807e-02,  9.3861e-03],\n",
            "          [-1.2876e-03,  1.3974e-03,  1.3434e-02]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.4474, 0.5138, 0.4335, 0.3421, 0.3855, 0.3495, 0.3741, 0.5836, 0.4327,\n",
            "        0.5043, 0.4618, 0.3866, 0.3498, 0.4798, 0.3310, 0.3913, 0.3880, 0.5225,\n",
            "        0.3975, 0.3292, 0.4151, 0.4458, 0.3970, 0.3614, 0.3914, 0.4633, 0.3463,\n",
            "        0.3644, 0.3272, 0.4584, 0.4280, 0.4538, 0.4030, 0.4673, 0.4209, 0.3987,\n",
            "        0.4233, 0.3876, 0.4212, 0.3460, 0.3522, 0.3744, 0.4550, 0.2888, 0.4590,\n",
            "        0.4817, 0.4450, 0.5110, 0.4052, 0.4247, 0.3558, 0.3075, 0.4462, 0.4724,\n",
            "        0.4253, 0.3884, 0.4492, 0.3727, 0.4630, 0.3985, 0.3512, 0.3665, 0.3860,\n",
            "        0.5082, 0.4022, 0.3458, 0.4805, 0.5390, 0.4223, 0.4275, 0.4590, 0.4736,\n",
            "        0.3673, 0.5405, 0.3243, 0.5178, 0.4743, 0.3506, 0.3759, 0.4328, 0.3867,\n",
            "        0.4591, 0.3843, 0.4982, 0.5288, 0.3946, 0.4589, 0.3197, 0.4676, 0.4806,\n",
            "        0.4308, 0.4235, 0.3284, 0.3877, 0.4140, 0.4469, 0.4041, 0.4407, 0.4356,\n",
            "        0.5120, 0.5059, 0.4628, 0.4585, 0.3311, 0.3424, 0.4150, 0.5170, 0.4593,\n",
            "        0.5228, 0.4252, 0.4214, 0.4995, 0.4098, 0.5380, 0.4874, 0.3719, 0.4649,\n",
            "        0.4320, 0.3277, 0.3743, 0.4360, 0.4838, 0.4399, 0.3763, 0.4150, 0.5147,\n",
            "        0.5012, 0.4382, 0.3655, 0.4037, 0.4498, 0.4720, 0.3914, 0.3237, 0.3208,\n",
            "        0.3224, 0.4291, 0.4009, 0.3947, 0.3779, 0.4349, 0.4120, 0.3274, 0.4334,\n",
            "        0.3740, 0.4189, 0.4288, 0.3071, 0.4260, 0.3410, 0.4375, 0.4407, 0.3750,\n",
            "        0.5853, 0.4518, 0.5045, 0.3005, 0.4968, 0.4155, 0.3755, 0.5514, 0.4146,\n",
            "        0.4677, 0.1404, 0.5001, 0.4193, 0.4246, 0.4452, 0.5109, 0.4488, 0.4574,\n",
            "        0.3896, 0.4145, 0.4497, 0.4245, 0.3971, 0.3957, 0.4072, 0.5305, 0.4986,\n",
            "        0.3733, 0.4280, 0.3469, 0.4178, 0.3766, 0.4029, 0.3814, 0.4493, 0.5132,\n",
            "        0.4080, 0.4155, 0.3635, 0.4391, 0.3489, 0.4228, 0.4833, 0.3494, 0.4406,\n",
            "        0.3795, 0.4298, 0.4910, 0.3878, 0.6299, 0.4322, 0.5436, 0.4140, 0.4312,\n",
            "        0.3161, 0.3612, 0.3597, 0.4281, 0.4506, 0.4294, 0.3646, 0.4110, 0.4038,\n",
            "        0.4098, 0.3901, 0.3928, 0.5421, 0.3629, 0.4078, 0.4586, 0.4217, 0.3953,\n",
            "        0.3997, 0.3838, 0.4374, 0.3576, 0.4217, 0.4128, 0.3904, 0.4137, 0.5145,\n",
            "        0.4039, 0.3577, 0.4429, 0.5639, 0.3848, 0.6104, 0.4482, 0.6203, 0.5336,\n",
            "        0.3480, 0.5401, 0.6044, 0.4077, 0.3469, 0.4281, 0.4631, 0.5948, 0.3479,\n",
            "        0.3689, 0.3658, 0.3191, 0.5492, 0.3410, 0.5386, 0.4041, 0.3373, 0.4186,\n",
            "        0.5187, 0.3933, 0.3188, 0.3502, 0.3736, 0.4238, 0.4752, 0.3322, 0.5078,\n",
            "        0.4317, 0.5318, 0.4413, 0.5510, 0.5648, 0.4130, 0.4017, 0.4304, 0.4077,\n",
            "        0.4285, 0.4360, 0.3749, 0.4261, 0.3905, 0.3030, 0.3412, 0.3768, 0.4507,\n",
            "        0.3127, 0.4592, 0.4298, 0.3936, 0.3106, 0.3869, 0.3594, 0.4046, 0.4722,\n",
            "        0.4373, 0.3902, 0.3515, 0.4448, 0.4299, 0.4347, 0.4693, 0.4807, 0.2549,\n",
            "        0.4171, 0.4387, 0.4156, 0.3976, 0.4092, 0.4953, 0.4824, 0.3468, 0.4382,\n",
            "        0.4179, 0.4668, 0.3299, 0.5986, 0.4949, 0.4167, 0.4996, 0.4528, 0.4550,\n",
            "        0.4945, 0.3415, 0.4658, 0.4356, 0.3976, 0.5439, 0.4643, 0.5122, 0.4669,\n",
            "        0.4463, 0.4810, 0.3492, 0.3961, 0.3593, 0.4053, 0.3878, 0.3959, 0.5001,\n",
            "        0.2808, 0.5470, 0.4448, 0.4894, 0.4621, 0.3417, 0.3485, 0.5060, 0.3637,\n",
            "        0.3774, 0.3248, 0.4520, 0.3936, 0.3403, 0.4660, 0.4114, 0.3643, 0.4196,\n",
            "        0.3903, 0.5128, 0.4221, 0.4115, 0.4240, 0.3610, 0.4999, 0.3672, 0.4721,\n",
            "        0.4252, 0.5590, 0.4694, 0.7322, 0.5849, 0.4749, 0.4426, 0.3934, 0.3909,\n",
            "        0.4576, 0.3636, 0.4146, 0.4129, 0.5081, 0.3681, 0.3652, 0.4254, 0.2945,\n",
            "        0.4142, 0.3145, 0.4304, 0.4252, 0.3493, 0.4257, 0.5133, 0.3261, 0.4367,\n",
            "        0.3637, 0.3712, 0.4183, 0.3772, 0.4418, 0.4231, 0.4133, 0.4731, 0.4955,\n",
            "        0.4046, 0.4079, 0.4719, 0.3875, 0.4673, 0.4129, 0.4569, 0.3530, 0.4793,\n",
            "        0.3844, 0.3785, 0.3343, 0.4351, 0.6512, 0.4295, 0.4122, 0.3788, 0.3692,\n",
            "        0.4343, 0.4214, 0.3873, 0.4566, 0.4456, 0.4107, 0.4596, 0.7082, 0.4452,\n",
            "        0.3515, 0.4785, 0.4217, 0.5756, 0.4312, 0.4047, 0.4043, 0.4764, 0.5489,\n",
            "        0.4430, 0.5559, 0.3744, 0.3951, 0.4376, 0.4752, 0.4340, 0.4399, 0.3586,\n",
            "        0.4161, 0.3930, 0.4599, 0.4354, 0.3448, 0.4649, 0.4442, 0.4275, 0.3881,\n",
            "        0.3247, 0.4909, 0.3426, 0.3989, 0.4320, 0.3363, 0.3991, 0.4732, 0.3514,\n",
            "        0.4736, 0.4244, 0.4603, 0.3298, 0.4357, 0.4353, 0.3742, 0.4191, 0.3880,\n",
            "        0.4212, 0.4527, 0.7213, 0.3969, 0.5217, 0.3786, 0.3512, 0.5318, 0.4138,\n",
            "        0.3243, 0.3244, 0.3652, 0.4774, 0.3997, 0.2800, 0.4562, 0.4463, 0.4816,\n",
            "        0.4290, 0.4399, 0.4633, 0.3575, 0.4774, 0.3105, 0.4356, 0.3797, 0.4304,\n",
            "        0.4261, 0.3740, 0.3370, 0.3917, 0.3637, 0.4347, 0.5235, 0.3845],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1759, -0.2156, -0.2047, -0.1695, -0.1628, -0.1473, -0.2158, -0.2905,\n",
            "        -0.1112, -0.2196, -0.1020, -0.1549, -0.1989, -0.0445, -0.1508, -0.1920,\n",
            "        -0.2114, -0.1655, -0.1854, -0.1733, -0.1289, -0.2376, -0.1965, -0.1965,\n",
            "        -0.1776, -0.1774, -0.1760, -0.1546, -0.1648, -0.2599, -0.1752, -0.2498,\n",
            "        -0.1741, -0.2410, -0.2498, -0.2938, -0.1496, -0.1578, -0.1800, -0.1851,\n",
            "        -0.1516, -0.1345, -0.2746, -0.1248, -0.2246, -0.2531, -0.2398, -0.1859,\n",
            "        -0.1739, -0.2393, -0.1214, -0.1803, -0.2729, -0.2617, -0.1855, -0.2316,\n",
            "        -0.2333, -0.1860, -0.2097, -0.0692, -0.1912, -0.2078, -0.1084, -0.2810,\n",
            "        -0.1303, -0.1654, -0.2119, -0.3641, -0.2951, -0.2384, -0.1632, -0.1892,\n",
            "        -0.1792, -0.2031, -0.1770, -0.2738, -0.3324, -0.1725, -0.1793, -0.2638,\n",
            "        -0.2207, -0.1609, -0.1534, -0.1414, -0.2992, -0.1450, -0.1838, -0.1779,\n",
            "        -0.1422, -0.2198, -0.1900, -0.1580, -0.1666, -0.2490, -0.1569, -0.1718,\n",
            "        -0.1660, -0.1972, -0.2287, -0.2366, -0.2230, -0.1543, -0.2030, -0.1431,\n",
            "        -0.1363, -0.2015, -0.1804, -0.2093, -0.2964, -0.1984, -0.2683, -0.2216,\n",
            "        -0.2147, -0.3404, -0.2668, -0.1890, -0.1733, -0.2226, -0.1772, -0.1698,\n",
            "        -0.1095, -0.2180, -0.1154, -0.1654, -0.1910, -0.3535, -0.3112, -0.2161,\n",
            "        -0.1496, -0.1667, -0.2849, -0.2207, -0.1529, -0.1807, -0.2118, -0.1869,\n",
            "        -0.1376, -0.1770, -0.1861, -0.1969, -0.1741, -0.3011, -0.0787, -0.2017,\n",
            "        -0.1947, -0.2247, -0.2459, -0.1058, -0.1401, -0.1213, -0.1199, -0.1760,\n",
            "        -0.2156, -0.3307, -0.3515, -0.2366, -0.1185, -0.2155, -0.1751, -0.1892,\n",
            "        -0.3365, -0.1598, -0.2554,  0.0644, -0.2856, -0.1198, -0.1583, -0.2297,\n",
            "        -0.3352, -0.1987, -0.2686, -0.1632, -0.2461, -0.2900, -0.2428, -0.1449,\n",
            "        -0.1900, -0.2149, -0.1541, -0.2917, -0.2504, -0.2213, -0.0463, -0.1547,\n",
            "        -0.1511, -0.1527, -0.1735, -0.1931, -0.1987, -0.2239, -0.2086, -0.2688,\n",
            "        -0.1845, -0.1797, -0.1833, -0.3880, -0.1539, -0.1553, -0.1567, -0.2238,\n",
            "        -0.1511, -0.2540, -0.2849, -0.1826, -0.2687, -0.2328, -0.2108, -0.2410,\n",
            "        -0.1022, -0.1507, -0.1978, -0.1734, -0.2282, -0.0985, -0.1847, -0.1770,\n",
            "        -0.1576, -0.1937, -0.1643, -0.2822, -0.1866, -0.2754, -0.2266, -0.2169,\n",
            "        -0.1352, -0.2194, -0.1060, -0.2139, -0.1322, -0.1889, -0.2130, -0.1913,\n",
            "        -0.2364, -0.1402, -0.2228, -0.2354, -0.1632, -0.1905, -0.1428, -0.1177,\n",
            "        -0.2419, -0.2733, -0.2963, -0.1600, -0.3558, -0.3673, -0.2201, -0.1505,\n",
            "        -0.2084, -0.0870, -0.2052, -0.2070, -0.1986, -0.2299, -0.0745, -0.1765,\n",
            "        -0.1412, -0.2180, -0.1450, -0.1426, -0.1452, -0.2916, -0.0871, -0.1359,\n",
            "        -0.2003, -0.1125, -0.2588, -0.1988, -0.2028, -0.2443, -0.0864, -0.3415,\n",
            "        -0.2579, -0.2343, -0.3552, -0.1859, -0.1153, -0.1732, -0.1780, -0.1909,\n",
            "        -0.2018, -0.1886, -0.2751, -0.1501,  0.1165, -0.1891, -0.1845, -0.2037,\n",
            "        -0.0339, -0.3464, -0.1956, -0.1962, -0.1537, -0.1902, -0.1431, -0.3022,\n",
            "        -0.1780, -0.1971, -0.2118, -0.0952, -0.1711, -0.2409, -0.2184, -0.2114,\n",
            "        -0.2042, -0.0566, -0.0700, -0.2081, -0.1872, -0.2079, -0.1540, -0.2266,\n",
            "        -0.1981, -0.1679, -0.2022, -0.2010, -0.1051, -0.1705, -0.2139,  0.0396,\n",
            "        -0.1077, -0.2745, -0.2690, -0.2603, -0.2819, -0.1917, -0.1940, -0.2944,\n",
            "        -0.1822, -0.2903, -0.1064, -0.2076, -0.2648, -0.3032, -0.2878, -0.1579,\n",
            "        -0.0071, -0.2142, -0.2022, -0.1516, -0.1123,  0.0246, -0.0978, -0.1382,\n",
            "        -0.1800, -0.3214, -0.2179, -0.1369, -0.0800,  0.0117, -0.1839, -0.1926,\n",
            "        -0.1614, -0.2769, -0.1909, -0.2101, -0.2305, -0.2055, -0.2017, -0.2741,\n",
            "        -0.1005, -0.3152, -0.1121, -0.1700, -0.1364, -0.2157, -0.2673, -0.1584,\n",
            "        -0.1997, -0.1745, -0.1886, -0.2307, -0.2024, -0.3376, -0.2266, -0.2355,\n",
            "        -0.2133, -0.2346, -0.2412, -0.2358, -0.1265, -0.2341, -0.1887, -0.1646,\n",
            "        -0.1417, -0.1882, -0.1076, -0.3048, -0.1162, -0.1651, -0.2046, -0.1833,\n",
            "        -0.3102, -0.1778, -0.1575, -0.2676, -0.1777, -0.1569, -0.1741, -0.1892,\n",
            "        -0.3028, -0.1457, -0.2179, -0.2226, -0.1609, -0.1423, -0.2683, -0.2920,\n",
            "        -0.1740, -0.2079, -0.1940, -0.2679, -0.1973, -0.1951, -0.1665, -0.2286,\n",
            "        -0.1903, -0.2667, -0.4010, -0.2550, -0.1817, -0.2025, -0.1589, -0.2476,\n",
            "        -0.0573, -0.2203, -0.2084, -0.1587, -0.1212, -0.1795, -0.3449, -0.1662,\n",
            "        -0.2523, -0.2435, -0.2878, -0.2797, -0.1897, -0.2113, -0.1943, -0.2050,\n",
            "        -0.1694, -0.2243, -0.2987, -0.1328, -0.1428, -0.2399, -0.1593, -0.1999,\n",
            "        -0.3225, -0.1860, -0.1763, -0.2691, -0.2097, -0.2396, -0.1140, -0.1897,\n",
            "        -0.1870, -0.1829, -0.2615, -0.2073, -0.1858, -0.0598, -0.1915, -0.2183,\n",
            "        -0.2088, -0.1742, -0.2715, -0.1999, -0.2117, -0.2492, -0.1717, -0.1566,\n",
            "        -0.1669, -0.3015, -0.1685, -0.2434, -0.2297, -0.1947, -0.2860, -0.3288,\n",
            "        -0.2197, -0.1862, -0.1755, -0.0987, -0.1756, -0.1304, -0.1555, -0.1679,\n",
            "        -0.2222, -0.2819, -0.2652, -0.0947, -0.2412, -0.2731, -0.2572, -0.2604,\n",
            "        -0.2934, -0.2470, -0.1820, -0.2740, -0.1336, -0.1698, -0.1919, -0.1796,\n",
            "        -0.2325, -0.1352, -0.1077, -0.2184, -0.1539, -0.2015, -0.3243, -0.1713],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 0.0057]],\n",
            "\n",
            "         [[ 0.0020]],\n",
            "\n",
            "         [[ 0.0167]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0085]],\n",
            "\n",
            "         [[-0.0274]],\n",
            "\n",
            "         [[ 0.0097]]],\n",
            "\n",
            "\n",
            "        [[[-0.0271]],\n",
            "\n",
            "         [[-0.0157]],\n",
            "\n",
            "         [[ 0.0543]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0206]],\n",
            "\n",
            "         [[-0.0308]],\n",
            "\n",
            "         [[ 0.0013]]],\n",
            "\n",
            "\n",
            "        [[[-0.0523]],\n",
            "\n",
            "         [[-0.0353]],\n",
            "\n",
            "         [[ 0.0394]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0382]],\n",
            "\n",
            "         [[-0.0264]],\n",
            "\n",
            "         [[-0.0443]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0494]],\n",
            "\n",
            "         [[ 0.0436]],\n",
            "\n",
            "         [[ 0.0103]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0072]],\n",
            "\n",
            "         [[ 0.0014]],\n",
            "\n",
            "         [[-0.0669]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0533]],\n",
            "\n",
            "         [[-0.0148]],\n",
            "\n",
            "         [[-0.0480]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0055]],\n",
            "\n",
            "         [[ 0.0429]],\n",
            "\n",
            "         [[ 0.0129]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0300]],\n",
            "\n",
            "         [[-0.0092]],\n",
            "\n",
            "         [[ 0.0090]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0101]],\n",
            "\n",
            "         [[-0.0111]],\n",
            "\n",
            "         [[-0.0080]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.1694,  0.3368,  0.2993,  0.3745,  0.1513,  0.1781,  0.3167,  0.3947,\n",
            "         0.1858,  0.2068,  0.1090,  0.2042,  0.2955,  0.0765,  0.2023,  0.2487,\n",
            "         0.3295,  0.3349,  0.2532,  0.2739,  0.1661,  0.3432,  0.3424,  0.2969,\n",
            "         0.2226,  0.0993,  0.3328,  0.2349,  0.2894,  0.2296,  0.2719,  0.3945,\n",
            "         0.1990,  0.2564,  0.2557,  0.3541,  0.1848,  0.2513,  0.3101,  0.2782,\n",
            "         0.2109,  0.2441,  0.3282,  0.3248,  0.2499,  0.1873,  0.2643,  0.3949,\n",
            "         0.1962,  0.2587,  0.1708,  0.3381,  0.2238,  0.2498,  0.2787,  0.3783,\n",
            "         0.3445,  0.2681,  0.2956,  0.1146,  0.2688,  0.3479,  0.1295,  0.2843,\n",
            "         0.1552,  0.3026,  0.2738,  0.1891,  0.3568,  0.2302,  0.2199,  0.2070,\n",
            "         0.2119,  0.0971,  0.2482,  0.2264,  0.3555,  0.3113,  0.2386,  0.2654,\n",
            "         0.2975,  0.2666,  0.2180,  0.1451,  0.2460,  0.1734,  0.2358,  0.2891,\n",
            "         0.2091,  0.1971,  0.2185,  0.2008,  0.2461,  0.3726,  0.2028,  0.1993,\n",
            "         0.3652,  0.2258,  0.2606,  0.1900,  0.2764,  0.2011,  0.1973,  0.2958,\n",
            "         0.3222,  0.4117,  0.1475,  0.2674,  0.1928,  0.3615,  0.2774,  0.2143,\n",
            "         0.2688,  0.4286,  0.2560,  0.2777,  0.1339,  0.5103,  0.3238,  0.2417,\n",
            "         0.1529,  0.1843,  0.0579,  0.2288,  0.1797,  0.2803,  0.2279,  0.1579,\n",
            "         0.3196,  0.1842,  0.3378,  0.1688,  0.1654,  0.3049,  0.3533,  0.2948,\n",
            "         0.1140,  0.2503,  0.1892,  0.2647,  0.2405,  0.3880,  0.1933,  0.1918,\n",
            "         0.2511,  0.2901,  0.3151,  0.3252,  0.1296,  0.2491,  0.1417,  0.1295,\n",
            "         0.3062,  0.2836,  0.3483,  0.2306,  0.2741,  0.2700,  0.1873,  0.2431,\n",
            "         0.3526,  0.3546,  0.2721,  0.2708,  0.3065,  0.0832,  0.2968,  0.2286,\n",
            "         0.3276,  0.2695,  0.2452,  0.2444,  0.2857,  0.3365,  0.2784,  0.2933,\n",
            "         0.3397,  0.2231,  0.2330,  0.1486,  0.3846,  0.3104,  0.1724,  0.1724,\n",
            "         0.3466,  0.2978,  0.2582,  0.1879,  0.2419,  0.2249,  0.2720,  0.3735,\n",
            "         0.4259,  0.3754,  0.1731,  0.3698,  0.2349,  0.2694,  0.3148,  0.1658,\n",
            "         0.1181,  0.2994,  0.4018,  0.2126,  0.3864,  0.2955,  0.1848,  0.3686,\n",
            "         0.1972,  0.3265,  0.2319,  0.1676,  0.1756,  0.2367,  0.2139,  0.1974,\n",
            "         0.2561,  0.2619,  0.2170,  0.2284,  0.3486,  0.4500,  0.2563,  0.2559,\n",
            "         0.2814,  0.1797,  0.1736,  0.2013,  0.3411,  0.2245,  0.1385,  0.2284,\n",
            "         0.2230,  0.2566,  0.2301,  0.3639,  0.1380,  0.2381,  0.2590,  0.0830,\n",
            "         0.1863,  0.1267,  0.4501,  0.2741,  0.2590,  0.2782,  0.2248,  0.2718,\n",
            "         0.1949,  0.1815,  0.2969,  0.3168,  0.3389,  0.2790,  0.1594,  0.2752,\n",
            "         0.2947,  0.2909,  0.1418,  0.3336,  0.1953,  0.2646,  0.0879,  0.2553,\n",
            "         0.3335,  0.1943,  0.2777,  0.2386,  0.3676,  0.3042,  0.1234,  0.2615,\n",
            "         0.2548,  0.3224,  0.3462,  0.2090,  0.2142,  0.2054,  0.2115,  0.2153,\n",
            "         0.2163,  0.2509,  0.2429,  0.3326, -0.0527,  0.2244,  0.2319,  0.2674,\n",
            "         0.1103,  0.2320,  0.2822,  0.3234,  0.2818,  0.2093,  0.2261,  0.2900,\n",
            "         0.3127,  0.3456,  0.2592,  0.1677,  0.3924,  0.2694,  0.1997,  0.2973,\n",
            "         0.3324,  0.2270,  0.0656,  0.2964,  0.1948,  0.2383,  0.3021,  0.2510,\n",
            "         0.3117,  0.3185,  0.1721,  0.1867,  0.1665,  0.2851,  0.3512, -0.0486,\n",
            "         0.1558,  0.2213,  0.3281,  0.3861,  0.2375,  0.3057,  0.1178,  0.2681,\n",
            "         0.1921,  0.2211,  0.1679,  0.2877,  0.2495,  0.2451,  0.2678,  0.2393,\n",
            "         0.0988,  0.2778,  0.2465,  0.1747,  0.1005,  0.0502,  0.2809,  0.2810,\n",
            "         0.1716,  0.2114,  0.2213,  0.2817,  0.1506,  0.0769,  0.2381,  0.2411,\n",
            "         0.2942,  0.2543,  0.2556,  0.3451,  0.2948,  0.3040,  0.3204,  0.2757,\n",
            "         0.1657,  0.2941,  0.1301,  0.1854,  0.2866,  0.3198,  0.2127,  0.3608,\n",
            "         0.3440,  0.0954,  0.2586,  0.1709,  0.2007,  0.1967,  0.1972,  0.1942,\n",
            "         0.3201,  0.3484,  0.3437,  0.3153,  0.2020,  0.3251,  0.3227,  0.3038,\n",
            "         0.2634,  0.2364,  0.2492,  0.3080,  0.2591,  0.2391,  0.2720,  0.2601,\n",
            "         0.3210,  0.1818,  0.3526,  0.3579,  0.2861,  0.2526,  0.1642,  0.2897,\n",
            "         0.3996,  0.2651,  0.2031,  0.2502,  0.3694,  0.2085,  0.2804,  0.2233,\n",
            "         0.2309,  0.1609,  0.2369,  0.2116,  0.3549,  0.1635,  0.1642,  0.3072,\n",
            "         0.3077,  0.2152,  0.2821,  0.2857,  0.1701,  0.2305,  0.2134,  0.3189,\n",
            "         0.1061,  0.2628,  0.2608,  0.1749,  0.0820,  0.1815,  0.3566,  0.1204,\n",
            "         0.3159,  0.1595,  0.3790,  0.3272,  0.2086,  0.3096,  0.2253,  0.1456,\n",
            "         0.1346,  0.2304,  0.2913,  0.2727,  0.2027,  0.2688,  0.1958,  0.2277,\n",
            "         0.3036,  0.3250,  0.3000,  0.3328,  0.2417,  0.2665,  0.2473,  0.0913,\n",
            "         0.2503,  0.2543,  0.3710,  0.3321,  0.3693,  0.1099,  0.1701,  0.1758,\n",
            "         0.3888,  0.2206,  0.2766,  0.2813,  0.1755,  0.2616,  0.1544,  0.2519,\n",
            "         0.1945,  0.2452,  0.3405,  0.2446,  0.2426,  0.1822,  0.3002,  0.3037,\n",
            "         0.3118,  0.2414,  0.2326,  0.1303,  0.3081,  0.0979,  0.2776,  0.2918,\n",
            "         0.3848,  0.1789,  0.3622,  0.3005,  0.1923,  0.2672,  0.1663,  0.2998,\n",
            "         0.2710,  0.2040,  0.2565,  0.2289,  0.2552,  0.2121,  0.3532,  0.2293,\n",
            "         0.2510,  0.3085,  0.2368,  0.3000,  0.2111,  0.3456,  0.3422,  0.1576],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1759, -0.2156, -0.2047, -0.1695, -0.1628, -0.1473, -0.2158, -0.2905,\n",
            "        -0.1112, -0.2196, -0.1020, -0.1549, -0.1989, -0.0445, -0.1508, -0.1920,\n",
            "        -0.2114, -0.1655, -0.1854, -0.1733, -0.1289, -0.2376, -0.1965, -0.1965,\n",
            "        -0.1776, -0.1774, -0.1760, -0.1546, -0.1648, -0.2599, -0.1752, -0.2498,\n",
            "        -0.1741, -0.2410, -0.2498, -0.2938, -0.1496, -0.1578, -0.1800, -0.1851,\n",
            "        -0.1516, -0.1345, -0.2746, -0.1248, -0.2246, -0.2531, -0.2398, -0.1859,\n",
            "        -0.1739, -0.2393, -0.1214, -0.1803, -0.2729, -0.2617, -0.1855, -0.2316,\n",
            "        -0.2333, -0.1860, -0.2097, -0.0692, -0.1912, -0.2078, -0.1084, -0.2810,\n",
            "        -0.1303, -0.1654, -0.2119, -0.3641, -0.2951, -0.2384, -0.1632, -0.1892,\n",
            "        -0.1792, -0.2031, -0.1770, -0.2738, -0.3324, -0.1725, -0.1793, -0.2638,\n",
            "        -0.2207, -0.1609, -0.1534, -0.1414, -0.2992, -0.1450, -0.1838, -0.1779,\n",
            "        -0.1422, -0.2198, -0.1900, -0.1580, -0.1666, -0.2490, -0.1569, -0.1718,\n",
            "        -0.1660, -0.1972, -0.2287, -0.2366, -0.2230, -0.1543, -0.2030, -0.1431,\n",
            "        -0.1363, -0.2015, -0.1804, -0.2093, -0.2964, -0.1984, -0.2683, -0.2216,\n",
            "        -0.2147, -0.3404, -0.2668, -0.1890, -0.1733, -0.2226, -0.1772, -0.1698,\n",
            "        -0.1095, -0.2180, -0.1154, -0.1654, -0.1910, -0.3535, -0.3112, -0.2161,\n",
            "        -0.1496, -0.1667, -0.2849, -0.2207, -0.1529, -0.1807, -0.2118, -0.1869,\n",
            "        -0.1376, -0.1770, -0.1861, -0.1969, -0.1741, -0.3011, -0.0787, -0.2017,\n",
            "        -0.1947, -0.2247, -0.2459, -0.1058, -0.1401, -0.1213, -0.1199, -0.1760,\n",
            "        -0.2156, -0.3307, -0.3515, -0.2366, -0.1185, -0.2155, -0.1751, -0.1892,\n",
            "        -0.3365, -0.1598, -0.2554,  0.0644, -0.2856, -0.1198, -0.1583, -0.2297,\n",
            "        -0.3352, -0.1987, -0.2686, -0.1632, -0.2461, -0.2900, -0.2428, -0.1449,\n",
            "        -0.1900, -0.2149, -0.1541, -0.2917, -0.2504, -0.2213, -0.0463, -0.1547,\n",
            "        -0.1511, -0.1527, -0.1735, -0.1931, -0.1987, -0.2239, -0.2086, -0.2688,\n",
            "        -0.1845, -0.1797, -0.1833, -0.3880, -0.1539, -0.1553, -0.1567, -0.2238,\n",
            "        -0.1511, -0.2540, -0.2849, -0.1826, -0.2687, -0.2328, -0.2108, -0.2410,\n",
            "        -0.1022, -0.1507, -0.1978, -0.1734, -0.2282, -0.0985, -0.1847, -0.1770,\n",
            "        -0.1576, -0.1937, -0.1643, -0.2822, -0.1866, -0.2754, -0.2266, -0.2169,\n",
            "        -0.1352, -0.2194, -0.1060, -0.2139, -0.1322, -0.1889, -0.2130, -0.1913,\n",
            "        -0.2364, -0.1402, -0.2228, -0.2354, -0.1632, -0.1905, -0.1428, -0.1177,\n",
            "        -0.2419, -0.2733, -0.2963, -0.1600, -0.3558, -0.3673, -0.2201, -0.1505,\n",
            "        -0.2084, -0.0870, -0.2052, -0.2070, -0.1986, -0.2299, -0.0745, -0.1765,\n",
            "        -0.1412, -0.2180, -0.1450, -0.1426, -0.1452, -0.2916, -0.0871, -0.1359,\n",
            "        -0.2003, -0.1125, -0.2588, -0.1988, -0.2028, -0.2443, -0.0864, -0.3415,\n",
            "        -0.2579, -0.2343, -0.3552, -0.1859, -0.1153, -0.1732, -0.1780, -0.1909,\n",
            "        -0.2018, -0.1886, -0.2751, -0.1501,  0.1165, -0.1891, -0.1845, -0.2037,\n",
            "        -0.0339, -0.3464, -0.1956, -0.1962, -0.1537, -0.1902, -0.1431, -0.3022,\n",
            "        -0.1780, -0.1971, -0.2118, -0.0952, -0.1711, -0.2409, -0.2184, -0.2114,\n",
            "        -0.2042, -0.0566, -0.0700, -0.2081, -0.1872, -0.2079, -0.1540, -0.2266,\n",
            "        -0.1981, -0.1679, -0.2022, -0.2010, -0.1051, -0.1705, -0.2139,  0.0396,\n",
            "        -0.1077, -0.2745, -0.2690, -0.2603, -0.2819, -0.1917, -0.1940, -0.2944,\n",
            "        -0.1822, -0.2903, -0.1064, -0.2076, -0.2648, -0.3032, -0.2878, -0.1579,\n",
            "        -0.0071, -0.2142, -0.2022, -0.1516, -0.1123,  0.0246, -0.0978, -0.1382,\n",
            "        -0.1800, -0.3214, -0.2179, -0.1369, -0.0800,  0.0117, -0.1839, -0.1926,\n",
            "        -0.1614, -0.2769, -0.1909, -0.2101, -0.2305, -0.2055, -0.2017, -0.2741,\n",
            "        -0.1005, -0.3152, -0.1121, -0.1700, -0.1364, -0.2157, -0.2673, -0.1584,\n",
            "        -0.1997, -0.1745, -0.1886, -0.2307, -0.2024, -0.3376, -0.2266, -0.2355,\n",
            "        -0.2133, -0.2346, -0.2412, -0.2358, -0.1265, -0.2341, -0.1887, -0.1646,\n",
            "        -0.1417, -0.1882, -0.1076, -0.3048, -0.1162, -0.1651, -0.2046, -0.1833,\n",
            "        -0.3102, -0.1778, -0.1575, -0.2676, -0.1777, -0.1569, -0.1741, -0.1892,\n",
            "        -0.3028, -0.1457, -0.2179, -0.2226, -0.1609, -0.1423, -0.2683, -0.2920,\n",
            "        -0.1740, -0.2079, -0.1940, -0.2679, -0.1973, -0.1951, -0.1665, -0.2286,\n",
            "        -0.1903, -0.2667, -0.4010, -0.2550, -0.1817, -0.2025, -0.1589, -0.2476,\n",
            "        -0.0573, -0.2203, -0.2084, -0.1587, -0.1212, -0.1795, -0.3449, -0.1662,\n",
            "        -0.2523, -0.2435, -0.2878, -0.2797, -0.1897, -0.2113, -0.1943, -0.2050,\n",
            "        -0.1694, -0.2243, -0.2987, -0.1328, -0.1428, -0.2399, -0.1593, -0.1999,\n",
            "        -0.3225, -0.1860, -0.1763, -0.2691, -0.2097, -0.2396, -0.1140, -0.1897,\n",
            "        -0.1870, -0.1829, -0.2615, -0.2073, -0.1858, -0.0598, -0.1915, -0.2183,\n",
            "        -0.2088, -0.1742, -0.2715, -0.1999, -0.2117, -0.2492, -0.1717, -0.1566,\n",
            "        -0.1669, -0.3015, -0.1685, -0.2434, -0.2297, -0.1947, -0.2860, -0.3288,\n",
            "        -0.2197, -0.1862, -0.1755, -0.0987, -0.1756, -0.1304, -0.1555, -0.1679,\n",
            "        -0.2222, -0.2819, -0.2652, -0.0947, -0.2412, -0.2731, -0.2572, -0.2604,\n",
            "        -0.2934, -0.2470, -0.1820, -0.2740, -0.1336, -0.1698, -0.1919, -0.1796,\n",
            "        -0.2325, -0.1352, -0.1077, -0.2184, -0.1539, -0.2015, -0.3243, -0.1713],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-8.0284e-03, -5.7776e-03,  6.4154e-03],\n",
            "          [ 5.0498e-03, -6.7796e-03,  1.2691e-02],\n",
            "          [ 1.3331e-02,  1.4523e-02,  2.4522e-02]],\n",
            "\n",
            "         [[-1.9876e-03,  1.2466e-02,  1.0494e-02],\n",
            "          [-1.9364e-02, -1.6696e-02, -1.1857e-02],\n",
            "          [-1.1569e-02, -3.7674e-03, -3.4679e-03]],\n",
            "\n",
            "         [[-1.1440e-02, -1.3884e-02,  1.1559e-03],\n",
            "          [-1.7906e-02, -2.9349e-02, -1.3876e-02],\n",
            "          [-1.4057e-02, -2.6989e-02, -2.3963e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-6.3040e-03, -3.1167e-03, -1.3304e-02],\n",
            "          [ 7.1623e-03,  6.4669e-03,  1.6063e-02],\n",
            "          [-1.0750e-02, -1.0480e-02, -6.1070e-03]],\n",
            "\n",
            "         [[ 7.4484e-03,  6.3878e-03, -1.2579e-02],\n",
            "          [-7.7356e-03,  1.8112e-03, -1.7890e-02],\n",
            "          [-2.9142e-03,  7.7705e-03, -9.7314e-03]],\n",
            "\n",
            "         [[ 2.1760e-02,  2.2364e-02,  2.2731e-02],\n",
            "          [ 2.6681e-02,  2.9127e-02,  3.3356e-02],\n",
            "          [ 1.2892e-02, -3.5818e-03,  5.3022e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.0597e-02, -9.1551e-03, -2.3418e-02],\n",
            "          [-1.0768e-02, -3.3171e-03, -1.8559e-02],\n",
            "          [-1.8607e-02, -4.2634e-03, -1.5591e-02]],\n",
            "\n",
            "         [[-2.6090e-02, -2.2517e-02, -3.0593e-02],\n",
            "          [-3.9406e-02, -2.6639e-02, -2.8202e-02],\n",
            "          [-2.6143e-02, -1.9647e-02, -2.1466e-02]],\n",
            "\n",
            "         [[-3.5259e-03,  1.6623e-03, -6.5624e-03],\n",
            "          [-5.0597e-03, -8.7162e-04, -5.3742e-03],\n",
            "          [-7.9651e-03, -9.7778e-03, -1.0736e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.8492e-02, -3.6799e-03,  1.0043e-02],\n",
            "          [-5.2974e-03, -2.0757e-02, -1.5120e-02],\n",
            "          [ 2.1435e-02,  6.4916e-03,  4.7660e-03]],\n",
            "\n",
            "         [[-1.8810e-02, -6.0469e-04, -7.6999e-03],\n",
            "          [-1.7697e-02, -7.8692e-03, -1.6543e-02],\n",
            "          [-1.7206e-02, -2.4746e-02, -3.0270e-02]],\n",
            "\n",
            "         [[-3.1191e-02, -1.4363e-02,  2.2032e-03],\n",
            "          [-1.2033e-02, -2.3699e-03, -1.6630e-02],\n",
            "          [-1.2905e-02, -1.5363e-02, -3.6297e-03]]],\n",
            "\n",
            "\n",
            "        [[[-3.2648e-02, -4.8158e-03, -2.0476e-02],\n",
            "          [-2.5846e-02, -1.4660e-03, -2.8170e-02],\n",
            "          [-2.6640e-02,  4.3022e-03, -2.7636e-02]],\n",
            "\n",
            "         [[-6.3289e-03, -1.5401e-02, -1.3096e-03],\n",
            "          [-1.7499e-02, -2.6212e-02, -2.3646e-02],\n",
            "          [-7.3207e-03, -1.5592e-02, -8.9578e-03]],\n",
            "\n",
            "         [[ 8.9701e-04, -6.6914e-03, -5.3129e-03],\n",
            "          [-1.1727e-03, -1.0726e-02, -9.0103e-03],\n",
            "          [ 3.2311e-03, -4.5854e-03,  4.3512e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.1822e-02, -3.6889e-02, -2.2588e-02],\n",
            "          [-1.3054e-02, -3.4191e-02, -2.7238e-02],\n",
            "          [-1.2383e-02, -2.3452e-02, -2.2486e-02]],\n",
            "\n",
            "         [[ 6.8177e-03,  2.1561e-02,  1.3674e-02],\n",
            "          [ 3.1192e-03,  1.0660e-02,  1.0409e-02],\n",
            "          [ 8.0477e-03, -4.6817e-03, -4.3912e-03]],\n",
            "\n",
            "         [[-1.1983e-02, -1.6201e-02, -2.2626e-02],\n",
            "          [-1.3461e-02, -7.0928e-03, -1.4384e-02],\n",
            "          [-2.4456e-02,  1.4885e-02,  1.2247e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-2.6347e-02, -2.9923e-02, -3.7810e-02],\n",
            "          [-1.5663e-02, -4.1126e-03, -1.1482e-02],\n",
            "          [-1.3415e-02, -1.5432e-02, -1.8204e-02]],\n",
            "\n",
            "         [[-3.8392e-03, -1.1093e-02, -8.0841e-04],\n",
            "          [-5.9634e-03, -5.9165e-03, -9.3332e-03],\n",
            "          [-2.2761e-03,  5.4781e-03, -5.6050e-03]],\n",
            "\n",
            "         [[-1.8406e-03, -2.8134e-03,  8.3246e-03],\n",
            "          [-1.2453e-03,  2.1453e-04,  7.4868e-03],\n",
            "          [ 1.3450e-02,  3.0599e-02,  2.6405e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.5268e-04,  2.3897e-03,  6.2558e-03],\n",
            "          [-1.4338e-02, -2.3146e-02, -1.9024e-02],\n",
            "          [-2.7306e-02, -3.0079e-02, -3.1762e-02]],\n",
            "\n",
            "         [[ 1.4584e-02,  4.3430e-03,  1.2053e-02],\n",
            "          [-6.1130e-03, -2.8539e-02, -1.8268e-02],\n",
            "          [-1.6844e-02, -4.7816e-02, -2.6274e-02]],\n",
            "\n",
            "         [[-1.8850e-02, -9.3396e-03,  7.8905e-03],\n",
            "          [-1.5322e-03,  8.3153e-03,  1.7783e-02],\n",
            "          [-8.3318e-03, -1.5759e-02, -1.2061e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 9.9578e-03,  7.4573e-03, -1.8738e-03],\n",
            "          [-1.7752e-03, -6.8015e-04, -7.4443e-03],\n",
            "          [-1.8319e-02, -1.4264e-02, -7.1446e-03]],\n",
            "\n",
            "         [[ 7.8524e-03, -2.6520e-03, -1.7556e-02],\n",
            "          [ 4.5240e-03, -4.8661e-03, -1.5215e-02],\n",
            "          [-5.0211e-03, -1.1864e-02, -1.4846e-02]],\n",
            "\n",
            "         [[ 2.9163e-02,  1.0344e-02,  2.4736e-02],\n",
            "          [ 1.2012e-02, -1.0346e-02,  3.5472e-03],\n",
            "          [ 8.2238e-03, -1.8237e-02, -5.4892e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-8.8434e-03, -4.3184e-03, -5.7536e-03],\n",
            "          [ 7.7230e-03, -4.1936e-04,  7.7260e-03],\n",
            "          [ 1.3536e-02,  1.5705e-02,  2.0893e-02]],\n",
            "\n",
            "         [[ 1.6743e-03,  1.9720e-03,  2.1567e-02],\n",
            "          [-8.0074e-03, -4.6606e-03,  4.0560e-03],\n",
            "          [-1.6688e-02, -1.3754e-02, -1.1708e-02]],\n",
            "\n",
            "         [[-9.7959e-03, -9.4502e-03, -9.3443e-03],\n",
            "          [ 6.9547e-03, -3.9134e-05,  6.2691e-03],\n",
            "          [-1.3193e-02,  9.3272e-04,  1.4579e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.4963e-03,  5.5133e-04,  1.1571e-02],\n",
            "          [ 1.0174e-02,  1.7889e-03,  1.1035e-02],\n",
            "          [ 7.0212e-03,  1.4651e-03,  1.2769e-03]],\n",
            "\n",
            "         [[-1.3021e-02,  6.4109e-03, -1.5199e-02],\n",
            "          [ 2.4775e-02,  2.1926e-02,  3.3679e-02],\n",
            "          [ 2.6471e-04, -3.0235e-03,  1.1690e-02]],\n",
            "\n",
            "         [[-2.9665e-02, -1.5314e-02, -1.7500e-02],\n",
            "          [-1.8339e-02, -2.0845e-02, -1.5494e-02],\n",
            "          [-1.6086e-03,  1.0831e-02, -1.4309e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-7.7044e-03, -2.1100e-02, -2.2816e-02],\n",
            "          [ 5.7688e-03,  1.9362e-04,  7.7105e-04],\n",
            "          [-6.1357e-03,  9.7275e-03, -2.5464e-03]],\n",
            "\n",
            "         [[ 1.1043e-02,  2.4205e-02,  3.4213e-02],\n",
            "          [ 2.9181e-02,  2.6904e-02,  4.5372e-02],\n",
            "          [-2.1594e-02, -1.1072e-03, -7.8312e-03]],\n",
            "\n",
            "         [[-8.3287e-03, -7.9521e-03, -5.3358e-03],\n",
            "          [-6.2527e-04, -5.3243e-03, -8.6296e-03],\n",
            "          [ 3.6094e-03, -1.2544e-03, -4.3801e-03]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.2587, 0.3073, 0.2595, 0.3223, 0.2662, 0.2652, 0.2575, 0.2660, 0.2766,\n",
            "        0.2414, 0.3045, 0.2853, 0.2821, 0.2880, 0.3094, 0.3444, 0.3155, 0.4129,\n",
            "        0.2110, 0.2903, 0.2496, 0.2601, 0.2967, 0.3033, 0.4152, 0.2719, 0.3661,\n",
            "        0.3251, 0.3898, 0.3346, 0.2753, 0.2712, 0.2414, 0.3351, 0.3394, 0.3167,\n",
            "        0.3360, 0.2666, 0.2109, 0.2705, 0.2587, 0.3070, 0.2720, 0.2316, 0.2885,\n",
            "        0.2884, 0.2955, 0.3057, 0.3043, 0.2596, 0.2673, 0.1929, 0.3136, 0.3593,\n",
            "        0.2622, 0.2931, 0.3295, 0.2514, 0.3208, 0.2798, 0.3259, 0.2939, 0.2390,\n",
            "        0.3105, 0.3471, 0.2812, 0.2148, 0.2997, 0.3061, 0.2740, 0.2791, 0.3790,\n",
            "        0.3592, 0.3247, 0.2995, 0.2735, 0.3356, 0.2703, 0.3255, 0.3127, 0.2783,\n",
            "        0.2702, 0.3900, 0.2942, 0.2899, 0.3461, 0.3432, 0.4685, 0.2634, 0.2553,\n",
            "        0.3019, 0.3961, 0.2742, 0.2995, 0.3858, 0.2785, 0.3212, 0.3109, 0.3642,\n",
            "        0.2193, 0.2643, 0.2333, 0.3151, 0.3102, 0.2936, 0.2374, 0.2419, 0.2976,\n",
            "        0.3335, 0.2619, 0.3984, 0.2721, 0.2718, 0.2678, 0.2757, 0.2445, 0.3508,\n",
            "        0.2174, 0.3309, 0.2653, 0.2564, 0.1748, 0.3177, 0.2751, 0.2067, 0.2905,\n",
            "        0.2762, 0.3329, 0.2738, 0.3224, 0.2199, 0.2997, 0.2206, 0.3213, 0.2760,\n",
            "        0.3927, 0.3174, 0.2698, 0.2988, 0.2610, 0.2550, 0.2788, 0.4445, 0.2862,\n",
            "        0.3606, 0.3279, 0.2869, 0.3294, 0.2244, 0.2338, 0.1754, 0.2318, 0.3186,\n",
            "        0.3322, 0.2255, 0.3041, 0.2837, 0.3276, 0.2392, 0.3668, 0.1971, 0.2946,\n",
            "        0.3613, 0.2736, 0.2554, 0.2860, 0.2511, 0.3490, 0.3253, 0.2934, 0.2027,\n",
            "        0.2580, 0.2200, 0.3089, 0.3074, 0.3332, 0.2943, 0.3375, 0.2330, 0.2611,\n",
            "        0.3383, 0.2837, 0.3546, 0.3093, 0.3791, 0.2197, 0.2648, 0.2830, 0.2587,\n",
            "        0.3588, 0.2830, 0.3971, 0.3194, 0.3066, 0.2754, 0.2647, 0.0970, 0.2182,\n",
            "        0.2334, 0.2624, 0.1829, 0.2933, 0.2747, 0.3001, 0.2996, 0.3107, 0.3256,\n",
            "        0.2940, 0.3901, 0.2790, 0.3030, 0.2838, 0.3010, 0.3044, 0.3479, 0.3087,\n",
            "        0.2611, 0.1958, 0.2941, 0.2558, 0.2889, 0.3148, 0.2516, 0.2664, 0.2862,\n",
            "        0.3940, 0.2933, 0.2781, 0.3796, 0.3022, 0.2583, 0.3021, 0.2784, 0.2967,\n",
            "        0.2994, 0.3856, 0.3277, 0.2587, 0.2539, 0.2824, 0.2634, 0.1489, 0.2205,\n",
            "        0.3929, 0.3401, 0.2717, 0.2789, 0.2917, 0.3177, 0.1992, 0.3684, 0.3120,\n",
            "        0.3201, 0.2810, 0.2302, 0.2779, 0.2865, 0.2858, 0.2713, 0.1601, 0.2496,\n",
            "        0.2895, 0.3154, 0.3443, 0.3285, 0.3444, 0.3251, 0.3235, 0.3375, 0.2282,\n",
            "        0.2128, 0.1795, 0.3077, 0.3005, 0.2775, 0.3054, 0.2914, 0.3535, 0.2871,\n",
            "        0.2669, 0.3961, 0.2674, 0.3898, 0.3183, 0.3242, 0.2789, 0.1911, 0.2569,\n",
            "        0.3427, 0.2464, 0.2778, 0.2098, 0.3019, 0.3145, 0.3271, 0.2914, 0.2619,\n",
            "        0.2643, 0.3039, 0.2520, 0.2099, 0.3643, 0.2915, 0.1957, 0.3286, 0.2355,\n",
            "        0.3210, 0.2982, 0.3388, 0.3450, 0.3716, 0.2898, 0.2846, 0.2805, 0.2219,\n",
            "        0.2910, 0.2681, 0.3163, 0.1964, 0.3176, 0.3092, 0.2706, 0.2505, 0.2508,\n",
            "        0.3166, 0.3583, 0.1563, 0.2608, 0.2892, 0.3401, 0.2891, 0.3126, 0.2172,\n",
            "        0.2459, 0.2651, 0.4052, 0.2986, 0.3026, 0.3773, 0.2262, 0.2675, 0.2900,\n",
            "        0.3759, 0.3201, 0.2567, 0.3443, 0.2348, 0.3057, 0.2347, 0.3277, 0.2938,\n",
            "        0.2746, 0.2805, 0.2421, 0.3590, 0.2622, 0.2773, 0.2396, 0.2134, 0.2727,\n",
            "        0.2984, 0.2744, 0.2591, 0.2628, 0.3568, 0.2009, 0.3220, 0.2868, 0.2561,\n",
            "        0.3113, 0.2138, 0.3136, 0.2745, 0.3046, 0.3042, 0.1972, 0.2815, 0.2542,\n",
            "        0.2983, 0.2613, 0.2668, 0.3142, 0.2930, 0.3800, 0.1966, 0.2948, 0.3363,\n",
            "        0.2713, 0.3625, 0.2909, 0.2695, 0.3111, 0.3242, 0.3009, 0.3231, 0.3051,\n",
            "        0.2012, 0.2716, 0.3692, 0.2694, 0.1481, 0.2858, 0.2819, 0.2391, 0.2867,\n",
            "        0.3466, 0.3431, 0.2365, 0.3357, 0.1685, 0.2925, 0.3092, 0.3127, 0.1883,\n",
            "        0.2561, 0.3086, 0.1732, 0.2989, 0.3235, 0.2693, 0.2630, 0.2913, 0.2786,\n",
            "        0.3124, 0.3098, 0.2695, 0.2403, 0.2906, 0.2784, 0.2654, 0.3485, 0.3939,\n",
            "        0.3033, 0.3145, 0.2622, 0.1540, 0.2790, 0.2967, 0.1954, 0.2632, 0.2957,\n",
            "        0.2581, 0.3231, 0.2795, 0.2859, 0.3139, 0.2488, 0.2404, 0.3714, 0.2649,\n",
            "        0.2267, 0.2878, 0.3462, 0.3063, 0.3180, 0.1726, 0.3153, 0.2625, 0.3020,\n",
            "        0.2996, 0.3632, 0.1541, 0.3192, 0.2200, 0.2894, 0.2622, 0.2534, 0.2935,\n",
            "        0.3208, 0.2231, 0.2743, 0.3023, 0.2829, 0.2394, 0.2506, 0.3512, 0.3366,\n",
            "        0.2666, 0.2930, 0.3049, 0.2321, 0.3397, 0.2727, 0.2900, 0.3146, 0.2682,\n",
            "        0.3094, 0.3718, 0.3387, 0.3202, 0.2423, 0.2745, 0.2966, 0.2500, 0.2329,\n",
            "        0.3419, 0.2928, 0.3536, 0.3739, 0.1935, 0.2670, 0.2846, 0.2583, 0.3783,\n",
            "        0.2826, 0.2929, 0.2728, 0.3645, 0.2770, 0.2756, 0.2523, 0.2500],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1668, -0.3019, -0.2187, -0.2917, -0.1971, -0.2325, -0.1869, -0.1857,\n",
            "        -0.2474, -0.1629, -0.2448, -0.2508, -0.1895, -0.2651, -0.3250, -0.3811,\n",
            "        -0.2953, -0.4963, -0.0294, -0.2724, -0.2007, -0.2220, -0.2945, -0.2579,\n",
            "        -0.5152, -0.1994, -0.5016, -0.2736, -0.4528, -0.3968, -0.2281, -0.1772,\n",
            "        -0.1293, -0.2655, -0.3252, -0.3232, -0.3337, -0.1901, -0.0692, -0.2196,\n",
            "        -0.2132, -0.2565, -0.1646, -0.1567, -0.2087, -0.2178, -0.2480, -0.2767,\n",
            "        -0.3071, -0.1988, -0.1985, -0.0235, -0.2458, -0.4156, -0.1660, -0.1923,\n",
            "        -0.3328, -0.1481, -0.3047, -0.2277, -0.3182, -0.2744, -0.1643, -0.3365,\n",
            "        -0.4050, -0.2082, -0.0621, -0.2671, -0.2809, -0.2185, -0.2148, -0.4465,\n",
            "        -0.3376, -0.3213, -0.2921, -0.1998, -0.3369, -0.2092, -0.2831, -0.2893,\n",
            "        -0.1719, -0.2189, -0.4016, -0.2484, -0.2070, -0.3849, -0.3753, -0.5874,\n",
            "        -0.1637, -0.1748, -0.2217, -0.5067, -0.2496, -0.2117, -0.4291, -0.1944,\n",
            "        -0.3089, -0.2621, -0.4096, -0.0602, -0.2009, -0.1316, -0.3336, -0.2627,\n",
            "        -0.2320, -0.0910, -0.1560, -0.2889, -0.3286, -0.1628, -0.5128, -0.2036,\n",
            "        -0.1726, -0.1844, -0.2285, -0.1925, -0.3432, -0.0929, -0.3138, -0.1912,\n",
            "        -0.1926, -0.0342, -0.3268, -0.1699, -0.0828, -0.2417, -0.2069, -0.3870,\n",
            "        -0.2210, -0.2867, -0.0526, -0.3092, -0.0655, -0.2594, -0.2160, -0.5062,\n",
            "        -0.2905, -0.2125, -0.3124, -0.2128, -0.1946, -0.2520, -0.5475, -0.2321,\n",
            "        -0.3350, -0.3473, -0.2158, -0.3603, -0.0759, -0.1472, -0.0327, -0.1404,\n",
            "        -0.3128, -0.3063, -0.1120, -0.2664, -0.2700, -0.3112, -0.1519, -0.3843,\n",
            "        -0.0645, -0.2373, -0.4227, -0.2546, -0.1611, -0.2350, -0.1524, -0.3494,\n",
            "        -0.3453, -0.2081, -0.0918, -0.2025, -0.1246, -0.2533, -0.2768, -0.3156,\n",
            "        -0.2530, -0.3957, -0.0981, -0.1257, -0.3697, -0.2333, -0.3664, -0.2829,\n",
            "        -0.4320, -0.0836, -0.1583, -0.2395, -0.1818, -0.4408, -0.2376, -0.4450,\n",
            "        -0.3232, -0.2787, -0.1858, -0.2137,  0.0481, -0.1058, -0.1093, -0.2035,\n",
            "        -0.0496, -0.2117, -0.1598, -0.2389, -0.2830, -0.2878, -0.3406, -0.2560,\n",
            "        -0.4468, -0.2444, -0.2492, -0.2222, -0.2792, -0.3005, -0.4180, -0.2568,\n",
            "        -0.1872, -0.0270, -0.2645, -0.1873, -0.3022, -0.3400, -0.1803, -0.1810,\n",
            "        -0.2079, -0.4775, -0.2047, -0.1878, -0.4504, -0.2516, -0.1657, -0.2765,\n",
            "        -0.2329, -0.2446, -0.2956, -0.4163, -0.2816, -0.1571, -0.2199, -0.2125,\n",
            "        -0.1684,  0.0356, -0.0914, -0.4484, -0.3535, -0.2212, -0.2550, -0.2509,\n",
            "        -0.2702, -0.0599, -0.3505, -0.2924, -0.2360, -0.2339, -0.1259, -0.2597,\n",
            "        -0.2267, -0.1978, -0.1371, -0.0129, -0.1175, -0.2527, -0.3099, -0.3231,\n",
            "        -0.3468, -0.3553, -0.3537, -0.3315, -0.3713, -0.1091, -0.0959, -0.0258,\n",
            "        -0.2756, -0.2808, -0.2012, -0.2812, -0.1991, -0.3948, -0.2257, -0.2469,\n",
            "        -0.4211, -0.2110, -0.4670, -0.3069, -0.3549, -0.2337, -0.0612, -0.1321,\n",
            "        -0.2968, -0.1870, -0.2316, -0.0686, -0.3113, -0.2895, -0.3149, -0.2686,\n",
            "        -0.2081, -0.2096, -0.3011, -0.1810, -0.0227, -0.3873, -0.2665, -0.0225,\n",
            "        -0.2973, -0.0973, -0.2980, -0.3219, -0.2926, -0.3196, -0.4332, -0.1980,\n",
            "        -0.2117, -0.2302, -0.0980, -0.2344, -0.2154, -0.2921, -0.0350, -0.3361,\n",
            "        -0.2620, -0.2188, -0.1566, -0.1795, -0.2726, -0.4103,  0.0413, -0.1507,\n",
            "        -0.2552, -0.3137, -0.2466, -0.2961, -0.0938, -0.1481, -0.2129, -0.5480,\n",
            "        -0.2915, -0.2802, -0.5077, -0.1306, -0.1862, -0.2400, -0.4362, -0.3017,\n",
            "        -0.1633, -0.3447, -0.1047, -0.2846, -0.1244, -0.3036, -0.2404, -0.2333,\n",
            "        -0.2494, -0.1866, -0.3294, -0.1677, -0.2540, -0.1295, -0.0512, -0.1966,\n",
            "        -0.2801, -0.1702, -0.1879, -0.1850, -0.3274, -0.0369, -0.2979, -0.2612,\n",
            "        -0.1889, -0.3270, -0.1377, -0.2787, -0.2201, -0.2417, -0.2834, -0.0555,\n",
            "        -0.2538, -0.1040, -0.2660, -0.1644, -0.1723, -0.2672, -0.2797, -0.4214,\n",
            "        -0.0378, -0.2386, -0.3498, -0.2435, -0.4348, -0.2554, -0.1719, -0.2836,\n",
            "        -0.3316, -0.2787, -0.2879, -0.2640, -0.0560, -0.1789, -0.4195, -0.2152,\n",
            "         0.0567, -0.2359, -0.2249, -0.0911, -0.2644, -0.3875, -0.3317, -0.1415,\n",
            "        -0.3425, -0.0020, -0.1941, -0.2821, -0.2809, -0.0965, -0.1841, -0.2971,\n",
            "        -0.0173, -0.3043, -0.3013, -0.1729, -0.1872, -0.2683, -0.2033, -0.3059,\n",
            "        -0.2939, -0.2163, -0.1889, -0.2581, -0.2296, -0.2066, -0.3462, -0.4298,\n",
            "        -0.2600, -0.3095, -0.1800, -0.0116, -0.2124, -0.2552, -0.0523, -0.2216,\n",
            "        -0.2605, -0.2134, -0.2867, -0.2556, -0.2275, -0.3437, -0.1698, -0.1560,\n",
            "        -0.4120, -0.2067, -0.1159, -0.2408, -0.3093, -0.2621, -0.2593, -0.0135,\n",
            "        -0.3099, -0.2179, -0.2766, -0.2400, -0.3934,  0.0072, -0.2982, -0.0930,\n",
            "        -0.2166, -0.1635, -0.1827, -0.2308, -0.2525, -0.0991, -0.2325, -0.2938,\n",
            "        -0.2480, -0.0934, -0.1911, -0.3772, -0.3369, -0.1606, -0.2752, -0.3005,\n",
            "        -0.1372, -0.2990, -0.2156, -0.2622, -0.3160, -0.1342, -0.2903, -0.3865,\n",
            "        -0.2916, -0.3243, -0.2051, -0.2656, -0.2359, -0.1508, -0.1063, -0.3595,\n",
            "        -0.2312, -0.3046, -0.4178, -0.0276, -0.2204, -0.2426, -0.1616, -0.4789,\n",
            "        -0.1713, -0.2802, -0.2305, -0.4327, -0.2413, -0.1862, -0.1486, -0.1507],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 2.8729e-04,  4.2632e-03, -2.0266e-03],\n",
            "          [ 1.9513e-04,  2.4381e-03, -5.8632e-03],\n",
            "          [ 4.4803e-03,  8.6577e-03,  8.5538e-04]],\n",
            "\n",
            "         [[-1.1335e-02, -1.3195e-02, -1.0305e-02],\n",
            "          [-4.9507e-03, -4.5898e-03, -3.1041e-03],\n",
            "          [-7.5883e-03, -8.3795e-03, -8.9239e-03]],\n",
            "\n",
            "         [[-1.1914e-02, -1.2104e-02, -1.0167e-02],\n",
            "          [-1.2093e-02, -1.1557e-02, -8.9600e-03],\n",
            "          [-1.2515e-02, -9.3296e-03, -6.4079e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-9.3573e-03, -1.0662e-02, -1.2672e-02],\n",
            "          [-8.0600e-03, -8.5423e-03, -1.2121e-02],\n",
            "          [-8.1498e-03, -8.8037e-03, -1.0611e-02]],\n",
            "\n",
            "         [[ 4.2632e-03,  5.6461e-03,  2.8460e-03],\n",
            "          [ 4.7070e-03,  6.2550e-03,  7.5862e-03],\n",
            "          [ 1.1504e-02,  1.1518e-02,  1.0728e-02]],\n",
            "\n",
            "         [[-6.2455e-03, -9.1693e-03, -9.6664e-03],\n",
            "          [-4.2935e-03, -6.5311e-03, -5.0513e-03],\n",
            "          [-3.1141e-03, -5.0124e-03, -5.8122e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 2.7483e-03,  3.7146e-04,  3.3262e-05],\n",
            "          [-4.5675e-03, -6.6689e-03, -6.4447e-03],\n",
            "          [-6.7610e-03, -7.3204e-03, -9.5855e-03]],\n",
            "\n",
            "         [[-1.4630e-02, -1.2320e-02, -1.4457e-02],\n",
            "          [-8.6197e-03, -5.8059e-03, -1.1075e-02],\n",
            "          [-6.2154e-03, -6.8218e-03, -9.3805e-03]],\n",
            "\n",
            "         [[ 1.0879e-03,  4.3850e-04, -1.9456e-03],\n",
            "          [-1.2517e-03,  3.2917e-04, -2.1435e-03],\n",
            "          [ 4.8136e-03,  2.5333e-03,  5.1504e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.4644e-02,  1.7434e-02,  2.0734e-02],\n",
            "          [ 2.3101e-02,  1.3487e-02,  2.0728e-02],\n",
            "          [ 1.9381e-02,  1.5243e-02,  1.7340e-02]],\n",
            "\n",
            "         [[ 1.2212e-02,  1.2448e-02,  1.5048e-02],\n",
            "          [ 5.2993e-03,  4.0090e-03,  9.3927e-03],\n",
            "          [ 6.6766e-03,  2.4941e-03,  8.3288e-03]],\n",
            "\n",
            "         [[ 3.1040e-02,  2.8243e-02,  3.2319e-02],\n",
            "          [ 3.8608e-02,  3.3099e-02,  3.8652e-02],\n",
            "          [ 2.5839e-02,  2.6524e-02,  2.4995e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.1761e-03,  4.5553e-03,  2.0612e-03],\n",
            "          [ 4.9747e-03,  1.1420e-02,  8.5734e-03],\n",
            "          [ 4.8583e-03,  1.1469e-02,  1.0039e-02]],\n",
            "\n",
            "         [[-6.2547e-05,  6.5336e-04,  9.4747e-04],\n",
            "          [ 5.0603e-03,  7.7136e-03,  6.5484e-03],\n",
            "          [-4.8432e-04,  2.3057e-03,  2.9219e-03]],\n",
            "\n",
            "         [[-3.2788e-02, -2.7615e-02, -3.2608e-02],\n",
            "          [-3.6296e-02, -2.8170e-02, -3.0277e-02],\n",
            "          [-3.6814e-02, -3.1547e-02, -3.0231e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.2998e-03, -2.8590e-04, -4.9266e-03],\n",
            "          [-7.0530e-03, -2.3684e-04, -1.5838e-03],\n",
            "          [-6.9291e-03,  4.8084e-04, -3.1548e-03]],\n",
            "\n",
            "         [[ 1.1854e-02,  8.4836e-03,  1.3839e-02],\n",
            "          [ 2.8741e-03, -9.7358e-05,  4.4888e-03],\n",
            "          [-2.5515e-03, -2.7788e-03, -3.2464e-03]],\n",
            "\n",
            "         [[-1.2408e-02, -1.5001e-02, -1.3377e-02],\n",
            "          [-1.4540e-02, -1.8537e-02, -1.7392e-02],\n",
            "          [-6.7315e-03, -9.5205e-03, -9.0692e-03]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 3.0369e-03,  1.9542e-03,  1.7140e-03],\n",
            "          [-7.6240e-03, -2.8765e-03, -5.1760e-03],\n",
            "          [-9.3019e-03, -4.8800e-03, -4.2932e-03]],\n",
            "\n",
            "         [[ 4.4836e-03,  2.4909e-03,  1.5746e-03],\n",
            "          [ 1.2065e-02,  1.2936e-02,  1.0344e-02],\n",
            "          [ 1.9010e-02,  1.7459e-02,  1.5988e-02]],\n",
            "\n",
            "         [[-1.4914e-03, -8.1727e-03, -8.0671e-03],\n",
            "          [-6.6247e-03, -6.2421e-03, -9.2717e-03],\n",
            "          [-8.7991e-03, -7.7528e-03, -8.6336e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.8040e-02, -1.5366e-02, -1.5334e-02],\n",
            "          [-1.3148e-02, -1.2180e-02, -1.0915e-02],\n",
            "          [-1.4545e-02, -1.4756e-02, -1.1787e-02]],\n",
            "\n",
            "         [[ 3.5762e-03,  6.6073e-03, -1.4055e-03],\n",
            "          [ 4.3975e-03,  7.8375e-03,  8.8085e-05],\n",
            "          [-5.0697e-03, -5.6633e-04, -5.9284e-03]],\n",
            "\n",
            "         [[-1.9234e-03, -8.8012e-03, -5.8821e-03],\n",
            "          [ 3.6685e-03, -1.3784e-03, -3.2117e-03],\n",
            "          [-4.7037e-04,  1.5340e-04, -3.4046e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.8305e-02, -1.7735e-02, -2.1683e-02],\n",
            "          [-1.6598e-02, -1.2508e-02, -2.0530e-02],\n",
            "          [-1.0800e-02, -9.8670e-03, -1.7195e-02]],\n",
            "\n",
            "         [[ 2.0721e-02,  2.2466e-02,  2.5049e-02],\n",
            "          [ 1.8682e-02,  1.3160e-02,  2.3696e-02],\n",
            "          [ 2.2104e-02,  1.7261e-02,  2.4877e-02]],\n",
            "\n",
            "         [[-5.7091e-03, -2.6876e-03, -9.2260e-04],\n",
            "          [-9.4530e-03, -7.0543e-03, -6.2770e-03],\n",
            "          [-4.5806e-03, -2.7182e-03, -2.5823e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.4150e-02,  1.4002e-02,  1.6559e-02],\n",
            "          [ 2.1363e-02,  1.4359e-02,  1.5854e-02],\n",
            "          [ 2.5786e-02,  2.7233e-02,  2.5104e-02]],\n",
            "\n",
            "         [[-4.6450e-03,  1.2419e-03, -1.8768e-03],\n",
            "          [ 1.3005e-03,  4.0888e-03, -6.5483e-04],\n",
            "          [-7.9783e-03, -6.6539e-03, -8.9957e-03]],\n",
            "\n",
            "         [[ 1.1494e-02,  2.6621e-02,  1.5649e-02],\n",
            "          [ 6.5960e-03,  1.7290e-02,  7.5466e-03],\n",
            "          [-8.0256e-03,  4.6246e-03, -5.7808e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 1.4232e-02,  1.1769e-02,  9.4342e-03],\n",
            "          [ 6.2592e-03,  5.1087e-03,  2.3311e-03],\n",
            "          [-1.9694e-03,  2.7110e-03, -2.8945e-03]],\n",
            "\n",
            "         [[-7.0772e-03,  1.0365e-03, -5.8451e-03],\n",
            "          [-9.1879e-03, -3.1388e-03, -8.1517e-03],\n",
            "          [-8.0300e-03, -5.1313e-03, -9.5734e-03]],\n",
            "\n",
            "         [[ 2.4314e-02,  1.8942e-02,  2.4256e-02],\n",
            "          [ 2.0090e-02,  1.1472e-02,  1.5993e-02],\n",
            "          [ 2.2910e-02,  2.0622e-02,  2.3820e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.6375e-02, -1.6928e-02, -1.9019e-02],\n",
            "          [-9.7367e-03, -1.1274e-02, -1.0261e-02],\n",
            "          [-1.2310e-02, -1.5931e-02, -1.4151e-02]],\n",
            "\n",
            "         [[ 4.7098e-03, -4.5205e-04,  2.8042e-03],\n",
            "          [ 2.1428e-03, -4.6175e-03, -1.6818e-03],\n",
            "          [-1.3336e-03, -5.5009e-03, -2.6237e-03]],\n",
            "\n",
            "         [[-1.4367e-02, -1.3520e-02, -1.1387e-02],\n",
            "          [-4.7420e-03, -1.7309e-03, -2.6426e-03],\n",
            "          [ 5.1448e-03,  7.0428e-03,  5.0202e-03]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([1.8419, 1.8307, 1.7650, 1.8288, 1.9505, 1.8026, 1.9536, 2.2790, 1.7662,\n",
            "        1.8902, 1.7768, 1.7749, 1.9055, 1.7328, 1.8762, 1.8211, 1.7967, 2.3428,\n",
            "        1.7985, 1.7271, 1.7915, 1.9512, 1.8928, 1.9017, 1.8784, 1.9809, 1.8569,\n",
            "        1.7830, 1.8911, 1.8859, 1.7764, 1.9832, 1.8389, 1.7616, 1.8728, 1.8753,\n",
            "        1.9008, 1.8209, 1.7039, 1.7377, 1.7786, 1.6944, 1.7829, 1.7815, 1.7594,\n",
            "        1.8428, 1.9238, 2.0871, 1.8980, 1.8413, 1.8471, 1.8584, 1.7640, 1.8453,\n",
            "        1.7606, 1.9504, 1.9620, 1.8755, 1.9424, 1.8731, 1.8674, 1.9422, 1.8750,\n",
            "        1.9208, 1.7464, 1.8558, 1.6539, 2.0660, 2.0298, 1.9174, 1.8972, 1.7589,\n",
            "        1.7551, 1.9560, 1.7909, 1.7971, 1.7851, 1.7733, 1.8061, 1.7949, 1.8169,\n",
            "        1.8089, 1.8641, 2.1542, 1.7739, 1.7913, 1.8022, 1.7155, 1.7679, 1.7704,\n",
            "        1.6266, 1.8645, 1.9076, 1.8576, 1.6924, 1.8020, 1.7100, 1.7713, 1.8572,\n",
            "        1.7103, 2.0664, 1.9054, 1.9422, 1.8078, 1.7412, 1.6061, 1.9105, 1.8947,\n",
            "        1.7954, 1.8989, 1.8239, 1.7619, 1.7951, 1.8149, 1.8539, 1.8502, 1.7095,\n",
            "        2.1831, 1.8599, 1.8252, 1.8193, 1.8460, 1.7968, 1.6229, 1.8450, 1.8290,\n",
            "        1.8706, 1.9293, 1.6881, 1.9725, 1.8981, 1.8925, 1.8851, 1.8445, 1.9764,\n",
            "        2.0674, 1.8384, 1.8414, 1.8762, 1.7931, 1.7131, 1.9644, 1.7854, 1.9369,\n",
            "        1.8972, 1.8940, 1.8700, 1.7967, 1.8775, 1.9409, 1.7391, 1.7944, 1.9678,\n",
            "        1.7678, 1.6851, 1.9414, 1.9663, 1.9882, 1.7915, 1.8141, 1.8325, 2.1200,\n",
            "        1.9256, 2.3592, 2.0304, 1.9594, 1.7334, 1.9048, 1.8221, 1.7811, 1.9084,\n",
            "        1.8053, 1.9171, 1.9644, 1.8256, 1.6432, 1.9173, 1.9094, 1.9923, 1.7963,\n",
            "        1.9077, 1.7619, 2.1724, 1.7931, 1.7564, 1.8889, 1.9832, 1.9136, 1.8035,\n",
            "        1.8419, 1.8278, 1.8057, 1.9063, 1.8646, 1.7848, 1.8230, 1.7986, 1.7091,\n",
            "        1.7724, 1.7939, 1.7611, 1.9325, 2.0162, 1.7295, 2.0196, 1.8876, 1.8325,\n",
            "        1.8225, 1.7870, 1.9160, 1.7197, 1.7170, 1.9133, 1.7770, 1.9943, 1.8389,\n",
            "        1.8070, 1.8516, 1.7857, 1.9648, 1.9553, 1.9232, 1.8086, 1.8114, 1.7141,\n",
            "        1.8058, 1.8532, 1.9255, 1.7682, 1.8314, 1.8495, 1.8296, 1.8278, 1.8819,\n",
            "        1.7698, 1.7838, 1.7807, 1.9974, 1.6994, 1.9483, 1.7793, 1.8029, 2.2210,\n",
            "        1.6455, 1.8357, 2.1706, 1.9204, 1.7414, 1.7809, 1.8648, 1.9145, 1.8849,\n",
            "        1.8346, 1.9368, 1.8169, 2.2302, 1.8262, 2.0651, 1.9888, 1.8169, 1.8462,\n",
            "        1.9681, 1.8083, 1.8595, 1.8539, 1.7699, 1.9001, 1.7285, 1.7553, 1.8924,\n",
            "        1.7829, 1.9428, 1.8724, 1.7228, 2.0548, 1.7732, 1.8561, 1.7699, 1.9269,\n",
            "        1.8171, 2.4075, 1.7257, 1.7819, 1.7244, 1.8521, 1.8302, 1.8797, 1.7617,\n",
            "        1.9650, 1.9807, 1.7102, 1.7486, 1.8350, 1.9919, 1.8505, 1.9000, 1.8269,\n",
            "        1.9787, 1.7635, 1.6071, 1.7998, 1.9545, 1.7348, 1.7140, 1.8851, 1.7981,\n",
            "        1.9100, 1.8315, 1.7864, 1.9165, 1.8839, 1.9017, 1.9334, 1.7405, 1.7661,\n",
            "        1.8015, 1.9987, 1.7622, 1.9107, 1.8444, 1.7128, 1.8726, 1.8529, 1.9270,\n",
            "        1.8769, 1.7261, 1.8393, 1.9075, 1.7953, 1.8246, 1.7605, 2.0470, 1.9221,\n",
            "        1.9205, 1.8910, 1.7666, 1.6801, 1.8308, 1.8845, 1.8339, 1.8238, 1.7616,\n",
            "        1.6114, 1.8411, 1.7437, 1.8423, 1.9540, 1.7465, 1.7741, 1.8746, 1.8856,\n",
            "        1.7740, 1.7603, 1.7682, 1.8396, 1.6869, 1.8080, 1.8836, 1.8283, 1.8341,\n",
            "        1.8522, 1.9749, 1.8707, 1.7719, 1.8993, 1.8108, 1.8480, 1.8267, 1.8731,\n",
            "        1.9576, 1.8347, 1.9509, 1.9641, 1.7997, 1.7652, 1.9253, 1.7126, 1.7551,\n",
            "        1.9427, 1.8559, 1.9163, 1.7681, 1.7803, 1.8500, 1.8535, 1.8865, 1.7599,\n",
            "        2.0692, 1.8021, 1.7077, 1.8890, 1.9457, 1.8516, 1.7882, 1.8356, 1.8472,\n",
            "        1.6708, 1.7435, 1.9080, 1.9653, 2.0401, 1.8935, 1.8450, 1.7536, 1.7733,\n",
            "        1.8135, 1.8534, 1.9368, 1.7348, 1.8738, 1.9632, 1.9033, 1.7422, 1.7842,\n",
            "        1.8516, 2.0218, 1.7044, 1.8793, 1.8655, 1.8516, 1.8002, 1.8687, 1.8460,\n",
            "        1.7589, 1.8174, 1.9830, 1.9034, 2.1222, 1.8460, 1.9209, 1.8893, 1.9422,\n",
            "        1.8489, 1.8396, 1.9953, 2.0865, 1.8253, 1.7700, 1.8035, 1.7535, 1.8923,\n",
            "        1.8620, 1.8627, 1.7264, 1.8140, 1.9613, 1.8812, 1.8729, 2.0050, 1.7092,\n",
            "        1.7726, 1.9410, 1.8381, 1.8366, 1.7276, 1.8796, 1.7548, 1.9536, 1.8062,\n",
            "        1.8883, 2.0278, 1.8775, 1.9446, 1.8676, 1.8423, 1.7798, 1.9403, 1.8375,\n",
            "        2.0473, 1.9507, 1.8337, 1.8184, 1.7791, 1.8993, 1.8781, 1.8691, 1.8493,\n",
            "        1.7623, 1.9458, 1.7564, 1.7448, 1.8633, 1.6863, 1.8062, 1.8702, 2.0048,\n",
            "        1.8504, 1.8964, 1.9489, 1.8264, 1.9019, 1.8196, 1.9712, 1.8969, 1.8652,\n",
            "        1.8709, 1.6984, 1.8677, 1.8846, 1.9256, 1.8620, 1.6366, 1.8434, 1.7506,\n",
            "        1.8438, 1.5788, 1.9316, 1.9535, 1.7878, 1.7354, 2.0920, 1.9456],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.2371, 0.3433, 0.3279, 0.4642, 0.2233, 0.2370, 0.2176, 0.3793, 0.3140,\n",
            "        0.2803, 0.2434, 0.2116, 0.2478, 0.2435, 0.2298, 0.3172, 0.2725, 0.6511,\n",
            "        0.2925, 0.2281, 0.2279, 0.4254, 0.2342, 0.3328, 0.2632, 0.2176, 0.3180,\n",
            "        0.3893, 0.1387, 0.2274, 0.3379, 0.0767, 0.2253, 0.2504, 0.1990, 0.1951,\n",
            "        0.2566, 0.3253, 0.2797, 0.3149, 0.2373, 0.2533, 0.1956, 0.3236, 0.2093,\n",
            "        0.2333, 0.2300, 0.5019, 0.2830, 0.1885, 0.3264, 0.2722, 0.2369, 0.2430,\n",
            "        0.3625, 0.2165, 0.4700, 0.3047, 0.3675, 0.2641, 0.1979, 0.2664, 0.3448,\n",
            "        0.2005, 0.2450, 0.4351, 0.2689, 0.1632, 0.3087, 0.1209, 0.2153, 0.1592,\n",
            "        0.2960, 0.1423, 0.2951, 0.2706, 0.2007, 0.2939, 0.2210, 0.2243, 0.2465,\n",
            "        0.3910, 0.4599, 0.5417, 0.2147, 0.3469, 0.2703, 0.2229, 0.3645, 0.2647,\n",
            "        0.2421, 0.2492, 0.1666, 0.2763, 0.2560, 0.2151, 0.3363, 0.2767, 0.2516,\n",
            "        0.2988, 0.2622, 0.3499, 0.3001, 0.3907, 0.3184, 0.2233, 0.2649, 0.2110,\n",
            "        0.2034, 0.2752, 0.2314, 0.3480, 0.2238, 0.2892, 0.1991, 0.2923, 0.3259,\n",
            "        0.0722, 0.3039, 0.3041, 0.3803, 0.2568, 0.2382, 0.3057, 0.2652, 0.1532,\n",
            "        0.2110, 0.2567, 0.3148, 0.2746, 0.1833, 0.1950, 0.1116, 0.2279, 0.3705,\n",
            "        0.2477, 0.2000, 0.3060, 0.2548, 0.2468, 0.3028, 0.1921, 0.2952, 0.1980,\n",
            "        0.2135, 0.1583, 0.1586, 0.3944, 0.2352, 0.3947, 0.2740, 0.2861, 0.1856,\n",
            "        0.2702, 0.2986, 0.1728, 0.2658, 0.2696, 0.2028, 0.1838, 0.3176, 0.6246,\n",
            "        0.2631, 0.3855, 0.2074, 0.2317, 0.4171, 0.2044, 0.2926, 0.3506, 0.2305,\n",
            "        0.2400, 0.1420, 0.1093, 0.2757, 0.3253, 0.2334, 0.1650, 0.4026, 0.2066,\n",
            "        0.1790, 0.3032, 0.5658, 0.3246, 0.3834, 0.3254, 0.1772, 0.2909, 0.2350,\n",
            "        0.2519, 0.1968, 0.2003, 0.3213, 0.4802, 0.2543, 0.2578, 0.3280, 0.2270,\n",
            "        0.3044, 0.2273, 0.2447, 0.2527, 0.4136, 0.2588, 0.3589, 0.2688, 0.2115,\n",
            "        0.2022, 0.3186, 0.3740, 0.1785, 0.2074, 0.2346, 0.3566, 0.2623, 0.2620,\n",
            "        0.2880, 0.1462, 0.1896, 0.2777, 0.1852, 0.3240, 0.2748, 0.2164, 0.3066,\n",
            "        0.1845, 0.3992, 0.1695, 0.4411, 0.2812, 0.2730, 0.2784, 0.1861, 0.3589,\n",
            "        0.1934, 0.3320, 0.3350, 0.2655, 0.2740, 0.3185, 0.2633, 0.2458, 0.2003,\n",
            "        0.2809, 0.3049, 0.2050, 0.2904, 0.2381, 0.3278, 0.3484, 0.4293, 0.2422,\n",
            "        0.2859, 0.1864, 0.2954, 0.5634, 0.2081, 0.3743, 0.2902, 0.3820, 0.3069,\n",
            "        0.2101, 0.2750, 0.2878, 0.1870, 0.3015, 0.1661, 0.2998, 0.3101, 0.2522,\n",
            "        0.2419, 0.1758, 0.2681, 0.2812, 0.1495, 0.2868, 0.3157, 0.2587, 0.2437,\n",
            "        0.1467, 0.5416, 0.2490, 0.2831, 0.2783, 0.1614, 0.1963, 0.2034, 0.2364,\n",
            "        0.2527, 0.1573, 0.3184, 0.2841, 0.1613, 0.1489, 0.2850, 0.1625, 0.3277,\n",
            "        0.4936, 0.2780, 0.3178, 0.1743, 0.2158, 0.2222, 0.2821, 0.4267, 0.2713,\n",
            "        0.1778, 0.3067, 0.2270, 0.1772, 0.3897, 0.2923, 0.4843, 0.2345, 0.2327,\n",
            "        0.2740, 0.2700, 0.2804, 0.4035, 0.1501, 0.3329, 0.3286, 0.2803, 0.2309,\n",
            "        0.1738, 0.3270, 0.3097, 0.1808, 0.2384, 0.2107, 0.3240, 0.3346, 0.2236,\n",
            "        0.2061, 0.2687, 0.2360, 0.3338, 0.2694, 0.3203, 0.2895, 0.1884, 0.1491,\n",
            "        0.3957, 0.5167, 0.3407, 0.1854, 0.1816, 0.2626, 0.1855, 0.2219, 0.1482,\n",
            "        0.2584, 0.2458, 0.2616, 0.2396, 0.2402, 0.2423, 0.3463, 0.2731, 0.1524,\n",
            "        0.2514, 0.2760, 0.1734, 0.2715, 0.4052, 0.2252, 0.3676, 0.3070, 0.3127,\n",
            "        0.1836, 0.4330, 0.2203, 0.2073, 0.2803, 0.2984, 0.2191, 0.3272, 0.2267,\n",
            "        0.2749, 0.3056, 0.4566, 0.2962, 0.3528, 0.3236, 0.4220, 0.2715, 0.2256,\n",
            "        0.2903, 0.1829, 0.3994, 0.2820, 0.2471, 0.1647, 0.3654, 0.4504, 0.2685,\n",
            "        0.2992, 0.2825, 0.2435, 0.2212, 0.4300, 0.4342, 0.1988, 0.2863, 0.3398,\n",
            "        0.2444, 0.2905, 0.2559, 0.2586, 0.1702, 0.1906, 0.2536, 0.2978, 0.2498,\n",
            "        0.3777, 0.2252, 0.2472, 0.2243, 0.1732, 0.2194, 0.2091, 0.2820, 0.2898,\n",
            "        0.2887, 0.3292, 0.1644, 0.2962, 0.3279, 0.2535, 0.2795, 0.2238, 0.2607,\n",
            "        0.1937, 0.2680, 0.2418, 0.5193, 0.2502, 0.3147, 0.2166, 0.2313, 0.2027,\n",
            "        0.1880, 0.2180, 0.3826, 0.3871, 0.2358, 0.3556, 0.2272, 0.3272, 0.3442,\n",
            "        0.3154, 0.1993, 0.3135, 0.2254, 0.3048, 0.2658, 0.3337, 0.2679, 0.2670,\n",
            "        0.2363, 0.4347, 0.1931, 0.1995, 0.2072, 0.3202, 0.2667, 0.2305, 0.2383,\n",
            "        0.2246, 0.2562, 0.2837, 0.4046, 0.2786, 0.2243, 0.1591, 0.1923, 0.1894,\n",
            "        0.2496, 0.1140, 0.3128, 0.3197, 0.3530, 0.2999, 0.2115, 0.4718, 0.2979,\n",
            "        0.3472, 0.2890, 0.4740, 0.2230, 0.3630, 0.4015, 0.2446, 0.1897, 0.1460,\n",
            "        0.1874, 0.2734, 0.2366, 0.3001, 0.2359, 0.2688, 0.3256, 0.2749, 0.2848,\n",
            "        0.2299, 0.3001, 0.4818, 0.3074, 0.3164, 0.3114, 0.3549, 0.2859],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.0185, -0.0705, -0.0518,  ..., -0.0390,  0.1735, -0.0410],\n",
            "        [-0.0818, -0.0944,  0.0174,  ...,  0.2028, -0.0248,  0.0372],\n",
            "        [-0.0332, -0.0566, -0.0242,  ..., -0.0344, -0.0227,  0.0197],\n",
            "        ...,\n",
            "        [-0.0103,  0.0033, -0.0359,  ..., -0.0279, -0.0115,  0.0128],\n",
            "        [-0.0359, -0.0353, -0.0296,  ..., -0.0330, -0.0110, -0.0513],\n",
            "        [ 0.0021, -0.0248, -0.0829,  ...,  0.0417, -0.0500,  0.0663]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-2.6341e-03,  3.0005e-03,  6.5581e-04, -2.6909e-02,  6.3637e-03,\n",
            "         1.3260e-02, -1.1178e-02,  2.0639e-02, -3.6373e-03, -1.2325e-02,\n",
            "        -1.2629e-02, -7.2057e-03, -1.9321e-02, -2.4960e-02, -1.1885e-02,\n",
            "        -8.3259e-03, -9.5745e-03, -1.6658e-02,  9.1804e-03, -1.5354e-02,\n",
            "         7.1358e-03,  3.0737e-02,  1.3239e-02, -7.7528e-03,  4.7448e-03,\n",
            "         1.1175e-02,  1.5949e-02, -1.6712e-02, -1.0130e-03, -3.7167e-03,\n",
            "         6.5269e-03, -1.2041e-02,  9.0427e-03, -8.3279e-04,  8.8647e-03,\n",
            "        -2.6307e-02, -1.4588e-02,  2.9433e-03,  2.9718e-03, -1.9125e-02,\n",
            "        -4.7922e-03,  1.3828e-02,  9.8802e-03, -1.8417e-02,  1.9734e-02,\n",
            "         1.6941e-03,  1.2420e-02, -5.5842e-03, -1.0612e-02,  3.9847e-04,\n",
            "         4.2733e-03, -1.3298e-02,  2.0661e-02,  1.6963e-02,  2.7952e-03,\n",
            "         7.4214e-04,  1.3168e-02,  3.2213e-03,  1.0458e-02,  1.6511e-02,\n",
            "         9.1717e-04,  3.9388e-03, -5.6534e-03,  1.9372e-02,  7.5238e-03,\n",
            "         1.3437e-02, -1.3185e-02, -1.0026e-02,  7.1920e-03, -2.3166e-03,\n",
            "        -1.8895e-02,  1.2519e-02,  1.9583e-03,  7.3836e-03, -9.6664e-03,\n",
            "         2.0189e-02,  7.6652e-03,  1.8529e-02,  1.5710e-02,  1.8582e-02,\n",
            "        -6.9314e-03,  1.7090e-02,  9.1268e-03, -3.8876e-02, -2.4116e-02,\n",
            "        -6.8715e-03, -1.1648e-02,  7.8817e-03,  1.8046e-03,  2.8480e-02,\n",
            "        -1.9379e-02, -1.6295e-02,  1.0468e-02, -1.3027e-02, -8.4211e-03,\n",
            "        -2.9210e-02, -2.4856e-03, -8.7141e-03, -1.6397e-02, -9.3054e-03,\n",
            "        -1.5931e-02, -2.6346e-02, -1.1091e-03,  2.2589e-02,  2.1387e-03,\n",
            "        -2.3212e-02, -1.4085e-02, -5.6224e-03, -2.0090e-02, -3.0284e-02,\n",
            "        -4.9574e-02,  2.3283e-02,  1.4954e-02, -7.7501e-03, -3.9482e-03,\n",
            "        -3.7629e-02, -2.4220e-02, -1.0194e-02, -7.7038e-03, -4.1312e-03,\n",
            "        -2.9553e-03, -6.2174e-03, -1.2076e-02, -7.0168e-03, -3.8948e-03,\n",
            "        -1.6953e-02, -2.4585e-02,  5.5353e-03, -8.3370e-03, -7.0759e-03,\n",
            "        -2.4023e-02, -6.3686e-03,  7.3420e-04,  5.2883e-03, -2.2181e-02,\n",
            "        -2.6972e-02, -1.7990e-02, -1.6393e-02,  2.1485e-03, -1.6122e-02,\n",
            "        -1.6112e-02,  6.5931e-03, -2.0045e-02,  6.4149e-03, -1.2601e-02,\n",
            "        -7.6238e-03,  1.1411e-02, -4.5084e-02, -9.2018e-03, -1.5563e-02,\n",
            "        -1.3590e-02, -1.4374e-03, -1.9466e-02,  2.0737e-02, -1.0476e-02,\n",
            "         6.3229e-03,  8.3229e-03, -1.0791e-02, -1.8903e-02,  5.8624e-03,\n",
            "        -2.0189e-03,  3.2436e-02,  4.0581e-02, -4.0820e-05,  1.0886e-02,\n",
            "        -1.6544e-02, -5.3365e-04, -2.2903e-02,  4.6295e-03, -4.8402e-03,\n",
            "         1.0187e-02,  1.7954e-02,  4.8211e-03,  6.1831e-03,  1.4419e-02,\n",
            "        -1.2094e-02, -8.7460e-03,  1.9488e-03,  1.4685e-02,  1.2464e-02,\n",
            "         7.0523e-03, -4.1783e-03,  1.2048e-02, -2.0199e-02,  9.9144e-03,\n",
            "         1.3978e-02, -1.0321e-03,  5.7394e-03,  1.4019e-03,  6.0113e-04,\n",
            "        -5.5790e-04,  2.4424e-02,  2.3076e-02, -1.4610e-02,  1.1185e-02,\n",
            "         3.4608e-02,  1.6944e-02,  4.3295e-03, -2.5606e-02,  1.2279e-02,\n",
            "        -2.5810e-02,  8.5365e-03,  2.0437e-02,  2.2557e-02,  2.2966e-02,\n",
            "         8.8420e-03, -1.3894e-02,  3.8719e-03, -9.3046e-03,  2.3220e-02,\n",
            "        -1.4949e-02,  6.9258e-03,  5.0070e-03, -1.7302e-02,  1.0364e-03,\n",
            "        -1.0223e-02, -9.6949e-03,  3.4534e-02,  6.1337e-03,  1.1582e-02,\n",
            "        -2.0529e-02, -2.1956e-02,  5.3109e-03,  3.4101e-02, -5.8079e-03,\n",
            "         2.9406e-02, -8.7954e-03, -5.2505e-03, -1.9088e-02,  3.0350e-02,\n",
            "         1.8445e-02, -2.1225e-02,  1.8432e-02,  1.3832e-02,  1.7848e-02,\n",
            "        -4.4762e-03,  3.5858e-02,  2.1762e-02,  1.0880e-02,  4.0255e-02,\n",
            "        -2.0049e-03, -3.0348e-03,  9.3293e-03, -1.6304e-02,  9.6253e-04,\n",
            "         1.8673e-02, -1.6567e-02,  1.4964e-02, -3.7206e-03, -7.6734e-03,\n",
            "        -7.9254e-06,  3.9732e-03, -9.5979e-03, -1.6833e-02,  5.8524e-05,\n",
            "        -6.4126e-03,  8.2977e-03,  4.8207e-03, -1.1467e-03,  4.8869e-03,\n",
            "         1.7349e-02,  3.9222e-03, -7.8080e-03,  1.6051e-02,  9.8802e-03,\n",
            "        -1.0144e-02,  2.0912e-02, -6.3203e-03, -2.3139e-02,  1.1646e-03,\n",
            "         2.2468e-02, -6.6953e-03,  1.8311e-02,  1.4623e-02, -1.1654e-02,\n",
            "        -1.4306e-02,  1.2974e-02, -9.6865e-03, -6.2351e-03,  1.3180e-02,\n",
            "         6.7543e-03,  4.6418e-02, -2.7962e-02, -1.5111e-02,  2.8716e-02,\n",
            "         9.1991e-03, -5.3710e-03, -6.0361e-03, -7.2140e-03, -9.2421e-03,\n",
            "         1.8536e-03, -3.1078e-03, -8.4004e-03, -1.6766e-02,  4.0936e-03,\n",
            "         6.2426e-03, -1.2470e-03, -1.2919e-02,  3.5819e-03,  1.1006e-02,\n",
            "        -1.3282e-02,  2.6395e-03,  8.9953e-03,  6.5421e-03, -1.2031e-02,\n",
            "         1.7149e-02,  1.7949e-02, -1.0581e-02, -2.6962e-02, -1.3564e-02,\n",
            "        -9.7173e-03, -2.1176e-03,  3.5370e-02,  1.8392e-02,  2.6676e-02,\n",
            "        -1.0594e-03, -3.3949e-03, -4.8838e-03,  1.3427e-02, -1.3948e-02,\n",
            "        -1.9559e-02, -2.3295e-02, -3.7834e-02, -1.4637e-02, -2.1323e-02,\n",
            "        -3.0952e-02, -3.0822e-02,  1.9438e-03,  2.8637e-03, -2.1198e-02,\n",
            "         1.0448e-02, -1.1316e-02, -4.2609e-03,  2.2647e-02, -1.2867e-02,\n",
            "        -1.1018e-02,  1.2336e-02, -2.0057e-02, -2.1837e-02, -6.8067e-03,\n",
            "        -1.0488e-02, -2.6298e-02, -9.9579e-03, -1.2966e-02, -2.4832e-03,\n",
            "        -7.0940e-03,  1.7997e-02, -6.4257e-03,  7.9069e-03, -1.2287e-02,\n",
            "         9.8176e-03,  2.6674e-03, -2.1524e-02,  2.7511e-03, -9.2075e-03,\n",
            "        -1.7541e-02, -1.7103e-03, -1.4588e-02,  4.4247e-03,  3.4405e-02,\n",
            "         1.2725e-02,  3.0885e-02,  1.3090e-03, -7.0084e-05, -2.3165e-03,\n",
            "        -3.7989e-03, -1.1148e-02,  1.7210e-02, -6.7575e-03, -1.3694e-04,\n",
            "        -8.9166e-03, -1.6281e-02, -4.4920e-03,  1.1332e-02, -1.5909e-03,\n",
            "        -8.8193e-03, -4.9399e-03,  4.5732e-03, -1.0949e-02,  1.2890e-02,\n",
            "        -6.6586e-03, -2.5605e-03,  2.7965e-03,  1.1225e-02, -2.2055e-02,\n",
            "        -3.9271e-03, -6.6467e-03, -1.8840e-02, -2.1687e-02, -7.4066e-04,\n",
            "        -2.7281e-02,  5.0448e-03, -2.0709e-02, -3.4103e-02, -2.2374e-02,\n",
            "        -1.6656e-02, -2.7916e-02, -9.8977e-03,  5.5252e-03,  1.6013e-02,\n",
            "        -1.4895e-02,  3.5091e-03,  9.0003e-03, -8.3982e-03, -3.7479e-02,\n",
            "         2.0727e-02, -5.8799e-03,  9.1768e-03, -2.0297e-02, -7.3148e-03,\n",
            "        -1.6966e-03, -1.4029e-03,  3.2229e-03,  2.9212e-02,  1.2487e-02,\n",
            "        -2.0100e-02,  2.1170e-02, -2.5300e-02,  3.1815e-02, -1.0645e-03,\n",
            "        -1.0449e-02, -2.3419e-02,  1.4564e-02,  2.1245e-02,  1.6530e-02,\n",
            "        -3.2436e-03, -2.0437e-02, -3.6982e-02, -8.7213e-03,  5.4575e-03,\n",
            "         1.1048e-03,  2.2012e-03,  2.9512e-03, -5.9939e-05,  5.5785e-04,\n",
            "        -3.6906e-03,  5.3763e-03, -2.4765e-02,  9.2729e-03,  9.6081e-03,\n",
            "         9.1647e-03,  9.0880e-03,  7.4842e-03, -1.1946e-02,  2.1395e-02,\n",
            "         2.7922e-02,  1.4692e-02, -2.4958e-03,  2.8887e-02,  1.3422e-02,\n",
            "         1.7173e-03,  2.5018e-03, -2.1253e-02, -8.2424e-04,  4.2183e-03,\n",
            "         8.5981e-03,  1.8735e-02,  8.5622e-03, -8.8255e-03,  1.7462e-02,\n",
            "        -1.3693e-02,  2.1955e-03,  1.0772e-02,  2.8693e-03,  3.1032e-02,\n",
            "         8.5460e-03, -1.4198e-02, -2.2472e-03,  1.8740e-02, -1.2905e-02,\n",
            "         4.0370e-02, -7.7538e-04,  1.8671e-03,  7.2793e-03, -2.6508e-02,\n",
            "        -1.7609e-02, -2.4142e-02,  2.9577e-03, -1.5917e-02,  1.6273e-03,\n",
            "         1.1132e-02,  1.4574e-02, -8.1919e-03, -7.6581e-03, -1.8452e-02,\n",
            "        -9.0419e-03,  4.0883e-03,  4.4482e-02, -2.3664e-02, -5.2547e-03,\n",
            "        -1.9529e-02,  3.2860e-03,  5.4667e-03, -4.9558e-03,  7.6805e-03,\n",
            "        -3.3026e-03, -2.6248e-03, -1.1094e-02,  2.3922e-02,  1.8079e-02,\n",
            "        -1.8135e-02,  5.2204e-03, -1.3559e-02,  1.9448e-02,  1.0981e-02,\n",
            "         2.6869e-02, -6.6801e-03, -8.9389e-04, -3.4924e-03, -1.9667e-02,\n",
            "        -1.8511e-02, -7.6262e-04, -1.6382e-02, -1.5862e-02, -1.3717e-02,\n",
            "         1.7528e-02, -1.1419e-03, -6.3346e-03, -1.1118e-02,  1.3159e-02,\n",
            "        -2.3464e-02,  2.7993e-04, -3.6273e-04,  2.3797e-02, -2.7353e-03,\n",
            "        -2.2223e-02,  1.3415e-02,  1.0443e-02, -2.3512e-02,  1.6832e-02,\n",
            "         4.3699e-03, -1.3243e-02, -2.8605e-03,  5.4212e-03,  1.9924e-03,\n",
            "        -6.8664e-04, -3.9092e-04,  1.7806e-02,  1.8391e-02,  2.8473e-02,\n",
            "        -3.3835e-02, -1.0778e-02, -1.2371e-02, -1.9110e-03, -1.6381e-03,\n",
            "         1.7288e-02, -3.9813e-03, -1.5167e-02, -1.0781e-02,  5.3808e-03,\n",
            "        -3.3947e-04,  3.3885e-04, -1.0162e-02, -4.0266e-03, -3.4751e-03,\n",
            "         4.2359e-03, -1.4677e-03,  1.3207e-02,  7.5580e-03,  1.9397e-04,\n",
            "         3.0048e-03,  8.6283e-03, -1.1193e-02,  3.8466e-02, -2.6220e-02,\n",
            "        -2.0251e-02, -6.3872e-03,  2.1906e-02, -7.3400e-03,  5.2753e-03,\n",
            "        -1.1709e-02,  8.4009e-03,  2.8530e-03, -4.7220e-03,  2.3118e-02,\n",
            "        -7.6039e-03,  2.8136e-03, -1.1701e-02, -4.4118e-03,  1.1846e-02,\n",
            "        -1.7632e-03, -1.2260e-02, -2.1210e-03,  1.2072e-02,  6.7523e-03,\n",
            "        -1.9128e-04, -2.5105e-02,  1.2693e-02,  1.6062e-02,  8.1264e-03,\n",
            "         1.3857e-03,  3.0087e-03, -1.4111e-02,  1.9784e-02, -9.2301e-04,\n",
            "        -1.8428e-02,  7.8059e-03,  1.5319e-02, -1.2768e-02, -9.0166e-03,\n",
            "         1.8031e-02,  2.4853e-02,  1.7788e-02,  8.8640e-03, -9.4422e-03,\n",
            "        -1.3652e-03,  1.2932e-02,  9.0133e-03,  1.6655e-02, -5.4321e-03,\n",
            "         2.7480e-02, -3.1781e-02, -1.3331e-02,  5.5792e-03, -1.3278e-02,\n",
            "        -1.9219e-02, -1.3307e-02,  4.2390e-03,  3.0246e-02, -8.1990e-03,\n",
            "         8.3008e-03,  1.8993e-02,  1.0643e-02,  3.1324e-02,  1.9283e-02,\n",
            "         3.3642e-03,  1.9669e-02,  2.2673e-03, -1.9630e-02,  2.0147e-02,\n",
            "        -1.1433e-02, -7.6073e-03,  1.5071e-02, -3.0395e-03, -9.3430e-03,\n",
            "        -4.1657e-03,  2.2972e-03, -5.0985e-03, -1.4499e-02, -2.7673e-02,\n",
            "        -3.8721e-02,  5.4249e-03,  1.3504e-02, -1.2811e-03,  3.7465e-02,\n",
            "         1.5154e-03,  2.4035e-02, -2.0557e-02,  9.8406e-03,  1.0352e-02,\n",
            "         3.8597e-02, -1.1905e-02, -2.1718e-02,  8.3778e-03,  1.4691e-02,\n",
            "         2.2631e-02, -3.7629e-03,  1.5570e-02, -9.3990e-03,  5.3536e-03,\n",
            "         1.9584e-02, -1.1156e-02,  1.5190e-02,  5.4622e-03,  2.2995e-02,\n",
            "         2.9260e-02, -1.5236e-03,  6.6009e-03, -3.1939e-02, -1.0486e-02,\n",
            "        -4.4617e-03,  3.1853e-02,  1.3736e-02,  1.3561e-02,  7.0907e-03,\n",
            "        -1.6753e-02, -2.5470e-02,  1.9752e-02,  2.6715e-02, -4.6859e-03,\n",
            "         1.7682e-02,  3.2496e-02,  1.4553e-02,  2.6101e-02,  1.1341e-02,\n",
            "        -2.2271e-03,  3.5237e-02, -1.1892e-02, -1.8683e-02, -5.5245e-03,\n",
            "        -7.0732e-03, -5.2670e-03,  7.5946e-03, -1.8465e-02, -1.6897e-02,\n",
            "         1.0127e-02,  1.3006e-02, -1.8251e-03,  6.6651e-04, -1.1207e-02,\n",
            "         1.3563e-02, -1.8153e-02, -2.6487e-02,  6.0652e-03,  3.9711e-02,\n",
            "        -1.4285e-02,  1.8001e-02, -1.4039e-02, -1.8762e-02, -1.1778e-02,\n",
            "        -1.6449e-02,  9.0423e-03, -7.2730e-03,  1.7517e-02, -7.3016e-04,\n",
            "         1.0212e-02,  2.3785e-02,  8.1286e-03,  8.0260e-03,  1.1922e-02,\n",
            "         6.2416e-03, -2.4625e-02,  2.9461e-02, -1.4183e-02, -1.8672e-02,\n",
            "        -1.4057e-02,  7.9872e-03, -2.1081e-02, -2.7560e-02, -3.5690e-03,\n",
            "         1.5993e-03,  9.4720e-03,  1.8272e-02, -2.3742e-03,  1.1843e-03,\n",
            "        -5.7722e-04,  8.7818e-03,  2.7804e-03,  6.7973e-04,  1.5877e-02,\n",
            "        -7.0359e-03,  2.5487e-03, -1.7925e-02,  8.4912e-03,  4.3375e-03,\n",
            "         2.4508e-02,  3.6686e-03,  1.0252e-02, -1.3396e-02,  4.5706e-04,\n",
            "         1.0313e-02,  1.5229e-02,  3.9907e-02, -8.0809e-03,  1.3760e-02,\n",
            "        -6.5863e-03,  6.6066e-03, -3.1480e-02,  2.4665e-02,  3.4374e-03,\n",
            "         2.0973e-02,  1.9384e-02, -2.0880e-02,  7.1465e-03,  1.0406e-02,\n",
            "         2.2273e-05, -1.9182e-02,  6.3135e-03, -1.6891e-04,  8.8664e-03,\n",
            "        -4.7666e-03, -1.4493e-02,  3.2176e-03,  7.3346e-03,  2.0694e-02,\n",
            "        -4.9972e-04,  1.8820e-02,  3.9147e-02, -2.7095e-02, -1.8293e-02,\n",
            "        -1.9868e-02, -9.4048e-03,  4.1552e-03,  5.3837e-03, -4.6663e-03,\n",
            "        -1.3019e-02, -2.4452e-02, -8.9231e-03, -1.4603e-02,  2.5529e-03,\n",
            "        -3.0766e-02,  1.1169e-02, -6.8113e-03, -7.5967e-03, -9.3191e-03,\n",
            "         1.4919e-03, -2.3428e-03,  4.4398e-04, -1.0810e-02,  8.8498e-03,\n",
            "        -2.1022e-02, -8.0380e-03, -1.0818e-02, -6.4815e-03, -2.0681e-03,\n",
            "         2.2326e-02, -1.9234e-02,  4.0844e-03,  7.7233e-04,  1.7226e-03,\n",
            "        -1.7454e-02, -1.3190e-02, -7.4112e-03, -1.7550e-03,  1.2926e-03,\n",
            "        -6.7029e-03, -7.0588e-03,  6.2745e-03, -1.8068e-02, -9.4855e-03,\n",
            "        -2.0856e-02,  8.9604e-03,  2.1294e-02,  1.7025e-02,  2.1015e-02,\n",
            "         8.8233e-03, -9.8277e-03, -2.2293e-02,  2.4295e-02, -1.1174e-02,\n",
            "        -7.5753e-03,  6.1182e-03, -2.0653e-02, -1.6264e-02,  2.6457e-02,\n",
            "        -1.4782e-02,  1.8654e-02,  2.5488e-02,  2.4106e-02,  4.7888e-03,\n",
            "         2.3329e-02,  3.5806e-04,  2.5154e-02,  1.7094e-02,  1.7803e-02,\n",
            "         2.4687e-02,  9.0085e-03,  2.3610e-03,  2.6088e-02, -1.4110e-02,\n",
            "        -5.4212e-04,  8.9498e-04,  2.1150e-02,  4.8484e-03, -3.0503e-02,\n",
            "        -7.5025e-03, -3.3718e-02, -2.8913e-02,  1.5691e-02,  6.2047e-03,\n",
            "        -1.0853e-02,  1.9524e-02, -1.6188e-02,  8.9890e-03,  9.1894e-03,\n",
            "        -2.8592e-03, -1.0911e-02,  1.0848e-02,  4.8784e-02, -1.9687e-03,\n",
            "         2.6843e-02, -4.8715e-03,  1.3489e-02, -1.4523e-02, -2.7585e-02,\n",
            "         6.1228e-03,  4.8171e-03,  2.1566e-03, -3.7561e-02,  3.0775e-02,\n",
            "         1.9977e-02,  1.8480e-02,  3.0368e-03,  9.3825e-03,  4.5243e-04,\n",
            "         6.1650e-02, -8.6416e-03, -2.6913e-02,  6.3527e-03,  7.7985e-03,\n",
            "         1.3180e-02, -1.6666e-03,  2.0865e-02,  9.9480e-03,  8.8136e-03,\n",
            "         1.4841e-02,  3.3211e-03,  3.6342e-03,  2.8740e-02, -2.2120e-02,\n",
            "        -7.1567e-03,  1.0352e-02,  1.6433e-02,  1.1683e-02, -5.8058e-03,\n",
            "        -6.9297e-04,  2.6578e-02,  8.7967e-03, -3.1689e-02,  1.8949e-02,\n",
            "        -8.5859e-03,  3.4228e-02, -1.5237e-02, -5.9709e-03,  1.1069e-03,\n",
            "        -1.8394e-02, -1.9246e-02, -3.6361e-02,  3.9839e-03,  4.1237e-02,\n",
            "         1.3816e-02, -7.3304e-03,  3.8832e-03,  2.4367e-03, -2.1625e-02,\n",
            "        -1.4523e-02, -1.6281e-04,  6.2566e-04, -1.6798e-02,  2.3083e-02,\n",
            "         9.7114e-03, -8.2207e-03,  1.1595e-03, -2.0983e-02, -6.6540e-03,\n",
            "        -1.4097e-02,  3.4067e-03, -7.7575e-03, -1.4738e-02, -2.1343e-02,\n",
            "         5.4123e-03,  3.9747e-03, -4.6185e-03, -1.5462e-02, -7.6229e-03,\n",
            "         1.2211e-02, -4.8453e-03, -8.8757e-03, -1.0275e-02,  7.3482e-03,\n",
            "        -6.0349e-03,  2.3658e-03,  2.1053e-02, -8.5688e-03, -1.1630e-02,\n",
            "        -2.7332e-02, -2.0648e-02,  4.4952e-03, -1.8649e-02, -1.1564e-02,\n",
            "         4.5905e-04, -6.1831e-03, -2.4435e-02, -7.1187e-03, -1.4394e-02,\n",
            "        -2.3544e-03,  2.1556e-02,  2.2924e-02, -1.3725e-02,  7.7785e-03,\n",
            "        -8.5513e-03,  2.4221e-02,  3.8192e-03,  7.0947e-04,  1.6114e-02,\n",
            "         2.5932e-02,  1.8108e-02,  2.9306e-02,  1.6773e-03, -3.0166e-03,\n",
            "         3.2015e-02, -1.4034e-02,  2.7365e-02, -1.8858e-02,  2.5832e-03,\n",
            "         1.3498e-02, -1.3502e-02, -1.4940e-02, -1.0904e-02,  1.8642e-02,\n",
            "         4.2593e-03, -1.6742e-02, -1.2638e-02, -4.5468e-02, -5.0823e-03,\n",
            "        -2.5093e-02,  6.7847e-03, -1.7868e-02, -7.8250e-04, -6.3448e-03],\n",
            "       requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Unfreeze the last two layers(layer 4)\n",
        "for param in model.layer4.parameters():\n",
        "  print(param)\n",
        "  param.requires_grad=True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIslrB-jNon3",
        "outputId": "5b3702b1-b72a-4cc8-e7e9-0aa86d45fa2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[[[-1.1645e-02, -1.9010e-02, -2.1876e-02],\n",
            "          [ 2.0482e-02,  2.3962e-02,  2.9161e-02],\n",
            "          [ 4.3672e-02,  3.3278e-02,  4.9908e-02]],\n",
            "\n",
            "         [[-7.4040e-03,  2.8083e-03, -4.7339e-03],\n",
            "          [ 6.9030e-03,  1.4271e-02, -3.6954e-03],\n",
            "          [-3.1341e-03,  1.3736e-02,  1.6127e-03]],\n",
            "\n",
            "         [[ 1.8676e-02, -1.0553e-02, -1.4233e-02],\n",
            "          [ 8.9944e-03, -2.5068e-03, -1.2145e-02],\n",
            "          [-4.9455e-03, -2.9206e-02, -9.6385e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.2655e-02,  1.7691e-02,  9.8264e-04],\n",
            "          [ 7.4271e-03,  7.6115e-03,  1.1135e-02],\n",
            "          [ 2.3242e-02,  1.1058e-02,  4.0498e-03]],\n",
            "\n",
            "         [[ 1.8557e-02,  1.2472e-02,  1.7220e-02],\n",
            "          [-4.8544e-03,  8.3627e-03,  2.2811e-02],\n",
            "          [-5.1675e-03,  2.3264e-02,  3.4068e-02]],\n",
            "\n",
            "         [[ 2.4934e-02,  2.2373e-02,  4.2614e-02],\n",
            "          [ 1.3486e-02,  1.6760e-03,  1.3019e-02],\n",
            "          [-6.2821e-03, -1.5112e-03, -8.9229e-03]]],\n",
            "\n",
            "\n",
            "        [[[-9.8089e-04, -6.3011e-03,  5.9932e-03],\n",
            "          [ 1.5936e-02,  1.3394e-02,  2.9934e-02],\n",
            "          [ 2.3149e-02,  2.0709e-02,  2.5485e-02]],\n",
            "\n",
            "         [[-2.0015e-02, -3.3349e-02, -8.0396e-03],\n",
            "          [-7.2800e-03, -1.2187e-02, -2.0389e-04],\n",
            "          [-1.3138e-02, -2.0427e-02, -1.6286e-02]],\n",
            "\n",
            "         [[-6.7681e-03,  5.0045e-03, -2.6683e-03],\n",
            "          [-2.1073e-02,  2.8275e-04, -1.8205e-02],\n",
            "          [-1.7382e-02, -5.0244e-03, -3.0386e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.1035e-02, -2.2964e-02, -1.1028e-02],\n",
            "          [-6.3256e-03, -4.1667e-03, -1.7323e-02],\n",
            "          [-1.3611e-02, -2.3468e-02, -1.6436e-02]],\n",
            "\n",
            "         [[ 7.3663e-03,  6.6219e-03,  5.2776e-03],\n",
            "          [-3.5464e-03,  3.2750e-03, -9.1126e-03],\n",
            "          [ 3.5593e-04, -1.0151e-02, -1.9123e-02]],\n",
            "\n",
            "         [[ 1.8193e-03,  8.8087e-03,  5.1361e-03],\n",
            "          [ 3.1915e-03,  2.5287e-02,  2.4939e-02],\n",
            "          [ 1.3968e-02,  1.9613e-02,  2.2382e-02]]],\n",
            "\n",
            "\n",
            "        [[[-4.1548e-03,  8.8964e-03,  2.0143e-03],\n",
            "          [ 1.1327e-02,  1.3251e-02,  1.4014e-02],\n",
            "          [ 7.2196e-03,  1.3045e-02,  2.4827e-02]],\n",
            "\n",
            "         [[-1.5025e-02,  5.0530e-03,  7.4766e-03],\n",
            "          [-2.4685e-02, -1.6732e-02, -1.0888e-02],\n",
            "          [-2.8064e-02, -1.1875e-02, -3.4120e-03]],\n",
            "\n",
            "         [[ 2.8449e-02,  1.4594e-02,  6.9441e-03],\n",
            "          [ 2.4799e-02,  1.9453e-02,  1.1294e-02],\n",
            "          [-1.0787e-02, -2.1006e-02, -1.0372e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.4967e-02,  8.2449e-03,  2.0244e-03],\n",
            "          [ 1.4287e-02, -6.3867e-03, -8.0757e-03],\n",
            "          [ 2.7547e-02,  1.0791e-02,  1.6567e-02]],\n",
            "\n",
            "         [[ 3.6191e-02,  3.8918e-02,  3.9028e-02],\n",
            "          [-8.3489e-04,  1.3273e-02,  2.0172e-02],\n",
            "          [-2.0652e-02, -5.4010e-03,  1.7147e-03]],\n",
            "\n",
            "         [[ 2.0373e-04,  3.5919e-03,  8.5592e-03],\n",
            "          [ 6.2363e-03, -9.3086e-05,  1.2940e-02],\n",
            "          [ 1.3152e-02,  1.0732e-02,  1.9896e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1.7400e-02, -6.7019e-03, -9.1787e-03],\n",
            "          [-9.9672e-03,  2.6298e-04,  3.3439e-03],\n",
            "          [ 1.5721e-02,  1.4216e-02,  2.0509e-02]],\n",
            "\n",
            "         [[ 2.1410e-02,  3.6914e-02,  2.8239e-02],\n",
            "          [ 3.8158e-02,  4.8944e-02,  3.4652e-02],\n",
            "          [ 3.1723e-02,  4.4208e-02,  4.0035e-02]],\n",
            "\n",
            "         [[-3.3437e-03, -1.0482e-02, -5.3990e-03],\n",
            "          [-5.3186e-03,  1.1394e-02,  1.7593e-03],\n",
            "          [-5.6652e-03, -6.6373e-03, -1.3492e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.7099e-02, -1.8145e-03, -1.3040e-02],\n",
            "          [-2.2750e-02, -3.6062e-03, -8.0294e-03],\n",
            "          [-1.6087e-02, -1.0175e-02, -1.3529e-02]],\n",
            "\n",
            "         [[ 4.1701e-04, -5.1785e-03, -2.1884e-02],\n",
            "          [ 2.6919e-03,  8.9139e-03, -1.4217e-04],\n",
            "          [-7.3746e-03, -6.6853e-03, -2.3725e-02]],\n",
            "\n",
            "         [[ 1.9425e-02,  1.3175e-02,  1.7511e-02],\n",
            "          [ 1.8235e-02,  4.4286e-02,  2.3767e-02],\n",
            "          [ 2.6504e-02,  3.3104e-02,  1.9696e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.0177e-02, -1.0701e-02, -2.0428e-02],\n",
            "          [-1.7986e-02,  5.9928e-03, -1.0584e-03],\n",
            "          [-1.8794e-02, -1.8773e-03, -6.9449e-03]],\n",
            "\n",
            "         [[-2.8498e-03,  1.6427e-03,  1.4575e-04],\n",
            "          [-5.4403e-03,  8.3667e-03, -9.4164e-03],\n",
            "          [-4.4999e-03,  5.4902e-03,  2.4863e-03]],\n",
            "\n",
            "         [[-1.3356e-02, -2.1525e-02,  5.3421e-04],\n",
            "          [-1.9160e-02, -2.4645e-02, -1.3791e-02],\n",
            "          [-6.1991e-03, -1.3174e-02, -3.6783e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.3993e-03, -2.7823e-03,  7.6715e-03],\n",
            "          [-2.0649e-02, -1.2731e-02, -9.4138e-03],\n",
            "          [-1.3678e-03, -3.4410e-02, -2.6984e-02]],\n",
            "\n",
            "         [[-3.5651e-04,  2.0102e-03,  1.4130e-02],\n",
            "          [-1.3073e-02, -1.6616e-02, -1.2690e-02],\n",
            "          [-3.5934e-02, -4.1700e-02, -3.3968e-02]],\n",
            "\n",
            "         [[ 2.0470e-02,  8.0159e-04, -1.1607e-03],\n",
            "          [ 9.5101e-03,  3.0336e-02,  2.7362e-02],\n",
            "          [ 1.5588e-02,  3.2851e-02,  1.3015e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.5574e-02, -3.2971e-02, -3.1939e-02],\n",
            "          [-2.2502e-02, -5.7187e-03, -5.6729e-03],\n",
            "          [-2.7309e-02, -1.6981e-02,  1.2832e-04]],\n",
            "\n",
            "         [[-1.1925e-02, -2.9479e-02, -2.0437e-02],\n",
            "          [-2.4408e-02, -2.2069e-02, -1.9965e-03],\n",
            "          [-2.3279e-02, -5.5140e-03,  2.5630e-02]],\n",
            "\n",
            "         [[-1.6100e-02, -8.2417e-03,  1.5266e-04],\n",
            "          [-2.6195e-03, -8.2754e-03, -2.9435e-02],\n",
            "          [-2.7493e-03, -2.4889e-02, -2.3583e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.7985e-02,  1.8594e-02,  8.9198e-04],\n",
            "          [-1.7319e-02,  7.8735e-03, -2.8659e-03],\n",
            "          [ 3.8596e-03,  2.9061e-02,  2.4188e-02]],\n",
            "\n",
            "         [[-2.6735e-02, -1.4391e-02, -4.0148e-02],\n",
            "          [-2.6728e-02, -2.4455e-02, -6.9176e-03],\n",
            "          [-5.7244e-02, -2.1995e-04,  5.5438e-02]],\n",
            "\n",
            "         [[ 2.3487e-02,  2.7157e-03, -8.4719e-04],\n",
            "          [ 1.7886e-02,  5.4860e-03,  2.8059e-02],\n",
            "          [ 4.6468e-03,  1.8598e-02,  1.3761e-03]]]])\n",
            "Parameter containing:\n",
            "tensor([0.2427, 0.2232, 0.2511, 0.2288, 0.2074, 0.2905, 0.2482, 0.3102, 0.2749,\n",
            "        0.2892, 0.2448, 0.1759, 0.2426, 0.2780, 0.2315, 0.2631, 0.3383, 0.2785,\n",
            "        0.2536, 0.2989, 0.2335, 0.2812, 0.3486, 0.2778, 0.2280, 0.2547, 0.3032,\n",
            "        0.2468, 0.2512, 0.2973, 0.2577, 0.3200, 0.2385, 0.2714, 0.2532, 0.2625,\n",
            "        0.3344, 0.2626, 0.1838, 0.2839, 0.2187, 0.2666, 0.2858, 0.2471, 0.2915,\n",
            "        0.2332, 0.2637, 0.2691, 0.2432, 0.2384, 0.2356, 0.2525, 0.2564, 0.2451,\n",
            "        0.2529, 0.2522, 0.2800, 0.3165, 0.2340, 0.2634, 0.2569, 0.1942, 0.2621,\n",
            "        0.2205, 0.2301, 0.2323, 0.2811, 0.1897, 0.2280, 0.3472, 0.2717, 0.3191,\n",
            "        0.2440, 0.2719, 0.2781, 0.2262, 0.3444, 0.2648, 0.2725, 0.2851, 0.2039,\n",
            "        0.2935, 0.2742, 0.2774, 0.2654, 0.2430, 0.2721, 0.2708, 0.3085, 0.2895,\n",
            "        0.2596, 0.2147, 0.3119, 0.3449, 0.2262, 0.2814, 0.2326, 0.2712, 0.2637,\n",
            "        0.2323, 0.3333, 0.2714, 0.2991, 0.2747, 0.2515, 0.2394, 0.2709, 0.2836,\n",
            "        0.2866, 0.2408, 0.2560, 0.2048, 0.2394, 0.2813, 0.3267, 0.2761, 0.2123,\n",
            "        0.2715, 0.2540, 0.2771, 0.3209, 0.1905, 0.3989, 0.2676, 0.2357, 0.2169,\n",
            "        0.3216, 0.3596, 0.2838, 0.2648, 0.2702, 0.2469, 0.2442, 0.2553, 0.2599,\n",
            "        0.2693, 0.2399, 0.2700, 0.2063, 0.2711, 0.2834, 0.2781, 0.2529, 0.2013,\n",
            "        0.2343, 0.2082, 0.3063, 0.1635, 0.2673, 0.2197, 0.2787, 0.2724, 0.2744,\n",
            "        0.2287, 0.2969, 0.2662, 0.2982, 0.2396, 0.3039, 0.2319, 0.2773, 0.2661,\n",
            "        0.2898, 0.2489, 0.3060, 0.2612, 0.2937, 0.3045, 0.2999, 0.2580, 0.2093,\n",
            "        0.2714, 0.2993, 0.2679, 0.2963, 0.2754, 0.2580, 0.2566, 0.2634, 0.2325,\n",
            "        0.2442, 0.2934, 0.2398, 0.2631, 0.2851, 0.2870, 0.2239, 0.2410, 0.2676,\n",
            "        0.2681, 0.2638, 0.2732, 0.2812, 0.2203, 0.2670, 0.2764, 0.2550, 0.3160,\n",
            "        0.2888, 0.2615, 0.2178, 0.2485, 0.2414, 0.2798, 0.2872, 0.2767, 0.2551,\n",
            "        0.2429, 0.2459, 0.3288, 0.3024, 0.2912, 0.2625, 0.3019, 0.2643, 0.2721,\n",
            "        0.2108, 0.2368, 0.2269, 0.1988, 0.2830, 0.2569, 0.2349, 0.2755, 0.2442,\n",
            "        0.2717, 0.2747, 0.2785, 0.2516, 0.2227, 0.2783, 0.2465, 0.2652, 0.2641,\n",
            "        0.2960, 0.2671, 0.2679, 0.2537, 0.2847, 0.2507, 0.2525, 0.2024, 0.2311,\n",
            "        0.2618, 0.2764, 0.3031, 0.2452, 0.2716, 0.2273, 0.2295, 0.2611, 0.2329,\n",
            "        0.2690, 0.2753, 0.2737, 0.2590, 0.2421, 0.2685, 0.3392, 0.3073, 0.1371,\n",
            "        0.3650, 0.2980, 0.2460, 0.2487, 0.2912, 0.2704, 0.2560, 0.2213, 0.2569,\n",
            "        0.2661, 0.2367, 0.2742, 0.2847, 0.3055, 0.2671, 0.2819, 0.2791, 0.2401,\n",
            "        0.2549, 0.2210, 0.3507, 0.2852, 0.2162, 0.2821, 0.2369, 0.2905, 0.2826,\n",
            "        0.2300, 0.2745, 0.2437, 0.2522, 0.2489, 0.2395, 0.2851, 0.2887, 0.2621,\n",
            "        0.2500, 0.2689, 0.2427, 0.3010, 0.3067, 0.2861, 0.2387, 0.2462, 0.2859,\n",
            "        0.2550, 0.2630, 0.2442, 0.2145, 0.2898, 0.2282, 0.2327, 0.2242, 0.2738,\n",
            "        0.2485, 0.2379, 0.3058, 0.2798, 0.2761, 0.2252, 0.2866, 0.2660, 0.3250,\n",
            "        0.2612, 0.2767, 0.3205, 0.2932, 0.3183, 0.2939, 0.3103, 0.2553, 0.2981,\n",
            "        0.3667, 0.3086, 0.2254, 0.2352, 0.2348, 0.2555, 0.2597, 0.2369, 0.3017,\n",
            "        0.2776, 0.2728, 0.3174, 0.2785, 0.2721, 0.2637, 0.2702, 0.3633, 0.2869,\n",
            "        0.2675, 0.3405, 0.2587, 0.2732, 0.2747, 0.2821, 0.2750, 0.2630, 0.2018,\n",
            "        0.2358, 0.3034, 0.3155, 0.3013, 0.2775, 0.2511, 0.2945, 0.1605, 0.2825,\n",
            "        0.2964, 0.2194, 0.2061, 0.2332, 0.2348, 0.2663, 0.2543, 0.2927, 0.2215,\n",
            "        0.2521, 0.2827, 0.1993, 0.2453, 0.2597, 0.2654, 0.2757, 0.2650, 0.2444,\n",
            "        0.2949, 0.2308, 0.3071, 0.1904, 0.3024, 0.2786, 0.3659, 0.2966, 0.2746,\n",
            "        0.2449, 0.2201, 0.2564, 0.2853, 0.2392, 0.2457, 0.2467, 0.2374, 0.2664,\n",
            "        0.2460, 0.3182, 0.1793, 0.2379, 0.2596, 0.2847, 0.2452, 0.1974, 0.2388,\n",
            "        0.2949, 0.2879, 0.2786, 0.2765, 0.3296, 0.2530, 0.2690, 0.2547, 0.2333,\n",
            "        0.2348, 0.2690, 0.2718, 0.2679, 0.2516, 0.2710, 0.2366, 0.2601, 0.2764,\n",
            "        0.2880, 0.2008, 0.2637, 0.2263, 0.2511, 0.2604, 0.2805, 0.2989, 0.2965,\n",
            "        0.2597, 0.2767, 0.2553, 0.2959, 0.2512, 0.2925, 0.3008, 0.2423, 0.2394,\n",
            "        0.2708, 0.3704, 0.2879, 0.2532, 0.2248, 0.2023, 0.2279, 0.2366, 0.3082,\n",
            "        0.2980, 0.2909, 0.2777, 0.4293, 0.2658, 0.2940, 0.2418, 0.2816, 0.3247,\n",
            "        0.2647, 0.2216, 0.2758, 0.2421, 0.2078, 0.2332, 0.2271, 0.2611, 0.3650,\n",
            "        0.2017, 0.2598, 0.2160, 0.2641, 0.1408, 0.2664, 0.2502, 0.2553, 0.2227,\n",
            "        0.2417, 0.2696, 0.2388, 0.2833, 0.2333, 0.2667, 0.2224, 0.2691, 0.2710,\n",
            "        0.2459, 0.2674, 0.2430, 0.2593, 0.1851, 0.2950, 0.3664, 0.2212, 0.3026,\n",
            "        0.1840, 0.3443, 0.2140, 0.3717, 0.2360, 0.3081, 0.2638, 0.2233])\n",
            "Parameter containing:\n",
            "tensor([-0.1986, -0.1593, -0.2054, -0.1598, -0.1268, -0.3226, -0.1597, -0.3477,\n",
            "        -0.2497, -0.2730, -0.2319, -0.0286, -0.1899, -0.2813, -0.1733, -0.2412,\n",
            "        -0.3712, -0.2747, -0.2053, -0.2585, -0.1535, -0.2748, -0.3241, -0.2525,\n",
            "        -0.1906, -0.2252, -0.3436, -0.2202, -0.1664, -0.2716, -0.1920, -0.3399,\n",
            "        -0.2026, -0.2972, -0.2616, -0.2238, -0.2486, -0.2606, -0.0893, -0.3572,\n",
            "        -0.1283, -0.2583, -0.2450, -0.1523, -0.3165, -0.1445, -0.2522, -0.1963,\n",
            "        -0.1794, -0.1071, -0.1662, -0.2053, -0.2530, -0.1447, -0.2517, -0.2062,\n",
            "        -0.2817, -0.3376, -0.1382, -0.2389, -0.2557, -0.0156, -0.2169, -0.1763,\n",
            "        -0.1486, -0.2122, -0.2002, -0.0716, -0.2089, -0.3580, -0.2588, -0.3599,\n",
            "        -0.1528, -0.2107, -0.2925, -0.1855, -0.3970, -0.1257, -0.2574, -0.2412,\n",
            "        -0.0863, -0.3065, -0.2701, -0.3380, -0.2485, -0.1935, -0.2987, -0.2279,\n",
            "        -0.3600, -0.2764, -0.2480, -0.1208, -0.3378, -0.2661, -0.1677, -0.2470,\n",
            "        -0.2152, -0.2591, -0.1936, -0.1543, -0.4117, -0.1570, -0.2372, -0.2997,\n",
            "        -0.2124, -0.2034, -0.1848, -0.3070, -0.3438, -0.1839, -0.1937, -0.0916,\n",
            "        -0.2338, -0.3558, -0.1967, -0.3303, -0.1398, -0.2177, -0.1665, -0.1857,\n",
            "        -0.3115, -0.1049, -0.4229, -0.2408, -0.1320, -0.1631, -0.3378, -0.3300,\n",
            "        -0.3183, -0.2268, -0.2787, -0.1950, -0.1950, -0.1463, -0.2437, -0.2297,\n",
            "        -0.1282, -0.2164, -0.1179, -0.2437, -0.2611, -0.2656, -0.1948, -0.1208,\n",
            "        -0.1668, -0.1351, -0.2713, -0.0560, -0.2243, -0.1318, -0.2356, -0.2720,\n",
            "        -0.2051, -0.1736, -0.2891, -0.2627, -0.3358, -0.1779, -0.2309, -0.1477,\n",
            "        -0.2685, -0.1882, -0.2629, -0.1983, -0.3522, -0.1905, -0.2778, -0.3395,\n",
            "        -0.2895, -0.2240, -0.1150, -0.2462, -0.2426, -0.2581, -0.3133, -0.2315,\n",
            "        -0.2271, -0.2077, -0.2109, -0.1371, -0.1323, -0.2529, -0.1716, -0.2532,\n",
            "        -0.2277, -0.2084, -0.1803, -0.1868, -0.2404, -0.2166, -0.2197, -0.2870,\n",
            "        -0.3062, -0.1507, -0.1054, -0.2199, -0.2415, -0.3310, -0.2700, -0.1568,\n",
            "        -0.1449, -0.2610, -0.1828, -0.2648, -0.3134, -0.2937, -0.2687, -0.2115,\n",
            "        -0.2164, -0.4522, -0.2999, -0.3032, -0.2292, -0.3099, -0.2642, -0.2695,\n",
            "        -0.1441, -0.1671, -0.1570, -0.1415, -0.2222, -0.1736, -0.1481, -0.2573,\n",
            "        -0.2060, -0.1703, -0.2360, -0.1770, -0.2132, -0.2016, -0.3001, -0.1518,\n",
            "        -0.2086, -0.2805, -0.2698, -0.2292, -0.1293, -0.2514, -0.2600, -0.2454,\n",
            "        -0.1744, -0.1029, -0.1679, -0.2353, -0.2007, -0.3363, -0.1640, -0.2430,\n",
            "        -0.1699, -0.1697, -0.1837, -0.1625, -0.2415, -0.2687, -0.2305, -0.2029,\n",
            "        -0.2209, -0.2240, -0.2675, -0.3233,  0.1462, -0.4777, -0.2376, -0.1489,\n",
            "        -0.1462, -0.3055, -0.2234, -0.1697, -0.1952, -0.2131, -0.2340, -0.2039,\n",
            "        -0.3054, -0.2596, -0.3470, -0.2176, -0.2706, -0.2897, -0.1729, -0.2300,\n",
            "        -0.1066, -0.3556, -0.2912, -0.1777, -0.2007, -0.1699, -0.3009, -0.3046,\n",
            "        -0.1693, -0.2602, -0.2053, -0.1810, -0.1808, -0.1730, -0.3757, -0.1808,\n",
            "        -0.1805, -0.1895, -0.2643, -0.2075, -0.2365, -0.1975, -0.3064, -0.1984,\n",
            "        -0.1811, -0.3676, -0.1198, -0.1485, -0.1770, -0.0781, -0.2052, -0.1360,\n",
            "        -0.1417, -0.1691, -0.2395, -0.1785, -0.1747, -0.2484, -0.2717, -0.3096,\n",
            "        -0.1465, -0.2239, -0.2584, -0.3572, -0.2311, -0.2878, -0.3841, -0.3475,\n",
            "        -0.3896, -0.1891, -0.2861, -0.2431, -0.2837, -0.4365, -0.3353, -0.1802,\n",
            "        -0.1976, -0.1529, -0.1978, -0.2535, -0.1954, -0.2667, -0.2813, -0.2487,\n",
            "        -0.3070, -0.2339, -0.2212, -0.1925, -0.2224, -0.4178, -0.3151, -0.2663,\n",
            "        -0.3581, -0.1935, -0.2385, -0.2424, -0.1850, -0.2265, -0.1803, -0.0777,\n",
            "        -0.1492, -0.3361, -0.4133, -0.3123, -0.2745, -0.1247, -0.3102,  0.0041,\n",
            "        -0.1981, -0.3301, -0.2047, -0.1053, -0.1653, -0.1634, -0.1116, -0.2314,\n",
            "        -0.3191, -0.1818, -0.2657, -0.2220, -0.1029, -0.1999, -0.2702, -0.2139,\n",
            "        -0.2256, -0.2653, -0.1630, -0.3322, -0.1617, -0.3446,  0.0288, -0.2456,\n",
            "        -0.3171, -0.3580, -0.2857, -0.2520, -0.2031, -0.1522, -0.2203, -0.3490,\n",
            "        -0.1685, -0.1424, -0.1602, -0.1553, -0.3057, -0.2420, -0.3536, -0.0551,\n",
            "        -0.0987, -0.2272, -0.2619, -0.2035, -0.0906, -0.1976, -0.3040, -0.2732,\n",
            "        -0.3161, -0.2102, -0.3384, -0.1740, -0.1475, -0.1842, -0.1823, -0.1151,\n",
            "        -0.2183, -0.2010, -0.2659, -0.2205, -0.2567, -0.1633, -0.2213, -0.2658,\n",
            "        -0.2938, -0.1069, -0.2522, -0.1103, -0.2216, -0.2244, -0.2908, -0.2176,\n",
            "        -0.3605, -0.2374, -0.2391, -0.2251, -0.2256, -0.1339, -0.1970, -0.2970,\n",
            "        -0.2206, -0.2051, -0.2229, -0.3602, -0.2923, -0.2498, -0.1466, -0.0979,\n",
            "        -0.1686, -0.2158, -0.2881, -0.3002, -0.2760, -0.2496, -0.3536, -0.2868,\n",
            "        -0.3251, -0.1847, -0.3062, -0.3861, -0.2650, -0.1339, -0.1846, -0.1630,\n",
            "        -0.0630, -0.1717, -0.1415, -0.1906, -0.4611, -0.1391, -0.1920, -0.1369,\n",
            "        -0.1647, -0.0055, -0.2598, -0.2653, -0.2319, -0.1780, -0.1913, -0.2055,\n",
            "        -0.1891, -0.2625, -0.1633, -0.2497, -0.1696, -0.1907, -0.2431, -0.1825,\n",
            "        -0.2607, -0.1943, -0.2361, -0.0581, -0.2758, -0.2593, -0.1466, -0.3589,\n",
            "        -0.0439, -0.3440, -0.1089, -0.4219, -0.1503, -0.2792, -0.3035, -0.1156])\n",
            "Parameter containing:\n",
            "tensor([[[[ 1.6218e-04, -1.4720e-02, -1.7000e-02],\n",
            "          [-1.2850e-02, -3.3085e-02, -3.6656e-02],\n",
            "          [ 2.7812e-02,  1.7691e-02, -1.8369e-02]],\n",
            "\n",
            "         [[ 1.0528e-02,  3.1379e-02,  2.4801e-02],\n",
            "          [-1.2698e-02, -2.9453e-02, -1.1834e-02],\n",
            "          [-9.4094e-03, -8.9462e-03, -3.1349e-02]],\n",
            "\n",
            "         [[-7.8447e-03, -2.9256e-02,  5.3590e-03],\n",
            "          [-1.3791e-02, -1.1116e-02,  5.0388e-03],\n",
            "          [-2.4919e-03,  7.3514e-03,  5.4013e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.0276e-03, -1.0275e-02, -2.9986e-02],\n",
            "          [-3.8465e-03,  1.9549e-03, -1.6291e-02],\n",
            "          [-1.8100e-03,  8.3778e-03, -8.5481e-03]],\n",
            "\n",
            "         [[-1.8196e-02, -1.3533e-02, -1.7457e-02],\n",
            "          [ 2.2457e-02,  5.7402e-02,  1.9325e-02],\n",
            "          [-2.4977e-02, -3.2113e-02, -8.1780e-03]],\n",
            "\n",
            "         [[ 3.6550e-03,  4.9358e-03, -5.7597e-03],\n",
            "          [-1.6875e-02,  1.3999e-04,  3.7629e-04],\n",
            "          [-2.6272e-03,  1.0947e-03,  1.1145e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 1.4018e-02,  3.9198e-03, -1.7189e-03],\n",
            "          [-1.3175e-03,  4.3503e-04, -1.1798e-02],\n",
            "          [-9.8003e-03, -1.7693e-02, -1.9910e-02]],\n",
            "\n",
            "         [[-1.4957e-02, -1.9796e-02, -2.8724e-02],\n",
            "          [ 5.8908e-03, -1.5228e-02, -5.6715e-03],\n",
            "          [ 2.9284e-03, -1.8028e-02, -7.1433e-03]],\n",
            "\n",
            "         [[-1.1625e-02, -3.3804e-02, -1.0025e-02],\n",
            "          [-1.6606e-02, -5.5716e-02, -2.3204e-02],\n",
            "          [-2.5758e-02, -4.3135e-02, -2.5901e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.5007e-02, -1.4333e-02, -2.5937e-03],\n",
            "          [-2.3078e-02, -1.5820e-02, -2.2818e-03],\n",
            "          [-4.1318e-03, -8.0353e-03, -2.3236e-03]],\n",
            "\n",
            "         [[-1.8531e-02, -1.8004e-02, -2.8084e-02],\n",
            "          [-3.6680e-02, -6.8641e-02, -5.2469e-02],\n",
            "          [-1.1712e-02, -2.4334e-02, -1.6733e-02]],\n",
            "\n",
            "         [[-2.2078e-02, -2.9163e-02, -3.8717e-03],\n",
            "          [-7.0301e-03,  1.6718e-02,  5.4339e-03],\n",
            "          [-1.3131e-02,  1.1999e-02, -1.7480e-02]]],\n",
            "\n",
            "\n",
            "        [[[-5.2378e-03, -3.4890e-03, -2.0851e-03],\n",
            "          [ 1.5306e-02, -2.1752e-02, -8.7682e-03],\n",
            "          [ 2.2460e-02,  9.9175e-03, -3.3635e-03]],\n",
            "\n",
            "         [[ 7.4677e-03, -9.1762e-03, -9.2569e-05],\n",
            "          [ 1.9441e-04,  1.2344e-03, -8.9978e-03],\n",
            "          [-5.1243e-04,  2.1850e-04, -4.8828e-03]],\n",
            "\n",
            "         [[ 1.7078e-02,  3.3955e-03,  9.3503e-03],\n",
            "          [ 2.0334e-02, -1.0621e-04, -8.2017e-05],\n",
            "          [ 1.0706e-02, -1.8414e-03,  1.0828e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.2008e-02,  2.3494e-02,  2.5386e-02],\n",
            "          [ 1.9307e-02,  2.3924e-02,  2.8972e-02],\n",
            "          [ 9.9003e-03,  2.0158e-02,  2.2655e-02]],\n",
            "\n",
            "         [[-9.8395e-03, -1.1114e-02, -3.7696e-03],\n",
            "          [-2.9508e-02, -3.6956e-02, -1.8228e-02],\n",
            "          [-1.3663e-03, -2.5845e-03,  1.0352e-02]],\n",
            "\n",
            "         [[-7.3867e-03, -2.5413e-02, -2.1942e-02],\n",
            "          [-1.6699e-02, -1.5133e-02, -1.3030e-02],\n",
            "          [-2.0090e-02,  3.7970e-03, -1.0341e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1.6157e-02, -1.6883e-02, -2.8328e-04],\n",
            "          [-7.7759e-03, -2.4465e-03, -1.4641e-02],\n",
            "          [ 2.4639e-02,  3.9862e-02,  2.1048e-02]],\n",
            "\n",
            "         [[ 2.4491e-03, -9.3885e-03, -1.1786e-02],\n",
            "          [ 2.5301e-02,  2.5625e-04,  7.1335e-03],\n",
            "          [ 2.2342e-02,  1.9042e-02,  7.2526e-03]],\n",
            "\n",
            "         [[-1.4652e-02, -2.7802e-02, -4.3564e-03],\n",
            "          [-1.7961e-02, -4.3846e-02,  2.7409e-03],\n",
            "          [-4.7968e-03, -8.4231e-03,  1.2070e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.0171e-02, -3.3546e-02, -1.6728e-02],\n",
            "          [-1.7847e-02, -5.1713e-02, -2.6780e-02],\n",
            "          [-1.3145e-03, -4.3181e-03, -9.6373e-03]],\n",
            "\n",
            "         [[-5.3917e-03, -2.0410e-04,  2.7798e-03],\n",
            "          [-9.6882e-04, -2.5141e-02,  1.4804e-02],\n",
            "          [ 2.8748e-02,  9.0832e-03,  4.2548e-02]],\n",
            "\n",
            "         [[-1.5698e-02, -1.9303e-02, -9.1469e-03],\n",
            "          [-2.0025e-02, -1.1131e-02, -3.3902e-02],\n",
            "          [-5.7436e-03, -7.3640e-03, -1.0044e-02]]],\n",
            "\n",
            "\n",
            "        [[[-8.8612e-03, -4.5370e-03, -1.2354e-02],\n",
            "          [-5.9245e-03, -1.7058e-02, -2.8041e-02],\n",
            "          [-1.0435e-02,  7.6695e-04, -1.0578e-02]],\n",
            "\n",
            "         [[ 9.5200e-03, -5.1975e-03,  1.2947e-02],\n",
            "          [ 4.4305e-03, -2.3992e-02, -8.4569e-04],\n",
            "          [ 4.6608e-03,  9.6787e-03,  8.2174e-03]],\n",
            "\n",
            "         [[ 5.1559e-03,  4.4635e-04, -7.9934e-03],\n",
            "          [ 3.3069e-03,  1.4450e-02,  8.9234e-03],\n",
            "          [ 6.3402e-03,  1.9043e-02,  1.9021e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.6964e-03, -1.3777e-02,  6.0539e-03],\n",
            "          [-1.5745e-03, -2.3391e-02, -1.0052e-02],\n",
            "          [ 9.5183e-03, -1.2251e-02,  2.2436e-03]],\n",
            "\n",
            "         [[ 1.0375e-02,  3.5875e-03, -5.7940e-04],\n",
            "          [ 7.0412e-03, -1.0673e-02, -4.9120e-03],\n",
            "          [-2.6034e-03,  1.1306e-02,  7.0696e-03]],\n",
            "\n",
            "         [[-1.7509e-02, -2.3182e-02, -1.7897e-02],\n",
            "          [-1.7769e-03,  1.9672e-03, -7.3220e-03],\n",
            "          [-6.6833e-03,  9.8286e-03,  2.0653e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 2.8375e-02, -8.1936e-03,  1.8009e-02],\n",
            "          [ 1.5829e-02, -1.3571e-02, -1.9335e-02],\n",
            "          [ 4.0766e-03, -1.5722e-02, -5.0620e-02]],\n",
            "\n",
            "         [[-5.5310e-03, -1.8996e-02, -7.9436e-03],\n",
            "          [ 1.3825e-03, -4.9608e-02,  1.7256e-03],\n",
            "          [ 7.6629e-03, -7.6101e-03,  1.2541e-02]],\n",
            "\n",
            "         [[ 1.8052e-02,  3.1718e-02,  4.2556e-03],\n",
            "          [-3.6760e-03,  3.0490e-03, -1.2264e-02],\n",
            "          [-8.9404e-03, -1.6604e-02,  1.6348e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.3192e-03,  1.8204e-02,  1.8114e-02],\n",
            "          [-6.1202e-03,  1.5905e-03,  2.0264e-02],\n",
            "          [-1.1471e-02, -1.5697e-02,  9.0871e-03]],\n",
            "\n",
            "         [[ 3.7707e-03,  8.0599e-03,  1.8290e-02],\n",
            "          [ 1.7257e-02,  6.9638e-03,  1.8746e-02],\n",
            "          [ 1.0751e-02,  1.3663e-02, -1.0081e-03]],\n",
            "\n",
            "         [[ 1.9711e-02, -1.4569e-02, -2.4663e-02],\n",
            "          [ 2.5966e-03, -2.4807e-02,  9.3861e-03],\n",
            "          [-1.2876e-03,  1.3974e-03,  1.3434e-02]]]])\n",
            "Parameter containing:\n",
            "tensor([0.4474, 0.5138, 0.4335, 0.3421, 0.3855, 0.3495, 0.3741, 0.5836, 0.4327,\n",
            "        0.5043, 0.4618, 0.3866, 0.3498, 0.4798, 0.3310, 0.3913, 0.3880, 0.5225,\n",
            "        0.3975, 0.3292, 0.4151, 0.4458, 0.3970, 0.3614, 0.3914, 0.4633, 0.3463,\n",
            "        0.3644, 0.3272, 0.4584, 0.4280, 0.4538, 0.4030, 0.4673, 0.4209, 0.3987,\n",
            "        0.4233, 0.3876, 0.4212, 0.3460, 0.3522, 0.3744, 0.4550, 0.2888, 0.4590,\n",
            "        0.4817, 0.4450, 0.5110, 0.4052, 0.4247, 0.3558, 0.3075, 0.4462, 0.4724,\n",
            "        0.4253, 0.3884, 0.4492, 0.3727, 0.4630, 0.3985, 0.3512, 0.3665, 0.3860,\n",
            "        0.5082, 0.4022, 0.3458, 0.4805, 0.5390, 0.4223, 0.4275, 0.4590, 0.4736,\n",
            "        0.3673, 0.5405, 0.3243, 0.5178, 0.4743, 0.3506, 0.3759, 0.4328, 0.3867,\n",
            "        0.4591, 0.3843, 0.4982, 0.5288, 0.3946, 0.4589, 0.3197, 0.4676, 0.4806,\n",
            "        0.4308, 0.4235, 0.3284, 0.3877, 0.4140, 0.4469, 0.4041, 0.4407, 0.4356,\n",
            "        0.5120, 0.5059, 0.4628, 0.4585, 0.3311, 0.3424, 0.4150, 0.5170, 0.4593,\n",
            "        0.5228, 0.4252, 0.4214, 0.4995, 0.4098, 0.5380, 0.4874, 0.3719, 0.4649,\n",
            "        0.4320, 0.3277, 0.3743, 0.4360, 0.4838, 0.4399, 0.3763, 0.4150, 0.5147,\n",
            "        0.5012, 0.4382, 0.3655, 0.4037, 0.4498, 0.4720, 0.3914, 0.3237, 0.3208,\n",
            "        0.3224, 0.4291, 0.4009, 0.3947, 0.3779, 0.4349, 0.4120, 0.3274, 0.4334,\n",
            "        0.3740, 0.4189, 0.4288, 0.3071, 0.4260, 0.3410, 0.4375, 0.4407, 0.3750,\n",
            "        0.5853, 0.4518, 0.5045, 0.3005, 0.4968, 0.4155, 0.3755, 0.5514, 0.4146,\n",
            "        0.4677, 0.1404, 0.5001, 0.4193, 0.4246, 0.4452, 0.5109, 0.4488, 0.4574,\n",
            "        0.3896, 0.4145, 0.4497, 0.4245, 0.3971, 0.3957, 0.4072, 0.5305, 0.4986,\n",
            "        0.3733, 0.4280, 0.3469, 0.4178, 0.3766, 0.4029, 0.3814, 0.4493, 0.5132,\n",
            "        0.4080, 0.4155, 0.3635, 0.4391, 0.3489, 0.4228, 0.4833, 0.3494, 0.4406,\n",
            "        0.3795, 0.4298, 0.4910, 0.3878, 0.6299, 0.4322, 0.5436, 0.4140, 0.4312,\n",
            "        0.3161, 0.3612, 0.3597, 0.4281, 0.4506, 0.4294, 0.3646, 0.4110, 0.4038,\n",
            "        0.4098, 0.3901, 0.3928, 0.5421, 0.3629, 0.4078, 0.4586, 0.4217, 0.3953,\n",
            "        0.3997, 0.3838, 0.4374, 0.3576, 0.4217, 0.4128, 0.3904, 0.4137, 0.5145,\n",
            "        0.4039, 0.3577, 0.4429, 0.5639, 0.3848, 0.6104, 0.4482, 0.6203, 0.5336,\n",
            "        0.3480, 0.5401, 0.6044, 0.4077, 0.3469, 0.4281, 0.4631, 0.5948, 0.3479,\n",
            "        0.3689, 0.3658, 0.3191, 0.5492, 0.3410, 0.5386, 0.4041, 0.3373, 0.4186,\n",
            "        0.5187, 0.3933, 0.3188, 0.3502, 0.3736, 0.4238, 0.4752, 0.3322, 0.5078,\n",
            "        0.4317, 0.5318, 0.4413, 0.5510, 0.5648, 0.4130, 0.4017, 0.4304, 0.4077,\n",
            "        0.4285, 0.4360, 0.3749, 0.4261, 0.3905, 0.3030, 0.3412, 0.3768, 0.4507,\n",
            "        0.3127, 0.4592, 0.4298, 0.3936, 0.3106, 0.3869, 0.3594, 0.4046, 0.4722,\n",
            "        0.4373, 0.3902, 0.3515, 0.4448, 0.4299, 0.4347, 0.4693, 0.4807, 0.2549,\n",
            "        0.4171, 0.4387, 0.4156, 0.3976, 0.4092, 0.4953, 0.4824, 0.3468, 0.4382,\n",
            "        0.4179, 0.4668, 0.3299, 0.5986, 0.4949, 0.4167, 0.4996, 0.4528, 0.4550,\n",
            "        0.4945, 0.3415, 0.4658, 0.4356, 0.3976, 0.5439, 0.4643, 0.5122, 0.4669,\n",
            "        0.4463, 0.4810, 0.3492, 0.3961, 0.3593, 0.4053, 0.3878, 0.3959, 0.5001,\n",
            "        0.2808, 0.5470, 0.4448, 0.4894, 0.4621, 0.3417, 0.3485, 0.5060, 0.3637,\n",
            "        0.3774, 0.3248, 0.4520, 0.3936, 0.3403, 0.4660, 0.4114, 0.3643, 0.4196,\n",
            "        0.3903, 0.5128, 0.4221, 0.4115, 0.4240, 0.3610, 0.4999, 0.3672, 0.4721,\n",
            "        0.4252, 0.5590, 0.4694, 0.7322, 0.5849, 0.4749, 0.4426, 0.3934, 0.3909,\n",
            "        0.4576, 0.3636, 0.4146, 0.4129, 0.5081, 0.3681, 0.3652, 0.4254, 0.2945,\n",
            "        0.4142, 0.3145, 0.4304, 0.4252, 0.3493, 0.4257, 0.5133, 0.3261, 0.4367,\n",
            "        0.3637, 0.3712, 0.4183, 0.3772, 0.4418, 0.4231, 0.4133, 0.4731, 0.4955,\n",
            "        0.4046, 0.4079, 0.4719, 0.3875, 0.4673, 0.4129, 0.4569, 0.3530, 0.4793,\n",
            "        0.3844, 0.3785, 0.3343, 0.4351, 0.6512, 0.4295, 0.4122, 0.3788, 0.3692,\n",
            "        0.4343, 0.4214, 0.3873, 0.4566, 0.4456, 0.4107, 0.4596, 0.7082, 0.4452,\n",
            "        0.3515, 0.4785, 0.4217, 0.5756, 0.4312, 0.4047, 0.4043, 0.4764, 0.5489,\n",
            "        0.4430, 0.5559, 0.3744, 0.3951, 0.4376, 0.4752, 0.4340, 0.4399, 0.3586,\n",
            "        0.4161, 0.3930, 0.4599, 0.4354, 0.3448, 0.4649, 0.4442, 0.4275, 0.3881,\n",
            "        0.3247, 0.4909, 0.3426, 0.3989, 0.4320, 0.3363, 0.3991, 0.4732, 0.3514,\n",
            "        0.4736, 0.4244, 0.4603, 0.3298, 0.4357, 0.4353, 0.3742, 0.4191, 0.3880,\n",
            "        0.4212, 0.4527, 0.7213, 0.3969, 0.5217, 0.3786, 0.3512, 0.5318, 0.4138,\n",
            "        0.3243, 0.3244, 0.3652, 0.4774, 0.3997, 0.2800, 0.4562, 0.4463, 0.4816,\n",
            "        0.4290, 0.4399, 0.4633, 0.3575, 0.4774, 0.3105, 0.4356, 0.3797, 0.4304,\n",
            "        0.4261, 0.3740, 0.3370, 0.3917, 0.3637, 0.4347, 0.5235, 0.3845])\n",
            "Parameter containing:\n",
            "tensor([-0.1759, -0.2156, -0.2047, -0.1695, -0.1628, -0.1473, -0.2158, -0.2905,\n",
            "        -0.1112, -0.2196, -0.1020, -0.1549, -0.1989, -0.0445, -0.1508, -0.1920,\n",
            "        -0.2114, -0.1655, -0.1854, -0.1733, -0.1289, -0.2376, -0.1965, -0.1965,\n",
            "        -0.1776, -0.1774, -0.1760, -0.1546, -0.1648, -0.2599, -0.1752, -0.2498,\n",
            "        -0.1741, -0.2410, -0.2498, -0.2938, -0.1496, -0.1578, -0.1800, -0.1851,\n",
            "        -0.1516, -0.1345, -0.2746, -0.1248, -0.2246, -0.2531, -0.2398, -0.1859,\n",
            "        -0.1739, -0.2393, -0.1214, -0.1803, -0.2729, -0.2617, -0.1855, -0.2316,\n",
            "        -0.2333, -0.1860, -0.2097, -0.0692, -0.1912, -0.2078, -0.1084, -0.2810,\n",
            "        -0.1303, -0.1654, -0.2119, -0.3641, -0.2951, -0.2384, -0.1632, -0.1892,\n",
            "        -0.1792, -0.2031, -0.1770, -0.2738, -0.3324, -0.1725, -0.1793, -0.2638,\n",
            "        -0.2207, -0.1609, -0.1534, -0.1414, -0.2992, -0.1450, -0.1838, -0.1779,\n",
            "        -0.1422, -0.2198, -0.1900, -0.1580, -0.1666, -0.2490, -0.1569, -0.1718,\n",
            "        -0.1660, -0.1972, -0.2287, -0.2366, -0.2230, -0.1543, -0.2030, -0.1431,\n",
            "        -0.1363, -0.2015, -0.1804, -0.2093, -0.2964, -0.1984, -0.2683, -0.2216,\n",
            "        -0.2147, -0.3404, -0.2668, -0.1890, -0.1733, -0.2226, -0.1772, -0.1698,\n",
            "        -0.1095, -0.2180, -0.1154, -0.1654, -0.1910, -0.3535, -0.3112, -0.2161,\n",
            "        -0.1496, -0.1667, -0.2849, -0.2207, -0.1529, -0.1807, -0.2118, -0.1869,\n",
            "        -0.1376, -0.1770, -0.1861, -0.1969, -0.1741, -0.3011, -0.0787, -0.2017,\n",
            "        -0.1947, -0.2247, -0.2459, -0.1058, -0.1401, -0.1213, -0.1199, -0.1760,\n",
            "        -0.2156, -0.3307, -0.3515, -0.2366, -0.1185, -0.2155, -0.1751, -0.1892,\n",
            "        -0.3365, -0.1598, -0.2554,  0.0644, -0.2856, -0.1198, -0.1583, -0.2297,\n",
            "        -0.3352, -0.1987, -0.2686, -0.1632, -0.2461, -0.2900, -0.2428, -0.1449,\n",
            "        -0.1900, -0.2149, -0.1541, -0.2917, -0.2504, -0.2213, -0.0463, -0.1547,\n",
            "        -0.1511, -0.1527, -0.1735, -0.1931, -0.1987, -0.2239, -0.2086, -0.2688,\n",
            "        -0.1845, -0.1797, -0.1833, -0.3880, -0.1539, -0.1553, -0.1567, -0.2238,\n",
            "        -0.1511, -0.2540, -0.2849, -0.1826, -0.2687, -0.2328, -0.2108, -0.2410,\n",
            "        -0.1022, -0.1507, -0.1978, -0.1734, -0.2282, -0.0985, -0.1847, -0.1770,\n",
            "        -0.1576, -0.1937, -0.1643, -0.2822, -0.1866, -0.2754, -0.2266, -0.2169,\n",
            "        -0.1352, -0.2194, -0.1060, -0.2139, -0.1322, -0.1889, -0.2130, -0.1913,\n",
            "        -0.2364, -0.1402, -0.2228, -0.2354, -0.1632, -0.1905, -0.1428, -0.1177,\n",
            "        -0.2419, -0.2733, -0.2963, -0.1600, -0.3558, -0.3673, -0.2201, -0.1505,\n",
            "        -0.2084, -0.0870, -0.2052, -0.2070, -0.1986, -0.2299, -0.0745, -0.1765,\n",
            "        -0.1412, -0.2180, -0.1450, -0.1426, -0.1452, -0.2916, -0.0871, -0.1359,\n",
            "        -0.2003, -0.1125, -0.2588, -0.1988, -0.2028, -0.2443, -0.0864, -0.3415,\n",
            "        -0.2579, -0.2343, -0.3552, -0.1859, -0.1153, -0.1732, -0.1780, -0.1909,\n",
            "        -0.2018, -0.1886, -0.2751, -0.1501,  0.1165, -0.1891, -0.1845, -0.2037,\n",
            "        -0.0339, -0.3464, -0.1956, -0.1962, -0.1537, -0.1902, -0.1431, -0.3022,\n",
            "        -0.1780, -0.1971, -0.2118, -0.0952, -0.1711, -0.2409, -0.2184, -0.2114,\n",
            "        -0.2042, -0.0566, -0.0700, -0.2081, -0.1872, -0.2079, -0.1540, -0.2266,\n",
            "        -0.1981, -0.1679, -0.2022, -0.2010, -0.1051, -0.1705, -0.2139,  0.0396,\n",
            "        -0.1077, -0.2745, -0.2690, -0.2603, -0.2819, -0.1917, -0.1940, -0.2944,\n",
            "        -0.1822, -0.2903, -0.1064, -0.2076, -0.2648, -0.3032, -0.2878, -0.1579,\n",
            "        -0.0071, -0.2142, -0.2022, -0.1516, -0.1123,  0.0246, -0.0978, -0.1382,\n",
            "        -0.1800, -0.3214, -0.2179, -0.1369, -0.0800,  0.0117, -0.1839, -0.1926,\n",
            "        -0.1614, -0.2769, -0.1909, -0.2101, -0.2305, -0.2055, -0.2017, -0.2741,\n",
            "        -0.1005, -0.3152, -0.1121, -0.1700, -0.1364, -0.2157, -0.2673, -0.1584,\n",
            "        -0.1997, -0.1745, -0.1886, -0.2307, -0.2024, -0.3376, -0.2266, -0.2355,\n",
            "        -0.2133, -0.2346, -0.2412, -0.2358, -0.1265, -0.2341, -0.1887, -0.1646,\n",
            "        -0.1417, -0.1882, -0.1076, -0.3048, -0.1162, -0.1651, -0.2046, -0.1833,\n",
            "        -0.3102, -0.1778, -0.1575, -0.2676, -0.1777, -0.1569, -0.1741, -0.1892,\n",
            "        -0.3028, -0.1457, -0.2179, -0.2226, -0.1609, -0.1423, -0.2683, -0.2920,\n",
            "        -0.1740, -0.2079, -0.1940, -0.2679, -0.1973, -0.1951, -0.1665, -0.2286,\n",
            "        -0.1903, -0.2667, -0.4010, -0.2550, -0.1817, -0.2025, -0.1589, -0.2476,\n",
            "        -0.0573, -0.2203, -0.2084, -0.1587, -0.1212, -0.1795, -0.3449, -0.1662,\n",
            "        -0.2523, -0.2435, -0.2878, -0.2797, -0.1897, -0.2113, -0.1943, -0.2050,\n",
            "        -0.1694, -0.2243, -0.2987, -0.1328, -0.1428, -0.2399, -0.1593, -0.1999,\n",
            "        -0.3225, -0.1860, -0.1763, -0.2691, -0.2097, -0.2396, -0.1140, -0.1897,\n",
            "        -0.1870, -0.1829, -0.2615, -0.2073, -0.1858, -0.0598, -0.1915, -0.2183,\n",
            "        -0.2088, -0.1742, -0.2715, -0.1999, -0.2117, -0.2492, -0.1717, -0.1566,\n",
            "        -0.1669, -0.3015, -0.1685, -0.2434, -0.2297, -0.1947, -0.2860, -0.3288,\n",
            "        -0.2197, -0.1862, -0.1755, -0.0987, -0.1756, -0.1304, -0.1555, -0.1679,\n",
            "        -0.2222, -0.2819, -0.2652, -0.0947, -0.2412, -0.2731, -0.2572, -0.2604,\n",
            "        -0.2934, -0.2470, -0.1820, -0.2740, -0.1336, -0.1698, -0.1919, -0.1796,\n",
            "        -0.2325, -0.1352, -0.1077, -0.2184, -0.1539, -0.2015, -0.3243, -0.1713])\n",
            "Parameter containing:\n",
            "tensor([[[[ 0.0057]],\n",
            "\n",
            "         [[ 0.0020]],\n",
            "\n",
            "         [[ 0.0167]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0085]],\n",
            "\n",
            "         [[-0.0274]],\n",
            "\n",
            "         [[ 0.0097]]],\n",
            "\n",
            "\n",
            "        [[[-0.0271]],\n",
            "\n",
            "         [[-0.0157]],\n",
            "\n",
            "         [[ 0.0543]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0206]],\n",
            "\n",
            "         [[-0.0308]],\n",
            "\n",
            "         [[ 0.0013]]],\n",
            "\n",
            "\n",
            "        [[[-0.0523]],\n",
            "\n",
            "         [[-0.0353]],\n",
            "\n",
            "         [[ 0.0394]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0382]],\n",
            "\n",
            "         [[-0.0264]],\n",
            "\n",
            "         [[-0.0443]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0494]],\n",
            "\n",
            "         [[ 0.0436]],\n",
            "\n",
            "         [[ 0.0103]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0072]],\n",
            "\n",
            "         [[ 0.0014]],\n",
            "\n",
            "         [[-0.0669]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0533]],\n",
            "\n",
            "         [[-0.0148]],\n",
            "\n",
            "         [[-0.0480]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0055]],\n",
            "\n",
            "         [[ 0.0429]],\n",
            "\n",
            "         [[ 0.0129]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0300]],\n",
            "\n",
            "         [[-0.0092]],\n",
            "\n",
            "         [[ 0.0090]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0101]],\n",
            "\n",
            "         [[-0.0111]],\n",
            "\n",
            "         [[-0.0080]]]])\n",
            "Parameter containing:\n",
            "tensor([ 0.1694,  0.3368,  0.2993,  0.3745,  0.1513,  0.1781,  0.3167,  0.3947,\n",
            "         0.1858,  0.2068,  0.1090,  0.2042,  0.2955,  0.0765,  0.2023,  0.2487,\n",
            "         0.3295,  0.3349,  0.2532,  0.2739,  0.1661,  0.3432,  0.3424,  0.2969,\n",
            "         0.2226,  0.0993,  0.3328,  0.2349,  0.2894,  0.2296,  0.2719,  0.3945,\n",
            "         0.1990,  0.2564,  0.2557,  0.3541,  0.1848,  0.2513,  0.3101,  0.2782,\n",
            "         0.2109,  0.2441,  0.3282,  0.3248,  0.2499,  0.1873,  0.2643,  0.3949,\n",
            "         0.1962,  0.2587,  0.1708,  0.3381,  0.2238,  0.2498,  0.2787,  0.3783,\n",
            "         0.3445,  0.2681,  0.2956,  0.1146,  0.2688,  0.3479,  0.1295,  0.2843,\n",
            "         0.1552,  0.3026,  0.2738,  0.1891,  0.3568,  0.2302,  0.2199,  0.2070,\n",
            "         0.2119,  0.0971,  0.2482,  0.2264,  0.3555,  0.3113,  0.2386,  0.2654,\n",
            "         0.2975,  0.2666,  0.2180,  0.1451,  0.2460,  0.1734,  0.2358,  0.2891,\n",
            "         0.2091,  0.1971,  0.2185,  0.2008,  0.2461,  0.3726,  0.2028,  0.1993,\n",
            "         0.3652,  0.2258,  0.2606,  0.1900,  0.2764,  0.2011,  0.1973,  0.2958,\n",
            "         0.3222,  0.4117,  0.1475,  0.2674,  0.1928,  0.3615,  0.2774,  0.2143,\n",
            "         0.2688,  0.4286,  0.2560,  0.2777,  0.1339,  0.5103,  0.3238,  0.2417,\n",
            "         0.1529,  0.1843,  0.0579,  0.2288,  0.1797,  0.2803,  0.2279,  0.1579,\n",
            "         0.3196,  0.1842,  0.3378,  0.1688,  0.1654,  0.3049,  0.3533,  0.2948,\n",
            "         0.1140,  0.2503,  0.1892,  0.2647,  0.2405,  0.3880,  0.1933,  0.1918,\n",
            "         0.2511,  0.2901,  0.3151,  0.3252,  0.1296,  0.2491,  0.1417,  0.1295,\n",
            "         0.3062,  0.2836,  0.3483,  0.2306,  0.2741,  0.2700,  0.1873,  0.2431,\n",
            "         0.3526,  0.3546,  0.2721,  0.2708,  0.3065,  0.0832,  0.2968,  0.2286,\n",
            "         0.3276,  0.2695,  0.2452,  0.2444,  0.2857,  0.3365,  0.2784,  0.2933,\n",
            "         0.3397,  0.2231,  0.2330,  0.1486,  0.3846,  0.3104,  0.1724,  0.1724,\n",
            "         0.3466,  0.2978,  0.2582,  0.1879,  0.2419,  0.2249,  0.2720,  0.3735,\n",
            "         0.4259,  0.3754,  0.1731,  0.3698,  0.2349,  0.2694,  0.3148,  0.1658,\n",
            "         0.1181,  0.2994,  0.4018,  0.2126,  0.3864,  0.2955,  0.1848,  0.3686,\n",
            "         0.1972,  0.3265,  0.2319,  0.1676,  0.1756,  0.2367,  0.2139,  0.1974,\n",
            "         0.2561,  0.2619,  0.2170,  0.2284,  0.3486,  0.4500,  0.2563,  0.2559,\n",
            "         0.2814,  0.1797,  0.1736,  0.2013,  0.3411,  0.2245,  0.1385,  0.2284,\n",
            "         0.2230,  0.2566,  0.2301,  0.3639,  0.1380,  0.2381,  0.2590,  0.0830,\n",
            "         0.1863,  0.1267,  0.4501,  0.2741,  0.2590,  0.2782,  0.2248,  0.2718,\n",
            "         0.1949,  0.1815,  0.2969,  0.3168,  0.3389,  0.2790,  0.1594,  0.2752,\n",
            "         0.2947,  0.2909,  0.1418,  0.3336,  0.1953,  0.2646,  0.0879,  0.2553,\n",
            "         0.3335,  0.1943,  0.2777,  0.2386,  0.3676,  0.3042,  0.1234,  0.2615,\n",
            "         0.2548,  0.3224,  0.3462,  0.2090,  0.2142,  0.2054,  0.2115,  0.2153,\n",
            "         0.2163,  0.2509,  0.2429,  0.3326, -0.0527,  0.2244,  0.2319,  0.2674,\n",
            "         0.1103,  0.2320,  0.2822,  0.3234,  0.2818,  0.2093,  0.2261,  0.2900,\n",
            "         0.3127,  0.3456,  0.2592,  0.1677,  0.3924,  0.2694,  0.1997,  0.2973,\n",
            "         0.3324,  0.2270,  0.0656,  0.2964,  0.1948,  0.2383,  0.3021,  0.2510,\n",
            "         0.3117,  0.3185,  0.1721,  0.1867,  0.1665,  0.2851,  0.3512, -0.0486,\n",
            "         0.1558,  0.2213,  0.3281,  0.3861,  0.2375,  0.3057,  0.1178,  0.2681,\n",
            "         0.1921,  0.2211,  0.1679,  0.2877,  0.2495,  0.2451,  0.2678,  0.2393,\n",
            "         0.0988,  0.2778,  0.2465,  0.1747,  0.1005,  0.0502,  0.2809,  0.2810,\n",
            "         0.1716,  0.2114,  0.2213,  0.2817,  0.1506,  0.0769,  0.2381,  0.2411,\n",
            "         0.2942,  0.2543,  0.2556,  0.3451,  0.2948,  0.3040,  0.3204,  0.2757,\n",
            "         0.1657,  0.2941,  0.1301,  0.1854,  0.2866,  0.3198,  0.2127,  0.3608,\n",
            "         0.3440,  0.0954,  0.2586,  0.1709,  0.2007,  0.1967,  0.1972,  0.1942,\n",
            "         0.3201,  0.3484,  0.3437,  0.3153,  0.2020,  0.3251,  0.3227,  0.3038,\n",
            "         0.2634,  0.2364,  0.2492,  0.3080,  0.2591,  0.2391,  0.2720,  0.2601,\n",
            "         0.3210,  0.1818,  0.3526,  0.3579,  0.2861,  0.2526,  0.1642,  0.2897,\n",
            "         0.3996,  0.2651,  0.2031,  0.2502,  0.3694,  0.2085,  0.2804,  0.2233,\n",
            "         0.2309,  0.1609,  0.2369,  0.2116,  0.3549,  0.1635,  0.1642,  0.3072,\n",
            "         0.3077,  0.2152,  0.2821,  0.2857,  0.1701,  0.2305,  0.2134,  0.3189,\n",
            "         0.1061,  0.2628,  0.2608,  0.1749,  0.0820,  0.1815,  0.3566,  0.1204,\n",
            "         0.3159,  0.1595,  0.3790,  0.3272,  0.2086,  0.3096,  0.2253,  0.1456,\n",
            "         0.1346,  0.2304,  0.2913,  0.2727,  0.2027,  0.2688,  0.1958,  0.2277,\n",
            "         0.3036,  0.3250,  0.3000,  0.3328,  0.2417,  0.2665,  0.2473,  0.0913,\n",
            "         0.2503,  0.2543,  0.3710,  0.3321,  0.3693,  0.1099,  0.1701,  0.1758,\n",
            "         0.3888,  0.2206,  0.2766,  0.2813,  0.1755,  0.2616,  0.1544,  0.2519,\n",
            "         0.1945,  0.2452,  0.3405,  0.2446,  0.2426,  0.1822,  0.3002,  0.3037,\n",
            "         0.3118,  0.2414,  0.2326,  0.1303,  0.3081,  0.0979,  0.2776,  0.2918,\n",
            "         0.3848,  0.1789,  0.3622,  0.3005,  0.1923,  0.2672,  0.1663,  0.2998,\n",
            "         0.2710,  0.2040,  0.2565,  0.2289,  0.2552,  0.2121,  0.3532,  0.2293,\n",
            "         0.2510,  0.3085,  0.2368,  0.3000,  0.2111,  0.3456,  0.3422,  0.1576])\n",
            "Parameter containing:\n",
            "tensor([-0.1759, -0.2156, -0.2047, -0.1695, -0.1628, -0.1473, -0.2158, -0.2905,\n",
            "        -0.1112, -0.2196, -0.1020, -0.1549, -0.1989, -0.0445, -0.1508, -0.1920,\n",
            "        -0.2114, -0.1655, -0.1854, -0.1733, -0.1289, -0.2376, -0.1965, -0.1965,\n",
            "        -0.1776, -0.1774, -0.1760, -0.1546, -0.1648, -0.2599, -0.1752, -0.2498,\n",
            "        -0.1741, -0.2410, -0.2498, -0.2938, -0.1496, -0.1578, -0.1800, -0.1851,\n",
            "        -0.1516, -0.1345, -0.2746, -0.1248, -0.2246, -0.2531, -0.2398, -0.1859,\n",
            "        -0.1739, -0.2393, -0.1214, -0.1803, -0.2729, -0.2617, -0.1855, -0.2316,\n",
            "        -0.2333, -0.1860, -0.2097, -0.0692, -0.1912, -0.2078, -0.1084, -0.2810,\n",
            "        -0.1303, -0.1654, -0.2119, -0.3641, -0.2951, -0.2384, -0.1632, -0.1892,\n",
            "        -0.1792, -0.2031, -0.1770, -0.2738, -0.3324, -0.1725, -0.1793, -0.2638,\n",
            "        -0.2207, -0.1609, -0.1534, -0.1414, -0.2992, -0.1450, -0.1838, -0.1779,\n",
            "        -0.1422, -0.2198, -0.1900, -0.1580, -0.1666, -0.2490, -0.1569, -0.1718,\n",
            "        -0.1660, -0.1972, -0.2287, -0.2366, -0.2230, -0.1543, -0.2030, -0.1431,\n",
            "        -0.1363, -0.2015, -0.1804, -0.2093, -0.2964, -0.1984, -0.2683, -0.2216,\n",
            "        -0.2147, -0.3404, -0.2668, -0.1890, -0.1733, -0.2226, -0.1772, -0.1698,\n",
            "        -0.1095, -0.2180, -0.1154, -0.1654, -0.1910, -0.3535, -0.3112, -0.2161,\n",
            "        -0.1496, -0.1667, -0.2849, -0.2207, -0.1529, -0.1807, -0.2118, -0.1869,\n",
            "        -0.1376, -0.1770, -0.1861, -0.1969, -0.1741, -0.3011, -0.0787, -0.2017,\n",
            "        -0.1947, -0.2247, -0.2459, -0.1058, -0.1401, -0.1213, -0.1199, -0.1760,\n",
            "        -0.2156, -0.3307, -0.3515, -0.2366, -0.1185, -0.2155, -0.1751, -0.1892,\n",
            "        -0.3365, -0.1598, -0.2554,  0.0644, -0.2856, -0.1198, -0.1583, -0.2297,\n",
            "        -0.3352, -0.1987, -0.2686, -0.1632, -0.2461, -0.2900, -0.2428, -0.1449,\n",
            "        -0.1900, -0.2149, -0.1541, -0.2917, -0.2504, -0.2213, -0.0463, -0.1547,\n",
            "        -0.1511, -0.1527, -0.1735, -0.1931, -0.1987, -0.2239, -0.2086, -0.2688,\n",
            "        -0.1845, -0.1797, -0.1833, -0.3880, -0.1539, -0.1553, -0.1567, -0.2238,\n",
            "        -0.1511, -0.2540, -0.2849, -0.1826, -0.2687, -0.2328, -0.2108, -0.2410,\n",
            "        -0.1022, -0.1507, -0.1978, -0.1734, -0.2282, -0.0985, -0.1847, -0.1770,\n",
            "        -0.1576, -0.1937, -0.1643, -0.2822, -0.1866, -0.2754, -0.2266, -0.2169,\n",
            "        -0.1352, -0.2194, -0.1060, -0.2139, -0.1322, -0.1889, -0.2130, -0.1913,\n",
            "        -0.2364, -0.1402, -0.2228, -0.2354, -0.1632, -0.1905, -0.1428, -0.1177,\n",
            "        -0.2419, -0.2733, -0.2963, -0.1600, -0.3558, -0.3673, -0.2201, -0.1505,\n",
            "        -0.2084, -0.0870, -0.2052, -0.2070, -0.1986, -0.2299, -0.0745, -0.1765,\n",
            "        -0.1412, -0.2180, -0.1450, -0.1426, -0.1452, -0.2916, -0.0871, -0.1359,\n",
            "        -0.2003, -0.1125, -0.2588, -0.1988, -0.2028, -0.2443, -0.0864, -0.3415,\n",
            "        -0.2579, -0.2343, -0.3552, -0.1859, -0.1153, -0.1732, -0.1780, -0.1909,\n",
            "        -0.2018, -0.1886, -0.2751, -0.1501,  0.1165, -0.1891, -0.1845, -0.2037,\n",
            "        -0.0339, -0.3464, -0.1956, -0.1962, -0.1537, -0.1902, -0.1431, -0.3022,\n",
            "        -0.1780, -0.1971, -0.2118, -0.0952, -0.1711, -0.2409, -0.2184, -0.2114,\n",
            "        -0.2042, -0.0566, -0.0700, -0.2081, -0.1872, -0.2079, -0.1540, -0.2266,\n",
            "        -0.1981, -0.1679, -0.2022, -0.2010, -0.1051, -0.1705, -0.2139,  0.0396,\n",
            "        -0.1077, -0.2745, -0.2690, -0.2603, -0.2819, -0.1917, -0.1940, -0.2944,\n",
            "        -0.1822, -0.2903, -0.1064, -0.2076, -0.2648, -0.3032, -0.2878, -0.1579,\n",
            "        -0.0071, -0.2142, -0.2022, -0.1516, -0.1123,  0.0246, -0.0978, -0.1382,\n",
            "        -0.1800, -0.3214, -0.2179, -0.1369, -0.0800,  0.0117, -0.1839, -0.1926,\n",
            "        -0.1614, -0.2769, -0.1909, -0.2101, -0.2305, -0.2055, -0.2017, -0.2741,\n",
            "        -0.1005, -0.3152, -0.1121, -0.1700, -0.1364, -0.2157, -0.2673, -0.1584,\n",
            "        -0.1997, -0.1745, -0.1886, -0.2307, -0.2024, -0.3376, -0.2266, -0.2355,\n",
            "        -0.2133, -0.2346, -0.2412, -0.2358, -0.1265, -0.2341, -0.1887, -0.1646,\n",
            "        -0.1417, -0.1882, -0.1076, -0.3048, -0.1162, -0.1651, -0.2046, -0.1833,\n",
            "        -0.3102, -0.1778, -0.1575, -0.2676, -0.1777, -0.1569, -0.1741, -0.1892,\n",
            "        -0.3028, -0.1457, -0.2179, -0.2226, -0.1609, -0.1423, -0.2683, -0.2920,\n",
            "        -0.1740, -0.2079, -0.1940, -0.2679, -0.1973, -0.1951, -0.1665, -0.2286,\n",
            "        -0.1903, -0.2667, -0.4010, -0.2550, -0.1817, -0.2025, -0.1589, -0.2476,\n",
            "        -0.0573, -0.2203, -0.2084, -0.1587, -0.1212, -0.1795, -0.3449, -0.1662,\n",
            "        -0.2523, -0.2435, -0.2878, -0.2797, -0.1897, -0.2113, -0.1943, -0.2050,\n",
            "        -0.1694, -0.2243, -0.2987, -0.1328, -0.1428, -0.2399, -0.1593, -0.1999,\n",
            "        -0.3225, -0.1860, -0.1763, -0.2691, -0.2097, -0.2396, -0.1140, -0.1897,\n",
            "        -0.1870, -0.1829, -0.2615, -0.2073, -0.1858, -0.0598, -0.1915, -0.2183,\n",
            "        -0.2088, -0.1742, -0.2715, -0.1999, -0.2117, -0.2492, -0.1717, -0.1566,\n",
            "        -0.1669, -0.3015, -0.1685, -0.2434, -0.2297, -0.1947, -0.2860, -0.3288,\n",
            "        -0.2197, -0.1862, -0.1755, -0.0987, -0.1756, -0.1304, -0.1555, -0.1679,\n",
            "        -0.2222, -0.2819, -0.2652, -0.0947, -0.2412, -0.2731, -0.2572, -0.2604,\n",
            "        -0.2934, -0.2470, -0.1820, -0.2740, -0.1336, -0.1698, -0.1919, -0.1796,\n",
            "        -0.2325, -0.1352, -0.1077, -0.2184, -0.1539, -0.2015, -0.3243, -0.1713])\n",
            "Parameter containing:\n",
            "tensor([[[[-8.0284e-03, -5.7776e-03,  6.4154e-03],\n",
            "          [ 5.0498e-03, -6.7796e-03,  1.2691e-02],\n",
            "          [ 1.3331e-02,  1.4523e-02,  2.4522e-02]],\n",
            "\n",
            "         [[-1.9876e-03,  1.2466e-02,  1.0494e-02],\n",
            "          [-1.9364e-02, -1.6696e-02, -1.1857e-02],\n",
            "          [-1.1569e-02, -3.7674e-03, -3.4679e-03]],\n",
            "\n",
            "         [[-1.1440e-02, -1.3884e-02,  1.1559e-03],\n",
            "          [-1.7906e-02, -2.9349e-02, -1.3876e-02],\n",
            "          [-1.4057e-02, -2.6989e-02, -2.3963e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-6.3040e-03, -3.1167e-03, -1.3304e-02],\n",
            "          [ 7.1623e-03,  6.4669e-03,  1.6063e-02],\n",
            "          [-1.0750e-02, -1.0480e-02, -6.1070e-03]],\n",
            "\n",
            "         [[ 7.4484e-03,  6.3878e-03, -1.2579e-02],\n",
            "          [-7.7356e-03,  1.8112e-03, -1.7890e-02],\n",
            "          [-2.9142e-03,  7.7705e-03, -9.7314e-03]],\n",
            "\n",
            "         [[ 2.1760e-02,  2.2364e-02,  2.2731e-02],\n",
            "          [ 2.6681e-02,  2.9127e-02,  3.3356e-02],\n",
            "          [ 1.2892e-02, -3.5818e-03,  5.3022e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.0597e-02, -9.1551e-03, -2.3418e-02],\n",
            "          [-1.0768e-02, -3.3171e-03, -1.8559e-02],\n",
            "          [-1.8607e-02, -4.2634e-03, -1.5591e-02]],\n",
            "\n",
            "         [[-2.6090e-02, -2.2517e-02, -3.0593e-02],\n",
            "          [-3.9406e-02, -2.6639e-02, -2.8202e-02],\n",
            "          [-2.6143e-02, -1.9647e-02, -2.1466e-02]],\n",
            "\n",
            "         [[-3.5259e-03,  1.6623e-03, -6.5624e-03],\n",
            "          [-5.0597e-03, -8.7162e-04, -5.3742e-03],\n",
            "          [-7.9651e-03, -9.7778e-03, -1.0736e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.8492e-02, -3.6799e-03,  1.0043e-02],\n",
            "          [-5.2974e-03, -2.0757e-02, -1.5120e-02],\n",
            "          [ 2.1435e-02,  6.4916e-03,  4.7660e-03]],\n",
            "\n",
            "         [[-1.8810e-02, -6.0469e-04, -7.6999e-03],\n",
            "          [-1.7697e-02, -7.8692e-03, -1.6543e-02],\n",
            "          [-1.7206e-02, -2.4746e-02, -3.0270e-02]],\n",
            "\n",
            "         [[-3.1191e-02, -1.4363e-02,  2.2032e-03],\n",
            "          [-1.2033e-02, -2.3699e-03, -1.6630e-02],\n",
            "          [-1.2905e-02, -1.5363e-02, -3.6297e-03]]],\n",
            "\n",
            "\n",
            "        [[[-3.2648e-02, -4.8158e-03, -2.0476e-02],\n",
            "          [-2.5846e-02, -1.4660e-03, -2.8170e-02],\n",
            "          [-2.6640e-02,  4.3022e-03, -2.7636e-02]],\n",
            "\n",
            "         [[-6.3289e-03, -1.5401e-02, -1.3096e-03],\n",
            "          [-1.7499e-02, -2.6212e-02, -2.3646e-02],\n",
            "          [-7.3207e-03, -1.5592e-02, -8.9578e-03]],\n",
            "\n",
            "         [[ 8.9701e-04, -6.6914e-03, -5.3129e-03],\n",
            "          [-1.1727e-03, -1.0726e-02, -9.0103e-03],\n",
            "          [ 3.2311e-03, -4.5854e-03,  4.3512e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.1822e-02, -3.6889e-02, -2.2588e-02],\n",
            "          [-1.3054e-02, -3.4191e-02, -2.7238e-02],\n",
            "          [-1.2383e-02, -2.3452e-02, -2.2486e-02]],\n",
            "\n",
            "         [[ 6.8177e-03,  2.1561e-02,  1.3674e-02],\n",
            "          [ 3.1192e-03,  1.0660e-02,  1.0409e-02],\n",
            "          [ 8.0477e-03, -4.6817e-03, -4.3912e-03]],\n",
            "\n",
            "         [[-1.1983e-02, -1.6201e-02, -2.2626e-02],\n",
            "          [-1.3461e-02, -7.0928e-03, -1.4384e-02],\n",
            "          [-2.4456e-02,  1.4885e-02,  1.2247e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-2.6347e-02, -2.9923e-02, -3.7810e-02],\n",
            "          [-1.5663e-02, -4.1126e-03, -1.1482e-02],\n",
            "          [-1.3415e-02, -1.5432e-02, -1.8204e-02]],\n",
            "\n",
            "         [[-3.8392e-03, -1.1093e-02, -8.0841e-04],\n",
            "          [-5.9634e-03, -5.9165e-03, -9.3332e-03],\n",
            "          [-2.2761e-03,  5.4781e-03, -5.6050e-03]],\n",
            "\n",
            "         [[-1.8406e-03, -2.8134e-03,  8.3246e-03],\n",
            "          [-1.2453e-03,  2.1453e-04,  7.4868e-03],\n",
            "          [ 1.3450e-02,  3.0599e-02,  2.6405e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.5268e-04,  2.3897e-03,  6.2558e-03],\n",
            "          [-1.4338e-02, -2.3146e-02, -1.9024e-02],\n",
            "          [-2.7306e-02, -3.0079e-02, -3.1762e-02]],\n",
            "\n",
            "         [[ 1.4584e-02,  4.3430e-03,  1.2053e-02],\n",
            "          [-6.1130e-03, -2.8539e-02, -1.8268e-02],\n",
            "          [-1.6844e-02, -4.7816e-02, -2.6274e-02]],\n",
            "\n",
            "         [[-1.8850e-02, -9.3396e-03,  7.8905e-03],\n",
            "          [-1.5322e-03,  8.3153e-03,  1.7783e-02],\n",
            "          [-8.3318e-03, -1.5759e-02, -1.2061e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 9.9578e-03,  7.4573e-03, -1.8738e-03],\n",
            "          [-1.7752e-03, -6.8015e-04, -7.4443e-03],\n",
            "          [-1.8319e-02, -1.4264e-02, -7.1446e-03]],\n",
            "\n",
            "         [[ 7.8524e-03, -2.6520e-03, -1.7556e-02],\n",
            "          [ 4.5240e-03, -4.8661e-03, -1.5215e-02],\n",
            "          [-5.0211e-03, -1.1864e-02, -1.4846e-02]],\n",
            "\n",
            "         [[ 2.9163e-02,  1.0344e-02,  2.4736e-02],\n",
            "          [ 1.2012e-02, -1.0346e-02,  3.5472e-03],\n",
            "          [ 8.2238e-03, -1.8237e-02, -5.4892e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-8.8434e-03, -4.3184e-03, -5.7536e-03],\n",
            "          [ 7.7230e-03, -4.1936e-04,  7.7260e-03],\n",
            "          [ 1.3536e-02,  1.5705e-02,  2.0893e-02]],\n",
            "\n",
            "         [[ 1.6743e-03,  1.9720e-03,  2.1567e-02],\n",
            "          [-8.0074e-03, -4.6606e-03,  4.0560e-03],\n",
            "          [-1.6688e-02, -1.3754e-02, -1.1708e-02]],\n",
            "\n",
            "         [[-9.7959e-03, -9.4502e-03, -9.3443e-03],\n",
            "          [ 6.9547e-03, -3.9134e-05,  6.2691e-03],\n",
            "          [-1.3193e-02,  9.3272e-04,  1.4579e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.4963e-03,  5.5133e-04,  1.1571e-02],\n",
            "          [ 1.0174e-02,  1.7889e-03,  1.1035e-02],\n",
            "          [ 7.0212e-03,  1.4651e-03,  1.2769e-03]],\n",
            "\n",
            "         [[-1.3021e-02,  6.4109e-03, -1.5199e-02],\n",
            "          [ 2.4775e-02,  2.1926e-02,  3.3679e-02],\n",
            "          [ 2.6471e-04, -3.0235e-03,  1.1690e-02]],\n",
            "\n",
            "         [[-2.9665e-02, -1.5314e-02, -1.7500e-02],\n",
            "          [-1.8339e-02, -2.0845e-02, -1.5494e-02],\n",
            "          [-1.6086e-03,  1.0831e-02, -1.4309e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-7.7044e-03, -2.1100e-02, -2.2816e-02],\n",
            "          [ 5.7688e-03,  1.9362e-04,  7.7105e-04],\n",
            "          [-6.1357e-03,  9.7275e-03, -2.5464e-03]],\n",
            "\n",
            "         [[ 1.1043e-02,  2.4205e-02,  3.4213e-02],\n",
            "          [ 2.9181e-02,  2.6904e-02,  4.5372e-02],\n",
            "          [-2.1594e-02, -1.1072e-03, -7.8312e-03]],\n",
            "\n",
            "         [[-8.3287e-03, -7.9521e-03, -5.3358e-03],\n",
            "          [-6.2527e-04, -5.3243e-03, -8.6296e-03],\n",
            "          [ 3.6094e-03, -1.2544e-03, -4.3801e-03]]]])\n",
            "Parameter containing:\n",
            "tensor([0.2587, 0.3073, 0.2595, 0.3223, 0.2662, 0.2652, 0.2575, 0.2660, 0.2766,\n",
            "        0.2414, 0.3045, 0.2853, 0.2821, 0.2880, 0.3094, 0.3444, 0.3155, 0.4129,\n",
            "        0.2110, 0.2903, 0.2496, 0.2601, 0.2967, 0.3033, 0.4152, 0.2719, 0.3661,\n",
            "        0.3251, 0.3898, 0.3346, 0.2753, 0.2712, 0.2414, 0.3351, 0.3394, 0.3167,\n",
            "        0.3360, 0.2666, 0.2109, 0.2705, 0.2587, 0.3070, 0.2720, 0.2316, 0.2885,\n",
            "        0.2884, 0.2955, 0.3057, 0.3043, 0.2596, 0.2673, 0.1929, 0.3136, 0.3593,\n",
            "        0.2622, 0.2931, 0.3295, 0.2514, 0.3208, 0.2798, 0.3259, 0.2939, 0.2390,\n",
            "        0.3105, 0.3471, 0.2812, 0.2148, 0.2997, 0.3061, 0.2740, 0.2791, 0.3790,\n",
            "        0.3592, 0.3247, 0.2995, 0.2735, 0.3356, 0.2703, 0.3255, 0.3127, 0.2783,\n",
            "        0.2702, 0.3900, 0.2942, 0.2899, 0.3461, 0.3432, 0.4685, 0.2634, 0.2553,\n",
            "        0.3019, 0.3961, 0.2742, 0.2995, 0.3858, 0.2785, 0.3212, 0.3109, 0.3642,\n",
            "        0.2193, 0.2643, 0.2333, 0.3151, 0.3102, 0.2936, 0.2374, 0.2419, 0.2976,\n",
            "        0.3335, 0.2619, 0.3984, 0.2721, 0.2718, 0.2678, 0.2757, 0.2445, 0.3508,\n",
            "        0.2174, 0.3309, 0.2653, 0.2564, 0.1748, 0.3177, 0.2751, 0.2067, 0.2905,\n",
            "        0.2762, 0.3329, 0.2738, 0.3224, 0.2199, 0.2997, 0.2206, 0.3213, 0.2760,\n",
            "        0.3927, 0.3174, 0.2698, 0.2988, 0.2610, 0.2550, 0.2788, 0.4445, 0.2862,\n",
            "        0.3606, 0.3279, 0.2869, 0.3294, 0.2244, 0.2338, 0.1754, 0.2318, 0.3186,\n",
            "        0.3322, 0.2255, 0.3041, 0.2837, 0.3276, 0.2392, 0.3668, 0.1971, 0.2946,\n",
            "        0.3613, 0.2736, 0.2554, 0.2860, 0.2511, 0.3490, 0.3253, 0.2934, 0.2027,\n",
            "        0.2580, 0.2200, 0.3089, 0.3074, 0.3332, 0.2943, 0.3375, 0.2330, 0.2611,\n",
            "        0.3383, 0.2837, 0.3546, 0.3093, 0.3791, 0.2197, 0.2648, 0.2830, 0.2587,\n",
            "        0.3588, 0.2830, 0.3971, 0.3194, 0.3066, 0.2754, 0.2647, 0.0970, 0.2182,\n",
            "        0.2334, 0.2624, 0.1829, 0.2933, 0.2747, 0.3001, 0.2996, 0.3107, 0.3256,\n",
            "        0.2940, 0.3901, 0.2790, 0.3030, 0.2838, 0.3010, 0.3044, 0.3479, 0.3087,\n",
            "        0.2611, 0.1958, 0.2941, 0.2558, 0.2889, 0.3148, 0.2516, 0.2664, 0.2862,\n",
            "        0.3940, 0.2933, 0.2781, 0.3796, 0.3022, 0.2583, 0.3021, 0.2784, 0.2967,\n",
            "        0.2994, 0.3856, 0.3277, 0.2587, 0.2539, 0.2824, 0.2634, 0.1489, 0.2205,\n",
            "        0.3929, 0.3401, 0.2717, 0.2789, 0.2917, 0.3177, 0.1992, 0.3684, 0.3120,\n",
            "        0.3201, 0.2810, 0.2302, 0.2779, 0.2865, 0.2858, 0.2713, 0.1601, 0.2496,\n",
            "        0.2895, 0.3154, 0.3443, 0.3285, 0.3444, 0.3251, 0.3235, 0.3375, 0.2282,\n",
            "        0.2128, 0.1795, 0.3077, 0.3005, 0.2775, 0.3054, 0.2914, 0.3535, 0.2871,\n",
            "        0.2669, 0.3961, 0.2674, 0.3898, 0.3183, 0.3242, 0.2789, 0.1911, 0.2569,\n",
            "        0.3427, 0.2464, 0.2778, 0.2098, 0.3019, 0.3145, 0.3271, 0.2914, 0.2619,\n",
            "        0.2643, 0.3039, 0.2520, 0.2099, 0.3643, 0.2915, 0.1957, 0.3286, 0.2355,\n",
            "        0.3210, 0.2982, 0.3388, 0.3450, 0.3716, 0.2898, 0.2846, 0.2805, 0.2219,\n",
            "        0.2910, 0.2681, 0.3163, 0.1964, 0.3176, 0.3092, 0.2706, 0.2505, 0.2508,\n",
            "        0.3166, 0.3583, 0.1563, 0.2608, 0.2892, 0.3401, 0.2891, 0.3126, 0.2172,\n",
            "        0.2459, 0.2651, 0.4052, 0.2986, 0.3026, 0.3773, 0.2262, 0.2675, 0.2900,\n",
            "        0.3759, 0.3201, 0.2567, 0.3443, 0.2348, 0.3057, 0.2347, 0.3277, 0.2938,\n",
            "        0.2746, 0.2805, 0.2421, 0.3590, 0.2622, 0.2773, 0.2396, 0.2134, 0.2727,\n",
            "        0.2984, 0.2744, 0.2591, 0.2628, 0.3568, 0.2009, 0.3220, 0.2868, 0.2561,\n",
            "        0.3113, 0.2138, 0.3136, 0.2745, 0.3046, 0.3042, 0.1972, 0.2815, 0.2542,\n",
            "        0.2983, 0.2613, 0.2668, 0.3142, 0.2930, 0.3800, 0.1966, 0.2948, 0.3363,\n",
            "        0.2713, 0.3625, 0.2909, 0.2695, 0.3111, 0.3242, 0.3009, 0.3231, 0.3051,\n",
            "        0.2012, 0.2716, 0.3692, 0.2694, 0.1481, 0.2858, 0.2819, 0.2391, 0.2867,\n",
            "        0.3466, 0.3431, 0.2365, 0.3357, 0.1685, 0.2925, 0.3092, 0.3127, 0.1883,\n",
            "        0.2561, 0.3086, 0.1732, 0.2989, 0.3235, 0.2693, 0.2630, 0.2913, 0.2786,\n",
            "        0.3124, 0.3098, 0.2695, 0.2403, 0.2906, 0.2784, 0.2654, 0.3485, 0.3939,\n",
            "        0.3033, 0.3145, 0.2622, 0.1540, 0.2790, 0.2967, 0.1954, 0.2632, 0.2957,\n",
            "        0.2581, 0.3231, 0.2795, 0.2859, 0.3139, 0.2488, 0.2404, 0.3714, 0.2649,\n",
            "        0.2267, 0.2878, 0.3462, 0.3063, 0.3180, 0.1726, 0.3153, 0.2625, 0.3020,\n",
            "        0.2996, 0.3632, 0.1541, 0.3192, 0.2200, 0.2894, 0.2622, 0.2534, 0.2935,\n",
            "        0.3208, 0.2231, 0.2743, 0.3023, 0.2829, 0.2394, 0.2506, 0.3512, 0.3366,\n",
            "        0.2666, 0.2930, 0.3049, 0.2321, 0.3397, 0.2727, 0.2900, 0.3146, 0.2682,\n",
            "        0.3094, 0.3718, 0.3387, 0.3202, 0.2423, 0.2745, 0.2966, 0.2500, 0.2329,\n",
            "        0.3419, 0.2928, 0.3536, 0.3739, 0.1935, 0.2670, 0.2846, 0.2583, 0.3783,\n",
            "        0.2826, 0.2929, 0.2728, 0.3645, 0.2770, 0.2756, 0.2523, 0.2500])\n",
            "Parameter containing:\n",
            "tensor([-0.1668, -0.3019, -0.2187, -0.2917, -0.1971, -0.2325, -0.1869, -0.1857,\n",
            "        -0.2474, -0.1629, -0.2448, -0.2508, -0.1895, -0.2651, -0.3250, -0.3811,\n",
            "        -0.2953, -0.4963, -0.0294, -0.2724, -0.2007, -0.2220, -0.2945, -0.2579,\n",
            "        -0.5152, -0.1994, -0.5016, -0.2736, -0.4528, -0.3968, -0.2281, -0.1772,\n",
            "        -0.1293, -0.2655, -0.3252, -0.3232, -0.3337, -0.1901, -0.0692, -0.2196,\n",
            "        -0.2132, -0.2565, -0.1646, -0.1567, -0.2087, -0.2178, -0.2480, -0.2767,\n",
            "        -0.3071, -0.1988, -0.1985, -0.0235, -0.2458, -0.4156, -0.1660, -0.1923,\n",
            "        -0.3328, -0.1481, -0.3047, -0.2277, -0.3182, -0.2744, -0.1643, -0.3365,\n",
            "        -0.4050, -0.2082, -0.0621, -0.2671, -0.2809, -0.2185, -0.2148, -0.4465,\n",
            "        -0.3376, -0.3213, -0.2921, -0.1998, -0.3369, -0.2092, -0.2831, -0.2893,\n",
            "        -0.1719, -0.2189, -0.4016, -0.2484, -0.2070, -0.3849, -0.3753, -0.5874,\n",
            "        -0.1637, -0.1748, -0.2217, -0.5067, -0.2496, -0.2117, -0.4291, -0.1944,\n",
            "        -0.3089, -0.2621, -0.4096, -0.0602, -0.2009, -0.1316, -0.3336, -0.2627,\n",
            "        -0.2320, -0.0910, -0.1560, -0.2889, -0.3286, -0.1628, -0.5128, -0.2036,\n",
            "        -0.1726, -0.1844, -0.2285, -0.1925, -0.3432, -0.0929, -0.3138, -0.1912,\n",
            "        -0.1926, -0.0342, -0.3268, -0.1699, -0.0828, -0.2417, -0.2069, -0.3870,\n",
            "        -0.2210, -0.2867, -0.0526, -0.3092, -0.0655, -0.2594, -0.2160, -0.5062,\n",
            "        -0.2905, -0.2125, -0.3124, -0.2128, -0.1946, -0.2520, -0.5475, -0.2321,\n",
            "        -0.3350, -0.3473, -0.2158, -0.3603, -0.0759, -0.1472, -0.0327, -0.1404,\n",
            "        -0.3128, -0.3063, -0.1120, -0.2664, -0.2700, -0.3112, -0.1519, -0.3843,\n",
            "        -0.0645, -0.2373, -0.4227, -0.2546, -0.1611, -0.2350, -0.1524, -0.3494,\n",
            "        -0.3453, -0.2081, -0.0918, -0.2025, -0.1246, -0.2533, -0.2768, -0.3156,\n",
            "        -0.2530, -0.3957, -0.0981, -0.1257, -0.3697, -0.2333, -0.3664, -0.2829,\n",
            "        -0.4320, -0.0836, -0.1583, -0.2395, -0.1818, -0.4408, -0.2376, -0.4450,\n",
            "        -0.3232, -0.2787, -0.1858, -0.2137,  0.0481, -0.1058, -0.1093, -0.2035,\n",
            "        -0.0496, -0.2117, -0.1598, -0.2389, -0.2830, -0.2878, -0.3406, -0.2560,\n",
            "        -0.4468, -0.2444, -0.2492, -0.2222, -0.2792, -0.3005, -0.4180, -0.2568,\n",
            "        -0.1872, -0.0270, -0.2645, -0.1873, -0.3022, -0.3400, -0.1803, -0.1810,\n",
            "        -0.2079, -0.4775, -0.2047, -0.1878, -0.4504, -0.2516, -0.1657, -0.2765,\n",
            "        -0.2329, -0.2446, -0.2956, -0.4163, -0.2816, -0.1571, -0.2199, -0.2125,\n",
            "        -0.1684,  0.0356, -0.0914, -0.4484, -0.3535, -0.2212, -0.2550, -0.2509,\n",
            "        -0.2702, -0.0599, -0.3505, -0.2924, -0.2360, -0.2339, -0.1259, -0.2597,\n",
            "        -0.2267, -0.1978, -0.1371, -0.0129, -0.1175, -0.2527, -0.3099, -0.3231,\n",
            "        -0.3468, -0.3553, -0.3537, -0.3315, -0.3713, -0.1091, -0.0959, -0.0258,\n",
            "        -0.2756, -0.2808, -0.2012, -0.2812, -0.1991, -0.3948, -0.2257, -0.2469,\n",
            "        -0.4211, -0.2110, -0.4670, -0.3069, -0.3549, -0.2337, -0.0612, -0.1321,\n",
            "        -0.2968, -0.1870, -0.2316, -0.0686, -0.3113, -0.2895, -0.3149, -0.2686,\n",
            "        -0.2081, -0.2096, -0.3011, -0.1810, -0.0227, -0.3873, -0.2665, -0.0225,\n",
            "        -0.2973, -0.0973, -0.2980, -0.3219, -0.2926, -0.3196, -0.4332, -0.1980,\n",
            "        -0.2117, -0.2302, -0.0980, -0.2344, -0.2154, -0.2921, -0.0350, -0.3361,\n",
            "        -0.2620, -0.2188, -0.1566, -0.1795, -0.2726, -0.4103,  0.0413, -0.1507,\n",
            "        -0.2552, -0.3137, -0.2466, -0.2961, -0.0938, -0.1481, -0.2129, -0.5480,\n",
            "        -0.2915, -0.2802, -0.5077, -0.1306, -0.1862, -0.2400, -0.4362, -0.3017,\n",
            "        -0.1633, -0.3447, -0.1047, -0.2846, -0.1244, -0.3036, -0.2404, -0.2333,\n",
            "        -0.2494, -0.1866, -0.3294, -0.1677, -0.2540, -0.1295, -0.0512, -0.1966,\n",
            "        -0.2801, -0.1702, -0.1879, -0.1850, -0.3274, -0.0369, -0.2979, -0.2612,\n",
            "        -0.1889, -0.3270, -0.1377, -0.2787, -0.2201, -0.2417, -0.2834, -0.0555,\n",
            "        -0.2538, -0.1040, -0.2660, -0.1644, -0.1723, -0.2672, -0.2797, -0.4214,\n",
            "        -0.0378, -0.2386, -0.3498, -0.2435, -0.4348, -0.2554, -0.1719, -0.2836,\n",
            "        -0.3316, -0.2787, -0.2879, -0.2640, -0.0560, -0.1789, -0.4195, -0.2152,\n",
            "         0.0567, -0.2359, -0.2249, -0.0911, -0.2644, -0.3875, -0.3317, -0.1415,\n",
            "        -0.3425, -0.0020, -0.1941, -0.2821, -0.2809, -0.0965, -0.1841, -0.2971,\n",
            "        -0.0173, -0.3043, -0.3013, -0.1729, -0.1872, -0.2683, -0.2033, -0.3059,\n",
            "        -0.2939, -0.2163, -0.1889, -0.2581, -0.2296, -0.2066, -0.3462, -0.4298,\n",
            "        -0.2600, -0.3095, -0.1800, -0.0116, -0.2124, -0.2552, -0.0523, -0.2216,\n",
            "        -0.2605, -0.2134, -0.2867, -0.2556, -0.2275, -0.3437, -0.1698, -0.1560,\n",
            "        -0.4120, -0.2067, -0.1159, -0.2408, -0.3093, -0.2621, -0.2593, -0.0135,\n",
            "        -0.3099, -0.2179, -0.2766, -0.2400, -0.3934,  0.0072, -0.2982, -0.0930,\n",
            "        -0.2166, -0.1635, -0.1827, -0.2308, -0.2525, -0.0991, -0.2325, -0.2938,\n",
            "        -0.2480, -0.0934, -0.1911, -0.3772, -0.3369, -0.1606, -0.2752, -0.3005,\n",
            "        -0.1372, -0.2990, -0.2156, -0.2622, -0.3160, -0.1342, -0.2903, -0.3865,\n",
            "        -0.2916, -0.3243, -0.2051, -0.2656, -0.2359, -0.1508, -0.1063, -0.3595,\n",
            "        -0.2312, -0.3046, -0.4178, -0.0276, -0.2204, -0.2426, -0.1616, -0.4789,\n",
            "        -0.1713, -0.2802, -0.2305, -0.4327, -0.2413, -0.1862, -0.1486, -0.1507])\n",
            "Parameter containing:\n",
            "tensor([[[[ 2.8729e-04,  4.2632e-03, -2.0266e-03],\n",
            "          [ 1.9513e-04,  2.4381e-03, -5.8632e-03],\n",
            "          [ 4.4803e-03,  8.6577e-03,  8.5538e-04]],\n",
            "\n",
            "         [[-1.1335e-02, -1.3195e-02, -1.0305e-02],\n",
            "          [-4.9507e-03, -4.5898e-03, -3.1041e-03],\n",
            "          [-7.5883e-03, -8.3795e-03, -8.9239e-03]],\n",
            "\n",
            "         [[-1.1914e-02, -1.2104e-02, -1.0167e-02],\n",
            "          [-1.2093e-02, -1.1557e-02, -8.9600e-03],\n",
            "          [-1.2515e-02, -9.3296e-03, -6.4079e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-9.3573e-03, -1.0662e-02, -1.2672e-02],\n",
            "          [-8.0600e-03, -8.5423e-03, -1.2121e-02],\n",
            "          [-8.1498e-03, -8.8037e-03, -1.0611e-02]],\n",
            "\n",
            "         [[ 4.2632e-03,  5.6461e-03,  2.8460e-03],\n",
            "          [ 4.7070e-03,  6.2550e-03,  7.5862e-03],\n",
            "          [ 1.1504e-02,  1.1518e-02,  1.0728e-02]],\n",
            "\n",
            "         [[-6.2455e-03, -9.1693e-03, -9.6664e-03],\n",
            "          [-4.2935e-03, -6.5311e-03, -5.0513e-03],\n",
            "          [-3.1141e-03, -5.0124e-03, -5.8122e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 2.7483e-03,  3.7146e-04,  3.3262e-05],\n",
            "          [-4.5675e-03, -6.6689e-03, -6.4447e-03],\n",
            "          [-6.7610e-03, -7.3204e-03, -9.5855e-03]],\n",
            "\n",
            "         [[-1.4630e-02, -1.2320e-02, -1.4457e-02],\n",
            "          [-8.6197e-03, -5.8059e-03, -1.1075e-02],\n",
            "          [-6.2154e-03, -6.8218e-03, -9.3805e-03]],\n",
            "\n",
            "         [[ 1.0879e-03,  4.3850e-04, -1.9456e-03],\n",
            "          [-1.2517e-03,  3.2917e-04, -2.1435e-03],\n",
            "          [ 4.8136e-03,  2.5333e-03,  5.1504e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.4644e-02,  1.7434e-02,  2.0734e-02],\n",
            "          [ 2.3101e-02,  1.3487e-02,  2.0728e-02],\n",
            "          [ 1.9381e-02,  1.5243e-02,  1.7340e-02]],\n",
            "\n",
            "         [[ 1.2212e-02,  1.2448e-02,  1.5048e-02],\n",
            "          [ 5.2993e-03,  4.0090e-03,  9.3927e-03],\n",
            "          [ 6.6766e-03,  2.4941e-03,  8.3288e-03]],\n",
            "\n",
            "         [[ 3.1040e-02,  2.8243e-02,  3.2319e-02],\n",
            "          [ 3.8608e-02,  3.3099e-02,  3.8652e-02],\n",
            "          [ 2.5839e-02,  2.6524e-02,  2.4995e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.1761e-03,  4.5553e-03,  2.0612e-03],\n",
            "          [ 4.9747e-03,  1.1420e-02,  8.5734e-03],\n",
            "          [ 4.8583e-03,  1.1469e-02,  1.0039e-02]],\n",
            "\n",
            "         [[-6.2547e-05,  6.5336e-04,  9.4747e-04],\n",
            "          [ 5.0603e-03,  7.7136e-03,  6.5484e-03],\n",
            "          [-4.8432e-04,  2.3057e-03,  2.9219e-03]],\n",
            "\n",
            "         [[-3.2788e-02, -2.7615e-02, -3.2608e-02],\n",
            "          [-3.6296e-02, -2.8170e-02, -3.0277e-02],\n",
            "          [-3.6814e-02, -3.1547e-02, -3.0231e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.2998e-03, -2.8590e-04, -4.9266e-03],\n",
            "          [-7.0530e-03, -2.3684e-04, -1.5838e-03],\n",
            "          [-6.9291e-03,  4.8084e-04, -3.1548e-03]],\n",
            "\n",
            "         [[ 1.1854e-02,  8.4836e-03,  1.3839e-02],\n",
            "          [ 2.8741e-03, -9.7358e-05,  4.4888e-03],\n",
            "          [-2.5515e-03, -2.7788e-03, -3.2464e-03]],\n",
            "\n",
            "         [[-1.2408e-02, -1.5001e-02, -1.3377e-02],\n",
            "          [-1.4540e-02, -1.8537e-02, -1.7392e-02],\n",
            "          [-6.7315e-03, -9.5205e-03, -9.0692e-03]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 3.0369e-03,  1.9542e-03,  1.7140e-03],\n",
            "          [-7.6240e-03, -2.8765e-03, -5.1760e-03],\n",
            "          [-9.3019e-03, -4.8800e-03, -4.2932e-03]],\n",
            "\n",
            "         [[ 4.4836e-03,  2.4909e-03,  1.5746e-03],\n",
            "          [ 1.2065e-02,  1.2936e-02,  1.0344e-02],\n",
            "          [ 1.9010e-02,  1.7459e-02,  1.5988e-02]],\n",
            "\n",
            "         [[-1.4914e-03, -8.1727e-03, -8.0671e-03],\n",
            "          [-6.6247e-03, -6.2421e-03, -9.2717e-03],\n",
            "          [-8.7991e-03, -7.7528e-03, -8.6336e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.8040e-02, -1.5366e-02, -1.5334e-02],\n",
            "          [-1.3148e-02, -1.2180e-02, -1.0915e-02],\n",
            "          [-1.4545e-02, -1.4756e-02, -1.1787e-02]],\n",
            "\n",
            "         [[ 3.5762e-03,  6.6073e-03, -1.4055e-03],\n",
            "          [ 4.3975e-03,  7.8375e-03,  8.8085e-05],\n",
            "          [-5.0697e-03, -5.6633e-04, -5.9284e-03]],\n",
            "\n",
            "         [[-1.9234e-03, -8.8012e-03, -5.8821e-03],\n",
            "          [ 3.6685e-03, -1.3784e-03, -3.2117e-03],\n",
            "          [-4.7037e-04,  1.5340e-04, -3.4046e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.8305e-02, -1.7735e-02, -2.1683e-02],\n",
            "          [-1.6598e-02, -1.2508e-02, -2.0530e-02],\n",
            "          [-1.0800e-02, -9.8670e-03, -1.7195e-02]],\n",
            "\n",
            "         [[ 2.0721e-02,  2.2466e-02,  2.5049e-02],\n",
            "          [ 1.8682e-02,  1.3160e-02,  2.3696e-02],\n",
            "          [ 2.2104e-02,  1.7261e-02,  2.4877e-02]],\n",
            "\n",
            "         [[-5.7091e-03, -2.6876e-03, -9.2260e-04],\n",
            "          [-9.4530e-03, -7.0543e-03, -6.2770e-03],\n",
            "          [-4.5806e-03, -2.7182e-03, -2.5823e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.4150e-02,  1.4002e-02,  1.6559e-02],\n",
            "          [ 2.1363e-02,  1.4359e-02,  1.5854e-02],\n",
            "          [ 2.5786e-02,  2.7233e-02,  2.5104e-02]],\n",
            "\n",
            "         [[-4.6450e-03,  1.2419e-03, -1.8768e-03],\n",
            "          [ 1.3005e-03,  4.0888e-03, -6.5483e-04],\n",
            "          [-7.9783e-03, -6.6539e-03, -8.9957e-03]],\n",
            "\n",
            "         [[ 1.1494e-02,  2.6621e-02,  1.5649e-02],\n",
            "          [ 6.5960e-03,  1.7290e-02,  7.5466e-03],\n",
            "          [-8.0256e-03,  4.6246e-03, -5.7808e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 1.4232e-02,  1.1769e-02,  9.4342e-03],\n",
            "          [ 6.2592e-03,  5.1087e-03,  2.3311e-03],\n",
            "          [-1.9694e-03,  2.7110e-03, -2.8945e-03]],\n",
            "\n",
            "         [[-7.0772e-03,  1.0365e-03, -5.8451e-03],\n",
            "          [-9.1879e-03, -3.1388e-03, -8.1517e-03],\n",
            "          [-8.0300e-03, -5.1313e-03, -9.5734e-03]],\n",
            "\n",
            "         [[ 2.4314e-02,  1.8942e-02,  2.4256e-02],\n",
            "          [ 2.0090e-02,  1.1472e-02,  1.5993e-02],\n",
            "          [ 2.2910e-02,  2.0622e-02,  2.3820e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.6375e-02, -1.6928e-02, -1.9019e-02],\n",
            "          [-9.7367e-03, -1.1274e-02, -1.0261e-02],\n",
            "          [-1.2310e-02, -1.5931e-02, -1.4151e-02]],\n",
            "\n",
            "         [[ 4.7098e-03, -4.5205e-04,  2.8042e-03],\n",
            "          [ 2.1428e-03, -4.6175e-03, -1.6818e-03],\n",
            "          [-1.3336e-03, -5.5009e-03, -2.6237e-03]],\n",
            "\n",
            "         [[-1.4367e-02, -1.3520e-02, -1.1387e-02],\n",
            "          [-4.7420e-03, -1.7309e-03, -2.6426e-03],\n",
            "          [ 5.1448e-03,  7.0428e-03,  5.0202e-03]]]])\n",
            "Parameter containing:\n",
            "tensor([1.8419, 1.8307, 1.7650, 1.8288, 1.9505, 1.8026, 1.9536, 2.2790, 1.7662,\n",
            "        1.8902, 1.7768, 1.7749, 1.9055, 1.7328, 1.8762, 1.8211, 1.7967, 2.3428,\n",
            "        1.7985, 1.7271, 1.7915, 1.9512, 1.8928, 1.9017, 1.8784, 1.9809, 1.8569,\n",
            "        1.7830, 1.8911, 1.8859, 1.7764, 1.9832, 1.8389, 1.7616, 1.8728, 1.8753,\n",
            "        1.9008, 1.8209, 1.7039, 1.7377, 1.7786, 1.6944, 1.7829, 1.7815, 1.7594,\n",
            "        1.8428, 1.9238, 2.0871, 1.8980, 1.8413, 1.8471, 1.8584, 1.7640, 1.8453,\n",
            "        1.7606, 1.9504, 1.9620, 1.8755, 1.9424, 1.8731, 1.8674, 1.9422, 1.8750,\n",
            "        1.9208, 1.7464, 1.8558, 1.6539, 2.0660, 2.0298, 1.9174, 1.8972, 1.7589,\n",
            "        1.7551, 1.9560, 1.7909, 1.7971, 1.7851, 1.7733, 1.8061, 1.7949, 1.8169,\n",
            "        1.8089, 1.8641, 2.1542, 1.7739, 1.7913, 1.8022, 1.7155, 1.7679, 1.7704,\n",
            "        1.6266, 1.8645, 1.9076, 1.8576, 1.6924, 1.8020, 1.7100, 1.7713, 1.8572,\n",
            "        1.7103, 2.0664, 1.9054, 1.9422, 1.8078, 1.7412, 1.6061, 1.9105, 1.8947,\n",
            "        1.7954, 1.8989, 1.8239, 1.7619, 1.7951, 1.8149, 1.8539, 1.8502, 1.7095,\n",
            "        2.1831, 1.8599, 1.8252, 1.8193, 1.8460, 1.7968, 1.6229, 1.8450, 1.8290,\n",
            "        1.8706, 1.9293, 1.6881, 1.9725, 1.8981, 1.8925, 1.8851, 1.8445, 1.9764,\n",
            "        2.0674, 1.8384, 1.8414, 1.8762, 1.7931, 1.7131, 1.9644, 1.7854, 1.9369,\n",
            "        1.8972, 1.8940, 1.8700, 1.7967, 1.8775, 1.9409, 1.7391, 1.7944, 1.9678,\n",
            "        1.7678, 1.6851, 1.9414, 1.9663, 1.9882, 1.7915, 1.8141, 1.8325, 2.1200,\n",
            "        1.9256, 2.3592, 2.0304, 1.9594, 1.7334, 1.9048, 1.8221, 1.7811, 1.9084,\n",
            "        1.8053, 1.9171, 1.9644, 1.8256, 1.6432, 1.9173, 1.9094, 1.9923, 1.7963,\n",
            "        1.9077, 1.7619, 2.1724, 1.7931, 1.7564, 1.8889, 1.9832, 1.9136, 1.8035,\n",
            "        1.8419, 1.8278, 1.8057, 1.9063, 1.8646, 1.7848, 1.8230, 1.7986, 1.7091,\n",
            "        1.7724, 1.7939, 1.7611, 1.9325, 2.0162, 1.7295, 2.0196, 1.8876, 1.8325,\n",
            "        1.8225, 1.7870, 1.9160, 1.7197, 1.7170, 1.9133, 1.7770, 1.9943, 1.8389,\n",
            "        1.8070, 1.8516, 1.7857, 1.9648, 1.9553, 1.9232, 1.8086, 1.8114, 1.7141,\n",
            "        1.8058, 1.8532, 1.9255, 1.7682, 1.8314, 1.8495, 1.8296, 1.8278, 1.8819,\n",
            "        1.7698, 1.7838, 1.7807, 1.9974, 1.6994, 1.9483, 1.7793, 1.8029, 2.2210,\n",
            "        1.6455, 1.8357, 2.1706, 1.9204, 1.7414, 1.7809, 1.8648, 1.9145, 1.8849,\n",
            "        1.8346, 1.9368, 1.8169, 2.2302, 1.8262, 2.0651, 1.9888, 1.8169, 1.8462,\n",
            "        1.9681, 1.8083, 1.8595, 1.8539, 1.7699, 1.9001, 1.7285, 1.7553, 1.8924,\n",
            "        1.7829, 1.9428, 1.8724, 1.7228, 2.0548, 1.7732, 1.8561, 1.7699, 1.9269,\n",
            "        1.8171, 2.4075, 1.7257, 1.7819, 1.7244, 1.8521, 1.8302, 1.8797, 1.7617,\n",
            "        1.9650, 1.9807, 1.7102, 1.7486, 1.8350, 1.9919, 1.8505, 1.9000, 1.8269,\n",
            "        1.9787, 1.7635, 1.6071, 1.7998, 1.9545, 1.7348, 1.7140, 1.8851, 1.7981,\n",
            "        1.9100, 1.8315, 1.7864, 1.9165, 1.8839, 1.9017, 1.9334, 1.7405, 1.7661,\n",
            "        1.8015, 1.9987, 1.7622, 1.9107, 1.8444, 1.7128, 1.8726, 1.8529, 1.9270,\n",
            "        1.8769, 1.7261, 1.8393, 1.9075, 1.7953, 1.8246, 1.7605, 2.0470, 1.9221,\n",
            "        1.9205, 1.8910, 1.7666, 1.6801, 1.8308, 1.8845, 1.8339, 1.8238, 1.7616,\n",
            "        1.6114, 1.8411, 1.7437, 1.8423, 1.9540, 1.7465, 1.7741, 1.8746, 1.8856,\n",
            "        1.7740, 1.7603, 1.7682, 1.8396, 1.6869, 1.8080, 1.8836, 1.8283, 1.8341,\n",
            "        1.8522, 1.9749, 1.8707, 1.7719, 1.8993, 1.8108, 1.8480, 1.8267, 1.8731,\n",
            "        1.9576, 1.8347, 1.9509, 1.9641, 1.7997, 1.7652, 1.9253, 1.7126, 1.7551,\n",
            "        1.9427, 1.8559, 1.9163, 1.7681, 1.7803, 1.8500, 1.8535, 1.8865, 1.7599,\n",
            "        2.0692, 1.8021, 1.7077, 1.8890, 1.9457, 1.8516, 1.7882, 1.8356, 1.8472,\n",
            "        1.6708, 1.7435, 1.9080, 1.9653, 2.0401, 1.8935, 1.8450, 1.7536, 1.7733,\n",
            "        1.8135, 1.8534, 1.9368, 1.7348, 1.8738, 1.9632, 1.9033, 1.7422, 1.7842,\n",
            "        1.8516, 2.0218, 1.7044, 1.8793, 1.8655, 1.8516, 1.8002, 1.8687, 1.8460,\n",
            "        1.7589, 1.8174, 1.9830, 1.9034, 2.1222, 1.8460, 1.9209, 1.8893, 1.9422,\n",
            "        1.8489, 1.8396, 1.9953, 2.0865, 1.8253, 1.7700, 1.8035, 1.7535, 1.8923,\n",
            "        1.8620, 1.8627, 1.7264, 1.8140, 1.9613, 1.8812, 1.8729, 2.0050, 1.7092,\n",
            "        1.7726, 1.9410, 1.8381, 1.8366, 1.7276, 1.8796, 1.7548, 1.9536, 1.8062,\n",
            "        1.8883, 2.0278, 1.8775, 1.9446, 1.8676, 1.8423, 1.7798, 1.9403, 1.8375,\n",
            "        2.0473, 1.9507, 1.8337, 1.8184, 1.7791, 1.8993, 1.8781, 1.8691, 1.8493,\n",
            "        1.7623, 1.9458, 1.7564, 1.7448, 1.8633, 1.6863, 1.8062, 1.8702, 2.0048,\n",
            "        1.8504, 1.8964, 1.9489, 1.8264, 1.9019, 1.8196, 1.9712, 1.8969, 1.8652,\n",
            "        1.8709, 1.6984, 1.8677, 1.8846, 1.9256, 1.8620, 1.6366, 1.8434, 1.7506,\n",
            "        1.8438, 1.5788, 1.9316, 1.9535, 1.7878, 1.7354, 2.0920, 1.9456])\n",
            "Parameter containing:\n",
            "tensor([0.2371, 0.3433, 0.3279, 0.4642, 0.2233, 0.2370, 0.2176, 0.3793, 0.3140,\n",
            "        0.2803, 0.2434, 0.2116, 0.2478, 0.2435, 0.2298, 0.3172, 0.2725, 0.6511,\n",
            "        0.2925, 0.2281, 0.2279, 0.4254, 0.2342, 0.3328, 0.2632, 0.2176, 0.3180,\n",
            "        0.3893, 0.1387, 0.2274, 0.3379, 0.0767, 0.2253, 0.2504, 0.1990, 0.1951,\n",
            "        0.2566, 0.3253, 0.2797, 0.3149, 0.2373, 0.2533, 0.1956, 0.3236, 0.2093,\n",
            "        0.2333, 0.2300, 0.5019, 0.2830, 0.1885, 0.3264, 0.2722, 0.2369, 0.2430,\n",
            "        0.3625, 0.2165, 0.4700, 0.3047, 0.3675, 0.2641, 0.1979, 0.2664, 0.3448,\n",
            "        0.2005, 0.2450, 0.4351, 0.2689, 0.1632, 0.3087, 0.1209, 0.2153, 0.1592,\n",
            "        0.2960, 0.1423, 0.2951, 0.2706, 0.2007, 0.2939, 0.2210, 0.2243, 0.2465,\n",
            "        0.3910, 0.4599, 0.5417, 0.2147, 0.3469, 0.2703, 0.2229, 0.3645, 0.2647,\n",
            "        0.2421, 0.2492, 0.1666, 0.2763, 0.2560, 0.2151, 0.3363, 0.2767, 0.2516,\n",
            "        0.2988, 0.2622, 0.3499, 0.3001, 0.3907, 0.3184, 0.2233, 0.2649, 0.2110,\n",
            "        0.2034, 0.2752, 0.2314, 0.3480, 0.2238, 0.2892, 0.1991, 0.2923, 0.3259,\n",
            "        0.0722, 0.3039, 0.3041, 0.3803, 0.2568, 0.2382, 0.3057, 0.2652, 0.1532,\n",
            "        0.2110, 0.2567, 0.3148, 0.2746, 0.1833, 0.1950, 0.1116, 0.2279, 0.3705,\n",
            "        0.2477, 0.2000, 0.3060, 0.2548, 0.2468, 0.3028, 0.1921, 0.2952, 0.1980,\n",
            "        0.2135, 0.1583, 0.1586, 0.3944, 0.2352, 0.3947, 0.2740, 0.2861, 0.1856,\n",
            "        0.2702, 0.2986, 0.1728, 0.2658, 0.2696, 0.2028, 0.1838, 0.3176, 0.6246,\n",
            "        0.2631, 0.3855, 0.2074, 0.2317, 0.4171, 0.2044, 0.2926, 0.3506, 0.2305,\n",
            "        0.2400, 0.1420, 0.1093, 0.2757, 0.3253, 0.2334, 0.1650, 0.4026, 0.2066,\n",
            "        0.1790, 0.3032, 0.5658, 0.3246, 0.3834, 0.3254, 0.1772, 0.2909, 0.2350,\n",
            "        0.2519, 0.1968, 0.2003, 0.3213, 0.4802, 0.2543, 0.2578, 0.3280, 0.2270,\n",
            "        0.3044, 0.2273, 0.2447, 0.2527, 0.4136, 0.2588, 0.3589, 0.2688, 0.2115,\n",
            "        0.2022, 0.3186, 0.3740, 0.1785, 0.2074, 0.2346, 0.3566, 0.2623, 0.2620,\n",
            "        0.2880, 0.1462, 0.1896, 0.2777, 0.1852, 0.3240, 0.2748, 0.2164, 0.3066,\n",
            "        0.1845, 0.3992, 0.1695, 0.4411, 0.2812, 0.2730, 0.2784, 0.1861, 0.3589,\n",
            "        0.1934, 0.3320, 0.3350, 0.2655, 0.2740, 0.3185, 0.2633, 0.2458, 0.2003,\n",
            "        0.2809, 0.3049, 0.2050, 0.2904, 0.2381, 0.3278, 0.3484, 0.4293, 0.2422,\n",
            "        0.2859, 0.1864, 0.2954, 0.5634, 0.2081, 0.3743, 0.2902, 0.3820, 0.3069,\n",
            "        0.2101, 0.2750, 0.2878, 0.1870, 0.3015, 0.1661, 0.2998, 0.3101, 0.2522,\n",
            "        0.2419, 0.1758, 0.2681, 0.2812, 0.1495, 0.2868, 0.3157, 0.2587, 0.2437,\n",
            "        0.1467, 0.5416, 0.2490, 0.2831, 0.2783, 0.1614, 0.1963, 0.2034, 0.2364,\n",
            "        0.2527, 0.1573, 0.3184, 0.2841, 0.1613, 0.1489, 0.2850, 0.1625, 0.3277,\n",
            "        0.4936, 0.2780, 0.3178, 0.1743, 0.2158, 0.2222, 0.2821, 0.4267, 0.2713,\n",
            "        0.1778, 0.3067, 0.2270, 0.1772, 0.3897, 0.2923, 0.4843, 0.2345, 0.2327,\n",
            "        0.2740, 0.2700, 0.2804, 0.4035, 0.1501, 0.3329, 0.3286, 0.2803, 0.2309,\n",
            "        0.1738, 0.3270, 0.3097, 0.1808, 0.2384, 0.2107, 0.3240, 0.3346, 0.2236,\n",
            "        0.2061, 0.2687, 0.2360, 0.3338, 0.2694, 0.3203, 0.2895, 0.1884, 0.1491,\n",
            "        0.3957, 0.5167, 0.3407, 0.1854, 0.1816, 0.2626, 0.1855, 0.2219, 0.1482,\n",
            "        0.2584, 0.2458, 0.2616, 0.2396, 0.2402, 0.2423, 0.3463, 0.2731, 0.1524,\n",
            "        0.2514, 0.2760, 0.1734, 0.2715, 0.4052, 0.2252, 0.3676, 0.3070, 0.3127,\n",
            "        0.1836, 0.4330, 0.2203, 0.2073, 0.2803, 0.2984, 0.2191, 0.3272, 0.2267,\n",
            "        0.2749, 0.3056, 0.4566, 0.2962, 0.3528, 0.3236, 0.4220, 0.2715, 0.2256,\n",
            "        0.2903, 0.1829, 0.3994, 0.2820, 0.2471, 0.1647, 0.3654, 0.4504, 0.2685,\n",
            "        0.2992, 0.2825, 0.2435, 0.2212, 0.4300, 0.4342, 0.1988, 0.2863, 0.3398,\n",
            "        0.2444, 0.2905, 0.2559, 0.2586, 0.1702, 0.1906, 0.2536, 0.2978, 0.2498,\n",
            "        0.3777, 0.2252, 0.2472, 0.2243, 0.1732, 0.2194, 0.2091, 0.2820, 0.2898,\n",
            "        0.2887, 0.3292, 0.1644, 0.2962, 0.3279, 0.2535, 0.2795, 0.2238, 0.2607,\n",
            "        0.1937, 0.2680, 0.2418, 0.5193, 0.2502, 0.3147, 0.2166, 0.2313, 0.2027,\n",
            "        0.1880, 0.2180, 0.3826, 0.3871, 0.2358, 0.3556, 0.2272, 0.3272, 0.3442,\n",
            "        0.3154, 0.1993, 0.3135, 0.2254, 0.3048, 0.2658, 0.3337, 0.2679, 0.2670,\n",
            "        0.2363, 0.4347, 0.1931, 0.1995, 0.2072, 0.3202, 0.2667, 0.2305, 0.2383,\n",
            "        0.2246, 0.2562, 0.2837, 0.4046, 0.2786, 0.2243, 0.1591, 0.1923, 0.1894,\n",
            "        0.2496, 0.1140, 0.3128, 0.3197, 0.3530, 0.2999, 0.2115, 0.4718, 0.2979,\n",
            "        0.3472, 0.2890, 0.4740, 0.2230, 0.3630, 0.4015, 0.2446, 0.1897, 0.1460,\n",
            "        0.1874, 0.2734, 0.2366, 0.3001, 0.2359, 0.2688, 0.3256, 0.2749, 0.2848,\n",
            "        0.2299, 0.3001, 0.4818, 0.3074, 0.3164, 0.3114, 0.3549, 0.2859])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Requires the classifier layer (fully connected layer)\n",
        "num_in_features = model.fc.in_features\n",
        "print(num_in_features)\n",
        "model.fc = nn.Linear(num_in_features, num_classes)\n",
        "print(model.fc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8Ttty72PKn-",
        "outputId": "7d38edb7-8de3-46ad-9e63-fe17437addac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512\n",
            "Linear(in_features=512, out_features=10, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Move Model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "0Wuf-CD4Qqbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print the model summary\n",
        "summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-InrwDfnV_yJ",
        "outputId": "2962fe48-8531-4cd7-bc8c-a39244462922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
            "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
            "             ReLU-14           [-1, 64, 56, 56]               0\n",
            "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
            "             ReLU-17           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
            "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
            "             ReLU-21          [-1, 128, 28, 28]               0\n",
            "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
            "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
            "             ReLU-26          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
            "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
            "             ReLU-30          [-1, 128, 28, 28]               0\n",
            "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
            "             ReLU-33          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
            "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
            "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
            "             ReLU-37          [-1, 256, 14, 14]               0\n",
            "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
            "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
            "             ReLU-42          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
            "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
            "             ReLU-46          [-1, 256, 14, 14]               0\n",
            "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
            "             ReLU-49          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
            "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
            "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-53            [-1, 512, 7, 7]               0\n",
            "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
            "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
            "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-58            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
            "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-62            [-1, 512, 7, 7]               0\n",
            "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-65            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
            "           Linear-68                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 11,181,642\n",
            "Trainable params: 8,398,858\n",
            "Non-trainable params: 2,782,784\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 62.79\n",
            "Params size (MB): 42.65\n",
            "Estimated Total Size (MB): 106.01\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Optimizer and Loss Function\n",
        "#Include parameters of both the classifier layer(Fully Connected Layer) and unfrozen layer(layer4)\n",
        "optimizer = optim.Adam([\n",
        "    {'params': model.fc.parameters()},\n",
        "    {'params': model.layer4.parameters()}\n",
        "], lr=learning_rate)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "4eN7BxM4WGTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training Function (Same as before)\n",
        "def train_model(model, train_loader, epochs):\n",
        "  model.train()     #Set the model into train mode\n",
        "  for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in train_loader:\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      optimizer.zero_grad()   #Reset the gradients\n",
        "      output = model(data)\n",
        "      loss = criterion(output, target)\n",
        "      loss.backward()     #Backpropagation\n",
        "      optimizer.step()    #Update weights\n",
        "      print(f'Loss of Item: {loss.item()}')\n",
        "      print(f'Batch Size of Tensor Data: {data.size(0)}')\n",
        "      total_loss += loss.item() * data.size(0)\n",
        "      pred = output.argmax(dim=1)     #Retrieve the indices of the maximum value\n",
        "      #Count Correct Prediction\n",
        "      correct += pred.eq(target).sum().item()     #eq() equalize the value of prediction and target, item() converts the value into python integer\n",
        "    print(f'Total Loss: {total_loss}')\n",
        "    print(f'Correct Predictions: {correct}')\n",
        "    avg_loss = total_loss / len(train_loader.dataset)\n",
        "    accuracy = correct / len(train_loader.dataset)\n",
        "    print(f'Epoch: [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "id": "pIHCXYNCaXvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_loader, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehYu2W_jduVt",
        "outputId": "716d2a70-1090-42d0-b6c0-be1799065ed2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss of Item: 2.6202597618103027\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.5329654216766357\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.0704424381256104\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.0701459646224976\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7780848145484924\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7508509159088135\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.650879442691803\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.9061073660850525\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 1.0104131698608398\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5890881419181824\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7249146699905396\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.8134014010429382\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.7557589411735535\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6513674855232239\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5578280687332153\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5508901476860046\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5452556610107422\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5918331742286682\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5330123901367188\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5949543714523315\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5928248763084412\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6428733468055725\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.45271438360214233\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6053421497344971\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6283982396125793\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6789858341217041\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.40170004963874817\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5311581492424011\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6953555941581726\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3991658091545105\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.593470573425293\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.62748783826828\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5618272423744202\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5577540993690491\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.48205962777137756\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.45412302017211914\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4910096526145935\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.567711591720581\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.39956387877464294\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.367903470993042\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.46029287576675415\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5376083254814148\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4024675190448761\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5226202011108398\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4349493682384491\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.47807949781417847\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4828468859195709\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6551765203475952\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.42775458097457886\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.47245267033576965\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4698520302772522\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.43728214502334595\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3815082609653473\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6046677827835083\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.43354836106300354\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4035566449165344\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.535582423210144\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5312695503234863\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.6363911032676697\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5209566354751587\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5432558059692383\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5410880446434021\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3525060713291168\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3966865837574005\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4052557051181793\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4351661205291748\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.33345651626586914\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4065084159374237\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5221157073974609\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5280272960662842\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4073370099067688\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.41442060470581055\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.43154701590538025\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.42416664958000183\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3364250361919403\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.39818382263183594\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2390281856060028\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5945025086402893\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.40284720063209534\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4953076243400574\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3799174427986145\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.36953532695770264\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.43786028027534485\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3925303518772125\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.34621456265449524\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.31459885835647583\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5578727722167969\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5313981771469116\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.45868980884552\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4521695077419281\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.44636985659599304\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3853270411491394\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.42111146450042725\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.43363723158836365\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.33047595620155334\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5045592784881592\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3123421370983124\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.47639939188957214\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3381415009498596\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.36765462160110474\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4810373783111572\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.29677754640579224\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3386325538158417\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.35019707679748535\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.380319207906723\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.40559905767440796\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.46195319294929504\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5105124711990356\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.36139237880706787\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.46037524938583374\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4425753653049469\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3612542450428009\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5183948874473572\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.33820635080337524\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.41692015528678894\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4275723993778229\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.35471418499946594\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4106453061103821\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.474599689245224\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.38929811120033264\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.36660027503967285\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.29653164744377136\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.36268022656440735\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.42410963773727417\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4048261046409607\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5101439356803894\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.47456851601600647\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4110982120037079\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.42327526211738586\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4322352707386017\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4562873840332031\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.41548749804496765\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3094859719276428\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.37730199098587036\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3077014088630676\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5343500971794128\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.40983888506889343\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5010945796966553\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5310424566268921\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5435091853141785\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2761003375053406\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5105116367340088\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4675567150115967\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.48851868510246277\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3168709874153137\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.47822678089141846\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.39096856117248535\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5656207799911499\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.31615951657295227\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.535516619682312\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.40289098024368286\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.34747323393821716\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.426828533411026\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3789352476596832\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3558182716369629\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.40776336193084717\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.287010133266449\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.43877798318862915\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.44040054082870483\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.32067951560020447\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3595709204673767\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4024089276790619\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.46853557229042053\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3333335518836975\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.43613943457603455\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4103128910064697\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4392054080963135\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3201899230480194\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4193628132343292\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3427906036376953\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.45046988129615784\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3375963568687439\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2666625380516052\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2659113109111786\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3996340036392212\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.30018311738967896\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3580981492996216\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.36925750970840454\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.37415608763694763\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.49808505177497864\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4637213945388794\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4116605818271637\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.327175110578537\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3694963753223419\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.27231818437576294\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4304494261741638\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4597874581813812\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.35888656973838806\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3813033401966095\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4008445143699646\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3275895416736603\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.35660940408706665\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.44004783034324646\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3951268494129181\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.27119988203048706\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4069783687591553\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3343695104122162\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.44700920581817627\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.25829315185546875\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2653806805610657\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.29858675599098206\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.46433931589126587\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3965858817100525\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.25119078159332275\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3810749351978302\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.41706010699272156\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2908916473388672\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.31719857454299927\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.47788503766059875\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5330637097358704\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3236592710018158\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3137441873550415\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3436400592327118\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4310400187969208\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3831157684326172\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.47755756974220276\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.295635849237442\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2636526823043823\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3657497465610504\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3016762137413025\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2650732696056366\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.41148099303245544\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3615191578865051\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.36736026406288147\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.42593103647232056\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.28065139055252075\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2312583029270172\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.35390689969062805\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4164324700832367\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.45073339343070984\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.46113866567611694\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.30498629808425903\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4231600761413574\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3381975591182709\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.299395352602005\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.40382999181747437\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3804929554462433\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.338647723197937\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.36271610856056213\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.38556814193725586\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.38001567125320435\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3206738829612732\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.37303224205970764\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3044605255126953\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.26752036809921265\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2853728234767914\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.37380146980285645\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.43037867546081543\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2537727653980255\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.536439061164856\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.43013450503349304\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.33804064989089966\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3751543462276459\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.39805930852890015\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3078615963459015\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.40306323766708374\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2162228375673294\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3916122317314148\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3445550203323364\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.22860807180404663\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4349838197231293\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.26692995429039\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.29556894302368164\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3423822224140167\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.36188384890556335\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2766130268573761\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.31300896406173706\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2084243893623352\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3168603777885437\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.33314210176467896\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2419903576374054\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.34035834670066833\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.38194945454597473\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.33683285117149353\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4247879683971405\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.30669718980789185\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.28847330808639526\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3405625522136688\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5002474784851074\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.30383050441741943\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.234614759683609\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4227466583251953\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2658176124095917\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3259628415107727\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.287516713142395\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4317205250263214\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.33124545216560364\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3619040250778198\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3044845461845398\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.23144230246543884\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.42092472314834595\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.38998377323150635\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.22120219469070435\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3349475562572479\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.28732872009277344\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3806166350841522\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4066793918609619\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4645252525806427\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.26969125866889954\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.325762540102005\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3428717255592346\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4205697178840637\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.15374796092510223\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.29162460565567017\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4428856670856476\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3211432099342346\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2582879662513733\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.29531893134117126\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2837006747722626\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.28143468499183655\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.34641870856285095\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.44210031628608704\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.252115398645401\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.5548086762428284\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.37339383363723755\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3341258764266968\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.395011842250824\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2938798666000366\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3374119997024536\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.21137148141860962\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3074144721031189\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2345447838306427\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.30392467975616455\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3028322458267212\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3962137699127197\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4079878628253937\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.32513150572776794\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3491182029247284\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.25073546171188354\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2729581296443939\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.36160022020339966\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.26736393570899963\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.30398085713386536\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.28324076533317566\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.34445515275001526\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.19553609192371368\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2861841022968292\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3242993950843811\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2504967451095581\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3654828667640686\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2504769265651703\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.34783026576042175\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3860524296760559\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.34487590193748474\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2771036624908447\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2176305502653122\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.32329460978507996\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3655203878879547\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2837038040161133\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2730291187763214\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.48287034034729004\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.30926549434661865\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3773757517337799\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.20947492122650146\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2816027104854584\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3458107113838196\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2919290065765381\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3118014931678772\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.41147124767303467\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4082687497138977\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2134873867034912\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.37979596853256226\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.30101191997528076\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.38853949308395386\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.20851872861385345\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.402578204870224\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2518341541290283\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3777904212474823\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3193400204181671\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.23465318977832794\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4077453017234802\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.31137073040008545\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.26893696188926697\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.28643444180488586\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2853483557701111\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.31620076298713684\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2395254671573639\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.39571788907051086\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.23007887601852417\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.222652405500412\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3660719096660614\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.39756059646606445\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.47136610746383667\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.42961597442626953\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3102896213531494\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.20941345393657684\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.23138615489006042\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.25971096754074097\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3430187404155731\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3208235502243042\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.4065311551094055\n",
            "Batch Size of Tensor Data: 80\n",
            "Total Loss: 20255.049479484558\n",
            "Correct Predictions: 43018\n",
            "Epoch: [1/5], Loss: 0.4051, Accuracy: 0.8604\n",
            "Loss of Item: 0.20746135711669922\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.21035407483577728\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12895072996616364\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12266987562179565\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.33484357595443726\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1728149801492691\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11809805035591125\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.19960880279541016\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1437392681837082\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1492515504360199\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.17638367414474487\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10155419260263443\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12214116752147675\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.25717365741729736\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.22901949286460876\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11928719282150269\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1003330647945404\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.16547098755836487\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1604139357805252\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1621636003255844\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08271389454603195\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09885470569133759\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.16524706780910492\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11454402655363083\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08342571556568146\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.18674486875534058\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1783326119184494\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.20576703548431396\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.24187596142292023\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1921524852514267\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12350744009017944\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1321973204612732\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.22300474345684052\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.17107972502708435\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.18548862636089325\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1454089879989624\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1426539123058319\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12187884747982025\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.20339056849479675\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.16132433712482452\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.16873937845230103\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08829022198915482\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11283242702484131\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12841835618019104\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14202940464019775\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.15258394181728363\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.20713907480239868\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06845401227474213\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14017623662948608\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.17586421966552734\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11326035857200623\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.17702507972717285\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.28896045684814453\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11358703672885895\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14199404418468475\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10772842913866043\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.15751425921916962\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.20767727494239807\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.20759618282318115\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.16661687195301056\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12409167736768723\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12398733198642731\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12130074948072433\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10096880048513412\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12163423746824265\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11920764297246933\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1164795458316803\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09719813615083694\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.16133341193199158\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.19822868704795837\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09758498519659042\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12347596883773804\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.18804039061069489\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1313946545124054\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.15021517872810364\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.13202406466007233\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.28621575236320496\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09870006889104843\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.16672950983047485\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.15903551876544952\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11903262883424759\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.16171811521053314\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.20891225337982178\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.16458402574062347\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.15126794576644897\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.17295929789543152\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11460176855325699\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1993904411792755\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.15677520632743835\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.15817967057228088\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1279078722000122\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.20665180683135986\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.15306386351585388\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.17641496658325195\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.13324742019176483\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11675800383090973\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1401827335357666\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.23441334068775177\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.23747888207435608\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.23818062245845795\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0915110632777214\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.18169791996479034\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11301989108324051\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0799155905842781\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14232133328914642\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1669897586107254\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14311064779758453\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08678829669952393\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.16848121583461761\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1814064085483551\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14199021458625793\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.25004342198371887\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.20396144688129425\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.15911078453063965\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14259324967861176\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.20297078788280487\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.13721585273742676\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.13276571035385132\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14491967856884003\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.20686596632003784\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.16353514790534973\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.21538765728473663\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14152352511882782\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.13467787206172943\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.19086644053459167\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09769288450479507\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.17905296385288239\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12078577280044556\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.21354937553405762\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.17251671850681305\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1925460249185562\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.19321304559707642\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11152024567127228\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.24437256157398224\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.15081341564655304\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2087593972682953\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14831699430942535\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11724704504013062\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.15935368835926056\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.20339293777942657\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.13260219991207123\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.15513843297958374\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.22928014397621155\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.18025469779968262\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12401270866394043\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.20630572736263275\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.17190198600292206\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10466111451387405\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.36305516958236694\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1890813708305359\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.18680386245250702\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.35111868381500244\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.23595735430717468\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.16620267927646637\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11798158288002014\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0973399356007576\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14693056046962738\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14461642503738403\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1970599889755249\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10236851125955582\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10032553970813751\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.24741779267787933\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.25274479389190674\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1395830363035202\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.29005399346351624\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.21544918417930603\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.27932050824165344\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.19924332201480865\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.15859416127204895\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.15179814398288727\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14008456468582153\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11530674248933792\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2175838053226471\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.17863699793815613\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.166404128074646\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.18242467939853668\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1367238461971283\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10638818144798279\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14485259354114532\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.16806088387966156\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.192163348197937\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12880131602287292\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.19580493867397308\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09331393986940384\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12960638105869293\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.17255213856697083\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.17644302546977997\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14606475830078125\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.16199204325675964\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1824694275856018\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10900316387414932\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.16122102737426758\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.26663246750831604\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14661534130573273\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12555080652236938\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1398817002773285\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1629146784543991\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14374001324176788\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.20161904394626617\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12836997210979462\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1433718353509903\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.16241535544395447\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.24131213128566742\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14191585779190063\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.13465049862861633\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10911595076322556\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2876885235309601\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.19753791391849518\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.31920579075813293\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3579552173614502\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14798414707183838\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.24149425327777863\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1497899293899536\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1474277526140213\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11673390865325928\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09984806925058365\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3036419451236725\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1767580807209015\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.19783581793308258\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12033272534608841\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10258032381534576\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.26262301206588745\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.20773811638355255\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.16964831948280334\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1442604809999466\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2072920948266983\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2526606023311615\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.20169098675251007\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.26629945635795593\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.29757195711135864\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.23807251453399658\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1573694944381714\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.18545560538768768\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.16077105700969696\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.20593200623989105\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12668806314468384\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14596574008464813\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.19162854552268982\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.186907097697258\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1377822905778885\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.16677457094192505\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10606297850608826\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.17766545712947845\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11587479710578918\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.18433637917041779\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.28743940591812134\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2680746912956238\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.15125799179077148\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2038622498512268\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1227441281080246\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.17666307091712952\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10030676424503326\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2257681041955948\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.16019447147846222\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.17583926022052765\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.23719310760498047\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1920316517353058\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.18918396532535553\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.15560805797576904\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14384986460208893\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.17269766330718994\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.23475879430770874\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.19844095408916473\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.15393276512622833\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.15557125210762024\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.17120690643787384\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.196457177400589\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14044873416423798\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14173845946788788\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.13068035244941711\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.18856225907802582\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10284589231014252\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.16160084307193756\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.17214412987232208\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.22760702669620514\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1565708965063095\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12826412916183472\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2052861452102661\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1395719349384308\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1610591858625412\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11457716673612595\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.19379229843616486\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.24735458195209503\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.17404188215732574\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.18368138372898102\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1541205495595932\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.19096706807613373\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1453925222158432\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14921078085899353\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.19739896059036255\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1422339230775833\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3744727671146393\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12712039053440094\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.26428717374801636\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.22004005312919617\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.16277095675468445\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2194257527589798\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14774881303310394\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06646887958049774\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1964949071407318\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12858538329601288\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.24963396787643433\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.18037757277488708\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.15613238513469696\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.34570908546447754\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.24169716238975525\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.15115171670913696\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1970096379518509\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1662459373474121\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.25707995891571045\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.20784549415111542\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.16615422070026398\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.24327856302261353\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2545989155769348\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2938041388988495\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.17250008881092072\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.18303282558918\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2547284960746765\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.27067214250564575\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12654425203800201\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.21279099583625793\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1298532634973526\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.15259309113025665\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11062514036893845\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1916780322790146\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.16264908015727997\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2814464867115021\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.18307434022426605\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1815650910139084\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.17158900201320648\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.150956928730011\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.20321546494960785\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.20242059230804443\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1899040937423706\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3153221607208252\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.13109366595745087\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.23755595088005066\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14349733293056488\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.151298388838768\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.13740696012973785\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.20143188536167145\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.21030846238136292\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14798051118850708\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2141120731830597\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.24669267237186432\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1713363528251648\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14820367097854614\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1826213151216507\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.19154612720012665\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.19783002138137817\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2485741525888443\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.13649344444274902\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.21846698224544525\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12177986651659012\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.22040636837482452\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.221231609582901\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.31993725895881653\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2164071649312973\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.18419408798217773\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.26389628648757935\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.35318559408187866\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.3108566403388977\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1327235996723175\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12000390142202377\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.183014914393425\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.18113160133361816\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.24975652992725372\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.22045643627643585\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2845900058746338\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1311691403388977\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.18754297494888306\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08734146505594254\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2994404435157776\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.15571466088294983\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2679680287837982\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2151108682155609\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2488916963338852\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.23028464615345\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.18604826927185059\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.22383075952529907\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.20667201280593872\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.18314310908317566\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.17982527613639832\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.31825223565101624\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14632965624332428\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14890117943286896\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.21722550690174103\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.17955946922302246\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.21643999218940735\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.27825745940208435\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.21790125966072083\n",
            "Batch Size of Tensor Data: 80\n",
            "Total Loss: 8860.283969402313\n",
            "Correct Predictions: 46970\n",
            "Epoch: [2/5], Loss: 0.1772, Accuracy: 0.9394\n",
            "Loss of Item: 0.05530213564634323\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.18266341090202332\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09990750998258591\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.056658387184143066\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07502570748329163\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06735201179981232\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.103866808116436\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.15058119595050812\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08522570133209229\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.17077872157096863\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.19292551279067993\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1246471181511879\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.21070949733257294\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07789869606494904\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07760041952133179\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07573729008436203\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10883437842130661\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07043007016181946\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0585143007338047\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08149182796478271\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07059027999639511\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.13518717885017395\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.027956318110227585\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10530901700258255\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07885217666625977\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10150555521249771\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06897591799497604\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11307238042354584\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07032588869333267\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08036313205957413\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11372294276952744\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0839841365814209\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12526772916316986\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10831086337566376\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10112304240465164\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06088301166892052\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05784386023879051\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06879164278507233\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.030233629047870636\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14556927978992462\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.052219703793525696\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05409116670489311\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0678272545337677\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08768656849861145\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08302080631256104\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.056197937577962875\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.114803247153759\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03644809126853943\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1295151263475418\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07552900165319443\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10559603571891785\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09827876091003418\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0886736661195755\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07766803354024887\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06888498365879059\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.047598134726285934\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06527746468782425\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06191258132457733\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0803065225481987\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.13725249469280243\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0556323267519474\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09100596606731415\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06241888552904129\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.052655287086963654\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.068910151720047\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08666648715734482\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05072512850165367\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03899737074971199\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07278132438659668\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09010671079158783\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0839315876364708\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07760085165500641\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06477050483226776\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.032691314816474915\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04782101511955261\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03374267742037773\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09636424481868744\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04990524426102638\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06020401418209076\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04038360342383385\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02602487988770008\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08893031626939774\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03306553512811661\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.051246337592601776\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06147703155875206\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03138967230916023\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.020087704062461853\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.039413951337337494\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.043106064200401306\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.061028722673654556\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06285393983125687\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05732782185077667\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07256947457790375\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06743992865085602\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08443412184715271\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06234817951917648\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.018356818705797195\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04769530147314072\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07640375196933746\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03908969461917877\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06847673654556274\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07054980844259262\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04001407325267792\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05484575033187866\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09853845834732056\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07783479988574982\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06320098787546158\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10971029102802277\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07260177284479141\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.050388362258672714\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04401146620512009\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.041176967322826385\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.01883845031261444\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08246104419231415\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06795652210712433\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.054007336497306824\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.033075377345085144\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07306231558322906\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0588940754532814\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.016843201592564583\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08806204795837402\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04235609248280525\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.053146738559007645\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14666900038719177\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05100499466061592\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06940316408872604\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03884170576930046\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1175372526049614\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09431228041648865\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06989678740501404\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.028372135013341904\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.029449457302689552\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03613670915365219\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12313653528690338\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.047157663851976395\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04890921711921692\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.032128315418958664\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04168299585580826\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14818818867206573\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07287412881851196\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08202406018972397\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04711190238595009\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08464546501636505\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12231525778770447\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0997765064239502\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06913430988788605\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04789455235004425\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08811694383621216\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.037867166101932526\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.058499861508607864\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.042678382247686386\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10124004632234573\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08643552660942078\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06848657876253128\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09015677869319916\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03575774282217026\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03255118802189827\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.028390895575284958\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04755184426903725\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03127286955714226\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07105883210897446\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05466919019818306\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.055558942258358\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08173421025276184\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08956706523895264\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.052233580499887466\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.16705158352851868\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04227659851312637\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04325368255376816\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07964873313903809\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04407482221722603\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08276892453432083\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08494167774915695\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04653364047408104\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.053166463971138\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10945175588130951\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0885472446680069\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11351409554481506\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07824710011482239\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09907637536525726\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11729691922664642\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07079499959945679\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0736008808016777\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11051182448863983\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08601146191358566\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08026392757892609\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08370519429445267\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12461099773645401\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10300921648740768\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09199371933937073\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12568898499011993\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.13110658526420593\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10456623136997223\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07794862240552902\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1852704882621765\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.17627984285354614\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.065508633852005\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05402132868766785\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11591086536645889\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04462470859289169\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14879050850868225\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.13207519054412842\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08273819088935852\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.054063521325588226\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06021907553076744\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04656567797064781\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.18637455999851227\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05313926562666893\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1368522197008133\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1977432519197464\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0877576470375061\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.18380656838417053\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07587264478206635\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06729919463396072\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07468894124031067\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11586432158946991\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08027513325214386\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1399884819984436\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0774933248758316\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12345173954963684\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0998549833893776\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09267590939998627\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11096060276031494\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07281274348497391\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11930322647094727\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.2428058385848999\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10454925149679184\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11604584753513336\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09087534248828888\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07239919155836105\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07226598262786865\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07660230994224548\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09347522258758545\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05509684607386589\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12222212553024292\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1261299103498459\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08516253530979156\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04949444159865379\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09149648994207382\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12298984080553055\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12475879490375519\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1266312599182129\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07860468327999115\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09208504110574722\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.059790175408124924\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1856364905834198\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06792084127664566\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09293479472398758\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10114864259958267\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.044975414872169495\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09564860165119171\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0723441019654274\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.18321648240089417\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06809285283088684\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07197395712137222\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11164792627096176\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10873375833034515\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14636822044849396\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04067663103342056\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09961766004562378\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08672717958688736\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05615219473838806\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12508524954319\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12140825390815735\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.181778222322464\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08087687194347382\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11668506264686584\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06716887652873993\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.106735460460186\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07220000773668289\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10395535826683044\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0845169797539711\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.13112232089042664\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06669122725725174\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1130203828215599\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08277977257966995\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07743261009454727\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06458225846290588\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10452589392662048\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06276459246873856\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12225204706192017\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10082639753818512\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.22244331240653992\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.029324304312467575\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07360925525426865\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.13145889341831207\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06718650460243225\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11708998680114746\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04360385611653328\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11816155910491943\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10843528807163239\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05033722519874573\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.15901415050029755\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06152554601430893\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12728352844715118\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08068510890007019\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.13664095103740692\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12446112930774689\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08950105309486389\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07734489440917969\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08667757362127304\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07376352697610855\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0754760131239891\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10627614706754684\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.030654426664114\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06160195916891098\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07683569937944412\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05276399850845337\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1380327343940735\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05684250220656395\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14598779380321503\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11575325578451157\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.18890541791915894\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04864627122879028\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11099178344011307\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09419552236795425\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.088341124355793\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.13884620368480682\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03627774491906166\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.19172056019306183\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08000589162111282\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04098281264305115\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08967272937297821\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05298097804188728\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.22861334681510925\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11405284702777863\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06885506212711334\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0384083017706871\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14402034878730774\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09361602365970612\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12877078354358673\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11426135152578354\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06893020868301392\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14716506004333496\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08631297945976257\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14720332622528076\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.16719740629196167\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07303739339113235\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10450823605060577\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.15556339919567108\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1235620304942131\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06766587495803833\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06828758120536804\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.091584213078022\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09444425255060196\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10985608398914337\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.021586252376437187\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.15831413865089417\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1943659782409668\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09583265334367752\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08374454081058502\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08911079168319702\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10159049183130264\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1574901044368744\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1349591612815857\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05378309264779091\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09831976890563965\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10738923400640488\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1295333206653595\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07174714654684067\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.136349156498909\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07452785968780518\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07679571211338043\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10403553396463394\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.15166276693344116\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08660394698381424\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1004122644662857\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.24313335120677948\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1378646343946457\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12709401547908783\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1476721465587616\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08816678822040558\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12113490700721741\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.20896583795547485\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1035073846578598\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1534471958875656\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09802427142858505\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10602278262376785\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06146073713898659\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14741933345794678\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08562957495450974\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.17409725487232208\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05783572420477867\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12366840988397598\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.14105844497680664\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08604510873556137\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07349961251020432\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1698073446750641\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08530832082033157\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12159211933612823\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.17644555866718292\n",
            "Batch Size of Tensor Data: 80\n",
            "Total Loss: 4490.711797952652\n",
            "Correct Predictions: 48462\n",
            "Epoch: [3/5], Loss: 0.0898, Accuracy: 0.9692\n",
            "Loss of Item: 0.07837184518575668\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.030630212277173996\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03790053725242615\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08620832115411758\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02253792993724346\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05808144062757492\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03630350902676582\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09171655029058456\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0584382526576519\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05498269945383072\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0523422472178936\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07470164448022842\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05007348954677582\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05143916234374046\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04673736169934273\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.030226947739720345\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0259400624781847\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06872881203889847\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12347539514303207\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02217533253133297\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04371102526783943\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08114174008369446\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03839913755655289\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07746346294879913\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.012661467306315899\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.048273686319589615\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03268810734152794\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.020912732928991318\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05798906460404396\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.026451807469129562\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03618542477488518\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04144059121608734\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05732136219739914\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.047682471573352814\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06306925415992737\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.028717463836073875\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04545239731669426\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.024205239489674568\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03976075351238251\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02930920012295246\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.024424904957413673\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.017302222549915314\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0354783833026886\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.01756378263235092\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04979253560304642\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.028920616954565048\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.025249846279621124\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.012135066092014313\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.037593401968479156\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.050040103495121\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02636018581688404\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.023042399436235428\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.056962352246046066\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.020840652287006378\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.054158762097358704\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.016117248684167862\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09875230491161346\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.033403486013412476\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03505578637123108\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05011805146932602\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02795843593776226\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04433364421129227\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.050399407744407654\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06766343861818314\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04589954763650894\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.005155713763087988\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.030046124011278152\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.027430953457951546\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.015703978016972542\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02092168480157852\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.041098952293395996\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06952381134033203\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.017809737473726273\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04034610092639923\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.019795047119259834\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.020454227924346924\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03796521574258804\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.017696229740977287\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06233895570039749\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07391146570444107\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.033412497490644455\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0457928329706192\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08636964857578278\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.040438175201416016\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04045073315501213\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.020785536617040634\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.062233224511146545\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02420528046786785\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08424852788448334\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.021151680499315262\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.013212732039391994\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05428978428244591\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08808282762765884\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0335649698972702\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.034485504031181335\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.010884977877140045\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05620022490620613\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10210920870304108\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.015142368152737617\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.023687783628702164\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03691069409251213\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05416020005941391\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.026572201400995255\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.047238968312740326\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06500056385993958\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.027849026024341583\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04952032119035721\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.011144437827169895\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03554707393050194\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04656611755490303\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.026065928861498833\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.017144327983260155\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0528964027762413\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04312547296285629\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03923141211271286\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.040278177708387375\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02101895585656166\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02698809653520584\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04882976412773132\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.014178678393363953\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03497534990310669\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0813138484954834\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0383220873773098\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.009740665555000305\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08170640468597412\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05258551612496376\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.035657137632369995\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05742380768060684\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11498301476240158\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02224995195865631\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.014522507786750793\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.01734062097966671\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.010276232846081257\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0387963131070137\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.020172761753201485\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.030615316703915596\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06297311931848526\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.040677741169929504\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05394580587744713\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06914644688367844\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.010882958769798279\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03328356146812439\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.031459782272577286\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04617657512426376\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03461441397666931\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.048520516604185104\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0336347296833992\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03531680628657341\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05317885801196098\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0317128486931324\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04289348050951958\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03599662333726883\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.029053444042801857\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0595807209610939\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.047988228499889374\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02184605412185192\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04033200442790985\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06561420857906342\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.057295553386211395\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07004926353693008\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0979677066206932\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.023144565522670746\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05091297999024391\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.058547984808683395\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.032804224640131\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05209833011031151\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.12131049484014511\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04292057082056999\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.021935468539595604\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.030769001692533493\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.023939356207847595\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.032100263983011246\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09018103778362274\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.054098550230264664\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.084593266248703\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.028973985463380814\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02662505768239498\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04799759015440941\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.046535804867744446\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03546808660030365\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06846790015697479\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04422858729958534\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07854260504245758\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0362682044506073\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.16637876629829407\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06296124309301376\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.006624208297580481\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.023688143119215965\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06835803389549255\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.057227954268455505\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03745391592383385\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05592348799109459\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05212792754173279\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.051150523126125336\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04458340257406235\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.049542516469955444\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.020284902304410934\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03629545494914055\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0857776552438736\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03236483782529831\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.021819161251187325\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.042277850210666656\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.01738375797867775\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06220051646232605\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08044342696666718\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.042191311717033386\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.006420151796191931\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.13727857172489166\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.01855851523578167\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03304050490260124\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05148877575993538\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0565892830491066\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.032493576407432556\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03810868039727211\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04789314791560173\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.025991290807724\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.01809459924697876\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.024357790127396584\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1485159695148468\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.058705177158117294\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04756075516343117\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.024773357436060905\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.013708589598536491\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06202182546257973\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07620389759540558\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10013613849878311\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.024656236171722412\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1368466466665268\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06943605840206146\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02639215625822544\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.019617123529314995\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.058314770460128784\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10000947117805481\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.032721612602472305\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.046585336327552795\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.048738379031419754\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10967826098203659\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.042509064078330994\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0300429854542017\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10223624110221863\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06593886017799377\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.019715268164873123\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07778041064739227\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.033799223601818085\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.015205021016299725\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10156241804361343\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06364607065916061\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03568650782108307\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1053609549999237\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06640271097421646\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09409799426794052\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09181352704763412\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0628775954246521\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08233276754617691\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04566990211606026\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.026948830112814903\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08923748135566711\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06509631872177124\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02612699568271637\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09824834764003754\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03187600523233414\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04672214388847351\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04368202015757561\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09322609752416611\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0361262783408165\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08836065977811813\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06749353557825089\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10676323622465134\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04196930676698685\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04194965213537216\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10336006432771683\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07599295675754547\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.012847636826336384\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0641489326953888\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10451508313417435\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10940476506948471\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0983913391828537\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03562647104263306\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0733502134680748\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04613178223371506\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05321965366601944\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.026827910915017128\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03272382915019989\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09492839127779007\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.13245965540409088\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04968433082103729\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.045570582151412964\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03518230468034744\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.019299788400530815\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0838765949010849\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10542614758014679\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05982493981719017\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.057266440242528915\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05881309509277344\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03226840868592262\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08259695768356323\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.21166200935840607\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05059533193707466\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05821723863482475\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03888304531574249\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.033976681530475616\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05052764713764191\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04837165027856827\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0795583501458168\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07795614749193192\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.051183827221393585\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06714276969432831\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06194564700126648\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09412635862827301\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07831604033708572\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03188171610236168\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03148676082491875\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.030925288796424866\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10189568996429443\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.020101383328437805\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03883029893040657\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.13276606798171997\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08359149843454361\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07823237776756287\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.027201242744922638\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10794238746166229\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0837787538766861\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10464688390493393\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02981182560324669\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07582974433898926\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.13432884216308594\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06582790613174438\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.025451544672250748\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07703849673271179\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07703866809606552\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07662376761436462\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08983473479747772\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03705219179391861\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08110277354717255\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08684118837118149\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08564603328704834\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06871522963047028\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.047127850353717804\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.047495219856500626\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03324589133262634\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06250825524330139\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05604126676917076\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08587264269590378\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06770497560501099\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07705987989902496\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1391850709915161\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05365873500704765\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.019285982474684715\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11361060291528702\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09457594901323318\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07998456805944443\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.13398291170597076\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07597760111093521\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09949016571044922\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07351631671190262\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.17799949645996094\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10450687259435654\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03324994444847107\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05996266007423401\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10602887719869614\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03801840916275978\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04227973520755768\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05459458753466606\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07426118850708008\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1347658485174179\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06171738728880882\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04371079057455063\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.042333051562309265\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06483916193246841\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07738343626260757\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.15889285504817963\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07095501571893692\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.024363551288843155\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09617288410663605\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.062448643147945404\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09768559783697128\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0645274668931961\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.057158757001161575\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07283036410808563\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1070370152592659\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06002701446413994\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1372700035572052\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04415220767259598\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06750740855932236\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07138235121965408\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05825209990143776\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07898055762052536\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10125207901000977\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0525231808423996\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.029205605387687683\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0967588871717453\n",
            "Batch Size of Tensor Data: 80\n",
            "Total Loss: 2746.126572430134\n",
            "Correct Predictions: 49037\n",
            "Epoch: [4/5], Loss: 0.0549, Accuracy: 0.9807\n",
            "Loss of Item: 0.016750389710068703\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08981268852949142\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10016871243715286\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07075554877519608\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08110574632883072\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1026763767004013\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06780088692903519\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08824125677347183\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04753083735704422\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.027065306901931763\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.020968155935406685\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.057658176869153976\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09667511284351349\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.018367672339081764\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.01196572370827198\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05682619661092758\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.014504819177091122\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03605705499649048\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03874857723712921\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.035279035568237305\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.13685272634029388\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04938509315252304\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.017956847324967384\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.014656967483460903\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03801834583282471\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.034078821539878845\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05399233102798462\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03092782385647297\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06578970700502396\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0782170295715332\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0793173760175705\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08091967552900314\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.027236342430114746\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03622853383421898\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.022983811795711517\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.027620907872915268\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.00955960527062416\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.024208953604102135\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.028012102469801903\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10188306868076324\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.01784832403063774\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.033671483397483826\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07295693457126617\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.038778793066740036\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.011781557463109493\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06414765864610672\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.027227183803915977\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05610443279147148\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07101471722126007\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.050401389598846436\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.039617378264665604\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.016934379935264587\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.007926095277071\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.020423367619514465\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06891591101884842\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05641595274209976\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.010127879679203033\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02922714501619339\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03402486443519592\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.015293480828404427\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05854365602135658\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04302116110920906\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08459892123937607\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0105879632756114\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.026475204154849052\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03613582253456116\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0198360625654459\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05984890088438988\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.01865328475832939\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08340825140476227\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02039002999663353\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03213607892394066\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.019087795168161392\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06646993011236191\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02386835217475891\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.030556276440620422\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02809898741543293\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04179929196834564\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.033955108374357224\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03456615284085274\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.058055367320775986\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.022143736481666565\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.020903758704662323\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.015240224078297615\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.041014526039361954\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0663517639040947\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.023060623556375504\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02179999276995659\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0562078021466732\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05213598906993866\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07775039970874786\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.021270234137773514\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06149346008896828\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0630853995680809\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03828462213277817\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.013472053222358227\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07062497735023499\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.008201632648706436\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.030602889135479927\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.021964816376566887\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.024562565609812737\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.026930099353194237\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04466427490115166\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07034581899642944\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.026916326954960823\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.014684837311506271\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04334353283047676\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09778022766113281\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.016375357285141945\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05117913708090782\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07676049321889877\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.029140984639525414\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02272193320095539\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04155374690890312\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03594352304935455\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08068159967660904\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07235546410083771\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.020790351554751396\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1009736880660057\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09858477115631104\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03643861040472984\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.01204323023557663\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.015473669394850731\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.01711667701601982\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09055275470018387\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08504576981067657\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07763160765171051\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07541846483945847\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.025269854813814163\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06719104945659637\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.010450314730405807\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04488654434680939\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.048896487802267075\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.01968006044626236\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.024616366252303123\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06241537630558014\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.061291009187698364\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.015697835013270378\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.024346189573407173\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02534784935414791\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03568476438522339\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04497061297297478\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.048979174345731735\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.028103839606046677\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.00871682446449995\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08844253420829773\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03597409278154373\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02164393663406372\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08657098561525345\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04807363450527191\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.019091380760073662\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.025321055203676224\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06772103905677795\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05744446441531181\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04262223094701767\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06175591051578522\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.033662255853414536\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02109381929039955\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.061888862401247025\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07617976516485214\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03975064679980278\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.01299571618437767\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02609374187886715\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.024346627295017242\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10821911692619324\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.024482455104589462\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.023988308385014534\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.045228634029626846\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02460387535393238\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07049008458852768\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.008029447868466377\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.021492501720786095\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.016040297225117683\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.011103450320661068\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0269298255443573\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.009759508073329926\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06192445755004883\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03698042780160904\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02215266413986683\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.009134283289313316\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02403232827782631\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.052604760974645615\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.023076290264725685\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04274079576134682\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03599689155817032\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.008903133682906628\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.046629924327135086\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.040016889572143555\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0634533166885376\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05181944742798805\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03562254458665848\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.010527830570936203\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1372080296278\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0601104311645031\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.034797027707099915\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.027340799570083618\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.053805459290742874\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11699795722961426\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03924153372645378\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03967957943677902\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05045958235859871\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06406231969594955\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.033217426389455795\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.00828701350837946\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.028628099709749222\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03427939862012863\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03249180316925049\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.023758752271533012\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.052790526300668716\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06174300238490105\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07444125413894653\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.052103448659181595\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06642446666955948\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.035688698291778564\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02543438971042633\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.016431182622909546\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.01494623813778162\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06171758100390434\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03984960541129112\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08501089364290237\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04986272007226944\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.025330573320388794\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.029366806149482727\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06680704653263092\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02356429398059845\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.013310709968209267\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.052233267575502396\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.011673828586935997\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02519373781979084\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.021887049078941345\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11986769735813141\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.009262518025934696\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04282956197857857\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.014032769948244095\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04515456035733223\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.035276126116514206\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02321321703493595\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.016979077830910683\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04318464919924736\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07346075773239136\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05719270184636116\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.015612604096531868\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04396600276231766\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04785068705677986\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03769921883940697\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.043033894151449203\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02853205054998398\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05825745314359665\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.046932876110076904\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.018816405907273293\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09866807609796524\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.014173448085784912\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03955288976430893\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0067492080852389336\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.029677260667085648\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05455673113465309\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.043157149106264114\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03566085919737816\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.044827900826931\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.056012291461229324\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07161036878824234\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.023367909714579582\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06491445004940033\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03163310885429382\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03015119396150112\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08715718239545822\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.025096749886870384\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07688907533884048\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0671629086136818\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07558150589466095\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07169944047927856\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06525319069623947\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1454884260892868\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04409131407737732\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.008885891176760197\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06379367411136627\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03694915398955345\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.044394008815288544\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02229139395058155\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.025570765137672424\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.024638192728161812\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06290263682603836\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03332916274666786\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05704928934574127\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.01242364663630724\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.031956735998392105\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.038362935185432434\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02643115632236004\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.033410266041755676\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.026469871401786804\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05706042796373367\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04837369546294212\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0331156849861145\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09390920400619507\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06972896307706833\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0745166689157486\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.053988657891750336\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.017802760004997253\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08979759365320206\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06372897326946259\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.021140504628419876\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05998406931757927\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07828807830810547\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.046232279390096664\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04927019774913788\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.052574895322322845\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.020922740921378136\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10724916309118271\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.008572183549404144\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.090814970433712\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05478563904762268\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03999842703342438\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.081892229616642\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03906256705522537\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02703302912414074\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.13975119590759277\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.014113957062363625\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09309937804937363\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04127225652337074\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07769865542650223\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.020971763879060745\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09836230427026749\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03180757910013199\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03427539020776749\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06232715770602226\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.01763433776795864\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06658297032117844\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04923337697982788\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05369380861520767\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04337969422340393\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0802939385175705\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07073920965194702\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.044217657297849655\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.10557052493095398\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.036699313670396805\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.01853792369365692\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.041536521166563034\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05732695385813713\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07449419051408768\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03280467540025711\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0838661640882492\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.037290532141923904\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02072140946984291\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04553338885307312\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03955134376883507\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.039364490658044815\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.11034324765205383\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04145903140306473\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05897624045610428\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0953831598162651\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0420965775847435\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.023053232580423355\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05191483721137047\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03582683578133583\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06816240400075912\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02940404787659645\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.13285066187381744\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06444934010505676\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.004674364347010851\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03196346014738083\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06590096652507782\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08905260264873505\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.030435988679528236\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.08152864873409271\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02726949378848076\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0223475880920887\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04068269208073616\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07990273088216782\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.010567777790129185\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06670115143060684\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0656537115573883\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05710957571864128\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.09024369716644287\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06379099190235138\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06185637414455414\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.034822992980480194\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05531473457813263\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.0462455078959465\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.04102196916937828\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.026323514059185982\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.05681871622800827\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.06923267245292664\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.051944196224212646\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.03922538459300995\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.037316348403692245\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.07079706341028214\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.030070407316088676\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.015246162191033363\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.02682468667626381\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.024329788982868195\n",
            "Batch Size of Tensor Data: 128\n",
            "Loss of Item: 0.1536896973848343\n",
            "Batch Size of Tensor Data: 80\n",
            "Total Loss: 2294.485084950924\n",
            "Correct Predictions: 49217\n",
            "Epoch: [5/5], Loss: 0.0459, Accuracy: 0.9843\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Evaluation\n",
        "def evaluate_model(model, test_loader):\n",
        "  model.eval()    #Set the model into evaluation model\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():       #Used to disable gradient computation within a block of code\n",
        "    for data, target in test_loader:\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      output = model(data)\n",
        "      print(f'Output: {output}')\n",
        "      loss = criterion(output, target)\n",
        "      test_loss += loss.item() * data.size(0)\n",
        "      pred = output.argmax(dim=1)\n",
        "      correct += pred.eq(target).sum().item()\n",
        "      print(f'Prediction: {pred}')\n",
        "      print(f'Target: {target}')\n",
        "  avg_loss = test_loss / len(test_loader.dataset)\n",
        "  accuracy = correct / len(test_loader.dataset)\n",
        "  print(f'Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "4NsIu6_meJ5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the model\n",
        "evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-FUlTHMgfRn",
        "outputId": "c3d79adc-1f83-44b8-acb7-560d96cd9a01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: tensor([[ -4.6545,  -6.5580,  -3.4353,  ..., -10.3584,  -4.8588,  -7.2316],\n",
            "        [  5.1671,  -3.1794,  -6.2857,  ...,  -8.1622,  15.0544,  -7.3549],\n",
            "        [ -0.9702,   2.6790,  -6.9634,  ...,  -5.2340,   8.1056,   1.8617],\n",
            "        ...,\n",
            "        [  2.7993,  -6.5426,   9.3534,  ..., -11.7028,  -1.5186,  -8.6289],\n",
            "        [ -3.9793,  -6.0363,  -0.7699,  ...,  -6.5423,  12.9062,  -1.7020],\n",
            "        [ -5.6009,  -5.3988,  -5.9996,  ...,  -7.0084,  -6.6051,  -7.0479]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 8, 8, 0, 6, 6, 9, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6, 0, 0, 4, 9,\n",
            "        5, 2, 4, 0, 9, 6, 2, 5, 4, 5, 9, 2, 4, 9, 9, 5, 4, 6, 5, 6, 0, 9, 3, 9,\n",
            "        7, 6, 9, 8, 0, 5, 8, 8, 7, 7, 5, 3, 7, 5, 6, 3, 6, 2, 1, 2, 6, 7, 2, 6,\n",
            "        8, 8, 0, 2, 9, 3, 3, 8, 8, 1, 1, 7, 2, 5, 2, 7, 8, 9, 0, 3, 8, 6, 4, 6,\n",
            "        6, 2, 0, 7, 4, 5, 6, 3, 1, 1, 3, 6, 8, 7, 4, 0, 6, 2, 1, 3, 0, 4, 2, 0,\n",
            "        8, 0, 1, 2, 8, 2, 8, 3], device='cuda:0')\n",
            "Target: tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6, 7, 0, 4, 9,\n",
            "        5, 2, 4, 0, 9, 6, 6, 5, 4, 5, 9, 2, 4, 1, 9, 5, 4, 6, 5, 6, 0, 9, 3, 9,\n",
            "        7, 6, 9, 8, 0, 3, 8, 8, 7, 7, 4, 6, 7, 3, 6, 3, 6, 2, 1, 2, 3, 7, 2, 6,\n",
            "        8, 8, 0, 2, 9, 3, 3, 8, 8, 1, 1, 7, 2, 5, 2, 7, 8, 9, 0, 3, 8, 6, 4, 6,\n",
            "        6, 0, 0, 7, 4, 5, 6, 3, 1, 1, 3, 6, 8, 7, 4, 0, 6, 2, 1, 3, 0, 4, 2, 7,\n",
            "        8, 3, 1, 2, 8, 0, 8, 3], device='cuda:0')\n",
            "Output: tensor([[ -4.3107,  -7.4393,  -4.8320,  ...,  -5.1291,  -6.5603,  -6.4753],\n",
            "        [  7.3547,  -9.8034,  14.8664,  ...,  -9.8822,  -4.7699,  -7.8996],\n",
            "        [ -6.9235,  -8.2828,   4.8486,  ...,  -7.3792, -10.9748,  -8.7483],\n",
            "        ...,\n",
            "        [ -3.7930,   2.2539,  -8.0543,  ...,  -5.9181,  -2.1563,   5.4515],\n",
            "        [ -8.3753,  -7.6891,   2.1604,  ...,  -2.0054,  -9.7024,  -7.6072],\n",
            "        [  4.3650,  -6.3395,  -3.9951,  ...,  -7.2179,   8.0843,  -0.2187]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 2, 4, 1, 8, 9, 1, 2, 9, 7, 2, 9, 6, 5, 6, 3, 8, 7, 6, 5, 3, 2, 0, 9,\n",
            "        6, 0, 0, 5, 3, 9, 3, 4, 2, 1, 6, 6, 0, 4, 8, 4, 5, 0, 9, 9, 9, 8, 9, 9,\n",
            "        3, 7, 5, 0, 0, 5, 2, 6, 3, 8, 6, 3, 3, 0, 5, 8, 0, 1, 7, 2, 8, 8, 7, 8,\n",
            "        5, 1, 8, 7, 1, 3, 0, 5, 7, 9, 7, 4, 5, 9, 8, 0, 7, 9, 8, 2, 7, 6, 9, 4,\n",
            "        3, 9, 9, 4, 3, 6, 5, 1, 7, 8, 8, 0, 4, 0, 5, 5, 1, 1, 8, 9, 0, 3, 1, 9,\n",
            "        2, 2, 5, 3, 9, 9, 4, 8], device='cuda:0')\n",
            "Target: tensor([5, 2, 4, 1, 8, 9, 1, 2, 9, 7, 2, 9, 6, 5, 6, 3, 8, 7, 6, 2, 5, 2, 8, 9,\n",
            "        6, 0, 0, 5, 2, 9, 5, 4, 2, 1, 6, 6, 8, 4, 8, 4, 5, 0, 9, 9, 9, 8, 9, 9,\n",
            "        3, 7, 5, 0, 0, 5, 2, 2, 3, 8, 6, 3, 4, 0, 5, 8, 0, 1, 7, 2, 8, 8, 7, 8,\n",
            "        5, 1, 8, 7, 1, 3, 0, 5, 7, 9, 7, 4, 5, 9, 8, 0, 7, 9, 8, 2, 7, 6, 9, 4,\n",
            "        3, 9, 6, 4, 7, 6, 5, 1, 5, 8, 8, 0, 4, 0, 5, 5, 1, 1, 8, 9, 0, 3, 1, 9,\n",
            "        2, 2, 5, 3, 9, 9, 4, 0], device='cuda:0')\n",
            "Output: tensor([[-6.7795e+00, -7.0967e+00, -4.4836e+00,  ..., -5.0167e+00,\n",
            "         -9.1846e+00, -8.4608e+00],\n",
            "        [ 1.7407e+01, -7.7447e+00,  1.9808e+00,  ..., -7.9447e+00,\n",
            "         -8.4930e-01, -7.2457e+00],\n",
            "        [ 1.0860e+01, -7.8311e-01, -7.0211e+00,  ..., -6.2392e+00,\n",
            "          1.8444e+00, -1.5582e+00],\n",
            "        ...,\n",
            "        [-6.4508e+00,  5.8279e+00, -1.2410e+01,  ..., -9.7097e+00,\n",
            "         -4.2570e+00,  1.9555e+01],\n",
            "        [ 1.8410e+01, -7.1130e+00,  9.9790e-01,  ..., -1.2530e+01,\n",
            "         -7.7372e+00, -4.4197e+00],\n",
            "        [-6.0988e-03, -5.7370e-02, -8.2887e+00,  ..., -6.1234e+00,\n",
            "         -1.0783e+00,  1.2950e+01]], device='cuda:0')\n",
            "Prediction: tensor([3, 0, 0, 9, 8, 1, 5, 7, 0, 2, 2, 4, 7, 0, 2, 3, 6, 3, 8, 6, 0, 2, 4, 3,\n",
            "        9, 0, 6, 1, 0, 9, 1, 8, 7, 9, 1, 2, 6, 1, 3, 4, 6, 0, 0, 6, 6, 6, 5, 2,\n",
            "        6, 1, 8, 2, 1, 4, 8, 6, 8, 2, 4, 0, 7, 7, 5, 5, 3, 5, 2, 3, 4, 1, 7, 5,\n",
            "        4, 6, 1, 9, 3, 6, 6, 9, 3, 8, 0, 7, 2, 6, 0, 5, 8, 5, 7, 6, 8, 9, 9, 1,\n",
            "        0, 2, 2, 4, 5, 2, 8, 0, 9, 5, 8, 1, 9, 4, 1, 3, 2, 1, 4, 7, 9, 2, 2, 7,\n",
            "        0, 7, 0, 6, 6, 9, 0, 9], device='cuda:0')\n",
            "Target: tensor([3, 0, 0, 9, 8, 1, 5, 7, 0, 8, 2, 4, 7, 0, 2, 3, 6, 3, 8, 5, 0, 3, 4, 3,\n",
            "        9, 0, 6, 1, 0, 9, 1, 0, 7, 9, 1, 2, 6, 9, 3, 4, 6, 0, 0, 6, 6, 6, 3, 2,\n",
            "        6, 1, 8, 2, 1, 6, 8, 6, 8, 0, 4, 0, 7, 7, 5, 5, 3, 5, 2, 3, 4, 1, 7, 5,\n",
            "        4, 6, 1, 9, 3, 6, 6, 9, 3, 8, 0, 7, 2, 6, 2, 5, 8, 5, 4, 6, 8, 9, 9, 1,\n",
            "        0, 2, 2, 7, 3, 2, 8, 0, 9, 5, 8, 1, 9, 4, 1, 3, 8, 1, 4, 7, 9, 4, 2, 7,\n",
            "        0, 7, 0, 6, 6, 9, 0, 9], device='cuda:0')\n",
            "Output: tensor([[-10.1351,  -4.7143,   1.5810,  ...,  -5.1857,  -5.3918,  -8.0449],\n",
            "        [  4.3471,  -4.4948,  -1.1486,  ...,  -6.2110,  11.0906,  -4.0054],\n",
            "        [ -3.8704,  -9.2788,  -6.6387,  ...,  14.8390,  -9.9626,  -3.0463],\n",
            "        ...,\n",
            "        [-10.6478, -10.0343,  -3.0170,  ...,  -0.8816, -12.2037,  -8.7196],\n",
            "        [-11.3750,  -9.1104, -11.1705,  ...,  -8.8917, -10.9247, -10.8782],\n",
            "        [-11.4423,  -7.9183,   0.5448,  ..., -12.1965,  -8.5127,  -9.1787]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([5, 8, 7, 2, 5, 5, 1, 2, 6, 2, 9, 6, 2, 3, 0, 3, 9, 8, 7, 8, 8, 4, 0, 1,\n",
            "        8, 2, 7, 9, 3, 6, 1, 9, 0, 7, 3, 7, 4, 5, 0, 3, 2, 9, 3, 4, 6, 6, 2, 5,\n",
            "        3, 0, 3, 7, 2, 5, 3, 1, 1, 4, 9, 9, 5, 7, 5, 0, 2, 2, 2, 9, 7, 3, 9, 4,\n",
            "        4, 5, 5, 6, 5, 6, 1, 4, 3, 4, 4, 3, 7, 8, 5, 7, 8, 0, 5, 7, 6, 0, 3, 4,\n",
            "        8, 6, 8, 5, 5, 9, 9, 9, 4, 0, 1, 0, 8, 1, 1, 8, 0, 5, 2, 0, 4, 6, 5, 4,\n",
            "        9, 4, 7, 9, 9, 4, 5, 6], device='cuda:0')\n",
            "Target: tensor([2, 8, 7, 2, 2, 5, 1, 2, 6, 2, 9, 6, 2, 3, 0, 3, 9, 8, 7, 8, 8, 4, 0, 1,\n",
            "        8, 2, 7, 9, 3, 6, 1, 9, 0, 7, 3, 7, 4, 5, 0, 0, 2, 9, 3, 4, 0, 6, 2, 5,\n",
            "        3, 7, 3, 7, 2, 5, 3, 1, 1, 4, 9, 9, 5, 7, 5, 0, 2, 2, 2, 9, 7, 3, 9, 4,\n",
            "        3, 5, 4, 6, 5, 6, 1, 4, 3, 4, 4, 3, 7, 8, 3, 7, 8, 0, 5, 7, 6, 0, 5, 4,\n",
            "        8, 6, 8, 5, 5, 9, 9, 9, 5, 0, 1, 0, 8, 1, 1, 8, 0, 2, 2, 0, 4, 6, 5, 4,\n",
            "        9, 4, 7, 9, 9, 4, 5, 6], device='cuda:0')\n",
            "Output: tensor([[-10.8651,  -8.4840,  -5.9157,  ...,  -9.8628,  -9.0006,  -8.0606],\n",
            "        [ -7.1485,  16.4922,  -8.6617,  ..., -11.3353,  -4.7809,   3.4276],\n",
            "        [ -5.0386,  -7.4985,  -7.4849,  ...,  -4.3679,  -6.5781,  -5.7446],\n",
            "        ...,\n",
            "        [ -4.4070,  -6.6253,   1.6230,  ...,  -8.2863,  -4.8238,  -1.9283],\n",
            "        [ 14.2797,  -2.5491,  -4.0944,  ...,  -6.6014,  -2.2465,  -1.3676],\n",
            "        [ -4.8470,  -7.4033,  -2.2518,  ...,   2.8754,  -9.8386,  -4.5354]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([6, 1, 5, 3, 8, 9, 5, 8, 5, 7, 0, 7, 0, 5, 0, 0, 4, 6, 9, 8, 9, 5, 6, 6,\n",
            "        6, 2, 9, 0, 1, 7, 6, 7, 5, 9, 1, 6, 2, 5, 5, 5, 8, 5, 9, 4, 6, 4, 3, 2,\n",
            "        0, 7, 6, 2, 2, 3, 9, 7, 9, 2, 6, 7, 1, 3, 6, 6, 8, 9, 7, 5, 4, 4, 8, 4,\n",
            "        0, 9, 3, 4, 8, 9, 6, 9, 2, 6, 1, 4, 7, 3, 3, 3, 9, 5, 0, 2, 1, 6, 4, 3,\n",
            "        3, 9, 6, 9, 8, 8, 5, 8, 6, 6, 2, 1, 7, 1, 1, 2, 7, 9, 9, 4, 4, 1, 2, 5,\n",
            "        6, 8, 7, 6, 8, 3, 0, 5], device='cuda:0')\n",
            "Target: tensor([6, 1, 5, 3, 8, 9, 5, 8, 5, 7, 0, 7, 0, 5, 0, 0, 4, 6, 9, 0, 9, 5, 6, 6,\n",
            "        6, 2, 9, 0, 1, 7, 6, 7, 5, 9, 1, 6, 2, 5, 5, 5, 8, 5, 9, 4, 6, 4, 3, 2,\n",
            "        0, 7, 6, 2, 2, 3, 9, 7, 9, 2, 6, 7, 1, 3, 6, 6, 8, 9, 7, 5, 4, 0, 8, 4,\n",
            "        0, 9, 3, 4, 8, 9, 6, 9, 2, 6, 1, 4, 7, 3, 5, 3, 8, 5, 0, 2, 1, 6, 4, 3,\n",
            "        3, 9, 6, 9, 8, 8, 5, 8, 6, 6, 2, 1, 7, 7, 1, 2, 7, 9, 9, 4, 4, 1, 2, 5,\n",
            "        6, 8, 7, 6, 8, 3, 0, 5], device='cuda:0')\n",
            "Output: tensor([[-6.9130, -5.0164,  1.0461,  ..., -5.5392, -9.1606, -5.7987],\n",
            "        [-7.4767, -6.0613, -2.0662,  ..., -7.9084, -3.4639, -7.5934],\n",
            "        [ 4.8680, -3.7994,  0.8585,  ..., -5.6310,  3.8257, -1.6140],\n",
            "        ...,\n",
            "        [-8.4392, -9.5216, 16.5977,  ..., -4.2352, -9.8098, -9.0301],\n",
            "        [-1.2350, -6.6660, -4.0152,  ...,  3.4898, -2.3229, -4.4959],\n",
            "        [ 4.4628, -3.6110, -6.9441,  ..., -4.1626, -1.1185,  5.0017]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 6, 0, 7, 9, 1, 3, 5, 4, 5, 3, 9, 5, 6, 9, 0, 1, 1, 4, 1, 9, 4, 7, 6,\n",
            "        3, 0, 9, 0, 1, 3, 6, 3, 6, 3, 2, 0, 3, 1, 0, 5, 9, 6, 4, 0, 9, 6, 9, 2,\n",
            "        3, 0, 3, 2, 0, 7, 8, 3, 8, 2, 7, 5, 7, 2, 4, 8, 7, 4, 2, 9, 8, 8, 6, 8,\n",
            "        8, 7, 4, 5, 3, 8, 4, 9, 4, 8, 8, 1, 8, 0, 1, 2, 6, 5, 4, 2, 7, 9, 1, 6,\n",
            "        1, 0, 1, 3, 2, 7, 0, 7, 9, 7, 6, 6, 2, 3, 9, 2, 9, 1, 2, 2, 6, 8, 2, 1,\n",
            "        3, 6, 6, 0, 1, 2, 7, 9], device='cuda:0')\n",
            "Target: tensor([5, 3, 0, 7, 9, 1, 3, 4, 4, 5, 3, 9, 5, 6, 9, 2, 1, 1, 4, 1, 9, 4, 7, 6,\n",
            "        3, 8, 9, 0, 1, 3, 6, 3, 6, 3, 2, 0, 3, 1, 0, 5, 9, 6, 4, 8, 9, 6, 9, 6,\n",
            "        3, 0, 3, 2, 2, 7, 8, 3, 8, 2, 7, 5, 7, 2, 4, 8, 7, 4, 2, 9, 8, 8, 6, 8,\n",
            "        8, 7, 4, 3, 3, 8, 4, 9, 4, 8, 8, 1, 8, 2, 1, 3, 6, 5, 4, 2, 7, 9, 9, 4,\n",
            "        1, 4, 1, 3, 2, 7, 0, 7, 9, 7, 6, 6, 2, 5, 9, 2, 9, 1, 2, 2, 6, 8, 2, 1,\n",
            "        3, 6, 6, 0, 1, 2, 7, 0], device='cuda:0')\n",
            "Output: tensor([[ -9.8542,  -8.8534,  -3.8756,  ...,  -0.9442,  -9.1951,  -5.9583],\n",
            "        [ -0.1286,  -7.5429,  -2.4525,  ...,  -3.5052,  -2.2514,  -3.6313],\n",
            "        [ -0.4490,  -1.1486,  -8.8497,  ...,  -4.7970,  -4.2257,   1.5081],\n",
            "        ...,\n",
            "        [ -1.1640,  -7.9656,  -5.5535,  ...,   2.7456,  -6.3625,  -4.6223],\n",
            "        [ -3.5102,  14.5261,  -7.4182,  ...,  -9.9775,  -6.5523,   4.7633],\n",
            "        [ -4.1276,   8.7073,  -8.1593,  ..., -10.5257,  -4.5213,   6.2636]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([5, 4, 9, 9, 6, 4, 0, 2, 5, 6, 0, 6, 9, 1, 7, 6, 7, 0, 3, 9, 6, 8, 4, 0,\n",
            "        5, 7, 7, 7, 2, 4, 7, 6, 7, 1, 4, 7, 4, 4, 0, 4, 7, 7, 2, 5, 7, 2, 0, 8,\n",
            "        9, 5, 8, 3, 6, 2, 0, 8, 7, 3, 7, 6, 5, 3, 1, 3, 2, 2, 5, 4, 1, 2, 9, 2,\n",
            "        7, 0, 7, 2, 1, 0, 2, 0, 2, 4, 7, 9, 8, 1, 0, 7, 7, 2, 7, 8, 4, 6, 6, 3,\n",
            "        0, 1, 5, 7, 0, 1, 3, 1, 4, 2, 3, 8, 4, 2, 3, 7, 8, 6, 2, 0, 9, 0, 0, 1,\n",
            "        8, 4, 4, 6, 7, 7, 1, 1], device='cuda:0')\n",
            "Target: tensor([5, 4, 6, 1, 6, 4, 0, 2, 2, 6, 0, 5, 9, 1, 7, 6, 7, 0, 3, 9, 6, 8, 3, 0,\n",
            "        3, 4, 7, 7, 1, 4, 7, 2, 7, 1, 4, 7, 4, 4, 8, 4, 7, 7, 5, 3, 7, 2, 0, 8,\n",
            "        9, 5, 8, 3, 6, 2, 0, 8, 7, 3, 7, 6, 5, 3, 1, 3, 2, 2, 5, 4, 1, 2, 9, 2,\n",
            "        7, 0, 7, 2, 1, 3, 2, 0, 2, 4, 7, 9, 8, 9, 0, 7, 7, 0, 7, 8, 4, 6, 3, 3,\n",
            "        0, 1, 3, 7, 0, 1, 3, 1, 4, 2, 3, 8, 4, 2, 3, 7, 8, 4, 3, 0, 9, 0, 0, 1,\n",
            "        0, 4, 4, 6, 7, 6, 1, 1], device='cuda:0')\n",
            "Output: tensor([[ -9.9456,  -4.7379,  -7.0667,  ...,  -9.1903,  -7.0838,  -5.7139],\n",
            "        [ -8.5888,  -6.3315,  -8.4765,  ...,  14.2583, -10.3502,  -6.6750],\n",
            "        [ -2.2009,  -4.5564,  -6.6416,  ...,  -1.8091,  -7.4460,  -0.3773],\n",
            "        ...,\n",
            "        [ -3.1879,  11.0829,  -2.4998,  ...,  -8.6161,  -7.8048,  -2.9883],\n",
            "        [ 13.4466,  -6.1858,  -4.4165,  ...,  -7.7201,   0.3769,  -4.2026],\n",
            "        [ 10.6688,  -4.9342,   3.0096,  ...,  -8.0258,  -2.1852,  -5.3469]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 7, 3, 5, 3, 6, 6, 5, 8, 7, 1, 6, 8, 8, 5, 3, 0, 4, 0, 1, 4, 8, 8, 0,\n",
            "        6, 9, 9, 9, 5, 3, 8, 6, 0, 0, 4, 2, 3, 2, 7, 2, 2, 5, 9, 8, 9, 1, 7, 4,\n",
            "        0, 3, 0, 1, 3, 8, 3, 9, 6, 8, 4, 7, 0, 3, 7, 8, 9, 9, 1, 6, 6, 6, 6, 9,\n",
            "        1, 9, 9, 4, 2, 1, 7, 0, 6, 8, 1, 9, 2, 9, 0, 4, 7, 8, 3, 1, 2, 0, 1, 5,\n",
            "        8, 4, 6, 3, 8, 9, 3, 8, 5, 0, 8, 4, 8, 1, 1, 0, 9, 6, 0, 8, 6, 1, 3, 4,\n",
            "        1, 6, 0, 0, 1, 1, 0, 0], device='cuda:0')\n",
            "Target: tensor([3, 7, 3, 5, 2, 6, 6, 5, 8, 7, 1, 6, 8, 8, 5, 3, 0, 4, 0, 1, 3, 8, 8, 0,\n",
            "        6, 9, 9, 9, 5, 5, 8, 6, 0, 0, 4, 2, 3, 2, 7, 2, 2, 5, 9, 8, 9, 1, 7, 4,\n",
            "        0, 3, 0, 1, 3, 8, 3, 9, 6, 1, 4, 7, 0, 3, 7, 8, 9, 1, 1, 6, 6, 6, 6, 9,\n",
            "        1, 9, 9, 4, 2, 1, 7, 0, 6, 8, 1, 9, 2, 9, 0, 4, 7, 8, 3, 1, 2, 0, 1, 5,\n",
            "        8, 4, 6, 3, 8, 1, 3, 8, 5, 0, 8, 4, 8, 1, 1, 8, 9, 6, 0, 8, 6, 1, 3, 4,\n",
            "        1, 6, 0, 5, 1, 1, 0, 0], device='cuda:0')\n",
            "Output: tensor([[ -5.6622,  -4.6186,  -5.9813,  ...,  -6.3738,  -8.7392,  -6.6291],\n",
            "        [ -7.9661, -12.5413, -13.8379,  ...,   1.8406,  -8.5565,  -4.7857],\n",
            "        [  7.9700,  -4.1423,  -2.2811,  ..., -10.4738,  -2.6517,   0.5779],\n",
            "        ...,\n",
            "        [ -3.5289,  -7.1306,  -4.7197,  ...,   4.2888, -10.0895,  -3.5262],\n",
            "        [  0.2128,  -4.5994,   4.9438,  ...,  -1.6815,  -7.9293,  -6.6829],\n",
            "        [ -1.8360,  -8.4351,  13.2462,  ...,  -7.0665, -11.0964,  -8.4459]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 5, 0, 0, 6, 6, 3, 3, 6, 3, 6, 6, 0, 7, 2, 2, 5, 5, 2, 2, 8, 5, 2, 1,\n",
            "        1, 1, 7, 2, 0, 3, 1, 5, 5, 7, 6, 8, 9, 1, 6, 4, 9, 3, 9, 0, 9, 6, 3, 6,\n",
            "        0, 7, 3, 8, 0, 0, 0, 6, 6, 6, 9, 2, 5, 4, 4, 6, 3, 6, 0, 8, 6, 0, 6, 2,\n",
            "        7, 5, 1, 2, 3, 8, 8, 0, 9, 5, 9, 7, 2, 0, 2, 8, 3, 8, 9, 1, 5, 5, 7, 7,\n",
            "        5, 3, 8, 3, 3, 6, 2, 8, 4, 3, 7, 9, 2, 4, 9, 6, 9, 0, 5, 8, 6, 1, 8, 6,\n",
            "        1, 4, 2, 6, 2, 4, 2, 2], device='cuda:0')\n",
            "Target: tensor([3, 5, 0, 0, 6, 6, 3, 3, 6, 3, 6, 6, 0, 7, 2, 2, 7, 5, 5, 2, 8, 5, 2, 1,\n",
            "        1, 4, 3, 2, 0, 3, 1, 5, 3, 7, 6, 8, 9, 1, 6, 4, 9, 3, 9, 0, 9, 6, 3, 6,\n",
            "        0, 7, 3, 8, 0, 0, 0, 6, 6, 6, 9, 2, 5, 4, 4, 6, 3, 6, 0, 8, 6, 0, 6, 2,\n",
            "        7, 5, 1, 2, 7, 8, 8, 0, 9, 4, 9, 7, 2, 0, 2, 8, 3, 8, 9, 1, 5, 5, 4, 7,\n",
            "        5, 3, 8, 3, 3, 6, 2, 8, 4, 3, 7, 1, 2, 4, 1, 6, 9, 0, 5, 8, 6, 1, 8, 6,\n",
            "        1, 4, 2, 6, 2, 7, 2, 2], device='cuda:0')\n",
            "Output: tensor([[ 22.7679,  -7.2809,  -5.9071,  ...,  -7.3867,  -3.7781,  -2.0893],\n",
            "        [ -4.1752,  -2.6019,  -6.0581,  ...,  -5.8899,  11.7935,  -1.3846],\n",
            "        [ -7.5150,  -7.0100,   3.1924,  ...,  -9.1051,  -7.9877,  -6.2436],\n",
            "        ...,\n",
            "        [ -0.5243,  -7.9516,   8.4266,  ...,  -4.8843,  -9.6967,  -7.7582],\n",
            "        [  7.4973,  -8.3158,  11.5256,  ...,  -5.1012,  -6.9766,  -8.8994],\n",
            "        [ -1.4422,  -2.8215, -10.5205,  ...,  -2.8299,   6.9412,  -3.6858]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([0, 8, 6, 9, 1, 7, 1, 8, 8, 0, 7, 5, 8, 0, 3, 3, 3, 7, 7, 9, 2, 3, 1, 9,\n",
            "        1, 1, 6, 4, 3, 6, 1, 0, 6, 1, 4, 1, 0, 0, 1, 1, 6, 5, 4, 6, 2, 0, 7, 9,\n",
            "        8, 7, 2, 0, 6, 8, 1, 4, 3, 7, 0, 6, 1, 8, 5, 7, 8, 5, 8, 3, 9, 9, 9, 8,\n",
            "        5, 6, 6, 3, 5, 1, 5, 9, 1, 4, 1, 5, 7, 0, 1, 3, 2, 0, 8, 8, 5, 6, 7, 2,\n",
            "        4, 4, 7, 2, 5, 8, 2, 4, 9, 2, 1, 8, 1, 9, 8, 0, 8, 1, 0, 4, 3, 3, 1, 8,\n",
            "        4, 6, 3, 3, 5, 2, 2, 8], device='cuda:0')\n",
            "Target: tensor([0, 8, 6, 9, 1, 7, 1, 8, 8, 0, 7, 3, 8, 0, 3, 4, 3, 7, 7, 9, 2, 3, 1, 9,\n",
            "        1, 9, 6, 3, 3, 3, 1, 0, 6, 1, 4, 1, 0, 0, 1, 1, 6, 5, 4, 6, 2, 0, 7, 9,\n",
            "        8, 7, 2, 0, 6, 8, 1, 4, 3, 7, 0, 6, 1, 8, 5, 7, 8, 4, 8, 3, 9, 9, 9, 8,\n",
            "        7, 6, 6, 3, 5, 1, 5, 9, 1, 4, 1, 5, 7, 0, 1, 5, 2, 0, 8, 8, 5, 6, 7, 3,\n",
            "        2, 4, 7, 2, 5, 8, 2, 4, 9, 2, 1, 8, 1, 9, 8, 8, 8, 9, 0, 4, 3, 3, 1, 8,\n",
            "        4, 6, 3, 3, 5, 2, 2, 8], device='cuda:0')\n",
            "Output: tensor([[ -2.2946,  -7.8243,  -8.8420,  ...,  -1.0565,  -6.5340,  -4.5627],\n",
            "        [ -3.6631,   1.2322,  -6.6798,  ..., -11.7990,  20.1490,  -8.9179],\n",
            "        [ -0.9445,  -4.0858,  -8.8984,  ...,  -6.3713,  -3.8863,  15.6864],\n",
            "        ...,\n",
            "        [  0.5436,   5.9410,  -8.3395,  ...,  -5.3302,  -4.6969,   6.8954],\n",
            "        [ -4.5474,  -3.2454, -13.7516,  ...,  -8.7667,  -3.2779,  23.9034],\n",
            "        [  3.7973,  -3.4563,  -1.7364,  ...,  -4.7490,  12.9168,  -4.9556]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 8, 9, 5, 8, 9, 8, 9, 1, 6, 5, 9, 5, 4, 8, 0, 7, 2, 9, 7, 4, 1, 6, 4,\n",
            "        8, 9, 1, 0, 5, 6, 0, 8, 6, 1, 9, 4, 5, 9, 5, 0, 7, 2, 0, 0, 4, 2, 6, 6,\n",
            "        3, 5, 2, 8, 1, 7, 3, 1, 4, 5, 6, 7, 1, 4, 7, 0, 9, 4, 5, 8, 2, 8, 4, 7,\n",
            "        2, 3, 1, 5, 2, 9, 8, 9, 7, 9, 5, 1, 4, 0, 8, 2, 3, 8, 9, 1, 1, 5, 2, 4,\n",
            "        9, 3, 1, 7, 6, 6, 0, 8, 9, 5, 3, 9, 5, 3, 6, 7, 2, 4, 6, 6, 1, 0, 7, 0,\n",
            "        5, 4, 7, 6, 1, 9, 9, 8], device='cuda:0')\n",
            "Target: tensor([3, 8, 9, 5, 8, 9, 8, 9, 1, 6, 5, 9, 4, 4, 8, 0, 7, 2, 9, 7, 4, 1, 6, 4,\n",
            "        4, 9, 1, 2, 5, 6, 0, 8, 6, 1, 9, 4, 5, 9, 5, 0, 7, 2, 0, 0, 4, 2, 6, 6,\n",
            "        5, 5, 2, 8, 1, 7, 3, 1, 4, 5, 6, 5, 1, 4, 7, 0, 9, 4, 3, 8, 2, 8, 4, 7,\n",
            "        2, 3, 1, 5, 2, 9, 8, 9, 7, 9, 5, 1, 4, 0, 8, 2, 3, 8, 9, 1, 1, 3, 2, 4,\n",
            "        9, 3, 1, 7, 4, 6, 2, 8, 9, 5, 3, 9, 5, 5, 6, 7, 2, 4, 6, 3, 1, 0, 7, 2,\n",
            "        5, 4, 7, 6, 1, 1, 9, 8], device='cuda:0')\n",
            "Output: tensor([[ -3.4131,  14.4618,  -8.6566,  ...,  -9.2914,  -8.1885,   1.2315],\n",
            "        [ 15.6044,  -5.1954,  -4.0720,  ...,  -7.7993,  -0.7640,  -2.5045],\n",
            "        [ -8.4403,  14.6785,  -5.8576,  ..., -13.8206,  -8.2223,   5.7537],\n",
            "        ...,\n",
            "        [-10.1954,  -6.8020,  -4.6817,  ...,   5.4113, -12.3211,  -4.8319],\n",
            "        [ -2.0088,  -5.5054,  -1.1451,  ...,  -6.0401, -10.8349,  -4.7600],\n",
            "        [ -9.0732,  -9.4160,   1.5055,  ...,  -0.9136,  -8.7866, -10.6684]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([1, 0, 1, 3, 1, 1, 1, 7, 3, 9, 6, 8, 4, 6, 8, 4, 9, 4, 7, 9, 7, 6, 8, 4,\n",
            "        9, 7, 0, 1, 6, 1, 5, 9, 0, 4, 3, 4, 1, 3, 0, 8, 4, 6, 2, 2, 6, 5, 3, 6,\n",
            "        2, 1, 1, 9, 6, 0, 2, 0, 1, 9, 7, 1, 3, 7, 7, 0, 7, 7, 3, 9, 7, 7, 7, 2,\n",
            "        1, 2, 8, 6, 4, 0, 7, 9, 8, 6, 8, 4, 8, 1, 2, 0, 2, 8, 3, 8, 1, 2, 2, 5,\n",
            "        1, 2, 7, 2, 8, 1, 8, 1, 8, 6, 0, 2, 4, 1, 3, 6, 7, 7, 4, 4, 3, 3, 4, 5,\n",
            "        2, 4, 3, 7, 8, 4, 4, 4], device='cuda:0')\n",
            "Target: tensor([1, 0, 1, 3, 1, 1, 1, 7, 3, 9, 6, 8, 4, 6, 8, 4, 9, 4, 7, 9, 7, 6, 8, 4,\n",
            "        9, 7, 0, 1, 6, 1, 5, 9, 0, 4, 3, 4, 1, 3, 0, 8, 4, 6, 2, 2, 6, 5, 3, 6,\n",
            "        2, 1, 1, 8, 6, 0, 4, 0, 1, 9, 7, 1, 3, 7, 7, 8, 7, 7, 3, 9, 7, 7, 7, 2,\n",
            "        1, 2, 8, 6, 4, 0, 7, 9, 8, 6, 8, 4, 9, 1, 7, 2, 2, 8, 5, 8, 1, 2, 2, 4,\n",
            "        1, 2, 5, 2, 8, 1, 8, 1, 8, 6, 0, 2, 4, 1, 3, 6, 7, 7, 4, 4, 3, 3, 4, 5,\n",
            "        2, 4, 3, 7, 8, 4, 4, 4], device='cuda:0')\n",
            "Output: tensor([[ -6.9194,  -3.7932,  -1.7060,  ...,  -2.9300,  -9.8819,  -4.6034],\n",
            "        [ -3.9025,  -7.4103,   2.0047,  ...,  -2.5861, -10.2608,  -8.6261],\n",
            "        [ -7.5117,  -5.7565,  -3.5902,  ...,  -0.5032,  -1.3453,  -3.1734],\n",
            "        ...,\n",
            "        [-11.3171,  -7.3468,   3.9342,  ...,  -9.9174, -10.7359, -10.0734],\n",
            "        [ -3.5167,  -5.1224,  -6.2660,  ...,  -2.2595,  -5.6103,  -5.1139],\n",
            "        [ -6.8627,  -7.1385,   4.0569,  ...,  -7.6874,  -9.2833,  -7.2835]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 4, 3, 2, 8, 4, 5, 5, 4, 1, 4, 2, 5, 1, 6, 4, 3, 4, 4, 0, 8, 8, 4, 3,\n",
            "        7, 5, 6, 9, 1, 6, 7, 2, 0, 1, 4, 5, 6, 9, 0, 2, 4, 4, 6, 0, 6, 8, 9, 1,\n",
            "        7, 7, 5, 2, 5, 6, 4, 1, 4, 3, 3, 3, 0, 3, 5, 5, 8, 9, 7, 3, 1, 3, 3, 0,\n",
            "        4, 4, 2, 3, 3, 8, 1, 7, 7, 0, 7, 4, 5, 1, 4, 4, 4, 3, 9, 9, 4, 9, 9, 1,\n",
            "        8, 1, 6, 7, 5, 5, 4, 9, 5, 6, 5, 9, 3, 4, 0, 7, 8, 5, 5, 0, 0, 9, 9, 8,\n",
            "        2, 5, 4, 8, 3, 6, 3, 6], device='cuda:0')\n",
            "Target: tensor([5, 4, 3, 2, 8, 4, 5, 5, 4, 1, 4, 2, 5, 1, 6, 4, 3, 4, 4, 0, 8, 8, 4, 5,\n",
            "        7, 5, 6, 9, 1, 6, 7, 2, 0, 1, 4, 5, 6, 0, 0, 2, 7, 5, 6, 0, 6, 2, 9, 1,\n",
            "        7, 7, 5, 2, 5, 6, 4, 1, 4, 3, 3, 3, 0, 3, 5, 5, 8, 9, 7, 3, 1, 3, 3, 3,\n",
            "        4, 4, 2, 3, 3, 8, 1, 7, 7, 0, 7, 4, 5, 1, 4, 2, 4, 3, 9, 9, 4, 9, 9, 1,\n",
            "        8, 1, 6, 7, 5, 5, 4, 9, 7, 6, 5, 9, 2, 4, 0, 7, 8, 5, 5, 0, 0, 9, 9, 8,\n",
            "        2, 5, 4, 8, 3, 6, 3, 6], device='cuda:0')\n",
            "Output: tensor([[  8.5637,  -3.6186,   0.6627,  ...,  -8.0660,  -2.2539,  -1.9171],\n",
            "        [-15.2077,  -9.3193,  -2.5929,  ..., -12.9825, -15.1548, -12.6407],\n",
            "        [ -8.7265,  -3.1722,   1.3713,  ..., -10.3153,  -3.6714,  -3.7736],\n",
            "        ...,\n",
            "        [ 10.4761,  -5.8391,  -6.1488,  ...,  -6.3045,   1.9517,  -2.0708],\n",
            "        [ -6.7634,  -5.0563,  -3.1653,  ...,   3.4733,  -6.9748,  -5.2378],\n",
            "        [-10.5134,  -6.5801,   0.5946,  ...,   0.6541,  -8.4069,  -4.5525]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([0, 6, 6, 6, 9, 6, 6, 8, 6, 2, 4, 5, 8, 1, 2, 7, 6, 5, 7, 0, 8, 6, 0, 8,\n",
            "        6, 9, 2, 8, 9, 4, 0, 9, 4, 9, 5, 7, 5, 5, 9, 5, 3, 0, 1, 9, 7, 2, 4, 1,\n",
            "        0, 8, 0, 3, 1, 7, 9, 0, 4, 8, 6, 2, 4, 0, 0, 9, 0, 8, 4, 6, 1, 3, 9, 0,\n",
            "        5, 7, 5, 6, 1, 4, 8, 1, 0, 5, 2, 1, 0, 2, 8, 1, 5, 6, 7, 7, 2, 6, 2, 5,\n",
            "        0, 1, 4, 2, 5, 2, 6, 2, 0, 1, 7, 2, 0, 5, 5, 3, 0, 4, 8, 5, 7, 6, 3, 0,\n",
            "        1, 0, 1, 3, 3, 0, 5, 6], device='cuda:0')\n",
            "Target: tensor([0, 6, 6, 6, 9, 6, 6, 8, 6, 2, 4, 5, 8, 1, 2, 7, 6, 5, 7, 8, 1, 8, 0, 8,\n",
            "        6, 9, 2, 8, 9, 4, 0, 9, 4, 9, 5, 7, 5, 5, 9, 5, 3, 0, 1, 9, 7, 2, 4, 1,\n",
            "        0, 8, 0, 3, 1, 7, 0, 0, 4, 8, 6, 2, 4, 0, 0, 9, 0, 8, 4, 5, 9, 3, 9, 0,\n",
            "        5, 6, 5, 0, 1, 4, 8, 1, 0, 5, 2, 1, 0, 2, 8, 1, 5, 6, 7, 7, 2, 6, 2, 5,\n",
            "        0, 1, 4, 2, 5, 4, 6, 2, 2, 1, 7, 2, 8, 5, 5, 3, 0, 4, 8, 3, 7, 6, 3, 8,\n",
            "        1, 0, 1, 3, 3, 0, 7, 4], device='cuda:0')\n",
            "Output: tensor([[ -4.7025,  -3.0841, -11.0544,  ..., -11.3933,  -5.1611,  26.9562],\n",
            "        [ -8.2292, -11.7592,   0.6926,  ...,   0.8873, -10.3025,  -9.1778],\n",
            "        [ -1.9025,  -7.1620,  -7.2602,  ...,  -9.9896,  -8.3986,  -4.8638],\n",
            "        ...,\n",
            "        [ -4.5747,   2.3099,  -5.3828,  ...,  -8.4083,  11.5553,  -7.3552],\n",
            "        [ -8.4411,  -8.0095,  16.7895,  ...,  -5.4026, -10.7041, -10.9887],\n",
            "        [ -7.2683,  -6.4515,   2.5618,  ...,  -1.3734,  -9.4347,  -7.6920]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([9, 5, 3, 6, 8, 1, 4, 4, 4, 4, 2, 2, 5, 8, 1, 5, 9, 8, 1, 1, 5, 3, 9, 9,\n",
            "        7, 2, 5, 0, 8, 4, 5, 0, 9, 2, 8, 4, 7, 1, 3, 9, 6, 8, 9, 0, 4, 9, 8, 7,\n",
            "        8, 9, 4, 8, 9, 7, 2, 5, 3, 7, 1, 0, 2, 9, 5, 5, 8, 5, 4, 2, 8, 3, 5, 5,\n",
            "        7, 7, 8, 6, 2, 8, 2, 3, 5, 6, 8, 0, 2, 3, 7, 0, 1, 9, 1, 3, 5, 0, 8, 3,\n",
            "        2, 9, 6, 8, 6, 9, 3, 0, 9, 0, 0, 7, 8, 5, 0, 0, 1, 6, 9, 1, 5, 2, 4, 4,\n",
            "        0, 9, 9, 9, 9, 8, 2, 4], device='cuda:0')\n",
            "Target: tensor([9, 5, 3, 6, 0, 1, 4, 4, 4, 4, 2, 2, 5, 8, 1, 5, 9, 8, 1, 1, 5, 3, 9, 9,\n",
            "        7, 6, 5, 0, 8, 4, 7, 0, 9, 2, 8, 4, 7, 1, 3, 9, 6, 8, 9, 0, 4, 9, 6, 7,\n",
            "        8, 9, 4, 8, 9, 7, 2, 5, 3, 7, 1, 0, 2, 9, 5, 5, 8, 5, 4, 2, 8, 3, 5, 5,\n",
            "        7, 7, 8, 6, 2, 8, 2, 3, 5, 6, 8, 0, 2, 3, 7, 0, 1, 9, 1, 3, 7, 5, 8, 3,\n",
            "        2, 9, 6, 8, 6, 9, 3, 8, 9, 8, 0, 7, 8, 5, 0, 0, 1, 3, 9, 1, 5, 3, 4, 4,\n",
            "        0, 9, 9, 9, 9, 8, 2, 4], device='cuda:0')\n",
            "Output: tensor([[ -7.3343,  -8.1277,  20.0778,  ..., -11.0716, -11.1625, -11.2855],\n",
            "        [  1.3689,  -5.9995,   3.1519,  ...,  -3.7320,  -6.2684,  -4.5576],\n",
            "        [ -4.7344,  -9.0808,  -3.1233,  ...,  -1.4415,  -9.6713,  -7.7121],\n",
            "        ...,\n",
            "        [ -4.1851,  19.0704,  -9.1157,  ...,  -9.1766,  -4.9504,  -0.8225],\n",
            "        [ -6.5276,  -2.3920,  -4.8876,  ...,  -7.1453,  -7.2441,  -8.9813],\n",
            "        [ -7.8790,  -4.3102,  -3.0627,  ...,  -4.3978,  -9.1470,  -8.7603]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([2, 2, 5, 1, 9, 1, 0, 9, 4, 2, 1, 6, 8, 3, 0, 2, 3, 9, 0, 5, 5, 7, 2, 8,\n",
            "        4, 4, 8, 5, 5, 0, 5, 7, 4, 4, 2, 2, 7, 7, 6, 0, 2, 7, 6, 2, 3, 0, 7, 7,\n",
            "        8, 9, 1, 4, 6, 0, 6, 6, 5, 5, 6, 6, 9, 3, 6, 8, 7, 6, 2, 9, 5, 6, 7, 1,\n",
            "        6, 3, 8, 2, 3, 9, 8, 5, 1, 5, 2, 5, 7, 5, 7, 8, 9, 1, 0, 2, 5, 6, 8, 4,\n",
            "        6, 5, 3, 9, 9, 8, 5, 5, 6, 4, 5, 9, 7, 3, 4, 1, 4, 3, 3, 6, 3, 5, 2, 8,\n",
            "        0, 0, 1, 8, 3, 1, 5, 5], device='cuda:0')\n",
            "Target: tensor([2, 2, 5, 1, 9, 1, 0, 9, 4, 2, 1, 6, 0, 3, 7, 6, 3, 1, 8, 6, 5, 7, 2, 8,\n",
            "        4, 4, 8, 3, 5, 0, 5, 7, 4, 4, 2, 2, 7, 3, 6, 0, 2, 7, 6, 2, 3, 0, 7, 7,\n",
            "        8, 1, 1, 4, 6, 0, 6, 6, 5, 5, 6, 3, 9, 3, 6, 8, 7, 6, 4, 9, 5, 6, 4, 1,\n",
            "        6, 3, 8, 2, 3, 9, 8, 5, 1, 5, 4, 5, 7, 5, 7, 8, 9, 1, 7, 2, 5, 6, 8, 4,\n",
            "        6, 5, 3, 9, 9, 8, 5, 5, 6, 4, 5, 9, 7, 3, 4, 1, 4, 2, 3, 6, 5, 5, 2, 8,\n",
            "        0, 0, 1, 8, 3, 1, 3, 5], device='cuda:0')\n",
            "Output: tensor([[ 2.8587, -3.4334, -3.8813,  ..., -9.7851, 18.6902, -5.3812],\n",
            "        [-1.3749, -7.0483, -4.5881,  ..., -8.0364, -8.7407, -7.9955],\n",
            "        [ 1.3391, -0.8144, -3.4141,  ..., -9.4608, 14.7542, -2.5243],\n",
            "        ...,\n",
            "        [-5.9943, -7.2010, -7.1359,  ..., -3.7298, -6.0756, -6.2275],\n",
            "        [-1.2498,  0.0568, -9.8829,  ..., -4.9739,  3.2620,  4.7808],\n",
            "        [-7.8771, -5.0475, -3.2978,  ..., -1.3260, -3.7863, -2.4084]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([8, 3, 8, 5, 8, 6, 3, 5, 5, 5, 0, 9, 5, 3, 7, 1, 8, 2, 4, 4, 3, 4, 2, 9,\n",
            "        6, 2, 2, 4, 3, 0, 2, 0, 1, 3, 2, 2, 7, 0, 0, 0, 7, 7, 2, 9, 1, 6, 5, 0,\n",
            "        2, 2, 0, 1, 2, 6, 0, 1, 6, 6, 5, 3, 5, 0, 0, 9, 9, 0, 2, 5, 9, 7, 8, 6,\n",
            "        4, 6, 0, 2, 0, 0, 9, 2, 6, 9, 0, 7, 2, 3, 4, 8, 4, 2, 6, 8, 9, 7, 1, 9,\n",
            "        3, 4, 2, 7, 9, 6, 9, 0, 7, 8, 5, 9, 8, 3, 9, 9, 0, 3, 5, 7, 1, 8, 0, 7,\n",
            "        5, 8, 2, 2, 5, 3, 9, 5], device='cuda:0')\n",
            "Target: tensor([8, 3, 8, 5, 8, 6, 3, 5, 5, 5, 0, 9, 5, 5, 7, 1, 8, 2, 2, 7, 3, 4, 2, 9,\n",
            "        6, 2, 2, 4, 3, 0, 2, 0, 1, 3, 2, 2, 7, 0, 1, 0, 7, 7, 2, 0, 1, 6, 5, 0,\n",
            "        2, 2, 0, 1, 2, 6, 0, 1, 6, 6, 5, 3, 4, 0, 0, 9, 1, 0, 2, 5, 9, 7, 8, 6,\n",
            "        4, 6, 0, 2, 0, 1, 9, 2, 4, 9, 0, 7, 2, 3, 4, 8, 0, 2, 6, 8, 9, 7, 1, 9,\n",
            "        3, 7, 2, 7, 9, 6, 9, 5, 7, 8, 5, 9, 8, 3, 9, 8, 0, 5, 5, 7, 1, 2, 0, 7,\n",
            "        5, 8, 2, 2, 5, 3, 9, 3], device='cuda:0')\n",
            "Output: tensor([[ -6.2658,   5.2466, -11.0289,  ...,  -6.6965,   9.2098,   5.9408],\n",
            "        [-10.0973,   1.1278, -13.0580,  ...,  -5.3926, -10.2529,  18.4477],\n",
            "        [ -2.6764,  -5.4736,  -5.5100,  ...,  -4.9881,  -4.3889,  -4.4707],\n",
            "        ...,\n",
            "        [ -6.2176,  -7.5590,  -1.5720,  ...,  -1.1222,  -7.6235,  -4.3230],\n",
            "        [ 13.5837,  -3.4661,  -4.7426,  ...,  -7.9832,  -6.2530,   2.4371],\n",
            "        [ -8.5488, -10.1160,  19.3292,  ...,  -5.9306,  -9.0434,  -9.4512]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([8, 9, 3, 1, 4, 3, 4, 4, 9, 0, 1, 5, 9, 2, 9, 4, 0, 2, 8, 4, 8, 6, 8, 3,\n",
            "        0, 5, 9, 6, 9, 3, 1, 3, 4, 4, 2, 7, 2, 6, 4, 0, 2, 8, 4, 5, 1, 2, 6, 8,\n",
            "        1, 7, 6, 8, 7, 4, 3, 5, 0, 2, 7, 2, 5, 6, 1, 7, 9, 0, 3, 3, 9, 8, 9, 5,\n",
            "        0, 0, 7, 3, 3, 5, 8, 1, 4, 0, 1, 5, 4, 3, 2, 6, 0, 8, 8, 3, 3, 3, 3, 9,\n",
            "        7, 5, 1, 6, 0, 5, 9, 9, 4, 5, 9, 4, 8, 3, 3, 2, 5, 1, 9, 5, 3, 1, 1, 8,\n",
            "        9, 7, 0, 3, 3, 5, 0, 2], device='cuda:0')\n",
            "Target: tensor([1, 9, 3, 1, 4, 3, 4, 4, 9, 0, 9, 5, 9, 2, 9, 4, 0, 2, 8, 4, 8, 6, 8, 2,\n",
            "        0, 5, 9, 6, 9, 5, 1, 3, 4, 4, 2, 7, 2, 4, 4, 0, 2, 8, 4, 5, 1, 2, 6, 8,\n",
            "        1, 7, 2, 8, 7, 4, 3, 3, 0, 3, 7, 2, 5, 6, 1, 7, 9, 0, 2, 3, 9, 8, 9, 5,\n",
            "        0, 0, 7, 6, 3, 3, 8, 1, 4, 0, 1, 5, 4, 3, 2, 6, 0, 8, 8, 6, 3, 3, 2, 9,\n",
            "        7, 5, 1, 6, 0, 5, 9, 9, 4, 5, 9, 4, 8, 3, 3, 2, 5, 1, 9, 5, 5, 8, 1, 8,\n",
            "        9, 7, 0, 6, 3, 2, 0, 2], device='cuda:0')\n",
            "Output: tensor([[ -7.4732,  -5.5204,  -3.2898,  ...,  -5.7453,  -3.1592,  -4.2990],\n",
            "        [ -2.9068,  -1.8402,  -1.5812,  ...,  -6.4240,  -0.2989,  10.4749],\n",
            "        [ -9.0373,  -7.2819, -10.3505,  ...,  -7.5933,  -7.8621,  -8.4624],\n",
            "        ...,\n",
            "        [  1.5103,  -5.0166,   1.6592,  ...,  -8.4195,  14.3468,  -5.4208],\n",
            "        [ -7.8164,  -7.8604,  -8.7885,  ...,  12.8643,  -9.4392,  -6.3824],\n",
            "        [ -0.7019,  -5.8852,  -2.9642,  ...,  -6.8533,  15.8594,  -5.2830]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([6, 9, 3, 9, 1, 6, 6, 7, 2, 6, 3, 2, 2, 5, 2, 7, 3, 2, 0, 8, 0, 7, 7, 1,\n",
            "        7, 4, 0, 4, 2, 6, 1, 5, 9, 7, 6, 2, 7, 0, 5, 6, 0, 1, 1, 8, 4, 5, 0, 1,\n",
            "        2, 4, 8, 9, 8, 1, 7, 2, 7, 2, 9, 2, 6, 7, 9, 4, 0, 1, 0, 4, 3, 0, 8, 0,\n",
            "        7, 6, 1, 8, 8, 5, 9, 2, 5, 4, 0, 9, 0, 6, 1, 2, 5, 1, 6, 7, 1, 5, 8, 8,\n",
            "        0, 3, 9, 4, 0, 6, 4, 9, 2, 4, 3, 0, 6, 6, 5, 6, 6, 7, 8, 4, 8, 8, 3, 2,\n",
            "        8, 0, 2, 5, 6, 8, 7, 8], device='cuda:0')\n",
            "Target: tensor([6, 9, 3, 9, 1, 6, 6, 7, 2, 6, 3, 2, 5, 5, 2, 7, 5, 2, 0, 8, 0, 7, 7, 1,\n",
            "        7, 4, 0, 2, 2, 6, 1, 5, 9, 7, 6, 2, 7, 0, 5, 6, 0, 1, 1, 8, 4, 5, 3, 1,\n",
            "        2, 4, 8, 9, 8, 1, 7, 2, 7, 2, 3, 2, 6, 7, 9, 4, 0, 1, 0, 4, 5, 0, 8, 0,\n",
            "        7, 6, 1, 0, 8, 5, 9, 2, 5, 4, 4, 9, 0, 6, 1, 2, 5, 1, 6, 7, 1, 5, 8, 8,\n",
            "        0, 3, 9, 4, 0, 3, 4, 9, 2, 4, 3, 0, 6, 4, 5, 6, 6, 7, 8, 4, 8, 8, 3, 2,\n",
            "        8, 0, 2, 5, 6, 8, 7, 8], device='cuda:0')\n",
            "Output: tensor([[  0.6526,  -0.9418, -10.2494,  ...,  -8.1801,  -5.4415,  16.6764],\n",
            "        [ -6.6145,   4.8346, -12.0057,  ...,  -8.1183,  -3.6761,  20.6040],\n",
            "        [  9.1765,  -1.4066,  -8.7118,  ...,  -7.3714,  -1.4709,   8.6424],\n",
            "        ...,\n",
            "        [ -3.3967,  -6.0745,  10.1563,  ...,  -6.7394,  -9.6096,  -7.9953],\n",
            "        [ -6.6739,  -9.3053,  -3.9170,  ...,  -4.0182,  -9.5224,  -8.2535],\n",
            "        [ -4.1149,  -7.5101,  -6.6187,  ...,   7.6561,  -9.4284,  -0.2715]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([9, 9, 0, 6, 6, 8, 1, 1, 7, 4, 4, 0, 3, 6, 9, 0, 2, 4, 1, 8, 2, 3, 1, 6,\n",
            "        8, 6, 8, 5, 9, 0, 1, 0, 2, 9, 2, 9, 0, 9, 7, 0, 5, 8, 6, 6, 8, 1, 6, 5,\n",
            "        8, 7, 9, 2, 1, 6, 7, 5, 6, 3, 0, 9, 8, 9, 0, 8, 9, 9, 3, 6, 4, 5, 8, 8,\n",
            "        1, 7, 3, 0, 6, 0, 5, 1, 2, 2, 9, 2, 2, 9, 7, 4, 1, 9, 1, 4, 5, 0, 3, 8,\n",
            "        9, 3, 6, 2, 0, 1, 9, 0, 5, 1, 5, 8, 2, 1, 2, 3, 2, 4, 8, 4, 0, 3, 4, 2,\n",
            "        9, 8, 4, 7, 6, 2, 4, 7], device='cuda:0')\n",
            "Target: tensor([9, 9, 0, 6, 6, 8, 1, 1, 7, 4, 4, 2, 3, 6, 9, 6, 2, 4, 1, 8, 2, 3, 1, 6,\n",
            "        8, 6, 8, 5, 9, 0, 1, 0, 2, 9, 2, 9, 0, 9, 7, 8, 5, 0, 6, 6, 8, 1, 6, 5,\n",
            "        8, 7, 9, 2, 1, 6, 7, 5, 6, 3, 0, 9, 8, 9, 5, 9, 9, 9, 3, 6, 4, 4, 8, 8,\n",
            "        1, 7, 3, 0, 6, 2, 4, 8, 2, 2, 9, 2, 2, 9, 7, 4, 1, 9, 1, 4, 3, 5, 3, 8,\n",
            "        9, 3, 6, 2, 0, 8, 9, 0, 5, 1, 5, 8, 2, 1, 2, 3, 2, 4, 8, 4, 0, 6, 4, 2,\n",
            "        9, 8, 4, 7, 6, 2, 4, 7], device='cuda:0')\n",
            "Output: tensor([[ -7.0910,  -3.2045,  -8.6000,  ...,   7.2345, -10.6610,  -5.7105],\n",
            "        [-13.6392,  -5.6615,   1.2023,  ..., -14.5398, -10.7812, -11.0305],\n",
            "        [ -2.7888,  -4.9083,  -0.8837,  ...,  -6.4122,  -4.1073,  -4.9884],\n",
            "        ...,\n",
            "        [  0.0991,  -7.2789,   9.6498,  ...,  -4.2836,  -5.5075,  -7.5533],\n",
            "        [ -5.4456,  -5.3840,  -7.0682,  ...,  11.2993,  -8.6116,  -5.2708],\n",
            "        [ -5.7205,  -0.6372,  -7.6838,  ...,  -9.7995,  19.4685,  -4.4158]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([7, 6, 6, 2, 2, 3, 2, 4, 9, 0, 0, 9, 6, 5, 8, 5, 2, 4, 8, 8, 6, 0, 5, 7,\n",
            "        6, 3, 4, 0, 4, 0, 9, 8, 4, 8, 0, 4, 9, 7, 3, 3, 8, 1, 6, 0, 8, 4, 5, 1,\n",
            "        4, 5, 2, 4, 5, 6, 0, 7, 1, 3, 5, 7, 1, 8, 0, 4, 8, 2, 7, 7, 0, 6, 6, 4,\n",
            "        3, 3, 9, 4, 1, 6, 4, 6, 7, 4, 7, 0, 8, 1, 3, 1, 4, 6, 7, 5, 7, 4, 5, 4,\n",
            "        9, 7, 7, 2, 3, 3, 6, 7, 4, 1, 6, 1, 0, 6, 9, 6, 0, 2, 0, 6, 4, 7, 0, 0,\n",
            "        0, 0, 7, 5, 0, 2, 7, 8], device='cuda:0')\n",
            "Target: tensor([7, 6, 4, 2, 2, 3, 2, 4, 9, 0, 0, 9, 6, 5, 8, 5, 2, 4, 8, 8, 6, 4, 5, 7,\n",
            "        6, 3, 7, 0, 4, 0, 5, 8, 2, 8, 0, 4, 9, 2, 3, 6, 8, 1, 6, 0, 8, 4, 3, 1,\n",
            "        4, 5, 2, 4, 3, 6, 0, 7, 1, 3, 5, 7, 1, 8, 0, 4, 8, 2, 7, 7, 0, 2, 2, 4,\n",
            "        3, 3, 1, 6, 1, 6, 4, 6, 7, 4, 7, 3, 8, 1, 3, 1, 4, 6, 5, 5, 7, 4, 5, 4,\n",
            "        9, 7, 7, 2, 3, 3, 6, 7, 4, 1, 6, 1, 0, 6, 9, 6, 0, 2, 0, 2, 4, 7, 0, 0,\n",
            "        0, 0, 7, 5, 3, 2, 7, 8], device='cuda:0')\n",
            "Output: tensor([[-10.0055,  -7.1510,   0.4602,  ...,  -8.2643,  -9.9312,  -8.5564],\n",
            "        [ -6.7069,  -5.9933,  -6.6469,  ...,  -6.1900,  -7.6902,  -6.4095],\n",
            "        [ -4.3707,  -6.5428,  12.8313,  ..., -11.0799,  -7.4447, -11.0757],\n",
            "        ...,\n",
            "        [ -3.1386,  14.6558,  -5.9974,  ...,  -8.4523,  -6.2675,   1.3964],\n",
            "        [ -3.2156,   7.2601,  -8.6110,  ...,  -4.6256,  -6.7183,   6.5527],\n",
            "        [ -2.2474,  -4.0337,  -4.2588,  ...,  -5.8793,  -2.6843,  -4.4977]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 5, 2, 2, 8, 9, 1, 2, 2, 0, 3, 4, 9, 1, 6, 9, 0, 8, 3, 6, 4, 6, 7, 5,\n",
            "        1, 8, 9, 0, 5, 0, 5, 2, 3, 8, 5, 2, 0, 8, 5, 9, 2, 3, 6, 5, 7, 3, 7, 2,\n",
            "        8, 8, 6, 2, 1, 6, 2, 9, 9, 8, 2, 3, 1, 0, 5, 2, 3, 7, 9, 5, 3, 1, 1, 3,\n",
            "        2, 7, 2, 4, 1, 9, 4, 5, 5, 2, 6, 7, 6, 1, 2, 3, 7, 5, 4, 3, 7, 7, 0, 0,\n",
            "        1, 7, 4, 3, 0, 9, 8, 7, 0, 9, 0, 3, 7, 6, 5, 8, 4, 5, 6, 5, 5, 4, 2, 9,\n",
            "        2, 4, 9, 2, 0, 1, 1, 3], device='cuda:0')\n",
            "Target: tensor([5, 5, 2, 2, 8, 9, 1, 2, 2, 0, 3, 4, 9, 1, 6, 9, 0, 8, 3, 6, 4, 6, 7, 5,\n",
            "        1, 8, 9, 0, 5, 0, 5, 4, 3, 8, 5, 2, 0, 8, 5, 9, 2, 5, 6, 5, 7, 3, 7, 2,\n",
            "        8, 8, 4, 2, 1, 6, 2, 9, 9, 8, 2, 3, 1, 0, 5, 2, 3, 7, 9, 5, 9, 1, 1, 3,\n",
            "        2, 7, 4, 4, 1, 9, 4, 5, 5, 2, 7, 7, 6, 1, 2, 3, 7, 5, 4, 3, 7, 7, 0, 0,\n",
            "        1, 4, 4, 3, 7, 9, 8, 7, 0, 9, 0, 3, 7, 6, 3, 8, 4, 5, 6, 5, 3, 4, 2, 9,\n",
            "        2, 4, 9, 2, 0, 1, 1, 3], device='cuda:0')\n",
            "Output: tensor([[ -5.4859,  -1.9651,  -6.2300,  ...,   7.6671,  -5.4624,  -6.1277],\n",
            "        [ -6.3458,  -1.5111, -11.2152,  ..., -10.9880,  -5.7169,  20.2136],\n",
            "        [ -8.3102,  -5.9394,  -5.5569,  ...,  -3.2271,  -9.8811,  -7.8568],\n",
            "        ...,\n",
            "        [ -0.8676,  -7.9122,  -7.3948,  ...,  -6.1711,  12.6631,  -3.1065],\n",
            "        [ -3.2861, -10.6970, -10.0827,  ...,  -3.1063,  -4.9189,  -3.8664],\n",
            "        [ -6.9118,  12.4384,  -9.4487,  ..., -10.5748,  -4.3808,   7.6028]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([7, 9, 5, 1, 6, 0, 3, 3, 7, 6, 4, 5, 2, 3, 6, 0, 3, 0, 4, 3, 3, 8, 1, 2,\n",
            "        3, 2, 7, 8, 0, 5, 6, 8, 6, 6, 5, 4, 2, 9, 9, 3, 2, 0, 4, 1, 5, 5, 6, 6,\n",
            "        5, 6, 9, 7, 9, 4, 8, 3, 1, 9, 8, 1, 9, 0, 3, 0, 4, 7, 7, 2, 6, 5, 6, 9,\n",
            "        7, 7, 5, 7, 0, 8, 6, 4, 8, 3, 8, 9, 2, 2, 9, 2, 7, 4, 1, 9, 7, 0, 8, 1,\n",
            "        2, 0, 6, 5, 1, 4, 2, 8, 2, 8, 3, 5, 1, 4, 9, 8, 1, 1, 7, 9, 9, 4, 6, 8,\n",
            "        5, 3, 2, 2, 0, 8, 5, 1], device='cuda:0')\n",
            "Target: tensor([7, 9, 5, 1, 6, 0, 3, 3, 7, 2, 4, 5, 2, 3, 6, 3, 3, 0, 4, 4, 3, 8, 1, 2,\n",
            "        3, 2, 7, 1, 4, 5, 6, 8, 6, 6, 5, 4, 2, 9, 9, 3, 2, 0, 4, 1, 5, 5, 6, 6,\n",
            "        5, 6, 9, 7, 9, 4, 8, 3, 1, 9, 8, 1, 9, 0, 3, 0, 4, 7, 7, 2, 6, 5, 6, 9,\n",
            "        7, 7, 5, 7, 0, 8, 6, 7, 8, 3, 8, 9, 3, 2, 9, 2, 7, 4, 1, 9, 7, 0, 8, 1,\n",
            "        2, 0, 6, 5, 1, 4, 2, 8, 2, 8, 3, 3, 1, 4, 9, 8, 1, 1, 7, 9, 9, 4, 6, 8,\n",
            "        5, 3, 2, 2, 3, 8, 5, 1], device='cuda:0')\n",
            "Output: tensor([[-7.9353e+00,  2.7091e+00, -9.3140e+00,  ..., -8.5505e+00,\n",
            "         -6.2818e+00,  1.4614e+01],\n",
            "        [-1.0712e+01, -8.4274e+00, -9.7581e+00,  ...,  1.2808e+01,\n",
            "         -8.9591e+00, -6.4682e+00],\n",
            "        [-1.9220e+00, -3.4218e+00, -1.6573e+01,  ..., -1.4060e+01,\n",
            "         -5.0205e+00,  2.7917e+01],\n",
            "        ...,\n",
            "        [-6.6036e+00, -1.0193e+01, -1.4188e+00,  ..., -1.0080e+01,\n",
            "         -9.2181e+00, -9.7874e+00],\n",
            "        [ 1.1925e+01, -4.1185e+00, -2.7953e-01,  ..., -9.4646e+00,\n",
            "          1.7534e-02, -3.3859e+00],\n",
            "        [-6.7430e+00, -8.9490e+00, -6.3812e+00,  ...,  9.4371e+00,\n",
            "         -9.1014e+00, -6.9355e+00]], device='cuda:0')\n",
            "Prediction: tensor([9, 7, 9, 1, 3, 3, 1, 0, 3, 8, 9, 3, 0, 8, 0, 7, 0, 4, 8, 3, 0, 7, 0, 7,\n",
            "        0, 5, 5, 4, 3, 0, 9, 4, 0, 2, 2, 0, 8, 6, 4, 5, 2, 2, 1, 1, 5, 5, 4, 3,\n",
            "        4, 0, 8, 4, 8, 3, 5, 6, 5, 1, 7, 2, 4, 0, 5, 6, 8, 7, 6, 3, 1, 3, 6, 6,\n",
            "        4, 5, 5, 5, 1, 8, 7, 3, 0, 2, 5, 1, 1, 1, 6, 7, 2, 1, 2, 3, 9, 8, 3, 1,\n",
            "        7, 4, 8, 4, 7, 5, 8, 6, 0, 5, 0, 8, 5, 4, 4, 9, 2, 6, 1, 0, 5, 7, 9, 2,\n",
            "        1, 3, 1, 3, 0, 4, 0, 7], device='cuda:0')\n",
            "Target: tensor([9, 7, 9, 1, 5, 3, 1, 0, 3, 8, 9, 2, 0, 8, 0, 7, 9, 4, 8, 3, 0, 7, 0, 7,\n",
            "        0, 5, 5, 4, 3, 0, 9, 9, 0, 2, 2, 0, 8, 4, 4, 5, 2, 2, 1, 1, 5, 5, 4, 3,\n",
            "        4, 0, 8, 4, 8, 2, 5, 6, 5, 1, 7, 2, 4, 0, 5, 6, 8, 7, 6, 3, 1, 3, 6, 6,\n",
            "        4, 5, 5, 5, 1, 8, 7, 3, 0, 2, 5, 1, 1, 1, 6, 7, 2, 1, 2, 3, 9, 8, 3, 1,\n",
            "        7, 4, 8, 4, 7, 5, 8, 6, 0, 3, 0, 8, 3, 4, 4, 9, 4, 6, 1, 5, 7, 7, 9, 0,\n",
            "        1, 3, 1, 3, 2, 4, 0, 7], device='cuda:0')\n",
            "Output: tensor([[  5.0374,   0.9163,  -5.5184,  ...,  -6.5684,   3.9049,  -4.8177],\n",
            "        [-11.4714,  -2.7302,  -5.0796,  ...,  -4.0211,  -7.3759,  -3.9119],\n",
            "        [ -8.3349,  17.1240,  -7.0927,  ..., -10.5274,  -9.8364,   4.3241],\n",
            "        ...,\n",
            "        [-12.8851,  -3.8311,  -4.3403,  ...,  -9.2936, -10.8871,  -5.5716],\n",
            "        [ -3.6990,  -5.4033,  -0.6297,  ...,  -5.0007,  -6.3559,  -5.4197],\n",
            "        [ -9.0674,  -6.6376,   5.6528,  ..., -10.1751, -10.6649,  -9.4766]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([0, 3, 1, 6, 2, 1, 8, 8, 1, 9, 7, 9, 3, 2, 9, 0, 6, 8, 4, 1, 5, 7, 0, 4,\n",
            "        7, 0, 4, 9, 2, 7, 0, 6, 7, 1, 8, 3, 8, 8, 7, 0, 0, 2, 2, 5, 7, 0, 6, 4,\n",
            "        4, 9, 1, 8, 6, 3, 9, 6, 9, 3, 3, 7, 5, 9, 7, 5, 4, 5, 8, 2, 8, 9, 5, 3,\n",
            "        1, 9, 2, 2, 0, 1, 9, 8, 1, 1, 9, 8, 9, 9, 8, 2, 6, 9, 1, 9, 0, 1, 9, 2,\n",
            "        9, 9, 9, 0, 9, 9, 2, 1, 7, 0, 5, 6, 3, 9, 8, 3, 0, 2, 7, 7, 4, 8, 3, 0,\n",
            "        4, 2, 3, 5, 7, 6, 6, 6], device='cuda:0')\n",
            "Target: tensor([8, 3, 1, 6, 2, 1, 8, 8, 1, 9, 7, 5, 3, 2, 9, 0, 6, 8, 4, 1, 5, 7, 2, 4,\n",
            "        7, 0, 4, 9, 0, 7, 0, 6, 7, 1, 8, 5, 8, 8, 7, 0, 0, 3, 2, 5, 7, 0, 6, 4,\n",
            "        4, 9, 1, 8, 6, 3, 9, 4, 9, 3, 3, 7, 5, 9, 7, 7, 4, 5, 8, 2, 8, 9, 5, 3,\n",
            "        1, 9, 2, 2, 0, 1, 1, 8, 1, 1, 9, 8, 9, 9, 0, 2, 6, 9, 1, 9, 0, 1, 9, 2,\n",
            "        9, 9, 9, 0, 9, 9, 2, 1, 7, 0, 5, 6, 3, 9, 8, 3, 0, 2, 7, 7, 4, 8, 3, 0,\n",
            "        5, 2, 3, 5, 7, 6, 6, 6], device='cuda:0')\n",
            "Output: tensor([[-0.3005, -7.0237, -3.0533,  ..., -2.5902, -7.2036, -7.6488],\n",
            "        [-5.1219, -4.7068,  0.6250,  ..., -8.6040, -5.9040, -7.6535],\n",
            "        [-5.1035, -5.4745, -3.8184,  ..., -7.3904, -4.7617, -5.7892],\n",
            "        ...,\n",
            "        [-6.8853, -5.5826, -2.8987,  ..., -4.6485,  0.9133, -7.0703],\n",
            "        [-6.3875, -7.8529,  2.1241,  ..., -4.5024, -9.3285, -7.2828],\n",
            "        [-8.5524, -7.5035, -6.4061,  ...,  6.6351, -8.0666, -3.1371]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([5, 6, 3, 4, 0, 8, 8, 0, 9, 5, 9, 0, 6, 9, 6, 1, 2, 7, 0, 0, 7, 5, 7, 7,\n",
            "        6, 9, 8, 2, 3, 9, 3, 3, 7, 8, 3, 5, 3, 9, 6, 1, 6, 7, 4, 8, 7, 7, 1, 3,\n",
            "        0, 3, 2, 5, 4, 9, 0, 5, 4, 7, 0, 3, 7, 0, 2, 1, 5, 8, 7, 3, 5, 7, 8, 5,\n",
            "        7, 8, 1, 5, 4, 7, 0, 8, 3, 2, 9, 7, 4, 1, 6, 5, 9, 8, 5, 5, 4, 1, 0, 9,\n",
            "        4, 3, 3, 0, 4, 8, 0, 8, 2, 9, 5, 9, 7, 4, 6, 7, 9, 2, 9, 5, 7, 7, 8, 2,\n",
            "        2, 0, 2, 5, 3, 6, 4, 7], device='cuda:0')\n",
            "Target: tensor([5, 6, 5, 4, 5, 8, 8, 0, 9, 7, 9, 0, 6, 9, 6, 1, 4, 7, 9, 0, 3, 5, 4, 7,\n",
            "        6, 9, 8, 2, 3, 9, 3, 3, 7, 8, 3, 5, 5, 9, 6, 1, 4, 7, 4, 8, 7, 7, 1, 3,\n",
            "        0, 3, 2, 5, 4, 9, 3, 5, 4, 7, 0, 3, 7, 0, 2, 1, 5, 8, 7, 3, 5, 7, 8, 5,\n",
            "        7, 8, 1, 5, 4, 7, 0, 8, 3, 2, 9, 7, 4, 1, 6, 5, 9, 8, 5, 5, 4, 1, 0, 9,\n",
            "        4, 4, 3, 0, 4, 8, 0, 8, 2, 9, 5, 9, 7, 4, 6, 7, 9, 2, 9, 3, 7, 7, 8, 2,\n",
            "        2, 0, 2, 5, 3, 6, 4, 7], device='cuda:0')\n",
            "Output: tensor([[ -0.6537,  -6.6167,   9.6852,  ...,  -8.4482,  -9.6198,  -7.5957],\n",
            "        [ -5.0603,  -5.9860,  -0.4030,  ...,  -9.2984,  -8.6352,  -7.6084],\n",
            "        [  0.8896,  -8.2360,  -6.0937,  ...,   4.6442,  -2.1534,  -2.6252],\n",
            "        ...,\n",
            "        [ 10.0864,  -7.8644,  12.8464,  ..., -11.7351,  -8.6971,  -6.7283],\n",
            "        [ -0.5470, -13.6887,  22.6993,  ..., -10.6445,  -9.4276, -11.6568],\n",
            "        [ -9.1186,  -6.0773,  -1.9618,  ...,  -9.0635,  -4.6851,  -8.4480]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([2, 3, 7, 8, 7, 2, 5, 0, 0, 7, 0, 9, 6, 1, 0, 3, 9, 7, 0, 9, 1, 6, 8, 1,\n",
            "        2, 3, 5, 5, 4, 8, 9, 5, 4, 4, 1, 2, 7, 9, 8, 7, 9, 5, 1, 2, 1, 6, 6, 4,\n",
            "        5, 7, 7, 5, 8, 3, 4, 8, 7, 8, 2, 3, 6, 1, 5, 3, 1, 5, 1, 9, 0, 9, 2, 0,\n",
            "        8, 2, 4, 8, 5, 7, 6, 1, 2, 9, 4, 5, 0, 3, 3, 7, 4, 7, 1, 4, 5, 0, 2, 8,\n",
            "        5, 0, 0, 6, 2, 0, 8, 4, 5, 9, 5, 6, 4, 7, 9, 4, 2, 0, 6, 2, 2, 0, 6, 4,\n",
            "        6, 1, 1, 5, 5, 2, 2, 6], device='cuda:0')\n",
            "Target: tensor([2, 3, 7, 8, 7, 2, 5, 0, 0, 7, 0, 9, 6, 1, 0, 3, 9, 7, 4, 9, 1, 6, 8, 1,\n",
            "        2, 3, 3, 5, 4, 8, 9, 7, 4, 4, 1, 2, 4, 9, 8, 7, 9, 5, 1, 2, 1, 6, 6, 4,\n",
            "        5, 7, 4, 5, 8, 5, 2, 8, 7, 8, 2, 3, 6, 1, 3, 3, 1, 5, 1, 9, 0, 9, 2, 0,\n",
            "        6, 2, 4, 8, 5, 7, 6, 1, 2, 9, 4, 5, 0, 3, 3, 7, 7, 7, 1, 4, 5, 0, 2, 8,\n",
            "        5, 0, 0, 6, 2, 0, 8, 4, 5, 4, 5, 6, 4, 7, 9, 4, 2, 0, 6, 4, 0, 0, 6, 4,\n",
            "        6, 1, 9, 5, 5, 2, 2, 6], device='cuda:0')\n",
            "Output: tensor([[-13.2976, -10.4070, -12.1654,  ...,  -3.7591,  -6.3910,  -8.3857],\n",
            "        [ -3.4658,  -6.6367,  -1.9263,  ...,  -0.5365,  -4.8333,  -5.3461],\n",
            "        [ -0.7210,  -8.2715,  -1.8981,  ...,  -5.9983,  -4.3970,  -6.2548],\n",
            "        ...,\n",
            "        [-10.9973,  -7.8795,   1.2949,  ...,  -7.5075, -13.3275,  -9.3143],\n",
            "        [ -1.9761,  -4.5612,  -1.4260,  ...,  -7.8351,  -7.3898,  -3.3017],\n",
            "        [ -8.3736,  22.3507, -10.7377,  ..., -12.7036,  -7.5668,  -0.3933]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 4, 5, 9, 1, 7, 2, 3, 9, 6, 5, 0, 2, 9, 7, 1, 7, 2, 9, 0, 8, 6, 4, 3,\n",
            "        2, 5, 7, 0, 4, 1, 6, 5, 1, 3, 0, 3, 9, 0, 0, 2, 5, 0, 4, 0, 1, 9, 8, 4,\n",
            "        9, 4, 2, 4, 3, 5, 4, 0, 4, 3, 5, 8, 9, 1, 5, 8, 1, 8, 2, 4, 4, 2, 4, 1,\n",
            "        1, 6, 6, 8, 5, 2, 2, 5, 0, 8, 2, 3, 6, 2, 9, 6, 1, 4, 5, 9, 0, 1, 4, 0,\n",
            "        8, 1, 1, 3, 6, 9, 5, 2, 9, 7, 8, 6, 9, 1, 7, 6, 0, 9, 3, 5, 3, 2, 5, 3,\n",
            "        4, 9, 7, 1, 4, 4, 6, 1], device='cuda:0')\n",
            "Target: tensor([3, 4, 5, 9, 1, 7, 2, 3, 9, 6, 5, 0, 2, 9, 7, 1, 7, 2, 2, 0, 8, 6, 4, 3,\n",
            "        2, 7, 7, 0, 4, 1, 6, 5, 1, 3, 0, 3, 9, 0, 0, 2, 5, 0, 4, 0, 1, 9, 8, 4,\n",
            "        9, 4, 2, 4, 3, 3, 4, 0, 4, 3, 2, 8, 9, 1, 5, 8, 1, 8, 2, 4, 5, 2, 4, 1,\n",
            "        1, 6, 6, 8, 5, 2, 2, 5, 0, 8, 2, 3, 6, 2, 9, 6, 1, 4, 5, 9, 0, 1, 0, 0,\n",
            "        8, 1, 1, 6, 6, 9, 5, 4, 1, 7, 8, 6, 9, 1, 7, 6, 0, 9, 3, 5, 3, 2, 5, 3,\n",
            "        4, 9, 7, 1, 4, 4, 6, 1], device='cuda:0')\n",
            "Output: tensor([[ -3.9807,  -8.9548, -10.1407,  ..., -11.7270,  -9.0884, -10.4938],\n",
            "        [  4.4569,  -5.7699,  -2.4751,  ...,  -7.2469,  14.5654,  -4.6615],\n",
            "        [  0.4245,  -8.3426,  -1.7748,  ...,  -8.7333,  21.3643,  -5.8828],\n",
            "        ...,\n",
            "        [ -1.3919,  -5.4591, -11.0259,  ...,  -0.8178,  -9.4966,  -7.3081],\n",
            "        [ -2.1265,  -8.1586,  16.4956,  ...,  -4.3908,  -5.7115,  -8.4637],\n",
            "        [ -7.3097,  -3.0762,  -0.4324,  ...,  -6.0769,  -2.4700,  -6.2549]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 8, 8, 0, 6, 7, 7, 6, 7, 2, 5, 2, 6, 6, 2, 7, 2, 0, 3, 6, 2, 6, 3, 2,\n",
            "        0, 9, 3, 1, 1, 5, 3, 2, 4, 3, 4, 1, 0, 4, 3, 5, 2, 8, 9, 3, 3, 1, 8, 0,\n",
            "        1, 3, 3, 5, 4, 2, 9, 7, 6, 8, 1, 8, 9, 9, 3, 1, 7, 3, 0, 0, 2, 8, 3, 9,\n",
            "        2, 7, 2, 6, 0, 1, 6, 1, 6, 7, 5, 5, 2, 5, 9, 4, 0, 2, 3, 4, 9, 4, 1, 0,\n",
            "        0, 2, 3, 8, 9, 2, 8, 9, 5, 7, 9, 1, 4, 6, 2, 8, 3, 4, 8, 9, 3, 1, 1, 6,\n",
            "        4, 8, 2, 6, 6, 5, 2, 6], device='cuda:0')\n",
            "Target: tensor([3, 8, 8, 0, 6, 7, 7, 6, 7, 2, 3, 2, 2, 6, 2, 7, 4, 0, 3, 6, 2, 6, 3, 3,\n",
            "        0, 9, 5, 1, 1, 5, 3, 6, 4, 3, 4, 1, 0, 4, 5, 5, 2, 8, 9, 4, 3, 1, 8, 0,\n",
            "        1, 3, 3, 4, 4, 2, 9, 7, 6, 8, 1, 8, 9, 1, 3, 1, 7, 3, 0, 0, 2, 8, 3, 9,\n",
            "        2, 7, 2, 6, 0, 1, 6, 1, 6, 7, 5, 5, 2, 5, 9, 4, 0, 2, 3, 4, 9, 4, 1, 0,\n",
            "        0, 2, 3, 8, 9, 2, 8, 9, 5, 7, 9, 1, 4, 6, 2, 8, 4, 4, 8, 9, 3, 1, 1, 6,\n",
            "        5, 8, 4, 6, 4, 5, 2, 6], device='cuda:0')\n",
            "Output: tensor([[ -9.5341,  -6.4480,  -3.5291,  ...,  -6.1996,  -6.3089,  -9.5451],\n",
            "        [ -7.1613,  -7.1352,  13.7644,  ...,  -6.0552, -10.1268, -10.4707],\n",
            "        [-10.1788,  -7.1701, -10.4593,  ...,  -9.7173,  -6.1022,  -4.8164],\n",
            "        ...,\n",
            "        [ -4.6727,  -4.5664, -13.2168,  ...,  -9.3701,  -6.6946,  24.6981],\n",
            "        [ -4.5098,  14.6145,  -4.5341,  ..., -10.7200,  -5.0361,   0.0475],\n",
            "        [-11.5459,  -9.7431,  -5.1592,  ...,  -6.0505,  -9.0705, -11.5200]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([4, 2, 3, 1, 6, 6, 8, 5, 2, 2, 7, 1, 3, 2, 5, 9, 6, 8, 1, 6, 9, 2, 7, 5,\n",
            "        3, 2, 9, 7, 0, 2, 9, 3, 4, 1, 7, 9, 5, 8, 9, 7, 8, 6, 0, 9, 7, 0, 2, 4,\n",
            "        7, 0, 1, 8, 1, 0, 4, 6, 1, 9, 9, 0, 9, 2, 5, 6, 9, 7, 7, 3, 4, 2, 0, 2,\n",
            "        5, 6, 7, 6, 7, 3, 4, 2, 0, 6, 1, 7, 5, 6, 5, 3, 9, 2, 7, 8, 5, 9, 5, 8,\n",
            "        5, 4, 3, 7, 9, 8, 1, 2, 2, 8, 9, 3, 0, 8, 4, 0, 2, 0, 1, 4, 0, 8, 5, 5,\n",
            "        7, 9, 8, 3, 7, 9, 1, 5], device='cuda:0')\n",
            "Target: tensor([4, 2, 3, 1, 3, 6, 8, 5, 2, 2, 7, 1, 6, 2, 5, 9, 2, 8, 1, 6, 9, 2, 7, 5,\n",
            "        3, 2, 9, 7, 0, 2, 9, 3, 4, 1, 7, 9, 5, 8, 9, 7, 3, 4, 0, 9, 7, 4, 2, 4,\n",
            "        7, 0, 1, 8, 1, 0, 4, 6, 1, 9, 9, 2, 1, 2, 5, 6, 9, 7, 7, 3, 4, 2, 0, 2,\n",
            "        5, 6, 7, 3, 7, 9, 4, 2, 0, 6, 1, 7, 5, 6, 5, 3, 9, 2, 7, 8, 5, 9, 5, 8,\n",
            "        5, 4, 3, 7, 9, 8, 1, 2, 2, 8, 9, 3, 0, 8, 4, 0, 2, 0, 1, 4, 1, 8, 5, 5,\n",
            "        7, 9, 8, 3, 7, 9, 1, 5], device='cuda:0')\n",
            "Output: tensor([[-10.2943,  -5.9526,  -1.1565,  ..., -10.8337,  -2.1283, -10.0047],\n",
            "        [ -4.1537,  -0.3585, -12.4902,  ..., -11.0239,  -6.4455,  24.4890],\n",
            "        [  3.9854,  -2.2249,  -4.4039,  ...,  -3.5413,   7.7515,   3.1548],\n",
            "        ...,\n",
            "        [ 11.3294,  -4.1991,  -0.2167,  ...,  -8.8525,   4.9708,  -5.2790],\n",
            "        [ -2.5017,  -8.1055,   6.2118,  ...,  -7.1857,  -5.7595,  -7.1018],\n",
            "        [ -8.7795,  -4.6920,  -4.2138,  ...,   0.5877,  -9.2970,  -1.4761]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([6, 9, 8, 7, 2, 0, 9, 0, 8, 5, 9, 2, 2, 9, 8, 1, 9, 1, 8, 3, 7, 6, 4, 2,\n",
            "        9, 7, 0, 3, 5, 8, 8, 8, 7, 9, 6, 2, 7, 4, 5, 7, 6, 7, 1, 2, 3, 6, 8, 3,\n",
            "        6, 6, 7, 1, 5, 9, 7, 1, 7, 0, 1, 2, 3, 3, 9, 0, 1, 2, 5, 2, 2, 5, 4, 9,\n",
            "        8, 7, 4, 4, 9, 7, 6, 7, 7, 1, 6, 3, 5, 1, 9, 0, 3, 3, 1, 5, 6, 6, 2, 4,\n",
            "        6, 8, 8, 9, 6, 6, 1, 0, 7, 5, 8, 2, 1, 5, 8, 1, 4, 7, 5, 0, 3, 0, 9, 5,\n",
            "        2, 8, 7, 1, 9, 0, 4, 4], device='cuda:0')\n",
            "Target: tensor([6, 9, 8, 7, 2, 0, 9, 0, 8, 5, 9, 4, 2, 9, 8, 1, 9, 1, 8, 3, 7, 6, 4, 2,\n",
            "        3, 7, 0, 3, 5, 8, 8, 8, 7, 9, 6, 2, 7, 4, 5, 7, 6, 7, 1, 7, 3, 6, 8, 2,\n",
            "        6, 6, 7, 1, 5, 9, 7, 1, 7, 0, 1, 6, 3, 3, 9, 0, 1, 2, 3, 2, 2, 5, 4, 9,\n",
            "        8, 7, 4, 4, 9, 7, 6, 7, 7, 1, 2, 3, 5, 1, 9, 0, 3, 3, 1, 5, 6, 6, 2, 4,\n",
            "        6, 8, 8, 9, 6, 6, 1, 0, 7, 5, 8, 2, 1, 5, 8, 1, 4, 7, 5, 0, 3, 9, 9, 5,\n",
            "        2, 8, 4, 1, 9, 0, 4, 4], device='cuda:0')\n",
            "Output: tensor([[ -3.4429,  -2.8246,  -3.8458,  ...,  -8.2998,  13.7345,  -0.5528],\n",
            "        [ -6.4611,  -4.4429,  18.0192,  ...,  -7.1129,  -9.1450, -10.8872],\n",
            "        [ -2.6968,   1.0706,  -4.9437,  ...,  -5.5789,  -2.8876,   9.1226],\n",
            "        ...,\n",
            "        [ -8.5070,  -4.6947,  -5.2788,  ...,  -6.6634,  -7.8683,  -6.3805],\n",
            "        [ -6.0531,  -5.9941,  -6.4625,  ...,  -0.5105,  -9.2966,  -6.7820],\n",
            "        [ -0.1101,  -4.3370,  -3.0089,  ...,  -2.9670,  11.3281,  -4.6467]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([8, 2, 9, 0, 7, 9, 8, 7, 1, 6, 2, 9, 2, 3, 9, 7, 6, 7, 7, 1, 1, 2, 4, 2,\n",
            "        0, 6, 8, 9, 6, 2, 2, 9, 7, 2, 4, 2, 0, 5, 8, 6, 8, 2, 7, 7, 0, 9, 8, 1,\n",
            "        6, 5, 9, 7, 0, 9, 6, 4, 8, 1, 9, 4, 0, 4, 1, 4, 5, 6, 2, 2, 2, 0, 0, 7,\n",
            "        0, 7, 4, 3, 6, 7, 7, 4, 5, 4, 3, 3, 5, 4, 0, 0, 0, 7, 7, 1, 6, 5, 0, 7,\n",
            "        7, 4, 6, 1, 8, 5, 9, 3, 5, 6, 5, 2, 7, 5, 5, 8, 1, 0, 6, 8, 7, 8, 8, 5,\n",
            "        7, 5, 4, 9, 7, 3, 5, 8], device='cuda:0')\n",
            "Target: tensor([8, 2, 9, 0, 7, 9, 8, 7, 1, 3, 2, 9, 9, 5, 9, 7, 6, 7, 7, 1, 1, 2, 4, 2,\n",
            "        0, 6, 8, 7, 6, 2, 2, 9, 8, 2, 4, 2, 0, 5, 8, 6, 8, 2, 7, 7, 3, 1, 8, 1,\n",
            "        6, 5, 9, 7, 8, 9, 6, 4, 8, 1, 9, 4, 0, 4, 1, 4, 3, 6, 2, 2, 7, 0, 0, 7,\n",
            "        0, 7, 4, 3, 6, 7, 7, 4, 5, 4, 3, 5, 5, 4, 7, 1, 0, 7, 7, 1, 6, 5, 0, 7,\n",
            "        7, 4, 6, 1, 8, 5, 9, 3, 5, 6, 2, 2, 7, 3, 5, 8, 1, 0, 6, 8, 7, 8, 8, 5,\n",
            "        7, 5, 4, 9, 7, 3, 3, 8], device='cuda:0')\n",
            "Output: tensor([[  3.3145,  -4.8742,   1.7693,  ...,  -5.7748,   9.8297,  -5.1166],\n",
            "        [ -4.5016,  -1.3661,  -5.3923,  ...,  -2.8882,  -8.6826,  -3.5594],\n",
            "        [ -4.6946,   0.6388, -14.4138,  ...,  -9.8420,  -2.3481,  20.5978],\n",
            "        ...,\n",
            "        [  5.8904,  -5.7427,   8.1957,  ...,  -7.6916,  -0.6538,  -7.1156],\n",
            "        [ -4.1483,  -0.3725,  -5.0321,  ...,  -8.8654,  14.7232,  -3.9478],\n",
            "        [  4.1669,  -6.6693,  -4.5290,  ...,  -6.9042,  18.5014,  -0.0759]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([8, 3, 9, 4, 7, 2, 0, 8, 0, 7, 3, 3, 2, 5, 2, 4, 5, 0, 2, 8, 2, 4, 0, 6,\n",
            "        4, 5, 6, 0, 8, 0, 0, 6, 1, 1, 6, 1, 4, 2, 1, 0, 2, 5, 4, 3, 7, 8, 8, 5,\n",
            "        1, 8, 2, 3, 3, 9, 6, 6, 1, 0, 7, 3, 3, 2, 7, 4, 5, 6, 0, 6, 1, 8, 1, 0,\n",
            "        9, 3, 1, 0, 5, 4, 2, 3, 3, 6, 7, 6, 8, 0, 5, 7, 4, 7, 7, 0, 6, 1, 3, 9,\n",
            "        0, 9, 0, 3, 8, 4, 0, 8, 4, 1, 0, 2, 2, 4, 1, 1, 2, 7, 3, 4, 1, 4, 7, 1,\n",
            "        0, 0, 9, 0, 4, 2, 8, 8], device='cuda:0')\n",
            "Target: tensor([8, 3, 9, 4, 7, 2, 0, 8, 0, 7, 3, 3, 2, 5, 2, 4, 4, 0, 4, 8, 2, 4, 0, 6,\n",
            "        4, 5, 6, 0, 8, 8, 0, 6, 1, 1, 6, 1, 4, 2, 1, 1, 2, 4, 4, 5, 5, 8, 8, 5,\n",
            "        1, 8, 2, 3, 3, 9, 6, 6, 5, 0, 7, 3, 3, 2, 7, 4, 5, 6, 0, 2, 1, 8, 1, 0,\n",
            "        9, 3, 1, 0, 5, 4, 2, 3, 3, 6, 7, 6, 0, 0, 5, 7, 4, 7, 7, 0, 6, 1, 3, 9,\n",
            "        0, 9, 0, 3, 8, 4, 8, 8, 4, 1, 0, 2, 2, 4, 1, 1, 2, 4, 3, 4, 1, 4, 7, 1,\n",
            "        0, 0, 9, 0, 4, 2, 8, 8], device='cuda:0')\n",
            "Output: tensor([[ -3.4636,  -8.1978,  -5.5383,  ...,  -5.0349,  -7.7021, -10.2192],\n",
            "        [ -8.3015,  -6.6357,   5.4616,  ..., -10.1899,  -6.4310, -11.6043],\n",
            "        [ 16.4471,  -3.6039,  -1.1134,  ...,  -9.4286,  -1.6991,  -5.6973],\n",
            "        ...,\n",
            "        [ 16.6536,  -5.5320,   3.3238,  ..., -10.6580,   0.0289,  -6.9647],\n",
            "        [ -5.2439,  -8.1163,  -9.0038,  ...,   4.5627,  -9.7981,  -4.5643],\n",
            "        [ -0.5318,  -6.2849,  -5.3653,  ...,  -5.8631,  17.3147,  -4.7256]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([5, 5, 0, 4, 0, 8, 2, 2, 5, 9, 2, 9, 1, 1, 5, 4, 7, 5, 8, 6, 6, 1, 5, 5,\n",
            "        3, 4, 1, 0, 9, 9, 8, 9, 8, 6, 8, 5, 8, 9, 4, 6, 2, 6, 5, 7, 4, 0, 0, 1,\n",
            "        7, 5, 1, 3, 1, 3, 1, 6, 8, 7, 3, 6, 9, 1, 2, 0, 1, 7, 2, 6, 1, 9, 0, 0,\n",
            "        8, 9, 9, 2, 8, 6, 5, 5, 6, 0, 3, 3, 0, 5, 4, 7, 5, 0, 1, 6, 8, 8, 1, 2,\n",
            "        0, 5, 4, 5, 9, 6, 7, 1, 0, 6, 9, 2, 7, 7, 5, 9, 9, 9, 9, 7, 0, 1, 3, 5,\n",
            "        4, 6, 3, 0, 8, 0, 4, 8], device='cuda:0')\n",
            "Target: tensor([5, 5, 0, 4, 0, 8, 6, 2, 5, 9, 2, 9, 1, 1, 5, 4, 7, 5, 8, 6, 2, 1, 5, 5,\n",
            "        3, 4, 1, 8, 9, 9, 8, 9, 8, 6, 8, 5, 8, 9, 4, 6, 2, 6, 3, 7, 4, 0, 0, 1,\n",
            "        7, 5, 1, 5, 9, 3, 1, 6, 8, 7, 3, 6, 9, 1, 2, 0, 1, 7, 2, 6, 1, 9, 0, 0,\n",
            "        8, 9, 9, 2, 8, 6, 2, 5, 6, 0, 3, 3, 0, 7, 4, 7, 5, 0, 1, 6, 8, 8, 1, 2,\n",
            "        1, 5, 4, 5, 9, 6, 7, 1, 0, 6, 9, 2, 7, 7, 3, 9, 9, 1, 9, 7, 0, 1, 3, 5,\n",
            "        4, 6, 3, 8, 8, 0, 4, 8], device='cuda:0')\n",
            "Output: tensor([[ -7.1474,  -3.4791,  -0.6806,  ...,  -0.3805,  -5.6195,  -3.0945],\n",
            "        [ -9.1309,  -6.5862,   0.3865,  ...,  -7.0138,  -8.7688,  -8.4047],\n",
            "        [ -4.2633,  -4.1878,  -1.7283,  ...,   4.3627,  -9.6211,  -7.0061],\n",
            "        ...,\n",
            "        [ -0.8893,  -6.0711,   4.9926,  ..., -11.3803,  -8.1138,  -9.0735],\n",
            "        [ -1.4691,   5.8080,  -5.9733,  ...,  -5.2886,   3.5941,  -0.4258],\n",
            "        [ -7.5542,  -3.6006,  -6.0430,  ...,   6.0591, -12.4168,  -7.4987]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 6, 7, 0, 0, 4, 5, 2, 6, 8, 4, 9, 9, 9, 6, 0, 4, 2, 8, 1, 1, 0, 0, 3,\n",
            "        7, 4, 1, 1, 9, 7, 7, 4, 6, 8, 6, 0, 2, 8, 5, 8, 5, 3, 5, 7, 9, 8, 4, 4,\n",
            "        5, 1, 4, 8, 3, 6, 5, 3, 0, 8, 9, 5, 7, 6, 2, 0, 2, 9, 9, 0, 5, 2, 3, 6,\n",
            "        1, 1, 0, 2, 8, 2, 1, 1, 7, 5, 2, 3, 4, 1, 2, 9, 2, 1, 9, 4, 8, 9, 0, 0,\n",
            "        4, 9, 0, 2, 2, 0, 6, 8, 7, 3, 3, 8, 9, 0, 2, 2, 3, 6, 1, 3, 9, 5, 0, 5,\n",
            "        4, 4, 0, 1, 1, 6, 1, 7], device='cuda:0')\n",
            "Target: tensor([3, 6, 7, 0, 0, 4, 5, 2, 6, 8, 4, 9, 9, 9, 2, 0, 4, 2, 8, 1, 1, 0, 0, 3,\n",
            "        7, 4, 1, 1, 9, 7, 7, 4, 6, 8, 6, 0, 2, 8, 5, 3, 5, 3, 5, 7, 9, 8, 4, 4,\n",
            "        3, 1, 4, 8, 3, 6, 5, 3, 0, 8, 9, 5, 7, 6, 2, 0, 4, 9, 9, 0, 5, 2, 3, 6,\n",
            "        1, 1, 0, 2, 8, 2, 1, 1, 7, 5, 2, 3, 4, 1, 2, 9, 2, 1, 3, 4, 8, 9, 0, 0,\n",
            "        4, 9, 0, 2, 2, 0, 6, 8, 7, 3, 3, 8, 9, 0, 2, 5, 3, 6, 1, 3, 9, 5, 0, 5,\n",
            "        4, 4, 0, 1, 1, 6, 1, 7], device='cuda:0')\n",
            "Output: tensor([[-11.4643,   6.4091, -12.0563,  ..., -14.0723,  -9.8379,  20.5813],\n",
            "        [ -2.3970,   1.7683,  -9.7549,  ...,  -6.8894,  -3.4420,   4.7660],\n",
            "        [ -7.3627,  -4.8627,  -4.1332,  ...,  -2.4091, -10.2002,  -2.9862],\n",
            "        ...,\n",
            "        [ -4.3027,  -6.4609,   4.0857,  ...,  -8.0364,  -3.7107,  -8.1690],\n",
            "        [  8.6193,  -4.8519,   0.1727,  ...,  -6.0368,  -0.9894,  -3.7960],\n",
            "        [ -5.5820,  11.1076,  -5.1756,  ..., -12.6763,  -8.2248,   2.8341]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([9, 9, 6, 2, 4, 5, 8, 3, 4, 7, 0, 2, 8, 4, 8, 3, 8, 8, 8, 3, 3, 5, 7, 7,\n",
            "        0, 4, 1, 5, 9, 7, 0, 6, 8, 4, 9, 0, 1, 8, 9, 6, 3, 9, 2, 4, 4, 0, 3, 3,\n",
            "        5, 4, 5, 1, 1, 8, 2, 2, 9, 3, 7, 8, 9, 2, 3, 1, 7, 4, 0, 6, 0, 1, 9, 5,\n",
            "        5, 3, 4, 5, 2, 0, 0, 3, 1, 3, 7, 3, 4, 4, 8, 2, 8, 9, 9, 2, 2, 4, 5, 6,\n",
            "        4, 6, 4, 6, 6, 3, 8, 7, 4, 8, 0, 5, 9, 5, 8, 0, 7, 5, 1, 9, 7, 9, 1, 8,\n",
            "        4, 2, 1, 6, 4, 2, 0, 1], device='cuda:0')\n",
            "Target: tensor([9, 9, 6, 2, 4, 3, 8, 3, 4, 7, 0, 2, 8, 4, 8, 3, 8, 8, 8, 3, 3, 5, 7, 7,\n",
            "        0, 4, 1, 5, 9, 7, 0, 6, 8, 4, 9, 0, 1, 8, 9, 6, 3, 9, 2, 4, 4, 0, 3, 3,\n",
            "        5, 4, 5, 1, 1, 8, 2, 2, 9, 3, 7, 8, 9, 2, 3, 1, 7, 3, 2, 3, 0, 1, 9, 5,\n",
            "        5, 3, 4, 5, 2, 0, 0, 3, 1, 3, 4, 7, 4, 2, 8, 4, 8, 9, 9, 4, 2, 4, 3, 6,\n",
            "        4, 6, 4, 6, 6, 3, 8, 7, 4, 8, 0, 5, 9, 3, 9, 0, 7, 5, 1, 9, 7, 9, 1, 8,\n",
            "        4, 2, 1, 6, 4, 3, 0, 1], device='cuda:0')\n",
            "Output: tensor([[ -7.6152,  -8.0096,  -5.2947,  ...,  -4.5443,  -6.4624,  -2.5460],\n",
            "        [  5.5667,  -5.5652,  -6.7621,  ...,  -7.7210,  12.7944,   1.9944],\n",
            "        [ -6.0313,   0.1654,  -4.9167,  ...,  -6.6088,  -5.5305,  -6.3179],\n",
            "        ...,\n",
            "        [ -1.6579,  -3.1767,  -3.0795,  ...,  -8.1985, -10.1724,  -6.6750],\n",
            "        [ -6.8909,  -6.6380,  -3.5368,  ...,  -5.2845,  -9.2475,  -7.7209],\n",
            "        [  5.2427,  -2.6207,  -5.2752,  ...,  -7.7343,   6.6815,   2.0552]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([4, 8, 6, 1, 7, 1, 3, 5, 4, 3, 3, 9, 7, 8, 2, 5, 5, 4, 5, 4, 5, 7, 3, 5,\n",
            "        7, 1, 7, 7, 8, 3, 6, 9, 2, 5, 2, 3, 7, 4, 2, 1, 6, 8, 6, 8, 3, 2, 7, 7,\n",
            "        9, 7, 1, 4, 7, 3, 6, 1, 7, 3, 0, 0, 6, 6, 8, 6, 6, 0, 3, 4, 7, 4, 9, 4,\n",
            "        9, 9, 3, 4, 1, 4, 0, 3, 7, 1, 2, 2, 8, 4, 7, 8, 5, 5, 6, 5, 6, 0, 6, 4,\n",
            "        9, 4, 2, 7, 3, 8, 6, 9, 4, 1, 6, 9, 9, 4, 3, 9, 8, 1, 6, 9, 5, 1, 0, 9,\n",
            "        7, 5, 2, 4, 6, 6, 4, 8], device='cuda:0')\n",
            "Target: tensor([4, 8, 6, 1, 7, 1, 3, 5, 4, 3, 3, 9, 7, 8, 2, 5, 5, 4, 5, 4, 5, 7, 2, 5,\n",
            "        5, 1, 7, 7, 8, 3, 6, 0, 2, 5, 2, 3, 7, 4, 2, 1, 6, 8, 6, 8, 3, 2, 7, 7,\n",
            "        9, 7, 1, 4, 7, 4, 6, 1, 7, 3, 0, 0, 6, 6, 8, 6, 6, 0, 3, 4, 7, 4, 9, 4,\n",
            "        9, 9, 3, 4, 1, 4, 0, 3, 7, 1, 2, 2, 8, 4, 7, 8, 5, 5, 6, 5, 6, 0, 6, 4,\n",
            "        9, 3, 2, 7, 3, 8, 3, 9, 4, 1, 6, 9, 9, 4, 3, 9, 8, 1, 6, 9, 5, 9, 0, 9,\n",
            "        7, 2, 2, 4, 2, 6, 4, 8], device='cuda:0')\n",
            "Output: tensor([[ -3.0735,  13.9068,  -4.1227,  ...,  -9.6056,  -4.3412,  -0.7631],\n",
            "        [ -5.0257,  -1.4211,  -6.4379,  ...,  -5.4701,  -2.5832,  14.3515],\n",
            "        [ -8.1197,  -4.8294,  -5.4871,  ..., -11.0874, -10.2774,  -8.4296],\n",
            "        ...,\n",
            "        [ 11.9045,  -2.5776,  -5.5400,  ...,  -7.8469,   1.4501,   1.2068],\n",
            "        [ -3.0665,  -7.0798,   6.3836,  ...,  -4.6716,  -6.1636,  -5.2105],\n",
            "        [ -5.6478,  13.2290,  -7.4272,  ...,  -9.1752,  -8.0406,   3.9305]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([1, 9, 5, 6, 3, 1, 8, 6, 0, 7, 6, 5, 0, 2, 9, 6, 1, 2, 8, 1, 6, 4, 9, 0,\n",
            "        3, 9, 1, 0, 5, 3, 1, 6, 9, 8, 9, 0, 6, 1, 6, 3, 0, 5, 6, 9, 0, 7, 2, 4,\n",
            "        8, 3, 6, 8, 6, 9, 1, 9, 0, 6, 2, 5, 9, 5, 4, 1, 9, 2, 0, 7, 3, 5, 1, 8,\n",
            "        5, 0, 5, 2, 8, 8, 3, 9, 6, 0, 9, 5, 3, 4, 1, 5, 0, 7, 3, 9, 4, 5, 3, 1,\n",
            "        4, 2, 4, 9, 9, 7, 1, 7, 0, 1, 2, 1, 5, 3, 8, 4, 1, 5, 1, 9, 9, 7, 0, 1,\n",
            "        7, 6, 2, 6, 5, 0, 2, 1], device='cuda:0')\n",
            "Target: tensor([1, 9, 5, 6, 5, 1, 8, 6, 6, 7, 6, 5, 0, 2, 7, 6, 1, 2, 3, 1, 6, 4, 9, 0,\n",
            "        3, 9, 1, 0, 5, 3, 1, 6, 9, 8, 9, 0, 6, 1, 6, 2, 3, 5, 6, 9, 0, 7, 2, 4,\n",
            "        0, 3, 6, 8, 6, 9, 1, 9, 0, 6, 4, 5, 9, 5, 4, 1, 9, 2, 0, 7, 3, 5, 1, 8,\n",
            "        3, 0, 5, 2, 8, 8, 3, 9, 6, 0, 3, 5, 3, 4, 1, 5, 0, 7, 3, 9, 4, 5, 3, 1,\n",
            "        4, 2, 4, 9, 9, 7, 1, 7, 0, 1, 2, 1, 5, 3, 8, 4, 1, 5, 1, 9, 9, 7, 0, 1,\n",
            "        7, 6, 2, 6, 5, 0, 3, 1], device='cuda:0')\n",
            "Output: tensor([[ -2.2264,  -3.6429,  -3.4674,  ...,  -5.7524,  -7.1691,  -5.2736],\n",
            "        [  0.8335,   0.6829,  -7.5870,  ...,  -7.4368,   1.9130,   5.7194],\n",
            "        [ -4.8888,  -0.4306,  -9.2745,  ...,  -6.5047,  -2.6684,  12.9255],\n",
            "        ...,\n",
            "        [-10.0473,  -5.9900,  -1.3801,  ..., -10.4068,  -6.1176, -10.5232],\n",
            "        [ -3.3658,  -6.8592,  -7.5033,  ...,   7.0097,  -7.5565,  -2.9157],\n",
            "        [ 12.0738,  -4.5200,   0.6789,  ...,  -7.5380,  -0.3379,  -3.1079]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 9, 9, 9, 0, 8, 8, 5, 9, 2, 9, 1, 3, 6, 7, 6, 0, 8, 6, 2, 4, 2, 3, 5,\n",
            "        5, 4, 1, 6, 5, 1, 2, 1, 7, 9, 6, 2, 3, 0, 4, 2, 9, 7, 3, 5, 6, 0, 8, 8,\n",
            "        1, 1, 3, 5, 6, 5, 3, 8, 7, 2, 0, 7, 7, 5, 7, 1, 9, 9, 1, 1, 9, 3, 5, 3,\n",
            "        3, 0, 3, 0, 6, 6, 0, 6, 8, 9, 4, 5, 1, 1, 2, 7, 9, 0, 1, 2, 5, 4, 4, 7,\n",
            "        4, 8, 3, 3, 4, 4, 5, 7, 0, 1, 2, 9, 8, 1, 7, 3, 5, 9, 1, 9, 1, 1, 3, 6,\n",
            "        7, 4, 8, 3, 5, 6, 7, 0], device='cuda:0')\n",
            "Target: tensor([3, 8, 9, 9, 0, 8, 8, 7, 9, 2, 9, 1, 3, 6, 7, 6, 0, 8, 6, 2, 4, 2, 3, 5,\n",
            "        5, 4, 1, 6, 7, 1, 6, 1, 7, 1, 6, 2, 3, 0, 4, 2, 9, 7, 5, 5, 6, 0, 8, 8,\n",
            "        1, 1, 3, 5, 6, 5, 3, 8, 7, 5, 0, 7, 7, 5, 7, 1, 9, 9, 1, 9, 9, 3, 5, 5,\n",
            "        3, 0, 3, 0, 6, 6, 5, 6, 8, 9, 4, 5, 1, 1, 2, 7, 9, 0, 1, 2, 5, 4, 4, 7,\n",
            "        4, 8, 3, 3, 5, 3, 5, 7, 0, 1, 2, 9, 8, 1, 7, 3, 5, 9, 1, 9, 1, 1, 3, 6,\n",
            "        7, 4, 8, 3, 5, 6, 7, 0], device='cuda:0')\n",
            "Output: tensor([[ -5.8091,  22.1006,  -9.2306,  ..., -11.9293,  -6.3004,   2.0855],\n",
            "        [ -1.4763, -10.3159,  -3.7852,  ...,   5.6174,  -6.6869,  -3.5257],\n",
            "        [-14.5889, -14.7107, -11.7584,  ...,  25.0706, -14.0898,  -6.5432],\n",
            "        ...,\n",
            "        [ -6.8995,  -7.2330,  -7.8473,  ...,   6.5664,  -6.5273,  -4.8300],\n",
            "        [ -7.0797,  -6.5117,   7.3413,  ...,  -5.1525,  -2.4361,  -3.6067],\n",
            "        [ -1.9718,  -4.3394,  -6.5965,  ...,  -0.3853,  -7.6809,  -1.5404]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([1, 7, 7, 2, 5, 3, 5, 9, 7, 6, 8, 4, 4, 5, 6, 3, 4, 7, 7, 3, 6, 0, 7, 2,\n",
            "        6, 7, 4, 1, 8, 0, 2, 6, 1, 5, 4, 0, 9, 3, 4, 4, 2, 3, 8, 5, 0, 0, 4, 6,\n",
            "        3, 9, 2, 0, 5, 7, 5, 5, 1, 7, 3, 1, 6, 7, 9, 0, 9, 2, 1, 4, 6, 1, 5, 9,\n",
            "        5, 0, 4, 9, 3, 9, 7, 1, 6, 8, 8, 6, 5, 2, 8, 1, 7, 5, 6, 9, 3, 2, 3, 9,\n",
            "        8, 6, 1, 0, 9, 0, 9, 8, 7, 7, 2, 2, 3, 5, 2, 7, 9, 7, 6, 9, 1, 9, 8, 6,\n",
            "        9, 6, 7, 5, 6, 7, 2, 5], device='cuda:0')\n",
            "Target: tensor([1, 7, 7, 2, 3, 3, 5, 1, 7, 6, 8, 4, 4, 5, 7, 3, 4, 7, 7, 3, 6, 0, 7, 2,\n",
            "        6, 7, 4, 1, 8, 0, 2, 2, 1, 5, 4, 0, 9, 3, 4, 4, 2, 3, 8, 5, 0, 0, 4, 6,\n",
            "        3, 9, 2, 0, 5, 7, 5, 5, 1, 7, 3, 1, 6, 7, 9, 0, 1, 2, 1, 4, 6, 1, 5, 9,\n",
            "        5, 0, 4, 9, 4, 9, 7, 1, 6, 8, 0, 6, 5, 0, 8, 1, 7, 5, 6, 9, 3, 2, 3, 9,\n",
            "        8, 6, 1, 0, 9, 0, 9, 8, 7, 7, 5, 2, 3, 5, 5, 7, 8, 7, 6, 9, 1, 9, 8, 6,\n",
            "        5, 6, 7, 5, 6, 7, 2, 5], device='cuda:0')\n",
            "Output: tensor([[ -0.4310,  -3.8405, -15.1553,  ..., -10.3232,  -3.6122,  25.4884],\n",
            "        [ -8.7752,  -7.6768,   3.3090,  ...,  -8.9178,  -9.3902,  -7.3886],\n",
            "        [-11.7809,  -9.4364,  -0.0969,  ...,  -7.0325, -12.2680,  -7.3340],\n",
            "        ...,\n",
            "        [ -2.5403,  15.7543,  -7.1069,  ...,  -7.7545,  -4.9300,  -0.6264],\n",
            "        [ -4.3020, -10.1607,  -3.5748,  ...,  10.7111,  -9.9845,  -5.3163],\n",
            "        [ -2.2569,  -7.8613,  -1.7120,  ...,   9.8138,  -7.4745,  -5.6609]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([9, 6, 6, 3, 0, 1, 0, 1, 3, 7, 5, 6, 1, 2, 5, 1, 9, 9, 8, 6, 8, 6, 8, 9,\n",
            "        0, 4, 0, 3, 3, 6, 4, 3, 9, 7, 2, 0, 5, 6, 5, 6, 0, 3, 2, 5, 0, 2, 9, 2,\n",
            "        7, 2, 9, 2, 4, 0, 6, 6, 3, 4, 5, 5, 9, 1, 1, 4, 4, 3, 5, 4, 8, 0, 0, 3,\n",
            "        9, 4, 7, 7, 2, 8, 3, 6, 5, 1, 9, 6, 3, 8, 3, 2, 4, 7, 5, 3, 5, 3, 1, 2,\n",
            "        6, 1, 5, 2, 0, 7, 5, 0, 3, 0, 1, 6, 3, 3, 8, 4, 3, 1, 7, 1, 0, 4, 2, 4,\n",
            "        1, 4, 0, 1, 2, 1, 7, 7], device='cuda:0')\n",
            "Target: tensor([9, 6, 6, 7, 0, 1, 0, 1, 3, 2, 5, 6, 1, 2, 5, 1, 9, 9, 8, 6, 8, 6, 8, 9,\n",
            "        0, 4, 0, 3, 3, 6, 4, 9, 9, 7, 2, 3, 5, 6, 4, 6, 0, 3, 7, 5, 0, 2, 9, 2,\n",
            "        7, 2, 9, 2, 4, 0, 6, 6, 3, 4, 5, 5, 9, 1, 1, 4, 4, 3, 5, 4, 8, 0, 0, 3,\n",
            "        9, 4, 7, 4, 2, 8, 3, 6, 3, 1, 9, 6, 3, 8, 3, 8, 4, 7, 5, 7, 5, 3, 1, 2,\n",
            "        6, 1, 3, 2, 0, 7, 5, 0, 0, 0, 1, 4, 3, 5, 8, 4, 3, 1, 7, 1, 0, 4, 2, 4,\n",
            "        1, 4, 0, 1, 2, 1, 7, 7], device='cuda:0')\n",
            "Output: tensor([[ -1.8241,  -4.5747, -13.4982,  ...,  -9.8307,  -4.6513,  24.7360],\n",
            "        [  5.0075,  -9.0293,  -2.5173,  ..., -11.5907,  21.3179,  -6.9136],\n",
            "        [ -5.5149, -10.7250,  -4.9134,  ...,  14.0075, -10.3507,  -6.4392],\n",
            "        ...,\n",
            "        [ -5.1256,  -6.3439,  19.2821,  ...,  -5.9277, -10.7388,  -9.0140],\n",
            "        [ 16.1448,  -5.2062,  -0.0442,  ..., -11.3904,  -1.2821,  -6.9296],\n",
            "        [ -5.5135,  14.9187,  -6.1319,  ...,  -7.9463,  -5.5508,  -0.9342]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([9, 8, 7, 5, 4, 0, 9, 0, 0, 8, 2, 0, 0, 7, 4, 8, 6, 2, 4, 6, 3, 5, 1, 5,\n",
            "        3, 7, 2, 2, 9, 8, 0, 0, 0, 3, 4, 4, 6, 1, 6, 3, 4, 4, 5, 9, 7, 0, 8, 0,\n",
            "        4, 6, 5, 7, 9, 7, 0, 5, 7, 7, 3, 1, 9, 7, 0, 9, 5, 3, 7, 9, 4, 4, 1, 7,\n",
            "        3, 1, 4, 1, 2, 8, 7, 0, 0, 4, 7, 2, 9, 7, 6, 9, 3, 5, 8, 0, 4, 6, 8, 3,\n",
            "        2, 4, 7, 1, 1, 3, 9, 7, 5, 1, 8, 8, 7, 0, 1, 6, 9, 0, 2, 7, 7, 8, 1, 0,\n",
            "        3, 6, 6, 7, 5, 2, 0, 1], device='cuda:0')\n",
            "Target: tensor([9, 8, 7, 5, 4, 0, 9, 0, 0, 8, 2, 0, 0, 2, 4, 8, 6, 2, 4, 6, 3, 5, 1, 5,\n",
            "        3, 7, 2, 2, 9, 8, 0, 0, 0, 3, 4, 4, 6, 1, 6, 7, 4, 4, 3, 9, 4, 0, 8, 0,\n",
            "        4, 6, 5, 7, 9, 7, 0, 5, 7, 7, 3, 1, 9, 3, 0, 9, 5, 3, 7, 9, 4, 4, 1, 7,\n",
            "        7, 1, 4, 1, 2, 8, 7, 0, 0, 4, 7, 2, 9, 7, 6, 9, 3, 5, 8, 0, 3, 6, 8, 3,\n",
            "        2, 4, 7, 1, 1, 3, 9, 7, 5, 1, 0, 8, 7, 0, 1, 6, 9, 3, 2, 7, 7, 8, 1, 0,\n",
            "        3, 4, 6, 7, 5, 2, 0, 1], device='cuda:0')\n",
            "Output: tensor([[ -8.6339,  -9.7315,  -7.6189,  ...,  -6.0448,  -1.9197,  -6.7869],\n",
            "        [ -6.3962,  -6.8888,  -3.6304,  ...,  -3.4836,  -1.9534,  -4.3764],\n",
            "        [ -6.4776,  10.1129,  -3.1600,  ...,  -6.4398,  -8.1591,  -2.9499],\n",
            "        ...,\n",
            "        [ -7.8604,  -7.1802,  -2.6392,  ...,  -5.4939,  -5.4252,  -5.9672],\n",
            "        [ -9.8037,  -9.0539,   2.8880,  ..., -11.8256, -12.0567, -10.4334],\n",
            "        [  1.5340,  -6.9016,  -5.9471,  ..., -11.2939,  19.3245, -11.1113]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([5, 5, 1, 4, 1, 3, 0, 8, 6, 2, 1, 6, 5, 4, 1, 9, 0, 4, 1, 0, 9, 9, 0, 6,\n",
            "        9, 2, 4, 7, 2, 0, 7, 4, 9, 1, 3, 6, 6, 3, 6, 4, 1, 3, 8, 2, 6, 6, 1, 6,\n",
            "        3, 6, 3, 8, 4, 6, 7, 1, 9, 5, 6, 7, 6, 0, 7, 1, 9, 5, 2, 6, 7, 7, 6, 5,\n",
            "        9, 1, 3, 6, 0, 2, 2, 9, 1, 8, 6, 5, 0, 0, 0, 5, 7, 7, 8, 0, 5, 5, 1, 6,\n",
            "        1, 5, 1, 0, 6, 2, 3, 2, 1, 7, 7, 1, 9, 8, 3, 2, 9, 7, 3, 0, 2, 3, 9, 4,\n",
            "        6, 3, 9, 6, 8, 6, 6, 8], device='cuda:0')\n",
            "Target: tensor([5, 5, 1, 4, 1, 3, 0, 8, 6, 2, 1, 3, 6, 4, 1, 9, 0, 4, 1, 0, 1, 9, 8, 6,\n",
            "        9, 2, 4, 7, 2, 2, 7, 4, 9, 1, 3, 2, 6, 3, 4, 4, 9, 4, 8, 2, 6, 6, 1, 6,\n",
            "        3, 6, 5, 8, 4, 6, 7, 1, 9, 3, 6, 7, 6, 0, 7, 1, 9, 5, 2, 6, 7, 7, 6, 5,\n",
            "        9, 1, 5, 6, 0, 2, 0, 9, 1, 8, 3, 5, 0, 0, 0, 5, 7, 7, 8, 5, 5, 5, 1, 6,\n",
            "        1, 5, 1, 0, 6, 2, 3, 2, 1, 7, 5, 1, 9, 8, 3, 6, 9, 7, 3, 0, 2, 3, 9, 4,\n",
            "        4, 3, 9, 6, 8, 6, 6, 8], device='cuda:0')\n",
            "Output: tensor([[ -3.3558,  -5.4873,  -4.3255,  ...,  -0.9751,  -5.4019,  -4.5367],\n",
            "        [ -9.2406,  -9.3712,  -0.3338,  ...,  -4.7132,  -9.7526,  -9.4591],\n",
            "        [-12.6565,  -6.1460,   0.1648,  ...,  -7.8143,  -3.9804,  -6.9715],\n",
            "        ...,\n",
            "        [ -6.1083,  -5.5476,  -0.7254,  ...,  -8.9046,  -8.8272,  -6.3052],\n",
            "        [ -8.6020,  -9.4355, -10.5691,  ...,   0.9515,  -9.5863,  -6.9245],\n",
            "        [ -5.6601,  -7.8254,   1.7709,  ...,  -7.8816, -10.3151,  -9.6264]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([5, 4, 3, 6, 7, 7, 4, 6, 9, 6, 2, 4, 0, 1, 3, 6, 4, 9, 2, 6, 0, 0, 8, 7,\n",
            "        5, 8, 3, 8, 3, 2, 6, 3, 6, 3, 7, 4, 5, 7, 9, 0, 0, 6, 7, 2, 0, 5, 5, 7,\n",
            "        5, 5, 9, 4, 6, 4, 7, 3, 3, 6, 4, 9, 6, 3, 1, 6, 7, 0, 5, 0, 1, 9, 7, 3,\n",
            "        5, 2, 3, 9, 5, 6, 7, 0, 0, 0, 8, 0, 3, 2, 4, 5, 3, 7, 9, 6, 9, 3, 1, 2,\n",
            "        6, 7, 4, 5, 1, 3, 7, 2, 9, 9, 8, 5, 8, 3, 8, 5, 6, 5, 0, 8, 5, 8, 1, 8,\n",
            "        1, 5, 0, 9, 8, 6, 3, 6], device='cuda:0')\n",
            "Target: tensor([5, 4, 3, 6, 7, 7, 4, 3, 9, 6, 2, 4, 0, 1, 3, 6, 4, 9, 2, 6, 0, 3, 8, 7,\n",
            "        5, 3, 3, 8, 3, 2, 6, 3, 6, 2, 7, 4, 5, 7, 9, 0, 0, 6, 7, 2, 8, 5, 5, 7,\n",
            "        5, 5, 9, 4, 6, 4, 7, 3, 3, 6, 4, 1, 6, 3, 1, 6, 7, 0, 5, 0, 1, 9, 7, 3,\n",
            "        5, 2, 3, 9, 5, 6, 4, 0, 0, 0, 8, 0, 3, 2, 4, 5, 3, 7, 9, 6, 9, 3, 1, 2,\n",
            "        6, 7, 4, 5, 1, 3, 7, 6, 9, 5, 8, 5, 8, 5, 8, 5, 6, 5, 0, 8, 3, 8, 1, 8,\n",
            "        1, 5, 0, 9, 8, 6, 3, 6], device='cuda:0')\n",
            "Output: tensor([[ -0.2268,  -0.0692,  -9.1018,  ...,  -7.6124, -10.7977,  -2.9505],\n",
            "        [ -7.2816,  -8.6661,   0.2960,  ...,   0.6887, -10.1510,  -8.1189],\n",
            "        [ -3.2404,  -9.5212,   5.8987,  ...,  -2.9031, -10.9979, -10.4720],\n",
            "        ...,\n",
            "        [ -3.8874,  -7.5425,   1.6631,  ...,   2.2141,  -8.3341,  -7.3883],\n",
            "        [ -2.7236,  -4.8467,  -4.6939,  ...,  -8.1928,  10.6081,  -0.9672],\n",
            "        [ -2.6809,  -1.6890,  -9.7071,  ...,  -8.4551,  -6.8546,  21.4348]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([3, 4, 2, 7, 4, 7, 4, 3, 3, 4, 4, 5, 7, 3, 4, 5, 8, 0, 5, 4, 0, 5, 7, 3,\n",
            "        3, 2, 4, 2, 9, 4, 2, 8, 6, 2, 6, 1, 7, 0, 4, 3, 8, 5, 9, 7, 6, 7, 1, 0,\n",
            "        2, 3, 7, 6, 7, 1, 6, 2, 8, 3, 7, 2, 5, 7, 5, 7, 7, 1, 0, 4, 3, 3, 4, 0,\n",
            "        4, 2, 7, 0, 5, 3, 6, 6, 2, 5, 2, 6, 0, 6, 9, 2, 9, 0, 0, 1, 5, 5, 1, 6,\n",
            "        7, 8, 3, 6, 3, 3, 6, 0, 9, 9, 2, 2, 6, 1, 4, 6, 2, 3, 7, 0, 8, 5, 6, 4,\n",
            "        9, 2, 3, 6, 4, 4, 8, 9], device='cuda:0')\n",
            "Target: tensor([3, 4, 4, 7, 4, 7, 4, 3, 2, 4, 5, 5, 7, 5, 4, 5, 8, 0, 5, 4, 0, 5, 4, 3,\n",
            "        3, 2, 4, 2, 9, 4, 8, 8, 6, 2, 6, 1, 7, 0, 4, 3, 8, 5, 9, 7, 6, 7, 1, 0,\n",
            "        2, 3, 5, 6, 7, 1, 6, 2, 8, 3, 7, 2, 5, 7, 5, 7, 7, 1, 7, 4, 3, 3, 4, 0,\n",
            "        4, 2, 7, 0, 2, 2, 6, 6, 2, 5, 2, 6, 0, 6, 1, 2, 9, 0, 0, 1, 5, 5, 1, 6,\n",
            "        7, 8, 5, 6, 3, 3, 6, 0, 9, 9, 2, 2, 6, 1, 4, 6, 2, 5, 7, 8, 8, 5, 6, 4,\n",
            "        3, 2, 3, 4, 4, 4, 8, 9], device='cuda:0')\n",
            "Output: tensor([[ -2.7565,  -8.3572,   9.7570,  ...,  -9.3868,  -7.4324,  -7.9452],\n",
            "        [ -9.7827,  -7.3434,   1.7813,  ..., -10.3830, -10.1760,  -7.9888],\n",
            "        [ 13.5694,  -3.5428,  -2.8381,  ...,  -9.9794,   2.5707,   0.2944],\n",
            "        ...,\n",
            "        [ -9.9197,  -8.7699,  -4.6766,  ...,   0.9411, -12.6088,  -8.9876],\n",
            "        [ -3.3738,  -4.8183,   4.0401,  ...,  -2.3189,  -5.2167,  -4.3061],\n",
            "        [ -5.0597,  18.5428,  -5.3990,  ..., -14.2151,  -9.0561,  -0.2723]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([2, 6, 0, 9, 7, 9, 8, 7, 7, 3, 8, 2, 9, 3, 3, 7, 5, 8, 7, 3, 7, 0, 7, 1,\n",
            "        6, 2, 6, 0, 0, 2, 9, 8, 2, 8, 0, 7, 5, 5, 0, 1, 2, 9, 6, 1, 0, 1, 0, 6,\n",
            "        3, 8, 5, 7, 4, 4, 8, 3, 5, 3, 7, 2, 5, 8, 3, 5, 7, 3, 3, 0, 5, 4, 3, 6,\n",
            "        9, 0, 5, 5, 8, 2, 4, 2, 2, 6, 7, 6, 0, 5, 5, 8, 8, 5, 6, 1, 3, 5, 4, 7,\n",
            "        3, 0, 7, 2, 0, 1, 3, 4, 1, 6, 4, 5, 0, 7, 7, 3, 7, 4, 3, 1, 6, 9, 4, 8,\n",
            "        0, 3, 9, 6, 4, 4, 2, 1], device='cuda:0')\n",
            "Target: tensor([2, 6, 0, 9, 7, 9, 8, 7, 7, 3, 8, 2, 9, 3, 5, 7, 5, 8, 7, 3, 7, 0, 7, 1,\n",
            "        6, 2, 4, 0, 0, 2, 9, 8, 2, 8, 7, 7, 5, 5, 0, 1, 2, 9, 6, 1, 0, 1, 0, 6,\n",
            "        5, 0, 5, 7, 4, 4, 8, 3, 5, 3, 7, 2, 5, 8, 3, 5, 7, 3, 7, 0, 5, 4, 5, 6,\n",
            "        9, 3, 5, 3, 8, 2, 4, 2, 2, 6, 7, 6, 0, 5, 5, 8, 4, 5, 6, 1, 3, 5, 4, 7,\n",
            "        3, 0, 7, 2, 0, 1, 2, 4, 9, 6, 4, 5, 9, 7, 7, 6, 7, 4, 3, 1, 6, 9, 4, 8,\n",
            "        0, 3, 1, 6, 4, 4, 2, 1], device='cuda:0')\n",
            "Output: tensor([[ -4.8726,  -8.1144,  -1.5781,  ...,  -0.2784,  -8.0140,  -6.9861],\n",
            "        [ -8.4809,  -4.9143,  10.2123,  ...,  -8.9398, -10.4943,  -9.1563],\n",
            "        [ -3.7673,  -2.6583,  -2.5696,  ...,  -2.4850,  -7.2899,   4.2751],\n",
            "        ...,\n",
            "        [ -7.8546,  -7.2186,   2.0269,  ...,  -7.3250,  -9.8247,  -6.4752],\n",
            "        [ -7.9438,  -7.4342,  -3.3491,  ...,  -0.0151,  -7.2556,  -7.3927],\n",
            "        [ 12.3594,  -3.7582,   1.2386,  ...,  -8.1732,  -0.1012,  -4.9598]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([4, 2, 9, 0, 8, 9, 7, 6, 8, 4, 8, 1, 2, 6, 9, 0, 7, 1, 4, 5, 2, 1, 3, 6,\n",
            "        4, 7, 1, 5, 9, 3, 1, 4, 8, 7, 7, 0, 2, 3, 4, 6, 7, 7, 5, 4, 4, 4, 3, 6,\n",
            "        6, 1, 9, 9, 6, 9, 4, 3, 2, 9, 6, 5, 0, 6, 9, 0, 9, 7, 8, 5, 4, 4, 9, 2,\n",
            "        2, 6, 6, 9, 9, 7, 9, 9, 0, 5, 1, 1, 0, 6, 9, 9, 5, 6, 5, 5, 9, 7, 9, 4,\n",
            "        5, 4, 5, 3, 9, 8, 8, 3, 4, 8, 3, 0, 4, 0, 7, 2, 9, 0, 2, 0, 7, 4, 0, 6,\n",
            "        6, 3, 1, 7, 4, 6, 5, 0], device='cuda:0')\n",
            "Target: tensor([4, 6, 3, 0, 8, 9, 7, 6, 8, 4, 8, 1, 4, 5, 9, 4, 7, 1, 4, 5, 3, 1, 2, 6,\n",
            "        4, 7, 1, 5, 9, 3, 1, 4, 8, 7, 7, 0, 4, 3, 4, 6, 7, 7, 5, 4, 4, 4, 3, 6,\n",
            "        6, 1, 9, 9, 7, 9, 4, 3, 2, 9, 8, 5, 0, 6, 9, 0, 9, 7, 8, 5, 4, 4, 9, 4,\n",
            "        2, 6, 6, 9, 9, 7, 9, 9, 0, 5, 1, 1, 8, 6, 9, 9, 5, 6, 5, 5, 9, 7, 9, 4,\n",
            "        5, 4, 3, 3, 9, 8, 8, 3, 4, 8, 4, 0, 4, 0, 7, 2, 8, 0, 2, 0, 7, 4, 0, 6,\n",
            "        3, 3, 1, 7, 4, 6, 5, 0], device='cuda:0')\n",
            "Output: tensor([[ -8.0658,  -6.0664,  -6.1628,  ...,  -1.0293,  -6.6715,  -5.9712],\n",
            "        [ -3.5935,   4.0917,  -4.4619,  ...,  -6.3972,  -8.4738,   7.9667],\n",
            "        [-10.3280,  -6.1837,  -2.0320,  ...,  -5.2837, -11.5706,  -7.3471],\n",
            "        ...,\n",
            "        [  1.4053,  -4.6042,   2.5695,  ...,  -9.2961,  14.6676,  -5.7978],\n",
            "        [ -5.3960,  -5.5636,  -5.6657,  ...,   1.0797,  -9.4308,  -3.7183],\n",
            "        [ -5.8497,   4.2574, -10.7754,  ...,  -5.6301,  -3.1189,  13.6700]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([6, 9, 5, 6, 6, 0, 0, 3, 2, 0, 3, 3, 0, 0, 8, 5, 4, 2, 9, 5, 0, 5, 4, 8,\n",
            "        3, 7, 6, 7, 7, 8, 2, 0, 1, 8, 2, 5, 8, 4, 5, 0, 4, 0, 1, 9, 3, 0, 3, 6,\n",
            "        5, 5, 6, 8, 3, 0, 9, 7, 5, 2, 4, 3, 7, 0, 2, 3, 1, 0, 0, 2, 1, 3, 1, 4,\n",
            "        6, 7, 1, 0, 5, 2, 5, 9, 2, 5, 6, 7, 1, 9, 3, 6, 1, 0, 2, 9, 4, 2, 9, 3,\n",
            "        8, 2, 9, 8, 5, 1, 0, 1, 5, 2, 3, 6, 8, 8, 3, 5, 0, 4, 1, 3, 7, 0, 3, 3,\n",
            "        9, 2, 3, 9, 9, 8, 3, 9], device='cuda:0')\n",
            "Target: tensor([4, 9, 5, 6, 6, 3, 0, 5, 2, 6, 3, 3, 8, 0, 8, 5, 4, 2, 9, 5, 0, 5, 4, 8,\n",
            "        2, 7, 6, 5, 7, 8, 2, 0, 1, 8, 2, 4, 8, 4, 3, 0, 4, 0, 1, 9, 3, 0, 3, 6,\n",
            "        5, 5, 2, 8, 5, 0, 8, 7, 5, 2, 4, 3, 7, 0, 2, 3, 1, 0, 0, 2, 1, 2, 1, 4,\n",
            "        6, 7, 1, 0, 5, 2, 5, 9, 2, 5, 6, 7, 1, 9, 3, 6, 1, 0, 2, 9, 4, 2, 9, 3,\n",
            "        8, 2, 9, 8, 5, 1, 0, 1, 5, 2, 5, 6, 8, 8, 3, 5, 0, 4, 1, 3, 7, 0, 3, 3,\n",
            "        9, 2, 3, 9, 9, 8, 5, 9], device='cuda:0')\n",
            "Output: tensor([[ -7.6581,  -9.0719,  -7.3103,  ...,   7.7240, -11.1309,  -8.6018],\n",
            "        [ -6.8972,  -6.7577,   6.6358,  ...,  -5.3789,  -8.7971,  -6.1336],\n",
            "        [  3.9189,  -0.6021,  -5.7206,  ...,  -4.0294,  -4.5916,   2.9321],\n",
            "        ...,\n",
            "        [ -5.6770,  -8.5621,  -6.8601,  ...,  14.8443, -10.3828,  -7.8708],\n",
            "        [ -9.9236,  -9.9400,  20.1657,  ..., -15.5116, -12.1649, -13.6512],\n",
            "        [ -6.1183,  -9.1077,   6.8357,  ...,  -8.0314, -10.4904,  -8.1417]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([7, 2, 0, 9, 4, 7, 1, 4, 6, 7, 4, 8, 9, 0, 7, 1, 2, 4, 7, 3, 7, 5, 1, 9,\n",
            "        3, 5, 2, 4, 1, 1, 3, 3, 9, 2, 5, 8, 0, 8, 1, 5, 1, 5, 9, 0, 9, 2, 4, 0,\n",
            "        6, 9, 1, 1, 3, 4, 9, 5, 3, 3, 6, 3, 3, 1, 5, 9, 8, 7, 4, 8, 7, 3, 5, 2,\n",
            "        5, 8, 9, 0, 2, 0, 0, 7, 5, 3, 5, 7, 4, 6, 2, 8, 2, 7, 7, 6, 1, 3, 8, 6,\n",
            "        2, 0, 2, 6, 8, 6, 4, 0, 2, 5, 4, 1, 2, 4, 4, 6, 9, 3, 0, 5, 2, 0, 8, 2,\n",
            "        6, 4, 7, 9, 5, 7, 2, 6], device='cuda:0')\n",
            "Target: tensor([7, 2, 0, 9, 4, 7, 1, 7, 6, 4, 4, 8, 9, 0, 7, 1, 2, 4, 0, 3, 7, 5, 1, 9,\n",
            "        3, 5, 2, 4, 1, 1, 3, 3, 9, 2, 5, 8, 0, 8, 1, 5, 1, 5, 9, 0, 9, 2, 4, 0,\n",
            "        6, 9, 1, 1, 3, 4, 9, 5, 3, 3, 3, 3, 4, 1, 5, 9, 8, 7, 4, 8, 7, 3, 5, 2,\n",
            "        5, 8, 9, 7, 2, 0, 0, 7, 5, 3, 5, 7, 4, 6, 2, 8, 2, 7, 7, 6, 1, 3, 8, 6,\n",
            "        2, 0, 4, 4, 8, 6, 4, 0, 2, 5, 4, 1, 2, 5, 4, 6, 9, 5, 0, 5, 2, 0, 8, 2,\n",
            "        6, 4, 7, 9, 5, 7, 2, 6], device='cuda:0')\n",
            "Output: tensor([[-10.1557,  -9.6568,   9.1413,  ..., -12.3323,  -9.0021, -11.2429],\n",
            "        [  0.6402,   8.0403,  -3.9451,  ...,  -8.0070,  -0.5090,  -2.8715],\n",
            "        [ -6.6798,  -1.2353, -14.6270,  ..., -11.7180,  -8.2280,  25.4847],\n",
            "        ...,\n",
            "        [ -9.2480,  -9.2636,  -2.6681,  ...,  -2.1940, -12.0604, -12.5966],\n",
            "        [ -6.6153,  -6.0650,  -4.0696,  ...,  -1.3096,  -6.9414,  -2.4957],\n",
            "        [ -5.2495,  16.1675,  -7.5070,  ..., -11.6156,  -6.7439,   8.0152]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([2, 1, 9, 9, 7, 6, 5, 1, 8, 1, 6, 6, 0, 8, 8, 5, 6, 9, 9, 9, 6, 3, 0, 8,\n",
            "        8, 5, 3, 3, 1, 1, 0, 7, 0, 6, 0, 4, 4, 6, 5, 0, 2, 9, 6, 6, 7, 9, 7, 7,\n",
            "        0, 8, 4, 2, 7, 7, 0, 4, 0, 1, 6, 0, 5, 3, 0, 2, 4, 9, 7, 3, 3, 2, 8, 8,\n",
            "        5, 3, 8, 0, 8, 0, 0, 0, 2, 2, 9, 0, 7, 7, 7, 0, 2, 0, 4, 0, 3, 7, 8, 4,\n",
            "        7, 9, 6, 9, 1, 6, 9, 3, 8, 9, 5, 8, 3, 9, 5, 2, 7, 5, 5, 2, 3, 5, 7, 9,\n",
            "        6, 2, 0, 8, 4, 5, 6, 1], device='cuda:0')\n",
            "Target: tensor([2, 1, 9, 9, 7, 2, 5, 1, 8, 1, 6, 3, 0, 8, 8, 5, 6, 9, 9, 9, 6, 3, 0, 8,\n",
            "        8, 5, 3, 3, 1, 1, 0, 7, 0, 6, 0, 4, 4, 6, 5, 3, 2, 9, 6, 6, 7, 9, 7, 7,\n",
            "        0, 8, 4, 2, 7, 7, 7, 4, 0, 1, 6, 0, 5, 3, 0, 2, 4, 9, 7, 3, 3, 2, 8, 8,\n",
            "        5, 3, 8, 0, 8, 0, 0, 0, 2, 2, 7, 0, 7, 7, 7, 2, 2, 0, 4, 0, 3, 7, 8, 4,\n",
            "        7, 9, 6, 1, 1, 6, 9, 3, 8, 9, 5, 8, 3, 9, 4, 3, 7, 5, 5, 2, 7, 5, 7, 9,\n",
            "        6, 3, 0, 8, 4, 5, 6, 1], device='cuda:0')\n",
            "Output: tensor([[ 4.4567, -4.0577, -1.8218,  ..., -3.7011,  3.2196, -4.2495],\n",
            "        [-4.7255, -6.9905, -2.6228,  ...,  3.5854, -6.4704, -2.2848],\n",
            "        [ 0.7105, -4.1984, -4.0777,  ..., -5.2402,  6.5471, -2.5720],\n",
            "        ...,\n",
            "        [-9.7930, -6.7890, -9.7971,  ..., -9.0150, -6.7356, -5.4817],\n",
            "        [ 3.1512, -0.8351, -6.9748,  ..., -6.7082, 15.6241, -0.4996],\n",
            "        [-7.6460, -6.2784, -3.9704,  ..., -2.8003, -8.0932, -9.2049]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([0, 7, 8, 4, 7, 9, 3, 2, 0, 9, 1, 8, 9, 6, 8, 1, 7, 1, 0, 1, 9, 6, 4, 5,\n",
            "        8, 2, 1, 2, 5, 8, 1, 1, 8, 8, 8, 9, 8, 0, 4, 3, 6, 5, 6, 6, 3, 1, 4, 4,\n",
            "        7, 8, 0, 1, 4, 8, 6, 6, 2, 2, 2, 6, 3, 4, 0, 8, 9, 3, 8, 3, 8, 9, 1, 6,\n",
            "        1, 9, 8, 2, 0, 4, 7, 4, 0, 0, 7, 6, 8, 9, 8, 6, 2, 8, 8, 0, 1, 1, 0, 7,\n",
            "        7, 2, 4, 7, 7, 2, 6, 6, 7, 4, 5, 2, 0, 9, 4, 7, 2, 5, 3, 2, 2, 7, 6, 2,\n",
            "        6, 4, 9, 6, 8, 3, 8, 5], device='cuda:0')\n",
            "Target: tensor([0, 4, 8, 4, 7, 1, 3, 2, 0, 9, 1, 8, 8, 6, 8, 1, 7, 1, 0, 9, 7, 6, 4, 5,\n",
            "        8, 2, 1, 2, 5, 8, 1, 1, 8, 8, 0, 9, 0, 0, 4, 3, 3, 5, 6, 6, 3, 1, 4, 4,\n",
            "        7, 8, 0, 1, 4, 8, 4, 6, 2, 2, 2, 6, 3, 4, 0, 8, 1, 3, 8, 3, 8, 9, 1, 6,\n",
            "        1, 9, 8, 2, 0, 4, 7, 4, 0, 0, 7, 6, 8, 9, 8, 6, 2, 8, 8, 0, 1, 1, 0, 7,\n",
            "        7, 4, 4, 7, 7, 2, 6, 6, 7, 4, 5, 4, 0, 9, 4, 7, 2, 5, 3, 2, 2, 7, 6, 2,\n",
            "        6, 4, 9, 6, 8, 3, 8, 5], device='cuda:0')\n",
            "Output: tensor([[ -5.4725,  -5.3060,  -5.0185,  ...,  -9.0028,  -6.7897,  -1.8108],\n",
            "        [ -0.1600,  -7.5129,  10.4515,  ...,  -7.8209,  -7.4396,  -8.0766],\n",
            "        [-10.3873,  -6.3300,  -3.0548,  ...,  -5.3441,  -6.6755,  -8.3458],\n",
            "        ...,\n",
            "        [  7.8481,  -3.7411,   2.7026,  ...,  -4.7186,   0.4253,  -3.1554],\n",
            "        [ -6.3324,  -8.0875,   1.5818,  ...,  -2.9699,  -9.8678,  -6.4061],\n",
            "        [ -5.4808,   9.0272,  -6.6932,  ...,  -8.1717,  -4.2288,   6.2365]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([5, 2, 3, 4, 5, 3, 7, 7, 1, 5, 3, 7, 8, 8, 9, 6, 0, 8, 2, 6, 0, 8, 1, 5,\n",
            "        1, 3, 5, 1, 8, 9, 3, 2, 6, 9, 8, 6, 5, 9, 1, 4, 2, 9, 9, 5, 4, 7, 0, 3,\n",
            "        7, 1, 6, 9, 0, 1, 4, 0, 5, 1, 6, 3, 8, 2, 3, 5, 5, 2, 8, 2, 3, 1, 6, 2,\n",
            "        7, 0, 8, 9, 2, 6, 4, 7, 5, 5, 8, 8, 7, 8, 5, 7, 4, 4, 0, 0, 7, 0, 4, 8,\n",
            "        4, 4, 2, 3, 4, 0, 7, 5, 7, 0, 4, 6, 2, 9, 1, 1, 3, 0, 9, 1, 0, 6, 3, 3,\n",
            "        0, 9, 0, 6, 2, 0, 4, 1], device='cuda:0')\n",
            "Target: tensor([5, 2, 3, 4, 5, 2, 7, 5, 1, 5, 3, 7, 8, 8, 9, 6, 0, 9, 2, 6, 0, 0, 1, 4,\n",
            "        1, 3, 5, 1, 8, 9, 6, 2, 6, 9, 5, 6, 5, 9, 1, 4, 2, 8, 9, 5, 4, 7, 0, 3,\n",
            "        7, 1, 6, 9, 1, 1, 4, 0, 5, 1, 6, 3, 0, 2, 5, 5, 5, 2, 8, 2, 3, 1, 6, 2,\n",
            "        7, 0, 8, 9, 2, 6, 4, 7, 5, 5, 8, 8, 7, 8, 5, 7, 4, 4, 0, 0, 7, 0, 4, 0,\n",
            "        4, 4, 2, 5, 4, 0, 7, 7, 7, 0, 4, 6, 2, 9, 1, 1, 5, 0, 9, 1, 0, 6, 5, 3,\n",
            "        0, 9, 0, 6, 2, 0, 4, 1], device='cuda:0')\n",
            "Output: tensor([[ -2.1301,  -5.4007,  -3.5498,  ...,  -2.8346,  -5.8949,  -2.6789],\n",
            "        [ -5.6211, -11.2383,  -7.5985,  ...,  18.4296,  -9.2041,  -5.7391],\n",
            "        [-10.4386,  -5.1933,  -3.0900,  ..., -10.3248,  -8.8123,  -3.6857],\n",
            "        ...,\n",
            "        [ -7.4969,  -4.2351,  -5.4677,  ...,  -9.5502,  -1.3629,  -5.1209],\n",
            "        [ -5.5437,  -1.7729,  -5.3357,  ...,  -7.2818,  -8.8454,  -1.7352],\n",
            "        [ -9.7613,  -4.1304,   3.8044,  ..., -12.2566,  -3.9960,  -8.2874]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([6, 7, 6, 1, 0, 4, 6, 0, 4, 4, 2, 7, 6, 2, 3, 5, 9, 2, 5, 2, 0, 5, 7, 4,\n",
            "        3, 9, 4, 5, 8, 1, 9, 3, 2, 8, 5, 8, 6, 6, 3, 4, 0, 2, 3, 2, 6, 1, 8, 2,\n",
            "        1, 3, 6, 2, 2, 3, 3, 5, 0, 2, 1, 2, 8, 0, 8, 2, 2, 7, 3, 5, 9, 8, 3, 6,\n",
            "        1, 0, 8, 6, 3, 5, 8, 9, 2, 8, 0, 0, 2, 9, 1, 8, 3, 7, 2, 8, 6, 9, 3, 8,\n",
            "        9, 2, 8, 9, 0, 4, 8, 9, 5, 3, 7, 2, 3, 8, 8, 1, 2, 3, 1, 5, 0, 2, 6, 0,\n",
            "        8, 2, 2, 8, 7, 3, 5, 6], device='cuda:0')\n",
            "Target: tensor([6, 7, 6, 1, 0, 4, 6, 0, 4, 4, 2, 7, 6, 5, 3, 5, 9, 4, 5, 2, 0, 5, 7, 2,\n",
            "        3, 9, 4, 5, 8, 1, 9, 3, 1, 8, 5, 8, 6, 6, 3, 4, 0, 2, 3, 2, 6, 1, 8, 2,\n",
            "        1, 3, 6, 2, 2, 3, 3, 5, 0, 2, 1, 2, 8, 0, 8, 2, 2, 7, 3, 5, 9, 8, 3, 6,\n",
            "        1, 0, 8, 6, 3, 5, 8, 9, 4, 8, 0, 0, 2, 9, 1, 8, 3, 7, 2, 8, 0, 9, 4, 8,\n",
            "        9, 4, 8, 9, 5, 4, 8, 1, 5, 5, 7, 2, 5, 8, 8, 1, 2, 3, 1, 5, 0, 2, 3, 0,\n",
            "        8, 2, 2, 8, 7, 3, 5, 3], device='cuda:0')\n",
            "Output: tensor([[ -5.3583,   1.9680, -15.1206,  ..., -11.8822,  -4.6341,  22.2982],\n",
            "        [ -9.0690,  -8.5308,   2.3645,  ...,  -9.6590, -11.8456,  -9.3396],\n",
            "        [ -4.3780,  -3.2075,   1.1409,  ...,  -3.6009,  -5.0664,  -3.3807],\n",
            "        ...,\n",
            "        [-10.2368,  -7.4652,   2.8136,  ...,   1.0544, -11.7932,  -8.2019],\n",
            "        [ -8.5321,  -9.2981,  -4.9102,  ...,  14.8975, -10.0056,  -7.3899],\n",
            "        [ -6.8747,  -5.8126,  -3.2927,  ...,  -9.3188,  -9.1846,  -8.0668]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([9, 6, 2, 5, 1, 1, 7, 7, 8, 3, 8, 0, 6, 0, 8, 0, 6, 6, 4, 6, 9, 1, 1, 7,\n",
            "        0, 6, 8, 9, 9, 6, 9, 0, 1, 1, 7, 2, 1, 0, 4, 5, 0, 3, 6, 3, 3, 8, 1, 8,\n",
            "        1, 3, 1, 5, 4, 5, 1, 8, 9, 1, 4, 1, 7, 4, 9, 8, 9, 4, 6, 0, 9, 4, 0, 4,\n",
            "        5, 2, 1, 1, 2, 8, 5, 6, 0, 3, 4, 6, 9, 6, 7, 9, 0, 3, 6, 5, 7, 0, 5, 9,\n",
            "        2, 2, 9, 7, 0, 0, 1, 4, 4, 5, 5, 9, 6, 3, 6, 2, 6, 9, 3, 7, 3, 7, 4, 9,\n",
            "        5, 2, 1, 6, 6, 4, 7, 6], device='cuda:0')\n",
            "Target: tensor([9, 6, 3, 5, 1, 1, 7, 7, 3, 3, 8, 9, 6, 0, 8, 0, 6, 6, 4, 6, 9, 1, 1, 7,\n",
            "        0, 6, 8, 9, 0, 6, 9, 0, 1, 1, 7, 2, 1, 0, 4, 5, 0, 3, 6, 3, 3, 8, 1, 8,\n",
            "        1, 3, 1, 5, 4, 5, 1, 8, 9, 1, 3, 1, 7, 2, 9, 8, 9, 4, 6, 0, 9, 4, 0, 4,\n",
            "        6, 2, 1, 3, 0, 8, 5, 6, 0, 3, 4, 6, 9, 5, 7, 9, 0, 3, 6, 5, 7, 0, 5, 1,\n",
            "        2, 2, 9, 7, 0, 0, 1, 4, 4, 5, 5, 9, 6, 3, 6, 2, 6, 9, 3, 2, 3, 2, 4, 9,\n",
            "        5, 2, 1, 6, 6, 4, 7, 6], device='cuda:0')\n",
            "Output: tensor([[ -6.1304,   0.0583,  -1.6528,  ...,   1.3065,  -8.2180,  -0.2188],\n",
            "        [-10.6579,  -4.8613,  -0.2223,  ..., -10.9888,  -6.2426,  -4.1995],\n",
            "        [ -2.6489,  -4.9211,  -1.0527,  ...,  -5.6615,   7.8982,  -4.5000],\n",
            "        ...,\n",
            "        [ -1.9641,  -8.6444,  -6.9120,  ...,   9.6408,  -6.8371,  -7.1465],\n",
            "        [ 13.0004,  -6.6384,  -0.3526,  ...,  -9.0168,  -1.3542,  -6.0614],\n",
            "        [ -3.6755,  -5.5806,  -1.9850,  ...,  -1.3863,  -9.7096,  -7.1643]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([7, 3, 8, 1, 4, 6, 4, 2, 5, 6, 2, 0, 5, 2, 7, 5, 2, 5, 0, 5, 9, 0, 6, 7,\n",
            "        9, 8, 3, 2, 1, 4, 3, 4, 5, 5, 6, 8, 6, 0, 8, 9, 8, 0, 0, 7, 6, 6, 0, 5,\n",
            "        9, 8, 4, 9, 5, 6, 0, 4, 8, 6, 0, 9, 1, 7, 9, 8, 3, 7, 4, 5, 8, 2, 1, 9,\n",
            "        1, 0, 5, 8, 4, 7, 3, 1, 4, 5, 5, 1, 0, 8, 8, 8, 2, 1, 6, 7, 3, 7, 6, 9,\n",
            "        1, 6, 0, 7, 7, 1, 3, 3, 3, 1, 0, 2, 7, 1, 2, 2, 9, 6, 3, 5, 9, 4, 8, 0,\n",
            "        4, 8, 0, 1, 3, 7, 0, 4], device='cuda:0')\n",
            "Target: tensor([3, 3, 8, 1, 4, 6, 4, 4, 5, 6, 2, 0, 5, 2, 7, 5, 2, 5, 2, 5, 9, 0, 6, 7,\n",
            "        9, 8, 3, 2, 1, 4, 3, 4, 5, 5, 6, 8, 6, 0, 8, 9, 8, 0, 7, 7, 6, 6, 7, 5,\n",
            "        9, 8, 7, 9, 5, 6, 0, 4, 8, 2, 0, 9, 1, 3, 9, 8, 3, 7, 4, 5, 0, 2, 1, 9,\n",
            "        1, 0, 5, 8, 4, 7, 3, 1, 4, 5, 3, 1, 0, 8, 8, 8, 2, 1, 3, 7, 3, 1, 6, 9,\n",
            "        1, 6, 0, 7, 7, 1, 3, 3, 3, 1, 0, 2, 7, 1, 2, 2, 9, 6, 3, 5, 9, 4, 8, 0,\n",
            "        4, 8, 0, 1, 3, 7, 0, 4], device='cuda:0')\n",
            "Output: tensor([[  8.8880,  -5.7449,  -0.7817,  ...,  -3.8202,  -0.6781,  -5.7562],\n",
            "        [  1.5556,  -5.3912,   2.9495,  ...,  -2.5792,   0.9472,  -3.6022],\n",
            "        [  1.6061,  -3.1899,  -3.5110,  ...,  -4.9502,   9.4552,  -0.5405],\n",
            "        ...,\n",
            "        [ 15.0262,  -8.5533,   5.7139,  ...,  -6.4621,  -9.6499,  -7.3683],\n",
            "        [ -5.0905,  18.5681,  -9.4551,  ..., -11.0275,  -8.5015,   3.5390],\n",
            "        [-10.2096, -11.5788,  -7.6923,  ...,  20.7698, -11.3908,  -7.3616]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([0, 4, 8, 6, 4, 3, 9, 8, 2, 2, 8, 3, 1, 1, 0, 2, 2, 6, 9, 2, 8, 4, 2, 1,\n",
            "        5, 2, 6, 9, 0, 0, 7, 1, 8, 9, 3, 9, 9, 0, 7, 7, 5, 4, 2, 6, 4, 5, 7, 7,\n",
            "        8, 7, 2, 6, 2, 2, 4, 4, 0, 7, 1, 3, 9, 6, 0, 0, 2, 3, 8, 5, 2, 4, 3, 5,\n",
            "        2, 9, 1, 0, 0, 6, 5, 0, 7, 9, 9, 6, 5, 5, 8, 5, 7, 1, 6, 6, 4, 1, 4, 4,\n",
            "        1, 5, 0, 0, 2, 5, 8, 2, 8, 3, 0, 5, 0, 5, 3, 1, 6, 0, 0, 9, 1, 5, 7, 6,\n",
            "        5, 5, 5, 6, 0, 0, 1, 7], device='cuda:0')\n",
            "Target: tensor([7, 4, 8, 6, 4, 3, 9, 8, 2, 2, 8, 3, 1, 1, 2, 8, 2, 6, 9, 4, 8, 4, 4, 1,\n",
            "        5, 2, 6, 9, 2, 0, 7, 1, 8, 9, 3, 9, 9, 0, 7, 7, 5, 4, 2, 6, 4, 5, 7, 7,\n",
            "        8, 7, 2, 6, 2, 2, 4, 4, 0, 7, 1, 3, 9, 6, 0, 0, 2, 3, 8, 2, 2, 4, 3, 5,\n",
            "        2, 9, 1, 0, 0, 6, 5, 5, 7, 9, 9, 6, 5, 5, 0, 5, 7, 1, 6, 6, 4, 1, 4, 4,\n",
            "        1, 5, 0, 0, 4, 5, 8, 4, 8, 3, 0, 5, 0, 5, 3, 1, 6, 7, 0, 9, 1, 5, 7, 6,\n",
            "        5, 5, 5, 6, 0, 0, 1, 7], device='cuda:0')\n",
            "Output: tensor([[ -7.6220,  -6.0568,  -4.0875,  ...,  -3.9704,  -7.8845,  -5.3793],\n",
            "        [ -3.5425,   7.2901,  -1.6284,  ...,  -8.1720,  -7.9585,   1.5905],\n",
            "        [ -2.6974,   2.6083,  -7.1125,  ...,  -6.7834,  -8.2885,  11.1248],\n",
            "        ...,\n",
            "        [ -9.4491,  -4.8527, -10.8937,  ...,  11.4497, -13.7753,  -3.2892],\n",
            "        [ -7.5541,  -8.6451,  -5.2020,  ...,   0.6489,  -9.3329,  -8.0921],\n",
            "        [-10.1424,  -7.4779,  -8.8335,  ...,  -7.1802,  -8.6610,  -9.5914]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([4, 1, 9, 2, 4, 1, 3, 7, 8, 0, 0, 9, 6, 6, 0, 6, 5, 8, 2, 7, 4, 0, 2, 7,\n",
            "        7, 8, 8, 7, 0, 4, 9, 1, 4, 4, 5, 5, 4, 6, 2, 3, 1, 0, 3, 3, 3, 6, 5, 1,\n",
            "        2, 0, 9, 7, 9, 3, 8, 7, 3, 1, 7, 7, 3, 2, 2, 8, 9, 5, 9, 2, 1, 7, 4, 4,\n",
            "        0, 5, 4, 1, 5, 4, 0, 8, 2, 9, 8, 7, 8, 4, 2, 3, 4, 0, 5, 4, 1, 8, 2, 5,\n",
            "        4, 7, 2, 5, 3, 7, 9, 7, 1, 4, 9, 3, 1, 4, 5, 5, 1, 7, 1, 3, 0, 9, 2, 5,\n",
            "        7, 0, 6, 3, 5, 7, 5, 5], device='cuda:0')\n",
            "Target: tensor([5, 1, 9, 2, 4, 1, 3, 7, 8, 2, 0, 9, 6, 6, 0, 6, 5, 8, 2, 7, 4, 0, 2, 7,\n",
            "        7, 8, 8, 7, 0, 4, 9, 1, 4, 4, 3, 5, 4, 6, 2, 3, 1, 0, 3, 3, 3, 6, 3, 1,\n",
            "        2, 8, 9, 7, 9, 3, 8, 7, 3, 1, 7, 7, 3, 2, 2, 8, 9, 5, 9, 2, 1, 7, 4, 4,\n",
            "        0, 5, 7, 1, 5, 4, 0, 8, 4, 9, 8, 7, 8, 4, 2, 3, 4, 0, 5, 4, 1, 8, 2, 5,\n",
            "        4, 5, 2, 5, 3, 7, 9, 7, 1, 4, 1, 3, 1, 4, 5, 5, 1, 7, 1, 3, 0, 1, 2, 5,\n",
            "        7, 0, 6, 3, 5, 7, 5, 5], device='cuda:0')\n",
            "Output: tensor([[-0.5292, -5.9862, -2.6029,  ..., -7.0992, 15.8635, -4.1846],\n",
            "        [-8.1887,  0.5830, -9.2208,  ..., -4.5285, -8.3939, 14.4144],\n",
            "        [-1.9369, -7.7113,  3.4990,  ..., -1.6936, -9.3356, -7.3903],\n",
            "        ...,\n",
            "        [-5.2005, -7.2806, -2.9258,  ..., -6.1658, -5.1812, -7.6940],\n",
            "        [-5.2147, -5.7364, -1.3087,  ..., -2.2526, -7.7530, -0.1002],\n",
            "        [-6.5461, -8.0498,  8.3048,  ..., -7.4833, -8.5287, -8.2055]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([8, 9, 4, 6, 3, 6, 6, 8, 2, 6, 4, 8, 4, 1, 3, 9, 2, 3, 5, 0, 1, 6, 4, 2,\n",
            "        8, 2, 4, 3, 1, 4, 4, 4, 4, 0, 5, 5, 7, 6, 5, 4, 3, 6, 5, 6, 2, 4, 7, 0,\n",
            "        4, 6, 5, 0, 2, 3, 9, 2, 3, 8, 0, 7, 0, 6, 8, 1, 2, 8, 9, 1, 5, 4, 2, 3,\n",
            "        5, 5, 5, 6, 5, 0, 3, 2, 1, 3, 5, 7, 7, 4, 9, 0, 5, 9, 5, 1, 7, 6, 6, 4,\n",
            "        8, 5, 1, 2, 8, 4, 3, 3, 5, 3, 0, 4, 0, 7, 7, 9, 3, 8, 4, 5, 9, 1, 0, 3,\n",
            "        1, 3, 8, 0, 5, 5, 4, 6], device='cuda:0')\n",
            "Target: tensor([8, 9, 4, 6, 3, 6, 6, 8, 2, 6, 4, 8, 4, 1, 3, 1, 2, 3, 3, 0, 1, 6, 4, 2,\n",
            "        8, 2, 4, 3, 1, 4, 4, 4, 4, 1, 5, 5, 7, 6, 5, 4, 5, 6, 5, 6, 2, 4, 7, 7,\n",
            "        4, 6, 5, 0, 2, 3, 9, 2, 3, 8, 0, 7, 0, 6, 8, 1, 2, 8, 9, 1, 5, 4, 2, 3,\n",
            "        5, 5, 3, 6, 5, 0, 3, 2, 1, 3, 3, 7, 7, 4, 9, 4, 3, 9, 5, 1, 7, 6, 6, 4,\n",
            "        8, 5, 1, 2, 8, 4, 5, 3, 3, 3, 0, 4, 8, 7, 7, 9, 4, 8, 4, 5, 9, 1, 8, 3,\n",
            "        1, 3, 8, 0, 3, 5, 4, 6], device='cuda:0')\n",
            "Output: tensor([[ -8.5486,  -3.5714, -12.7276,  ...,  -7.1040,  -7.4368,  24.1517],\n",
            "        [ -3.6066, -11.1658,  19.3262,  ...,  -7.5870,  -8.6461, -10.9877],\n",
            "        [ -7.7395, -11.3615,  -9.2888,  ...,  24.5047, -14.4188,  -7.5885],\n",
            "        ...,\n",
            "        [ -5.9886,  -5.0347,  -7.8629,  ...,  -5.8586, -10.9925,  -6.9044],\n",
            "        [ -6.3713,  -8.1729,  -2.6737,  ...,  -0.8850,  -7.5153,  -8.4833],\n",
            "        [ -8.4396,  -5.6558,  -0.3111,  ...,  -6.2314,  -2.6238,  -7.5981]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([9, 2, 7, 2, 0, 5, 2, 9, 7, 5, 0, 7, 8, 3, 5, 3, 3, 8, 7, 7, 9, 6, 6, 8,\n",
            "        9, 0, 1, 9, 4, 9, 0, 9, 7, 2, 2, 8, 5, 9, 8, 2, 8, 0, 6, 6, 8, 6, 9, 6,\n",
            "        1, 1, 5, 3, 5, 0, 7, 6, 1, 4, 6, 1, 1, 8, 1, 1, 2, 3, 4, 9, 9, 5, 2, 7,\n",
            "        3, 5, 6, 9, 6, 9, 8, 8, 6, 7, 4, 7, 2, 0, 8, 0, 9, 0, 7, 8, 1, 2, 3, 2,\n",
            "        4, 2, 9, 0, 8, 8, 9, 6, 5, 5, 2, 4, 1, 0, 6, 1, 8, 4, 5, 5, 5, 0, 8, 9,\n",
            "        0, 5, 7, 9, 6, 5, 4, 6], device='cuda:0')\n",
            "Target: tensor([9, 2, 7, 2, 0, 5, 2, 9, 7, 5, 0, 7, 0, 3, 5, 5, 3, 8, 7, 7, 9, 6, 5, 8,\n",
            "        9, 0, 1, 9, 4, 0, 0, 1, 7, 2, 2, 8, 5, 9, 8, 2, 8, 0, 6, 6, 8, 6, 9, 6,\n",
            "        1, 1, 5, 3, 5, 0, 7, 6, 1, 4, 4, 1, 1, 8, 1, 1, 2, 3, 4, 9, 9, 5, 2, 7,\n",
            "        3, 3, 6, 9, 6, 9, 8, 8, 6, 7, 4, 7, 2, 3, 8, 0, 9, 0, 7, 8, 1, 2, 3, 2,\n",
            "        4, 2, 9, 0, 3, 8, 9, 6, 5, 5, 2, 4, 1, 0, 6, 1, 8, 4, 5, 5, 5, 0, 8, 9,\n",
            "        0, 5, 7, 9, 6, 3, 4, 6], device='cuda:0')\n",
            "Output: tensor([[ -4.9999,  -8.3697,  -2.2345,  ...,   4.9877,  -7.9853,  -6.5279],\n",
            "        [ -6.5617,   1.2937,  -9.7170,  ..., -10.9171,  -3.2219,  17.7833],\n",
            "        [ -0.5919,  -5.8818,  -7.2274,  ...,   6.9094,  -5.0778,  -5.1342],\n",
            "        ...,\n",
            "        [-15.5230,  -6.1214,  -3.6261,  ..., -11.1731, -12.7097,  -6.2744],\n",
            "        [  1.1123,  -3.4869,  -3.9088,  ...,  -7.0031,  16.8283,  -4.8950],\n",
            "        [ -7.3156,  -5.8645,  -1.3994,  ...,  -6.6959, -11.6533,  -7.1182]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([4, 9, 7, 1, 3, 5, 2, 8, 0, 2, 2, 9, 8, 1, 9, 7, 7, 3, 1, 4, 1, 1, 3, 8,\n",
            "        8, 1, 9, 3, 8, 9, 5, 1, 1, 6, 2, 7, 2, 6, 4, 6, 9, 6, 0, 1, 3, 9, 3, 8,\n",
            "        8, 8, 2, 9, 1, 2, 6, 0, 5, 0, 7, 7, 7, 9, 4, 5, 0, 0, 3, 5, 5, 8, 9, 1,\n",
            "        2, 0, 1, 3, 3, 6, 9, 1, 0, 7, 1, 9, 5, 8, 8, 9, 9, 7, 9, 8, 3, 0, 2, 6,\n",
            "        9, 2, 2, 4, 0, 8, 4, 8, 1, 5, 7, 2, 7, 9, 5, 2, 2, 9, 2, 6, 5, 5, 2, 5,\n",
            "        6, 5, 1, 8, 2, 6, 8, 6], device='cuda:0')\n",
            "Target: tensor([4, 9, 7, 1, 2, 5, 2, 8, 0, 2, 2, 9, 8, 1, 9, 7, 7, 3, 1, 4, 1, 1, 3, 8,\n",
            "        8, 1, 9, 3, 8, 9, 5, 1, 1, 6, 2, 7, 2, 6, 2, 6, 9, 6, 0, 1, 3, 9, 3, 8,\n",
            "        8, 8, 2, 9, 1, 2, 6, 0, 5, 0, 7, 7, 7, 9, 4, 5, 0, 2, 3, 5, 5, 8, 9, 1,\n",
            "        2, 7, 1, 3, 4, 6, 9, 1, 0, 7, 1, 9, 5, 8, 1, 9, 9, 7, 9, 8, 3, 0, 2, 6,\n",
            "        0, 2, 2, 4, 0, 8, 4, 8, 1, 2, 7, 2, 7, 9, 5, 2, 2, 9, 2, 6, 5, 7, 2, 5,\n",
            "        6, 5, 1, 8, 2, 6, 8, 6], device='cuda:0')\n",
            "Output: tensor([[ -5.6600,  -6.0129, -11.3109,  ...,  -2.4306,  -7.8578,  -2.7210],\n",
            "        [ 28.7086,  -9.8565,  -8.6132,  ...,  -9.5310,  -4.1551,  -3.8750],\n",
            "        [  0.8986,  -1.9112,  -3.8461,  ...,  -7.2923,  14.5228,  -0.8122],\n",
            "        ...,\n",
            "        [ -7.0927,   2.5716,  -5.8157,  ...,  -9.5256,  -8.1853,  14.8505],\n",
            "        [ -3.0997,   4.5973, -10.7466,  ..., -10.0004,  -6.7532,  18.1908],\n",
            "        [-11.2856,  -7.6003,  -7.5645,  ..., -14.6616, -11.1465,  -7.6177]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([5, 0, 8, 0, 0, 9, 0, 6, 8, 8, 6, 6, 0, 7, 9, 8, 0, 5, 0, 9, 6, 0, 4, 7,\n",
            "        1, 7, 8, 8, 2, 3, 9, 2, 5, 3, 3, 1, 5, 3, 0, 2, 5, 2, 7, 3, 8, 4, 4, 6,\n",
            "        1, 9, 7, 1, 7, 0, 1, 2, 3, 0, 2, 9, 4, 5, 0, 3, 3, 0, 2, 3, 0, 1, 3, 7,\n",
            "        9, 9, 0, 7, 5, 6, 7, 1, 2, 3, 7, 4, 6, 8, 4, 9, 8, 7, 9, 2, 3, 6, 0, 2,\n",
            "        5, 6, 0, 5, 3, 5, 8, 3, 4, 2, 9, 5, 6, 3, 8, 6, 1, 8, 5, 5, 8, 6, 5, 9,\n",
            "        0, 4, 5, 0, 5, 9, 9, 3], device='cuda:0')\n",
            "Target: tensor([3, 0, 8, 0, 0, 9, 9, 6, 8, 0, 6, 6, 0, 7, 9, 8, 0, 5, 0, 9, 6, 0, 4, 7,\n",
            "        1, 7, 8, 8, 2, 3, 9, 6, 5, 5, 3, 1, 5, 3, 0, 2, 5, 2, 7, 3, 8, 4, 4, 6,\n",
            "        1, 9, 4, 1, 7, 0, 1, 4, 3, 0, 2, 9, 4, 5, 0, 3, 4, 7, 2, 3, 0, 1, 3, 7,\n",
            "        9, 9, 0, 7, 5, 6, 7, 1, 2, 2, 7, 4, 6, 8, 4, 9, 8, 4, 9, 2, 3, 4, 0, 2,\n",
            "        5, 6, 0, 5, 3, 5, 8, 3, 7, 4, 9, 5, 6, 3, 8, 6, 9, 8, 5, 5, 8, 3, 5, 9,\n",
            "        0, 4, 2, 0, 5, 9, 9, 3], device='cuda:0')\n",
            "Output: tensor([[ -2.5604,  -4.1889,  -5.0991,  ...,  -8.1414,  17.0744,  -3.0765],\n",
            "        [ 11.9462,  -6.5531,   1.9111,  ...,  -8.0626,  -4.1991,  -1.9203],\n",
            "        [ -7.8134,  -6.3047,   1.8962,  ..., -11.7033,  -8.9414,  -9.2108],\n",
            "        ...,\n",
            "        [ -3.3270,  -7.3027,   2.5934,  ...,  -8.6994,  -9.3776,  -5.6552],\n",
            "        [ -4.9107,  -1.0821,  -2.7156,  ...,  -8.5531,  15.9287,  -1.4478],\n",
            "        [ -5.3856,  -7.5974,   0.6664,  ...,   5.4252,  -6.4435,  -5.4176]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([8, 0, 6, 0, 2, 3, 7, 8, 4, 6, 9, 1, 5, 2, 2, 7, 9, 3, 9, 5, 5, 2, 2, 2,\n",
            "        8, 8, 5, 0, 0, 7, 0, 4, 1, 6, 3, 0, 9, 2, 6, 7, 4, 2, 0, 0, 7, 4, 3, 2,\n",
            "        6, 4, 2, 7, 8, 0, 1, 0, 1, 3, 6, 4, 8, 0, 6, 0, 9, 1, 6, 2, 1, 0, 1, 3,\n",
            "        0, 3, 7, 9, 9, 3, 7, 5, 7, 1, 6, 6, 9, 1, 1, 4, 4, 2, 1, 7, 0, 1, 2, 1,\n",
            "        1, 3, 2, 6, 9, 9, 3, 2, 0, 8, 7, 3, 4, 6, 4, 6, 9, 4, 8, 9, 3, 0, 5, 7,\n",
            "        2, 0, 0, 9, 9, 3, 8, 7], device='cuda:0')\n",
            "Target: tensor([8, 0, 6, 0, 2, 3, 7, 0, 0, 6, 9, 1, 5, 2, 2, 7, 9, 3, 9, 3, 5, 2, 2, 2,\n",
            "        8, 8, 5, 0, 0, 7, 0, 3, 1, 6, 5, 0, 9, 2, 6, 7, 4, 2, 0, 0, 5, 3, 3, 2,\n",
            "        6, 4, 2, 7, 8, 0, 1, 0, 1, 3, 6, 4, 8, 0, 6, 0, 9, 1, 6, 2, 1, 0, 1, 3,\n",
            "        0, 3, 7, 9, 9, 3, 7, 5, 7, 1, 6, 6, 1, 1, 1, 4, 4, 2, 1, 7, 0, 1, 2, 1,\n",
            "        1, 3, 2, 6, 9, 1, 3, 2, 0, 8, 7, 3, 4, 6, 4, 6, 9, 4, 8, 9, 3, 0, 5, 7,\n",
            "        2, 0, 2, 9, 9, 3, 8, 7], device='cuda:0')\n",
            "Output: tensor([[ -5.9500,  16.2845,  -7.9492,  ...,  -9.0829,  -4.9094,   1.9716],\n",
            "        [ -5.2235,  22.0210,  -7.9448,  ...,  -9.7270, -10.3549,  -1.4737],\n",
            "        [ -8.4136, -11.0795,  22.8190,  ...,  -7.3580,  -8.7978, -10.1569],\n",
            "        ...,\n",
            "        [ 16.0408,  -4.3019,  -2.6553,  ...,  -9.0996,  -7.7695,  -2.6072],\n",
            "        [ -4.2691, -11.7660,  18.9460,  ...,  -9.8100, -10.2777, -11.0363],\n",
            "        [ 11.2522,  -3.8270,  -7.4779,  ...,  -6.8293,   1.3853,   4.3775]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([1, 1, 2, 2, 2, 2, 9, 1, 2, 6, 1, 2, 1, 8, 9, 2, 2, 1, 9, 9, 3, 4, 3, 5,\n",
            "        0, 6, 3, 7, 1, 7, 7, 9, 8, 6, 3, 5, 5, 1, 6, 2, 7, 8, 4, 3, 3, 5, 9, 6,\n",
            "        8, 5, 1, 0, 6, 5, 0, 9, 6, 2, 0, 8, 7, 1, 2, 6, 9, 1, 6, 4, 7, 1, 9, 0,\n",
            "        0, 5, 2, 8, 3, 0, 3, 8, 4, 0, 8, 3, 6, 1, 3, 3, 0, 5, 0, 4, 1, 7, 2, 0,\n",
            "        1, 5, 9, 1, 6, 1, 6, 1, 2, 3, 7, 0, 9, 5, 8, 8, 0, 1, 2, 9, 7, 1, 9, 2,\n",
            "        1, 9, 0, 3, 6, 0, 2, 0], device='cuda:0')\n",
            "Target: tensor([1, 1, 2, 2, 2, 5, 9, 1, 2, 6, 1, 2, 1, 8, 9, 2, 3, 1, 9, 9, 4, 4, 5, 5,\n",
            "        0, 3, 3, 7, 1, 7, 7, 9, 8, 6, 3, 5, 5, 1, 6, 2, 7, 8, 4, 3, 3, 5, 9, 6,\n",
            "        8, 5, 1, 0, 6, 7, 0, 9, 6, 2, 0, 8, 7, 1, 2, 6, 9, 1, 6, 4, 7, 1, 9, 0,\n",
            "        0, 5, 2, 8, 3, 8, 2, 8, 4, 0, 8, 3, 6, 1, 2, 3, 0, 5, 0, 4, 1, 7, 2, 0,\n",
            "        1, 3, 9, 1, 6, 1, 6, 1, 2, 8, 7, 0, 9, 5, 8, 0, 0, 1, 7, 9, 7, 1, 9, 2,\n",
            "        1, 9, 4, 3, 6, 0, 2, 0], device='cuda:0')\n",
            "Output: tensor([[  9.3444,  -5.5544,   3.2010,  ...,  -7.6982,  -0.0422,  -5.1094],\n",
            "        [ -5.2348,  -1.4074,  -8.3570,  ...,  -8.1968,  -3.9910,  18.2833],\n",
            "        [ -2.5266,   2.0324,  -4.8586,  ...,  -8.6062,  -7.6195,   7.2961],\n",
            "        ...,\n",
            "        [  3.9814,  -5.6980,  -5.9404,  ...,  -2.2912,  11.4080,   0.9086],\n",
            "        [ -6.9926,  -9.3477,  -4.6852,  ...,   0.7588, -12.2051,  -6.8810],\n",
            "        [-11.6107,  -5.5126,   0.5400,  ...,  -6.3421, -10.5716,  -7.1375]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([0, 9, 9, 8, 9, 2, 5, 4, 3, 0, 7, 8, 7, 3, 7, 5, 4, 6, 5, 9, 3, 2, 0, 2,\n",
            "        7, 9, 6, 7, 4, 7, 1, 2, 3, 1, 9, 3, 6, 8, 3, 7, 7, 0, 8, 1, 5, 2, 3, 5,\n",
            "        6, 6, 3, 5, 9, 8, 2, 5, 8, 0, 4, 1, 7, 8, 4, 9, 3, 2, 6, 3, 8, 8, 6, 7,\n",
            "        3, 8, 9, 1, 5, 3, 7, 9, 9, 9, 8, 5, 9, 9, 6, 8, 8, 4, 8, 1, 0, 1, 8, 8,\n",
            "        6, 5, 9, 0, 6, 1, 2, 6, 6, 3, 5, 6, 5, 3, 7, 3, 6, 8, 9, 4, 3, 3, 8, 3,\n",
            "        6, 0, 8, 5, 7, 8, 4, 6], device='cuda:0')\n",
            "Target: tensor([0, 9, 9, 8, 9, 4, 5, 4, 3, 0, 7, 8, 7, 3, 7, 5, 4, 6, 5, 9, 5, 2, 4, 2,\n",
            "        7, 9, 6, 7, 4, 7, 1, 2, 3, 1, 1, 3, 6, 8, 3, 7, 7, 0, 8, 1, 5, 2, 5, 5,\n",
            "        3, 6, 3, 5, 9, 8, 4, 6, 8, 0, 4, 1, 7, 8, 4, 9, 7, 2, 6, 3, 8, 8, 6, 7,\n",
            "        3, 8, 1, 1, 5, 3, 7, 9, 9, 9, 8, 5, 9, 9, 6, 8, 8, 4, 8, 1, 0, 1, 8, 8,\n",
            "        6, 5, 9, 0, 6, 1, 2, 6, 6, 3, 5, 6, 5, 5, 3, 3, 6, 8, 9, 4, 4, 3, 8, 3,\n",
            "        6, 0, 8, 5, 7, 8, 4, 6], device='cuda:0')\n",
            "Output: tensor([[ -8.3415,  -5.9190,  -7.6558,  ...,  -3.0204,  -7.7633,  -5.5447],\n",
            "        [ -4.6436,  -4.0587,  -7.3336,  ...,  -6.5838,  -1.9847,  -5.9159],\n",
            "        [ -3.6521,  11.8280,  -9.5336,  ..., -11.2746,  -5.5993,   9.1382],\n",
            "        ...,\n",
            "        [ -5.4805,  -3.9384,  -3.3989,  ...,  -7.4237,  -5.2822,  -4.8822],\n",
            "        [ -4.6401,  -5.4959,   4.6574,  ...,  -3.0905,  -6.8729,  -7.0920],\n",
            "        [ -3.7166,  -2.4705,  -8.7626,  ...,   5.7170,  -9.6385,  -1.4678]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([5, 3, 1, 8, 3, 0, 4, 4, 0, 3, 5, 3, 3, 2, 7, 7, 8, 2, 0, 5, 3, 6, 1, 1,\n",
            "        3, 9, 9, 3, 6, 7, 8, 8, 3, 9, 8, 7, 8, 8, 0, 3, 2, 4, 8, 7, 5, 5, 1, 8,\n",
            "        7, 3, 5, 4, 4, 3, 8, 6, 1, 1, 6, 3, 2, 2, 8, 9, 8, 3, 3, 8, 1, 1, 0, 5,\n",
            "        2, 0, 3, 0, 3, 6, 6, 3, 9, 6, 6, 3, 9, 4, 8, 3, 4, 5, 1, 0, 2, 9, 7, 8,\n",
            "        5, 5, 8, 6, 9, 0, 8, 0, 5, 1, 2, 3, 5, 5, 3, 8, 9, 2, 6, 4, 2, 4, 3, 8,\n",
            "        9, 6, 6, 6, 5, 3, 2, 7], device='cuda:0')\n",
            "Target: tensor([5, 3, 1, 8, 3, 0, 4, 4, 3, 3, 5, 3, 3, 2, 7, 7, 8, 2, 0, 5, 5, 6, 1, 1,\n",
            "        3, 9, 9, 3, 6, 7, 1, 8, 3, 9, 8, 7, 8, 8, 0, 3, 2, 4, 8, 7, 5, 7, 1, 8,\n",
            "        7, 3, 5, 4, 4, 3, 8, 6, 1, 1, 6, 3, 2, 2, 8, 9, 8, 3, 3, 8, 1, 1, 0, 5,\n",
            "        2, 0, 5, 0, 5, 8, 6, 3, 9, 6, 6, 3, 9, 4, 8, 3, 4, 3, 1, 1, 2, 9, 7, 8,\n",
            "        5, 5, 8, 6, 9, 0, 8, 3, 5, 1, 3, 3, 5, 5, 3, 8, 1, 4, 6, 4, 2, 4, 3, 8,\n",
            "        9, 6, 6, 6, 5, 5, 2, 7], device='cuda:0')\n",
            "Output: tensor([[ -7.8823,  -9.4329,  -9.4172,  ...,  16.1961, -10.5732,  -5.8277],\n",
            "        [ -4.9818,  -7.0311,   5.8652,  ...,  -8.5352,  -9.7220,  -7.1476],\n",
            "        [ -8.1089, -10.2031,   0.3668,  ...,  -2.8411, -12.0937,  -7.6564],\n",
            "        ...,\n",
            "        [ -2.4612,  12.0423,  -1.6836,  ...,  -6.8238,  -5.9708,  -3.3175],\n",
            "        [ -0.8218,  -6.2896,  -4.3653,  ...,  -8.8223,  -2.3336,  -7.6398],\n",
            "        [ 10.4181,  -6.2203,   0.8176,  ...,  -8.3093,  -1.7412,   1.5687]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([7, 2, 4, 9, 3, 8, 3, 7, 7, 8, 5, 3, 9, 1, 6, 4, 3, 3, 3, 8, 5, 6, 1, 3,\n",
            "        7, 9, 6, 1, 6, 4, 9, 0, 5, 7, 4, 9, 1, 0, 0, 2, 7, 8, 5, 7, 7, 6, 1, 6,\n",
            "        6, 4, 4, 2, 1, 8, 4, 4, 7, 8, 5, 8, 9, 4, 3, 2, 2, 1, 3, 0, 1, 1, 1, 1,\n",
            "        6, 8, 5, 1, 3, 0, 4, 5, 0, 7, 0, 6, 2, 0, 9, 8, 6, 8, 5, 8, 2, 9, 6, 0,\n",
            "        6, 1, 8, 7, 6, 2, 2, 6, 5, 6, 9, 2, 3, 4, 9, 8, 5, 3, 9, 6, 5, 7, 1, 8,\n",
            "        5, 8, 6, 2, 5, 1, 5, 0], device='cuda:0')\n",
            "Target: tensor([7, 4, 4, 9, 3, 8, 3, 7, 7, 8, 5, 3, 9, 1, 6, 7, 3, 3, 3, 8, 5, 6, 1, 3,\n",
            "        7, 9, 6, 1, 6, 4, 9, 0, 5, 7, 4, 9, 1, 0, 0, 2, 7, 8, 3, 2, 7, 6, 1, 6,\n",
            "        6, 4, 4, 2, 1, 8, 4, 4, 7, 8, 5, 8, 9, 4, 3, 2, 2, 9, 3, 0, 1, 1, 1, 1,\n",
            "        6, 8, 7, 1, 3, 0, 4, 5, 0, 3, 0, 6, 2, 0, 9, 8, 6, 8, 5, 8, 2, 9, 6, 0,\n",
            "        6, 1, 8, 7, 3, 2, 2, 6, 5, 6, 9, 2, 5, 4, 9, 8, 5, 3, 9, 3, 5, 7, 1, 8,\n",
            "        5, 8, 6, 2, 5, 1, 3, 0], device='cuda:0')\n",
            "Output: tensor([[ -4.0229,  -6.8260,   0.0233,  ...,  -4.2921,  -5.5751,  -7.1424],\n",
            "        [ -5.1663,   4.1040,  -5.1030,  ...,  -6.3399,   0.7832,  -2.4020],\n",
            "        [ -4.8081,  -0.5023,  -6.8234,  ..., -10.0117,  -2.2642,   0.1163],\n",
            "        ...,\n",
            "        [  7.1964,  -9.5521,  -1.0224,  ...,  -9.8977,  -7.3318,  -6.2986],\n",
            "        [ -4.8901,  -7.6745,  -2.0280,  ...,   3.3957, -11.0419,  -8.9919],\n",
            "        [ -6.0128,  -0.8022,  -8.9951,  ...,  -8.6255,  -1.8098,  16.2751]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([4, 1, 3, 7, 4, 9, 2, 1, 4, 4, 5, 9, 8, 7, 2, 1, 6, 1, 7, 3, 6, 3, 8, 0,\n",
            "        2, 2, 4, 9, 0, 5, 5, 4, 6, 8, 1, 0, 2, 0, 4, 2, 9, 1, 3, 3, 0, 2, 1, 1,\n",
            "        0, 4, 1, 9, 4, 8, 1, 1, 2, 5, 9, 8, 1, 6, 7, 2, 1, 9, 3, 8, 8, 1, 9, 7,\n",
            "        8, 9, 1, 6, 6, 3, 3, 1, 3, 8, 8, 8, 0, 6, 6, 5, 1, 0, 7, 6, 4, 8, 6, 0,\n",
            "        6, 6, 0, 6, 7, 9, 1, 8, 6, 7, 0, 5, 4, 3, 2, 1, 7, 8, 2, 0, 1, 7, 4, 8,\n",
            "        7, 6, 6, 7, 0, 0, 4, 9], device='cuda:0')\n",
            "Target: tensor([4, 1, 9, 5, 4, 9, 2, 1, 4, 2, 5, 9, 8, 7, 2, 1, 6, 9, 7, 3, 6, 5, 8, 0,\n",
            "        2, 2, 4, 9, 0, 5, 5, 4, 2, 3, 1, 0, 3, 0, 4, 4, 9, 1, 3, 3, 0, 2, 1, 1,\n",
            "        0, 4, 1, 9, 4, 8, 1, 1, 1, 5, 9, 8, 1, 6, 7, 2, 9, 9, 7, 8, 8, 1, 9, 7,\n",
            "        8, 9, 1, 6, 6, 3, 3, 1, 3, 0, 8, 8, 3, 6, 6, 7, 1, 0, 7, 6, 4, 8, 2, 0,\n",
            "        6, 6, 8, 6, 3, 1, 1, 8, 6, 7, 0, 5, 4, 3, 2, 1, 7, 8, 2, 0, 1, 7, 4, 8,\n",
            "        7, 6, 6, 7, 0, 6, 4, 9], device='cuda:0')\n",
            "Output: tensor([[  6.9785,  -6.1082,  -1.0218,  ...,  -6.4080,  -0.6943,  -3.7049],\n",
            "        [ -7.8381,  -2.6342,  -3.7833,  ...,  -5.6002,  -3.4972,  -0.6071],\n",
            "        [ -7.5466,  -6.1512,   6.4108,  ...,  -2.6578,  -9.2046, -10.1421],\n",
            "        ...,\n",
            "        [ -0.2037,  -2.9051,  -6.2940,  ...,  -5.7760,  19.6492,  -7.0245],\n",
            "        [  7.3397,  -0.0605,  -7.4722,  ...,  -5.0682,   2.4096,   4.3074],\n",
            "        [ -5.1492,  -6.9146,  -0.0763,  ...,  -4.7023, -11.5289,  -6.4182]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([0, 6, 2, 1, 3, 8, 6, 8, 1, 9, 6, 7, 6, 3, 0, 6, 0, 1, 3, 1, 1, 5, 2, 2,\n",
            "        3, 3, 8, 1, 6, 6, 1, 8, 8, 6, 2, 7, 4, 6, 8, 9, 3, 5, 5, 5, 3, 1, 4, 1,\n",
            "        9, 5, 1, 7, 7, 7, 5, 2, 8, 3, 6, 6, 6, 4, 0, 0, 1, 7, 6, 0, 9, 1, 2, 4,\n",
            "        1, 2, 2, 4, 9, 8, 3, 8, 7, 6, 3, 4, 7, 2, 3, 1, 5, 3, 0, 4, 3, 4, 9, 4,\n",
            "        7, 9, 1, 7, 8, 5, 1, 8, 3, 2, 5, 7, 2, 7, 9, 6, 8, 6, 8, 6, 5, 9, 0, 4,\n",
            "        8, 5, 6, 5, 3, 8, 0, 4], device='cuda:0')\n",
            "Target: tensor([2, 3, 2, 1, 3, 8, 6, 8, 1, 9, 6, 7, 6, 5, 0, 6, 0, 1, 3, 1, 1, 5, 3, 4,\n",
            "        3, 3, 8, 1, 6, 6, 1, 8, 8, 6, 2, 7, 4, 6, 8, 9, 3, 3, 5, 5, 3, 1, 4, 1,\n",
            "        9, 5, 1, 7, 7, 7, 5, 2, 3, 3, 6, 6, 6, 4, 0, 7, 1, 7, 3, 0, 8, 1, 2, 4,\n",
            "        1, 2, 2, 4, 9, 8, 2, 8, 7, 6, 3, 4, 7, 2, 3, 1, 5, 3, 0, 4, 3, 4, 9, 4,\n",
            "        7, 9, 1, 7, 8, 3, 1, 8, 3, 2, 5, 7, 2, 7, 9, 6, 8, 6, 8, 6, 5, 9, 0, 4,\n",
            "        8, 5, 6, 4, 3, 8, 0, 4], device='cuda:0')\n",
            "Output: tensor([[  4.3999,  -0.9302,  -4.7715,  ...,  -7.0767,  10.5646,  -0.1978],\n",
            "        [ -4.0434,  -7.3712,  -3.1065,  ...,  -7.5278,  -1.0540,  -3.3311],\n",
            "        [ -8.3809,  -3.1860,  -5.9661,  ...,   5.7701, -10.6860,  -7.5030],\n",
            "        ...,\n",
            "        [ 22.6634,  -5.6468,  -5.1278,  ...,  -5.5564,  -1.6156,  -4.5575],\n",
            "        [ -5.4076,  -1.8375, -12.4277,  ..., -10.5203,  -5.4651,  25.1812],\n",
            "        [ -8.0152,  -9.2430,  -6.3013,  ...,  -4.5499, -10.0567,  -9.4625]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([8, 6, 5, 2, 7, 1, 7, 4, 1, 2, 4, 2, 2, 8, 7, 2, 2, 6, 3, 7, 6, 8, 9, 0,\n",
            "        5, 5, 2, 9, 8, 7, 2, 3, 3, 7, 4, 3, 2, 1, 9, 8, 1, 5, 5, 2, 6, 5, 9, 1,\n",
            "        2, 4, 3, 6, 2, 6, 9, 7, 8, 8, 8, 9, 1, 2, 5, 7, 2, 0, 6, 6, 1, 6, 7, 1,\n",
            "        5, 0, 0, 7, 4, 6, 5, 6, 4, 8, 3, 4, 5, 5, 0, 6, 6, 7, 0, 0, 9, 9, 6, 1,\n",
            "        0, 4, 6, 5, 6, 9, 3, 6, 6, 2, 2, 1, 0, 1, 9, 5, 7, 4, 3, 0, 2, 7, 6, 4,\n",
            "        3, 0, 8, 2, 0, 0, 9, 6], device='cuda:0')\n",
            "Target: tensor([8, 6, 7, 2, 7, 1, 7, 4, 1, 2, 4, 2, 2, 8, 7, 2, 2, 6, 7, 5, 6, 8, 9, 9,\n",
            "        6, 5, 2, 9, 8, 7, 2, 3, 3, 7, 4, 3, 2, 1, 9, 0, 1, 5, 3, 2, 6, 5, 9, 1,\n",
            "        5, 4, 3, 6, 2, 8, 9, 7, 8, 0, 8, 9, 1, 2, 5, 7, 2, 0, 6, 6, 1, 6, 3, 1,\n",
            "        5, 0, 3, 7, 4, 6, 5, 6, 4, 8, 3, 4, 5, 5, 0, 6, 6, 7, 5, 0, 9, 9, 6, 1,\n",
            "        0, 4, 6, 3, 6, 9, 3, 6, 6, 8, 2, 1, 0, 1, 9, 3, 7, 4, 3, 0, 2, 7, 6, 4,\n",
            "        3, 0, 8, 2, 0, 0, 9, 6], device='cuda:0')\n",
            "Output: tensor([[ -9.9818,  -4.2132,  -0.5302,  ...,  -5.9993, -10.1258,  -7.6306],\n",
            "        [-11.5388,  -5.5505,   1.7218,  ...,  -4.7450,  -9.6069,  -7.1191],\n",
            "        [ -2.8778,  -8.3935,  -2.2433,  ...,  -8.5474,  19.5651,  -3.1309],\n",
            "        ...,\n",
            "        [ -5.8190,  -6.0894,  -4.2540,  ...,   6.7902,  -6.4179,  -4.1694],\n",
            "        [ -2.4734,  -4.6615,   1.6350,  ...,  -6.4830,  -8.9291,  -5.4614],\n",
            "        [ -7.0739,  14.3644,  -8.4604,  ..., -10.1649,  -5.4083,   2.0399]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([6, 6, 8, 7, 4, 1, 8, 1, 2, 2, 4, 8, 5, 2, 6, 5, 3, 9, 1, 0, 7, 2, 2, 4,\n",
            "        0, 0, 6, 2, 2, 4, 0, 5, 9, 7, 1, 8, 4, 5, 5, 9, 8, 5, 7, 8, 0, 1, 8, 9,\n",
            "        1, 6, 3, 8, 0, 3, 4, 4, 8, 4, 8, 9, 8, 6, 0, 0, 8, 2, 0, 4, 2, 5, 6, 0,\n",
            "        5, 8, 4, 1, 9, 0, 1, 4, 4, 8, 4, 9, 6, 0, 7, 7, 6, 8, 9, 6, 2, 0, 4, 9,\n",
            "        4, 9, 3, 9, 7, 6, 5, 0, 9, 7, 1, 8, 6, 3, 6, 5, 0, 1, 9, 4, 6, 7, 9, 8,\n",
            "        3, 9, 2, 1, 2, 7, 6, 1], device='cuda:0')\n",
            "Target: tensor([6, 6, 8, 7, 4, 1, 8, 1, 2, 2, 4, 8, 5, 2, 6, 5, 3, 9, 1, 0, 7, 2, 4, 4,\n",
            "        0, 0, 6, 2, 2, 4, 0, 5, 9, 7, 1, 8, 4, 5, 5, 9, 8, 5, 7, 8, 0, 9, 8, 9,\n",
            "        1, 6, 3, 8, 0, 3, 4, 4, 8, 4, 8, 9, 8, 6, 0, 0, 8, 2, 7, 4, 2, 5, 6, 0,\n",
            "        5, 8, 4, 1, 9, 0, 1, 4, 4, 8, 4, 9, 6, 0, 7, 7, 6, 8, 9, 6, 2, 0, 4, 9,\n",
            "        4, 9, 3, 9, 6, 6, 7, 0, 9, 7, 1, 8, 6, 0, 6, 7, 4, 1, 9, 4, 6, 7, 9, 8,\n",
            "        3, 9, 2, 1, 2, 7, 6, 1], device='cuda:0')\n",
            "Output: tensor([[  5.8054,  -2.1121,   2.1208,  ...,  -5.9316,  -2.9341,  -2.4327],\n",
            "        [ 24.0864,  -6.1944,  -5.2309,  ...,  -4.8242,  -8.5239,  -4.2861],\n",
            "        [ -7.6205,  -8.2478,  -5.2245,  ...,  -7.9606,  -7.2469,  -6.7392],\n",
            "        ...,\n",
            "        [  2.9973,   0.7333,  -9.0344,  ...,  -5.7227,  -3.9644,  10.0372],\n",
            "        [ 33.4358,  -8.6304,  -8.1477,  ...,  -7.1092, -13.7733,  -7.9201],\n",
            "        [-12.6707, -11.9904,  -4.4142,  ..., -12.0903, -12.9319, -11.1249]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([0, 0, 5, 6, 0, 4, 3, 2, 8, 8, 0, 6, 0, 5, 2, 8, 0, 0, 6, 5, 9, 4, 2, 3,\n",
            "        6, 9, 6, 2, 2, 4, 1, 0, 5, 0, 8, 9, 3, 5, 9, 3, 8, 1, 6, 3, 7, 3, 6, 2,\n",
            "        0, 2, 8, 2, 8, 7, 7, 8, 1, 0, 8, 9, 7, 0, 3, 8, 0, 5, 9, 5, 8, 4, 2, 0,\n",
            "        9, 2, 2, 4, 7, 9, 2, 2, 2, 3, 1, 3, 2, 0, 0, 4, 0, 6, 5, 8, 0, 5, 8, 6,\n",
            "        3, 8, 5, 2, 9, 7, 9, 7, 1, 0, 1, 9, 6, 9, 2, 7, 9, 4, 4, 0, 6, 2, 4, 1,\n",
            "        5, 7, 2, 8, 5, 9, 0, 3], device='cuda:0')\n",
            "Target: tensor([0, 0, 5, 6, 0, 4, 3, 2, 8, 8, 0, 6, 9, 5, 2, 8, 7, 0, 6, 5, 9, 7, 2, 3,\n",
            "        6, 9, 6, 2, 2, 4, 1, 0, 5, 0, 8, 9, 3, 5, 9, 3, 8, 1, 6, 3, 7, 5, 6, 2,\n",
            "        0, 2, 8, 2, 8, 7, 7, 8, 1, 0, 8, 9, 7, 0, 3, 8, 0, 5, 9, 5, 8, 4, 2, 0,\n",
            "        9, 2, 2, 4, 4, 9, 2, 2, 2, 5, 1, 3, 2, 0, 0, 4, 0, 6, 5, 8, 0, 5, 8, 6,\n",
            "        4, 8, 5, 2, 9, 7, 9, 7, 1, 0, 1, 9, 6, 9, 2, 7, 9, 4, 4, 0, 6, 2, 4, 1,\n",
            "        3, 7, 2, 8, 5, 9, 0, 3], device='cuda:0')\n",
            "Output: tensor([[-10.3451,  -6.3307,  10.0344,  ...,  -5.3603,  -6.7878,  -7.6495],\n",
            "        [ -6.1619,  -7.3322,  -5.6508,  ...,  -7.5268,  -6.8529,  -8.6824],\n",
            "        [ -5.8357,  -7.0288,   9.8363,  ...,   0.0535, -10.9932,  -8.8224],\n",
            "        ...,\n",
            "        [ -1.6430,  -6.3144,   7.4556,  ...,  -0.8119,  -6.4450,  -6.7716],\n",
            "        [ -4.8613,  -9.1747, -11.2041,  ...,   1.7374,  -4.9597,   9.9543],\n",
            "        [ -9.5099,  -7.1880,  -9.0319,  ...,  -8.4351,  -8.2121,  -8.6470]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([2, 3, 2, 7, 2, 3, 2, 5, 9, 0, 5, 1, 9, 8, 7, 7, 4, 0, 6, 3, 2, 3, 1, 0,\n",
            "        4, 1, 8, 8, 4, 9, 4, 4, 0, 3, 9, 2, 0, 1, 1, 8, 4, 2, 8, 3, 6, 9, 5, 7,\n",
            "        6, 6, 5, 4, 7, 3, 3, 9, 0, 9, 5, 9, 5, 7, 6, 0, 4, 2, 2, 5, 6, 3, 8, 9,\n",
            "        3, 6, 1, 4, 5, 4, 6, 7, 2, 1, 0, 2, 0, 4, 9, 8, 8, 9, 1, 1, 5, 0, 0, 8,\n",
            "        7, 1, 7, 4, 5, 4, 4, 3, 2, 0, 6, 6, 0, 1, 3, 9, 8, 3, 7, 8, 9, 4, 8, 9,\n",
            "        0, 9, 7, 1, 6, 2, 9, 5], device='cuda:0')\n",
            "Target: tensor([2, 3, 2, 7, 6, 3, 2, 5, 9, 0, 5, 9, 9, 8, 7, 7, 4, 8, 6, 5, 2, 3, 1, 0,\n",
            "        4, 1, 8, 8, 4, 9, 4, 4, 3, 3, 9, 2, 0, 1, 1, 8, 4, 4, 8, 3, 2, 9, 5, 7,\n",
            "        6, 2, 5, 4, 7, 3, 3, 9, 0, 1, 5, 9, 3, 7, 6, 0, 4, 2, 2, 5, 6, 3, 8, 9,\n",
            "        5, 6, 1, 4, 5, 4, 6, 7, 2, 1, 0, 2, 0, 4, 9, 8, 8, 9, 1, 1, 5, 0, 0, 8,\n",
            "        7, 1, 7, 4, 5, 4, 3, 3, 2, 0, 6, 6, 0, 1, 3, 9, 8, 3, 7, 8, 9, 4, 8, 9,\n",
            "        0, 9, 7, 1, 6, 2, 9, 5], device='cuda:0')\n",
            "Output: tensor([[ -7.8790,  -5.8077,  -3.5059,  ...,  -7.0213,  -6.7816,  -4.4657],\n",
            "        [  1.5826,  -3.4627, -13.1943,  ..., -11.6048,  -2.4461,  23.8055],\n",
            "        [ -6.6215,  -3.1900,  -0.9113,  ...,  -8.3487,  -6.7143,  -7.8456],\n",
            "        ...,\n",
            "        [ -2.0587,  -8.6386,  10.8989,  ...,  -8.0606,  -8.8248,  -9.8019],\n",
            "        [ -4.9792,  -1.3829,  -7.1881,  ...,  -7.6714, -11.1987,  -2.6980],\n",
            "        [ -2.7196,   0.5427,  -3.8982,  ...,  -8.6012,  15.0572,  -2.9663]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([6, 9, 5, 9, 8, 7, 7, 1, 6, 5, 3, 9, 3, 1, 2, 2, 1, 8, 2, 0, 9, 3, 9, 8,\n",
            "        8, 6, 7, 3, 7, 1, 3, 9, 0, 9, 5, 6, 7, 2, 5, 8, 0, 5, 9, 7, 3, 5, 0, 6,\n",
            "        3, 1, 8, 2, 7, 5, 9, 0, 0, 0, 8, 8, 7, 3, 7, 8, 9, 3, 7, 9, 7, 8, 7, 9,\n",
            "        8, 5, 4, 8, 0, 7, 6, 3, 8, 0, 1, 9, 3, 7, 3, 9, 5, 5, 8, 7, 3, 5, 3, 5,\n",
            "        9, 7, 6, 7, 3, 6, 4, 3, 9, 4, 2, 1, 9, 6, 0, 2, 6, 7, 4, 7, 9, 0, 7, 4,\n",
            "        5, 5, 3, 1, 1, 2, 6, 8], device='cuda:0')\n",
            "Target: tensor([6, 9, 3, 9, 8, 7, 7, 1, 6, 5, 3, 1, 3, 1, 2, 7, 1, 8, 2, 0, 9, 7, 9, 8,\n",
            "        8, 6, 7, 3, 7, 1, 3, 9, 0, 9, 3, 6, 7, 2, 7, 3, 0, 5, 9, 7, 5, 5, 0, 6,\n",
            "        5, 1, 8, 2, 7, 5, 9, 0, 0, 0, 8, 8, 7, 3, 7, 8, 9, 3, 7, 9, 7, 8, 7, 9,\n",
            "        8, 5, 4, 8, 3, 7, 6, 3, 8, 2, 1, 9, 5, 7, 3, 9, 5, 5, 8, 7, 3, 5, 3, 5,\n",
            "        9, 7, 6, 7, 3, 6, 4, 3, 9, 4, 2, 1, 9, 6, 0, 2, 6, 7, 4, 7, 9, 0, 7, 4,\n",
            "        3, 5, 3, 1, 1, 2, 6, 8], device='cuda:0')\n",
            "Output: tensor([[ -5.4032,  -7.8433,  12.9586,  ...,  -5.6067,  -6.4277,  -9.5160],\n",
            "        [ -4.3840,  13.7921, -10.9067,  ...,  -8.4531,  -4.7044,   8.9348],\n",
            "        [ -6.3584,  -6.7289,  -3.4537,  ...,   6.6811,  -9.3907,  -5.6556],\n",
            "        ...,\n",
            "        [ 19.5110,  -4.1541,  -4.2109,  ...,  -5.3491,  -7.9926,  -3.9080],\n",
            "        [-10.6822,   1.8816, -16.1824,  ..., -15.2195,  -9.6801,  32.1322],\n",
            "        [ -1.4083,  -5.1410,   0.0689,  ...,  -2.1940,  -9.5222,  -5.4338]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([2, 1, 7, 8, 5, 9, 6, 1, 1, 4, 0, 6, 0, 9, 2, 6, 5, 8, 9, 5, 5, 6, 2, 9,\n",
            "        1, 6, 1, 8, 7, 1, 7, 5, 5, 4, 9, 7, 3, 2, 9, 9, 4, 2, 6, 1, 3, 8, 7, 9,\n",
            "        0, 4, 5, 7, 5, 2, 8, 7, 6, 9, 6, 9, 3, 8, 5, 6, 6, 9, 5, 7, 8, 0, 3, 0,\n",
            "        7, 4, 8, 2, 5, 1, 3, 2, 2, 6, 4, 1, 3, 2, 6, 3, 1, 3, 3, 3, 1, 3, 7, 0,\n",
            "        8, 4, 4, 5, 7, 9, 5, 4, 3, 9, 6, 8, 2, 3, 3, 1, 6, 1, 7, 0, 3, 4, 2, 9,\n",
            "        4, 5, 8, 2, 7, 0, 9, 6], device='cuda:0')\n",
            "Target: tensor([2, 1, 7, 8, 5, 9, 6, 1, 1, 5, 0, 6, 0, 9, 2, 6, 5, 8, 9, 5, 5, 6, 2, 9,\n",
            "        1, 5, 8, 8, 7, 1, 7, 3, 5, 4, 9, 7, 5, 2, 9, 9, 4, 7, 4, 1, 3, 8, 7, 9,\n",
            "        0, 4, 5, 7, 5, 2, 8, 7, 6, 9, 6, 9, 3, 8, 5, 6, 6, 9, 5, 7, 8, 0, 5, 0,\n",
            "        7, 4, 8, 2, 5, 1, 3, 2, 2, 6, 2, 1, 7, 4, 6, 3, 1, 3, 7, 2, 1, 3, 7, 0,\n",
            "        8, 4, 4, 5, 7, 9, 5, 4, 3, 9, 6, 8, 2, 3, 3, 1, 6, 1, 7, 0, 3, 4, 2, 9,\n",
            "        4, 5, 8, 2, 7, 0, 9, 6], device='cuda:0')\n",
            "Output: tensor([[ -2.2282,  -5.8394,  -4.9681,  ...,  -5.4980,  14.9792,  -0.6172],\n",
            "        [ 11.7877,  -4.4760,  -1.1958,  ...,  -8.2563,  -2.7080,   0.9557],\n",
            "        [  3.7321,  -5.6543,  -4.5242,  ...,  -7.5509,  18.5017,  -5.2220],\n",
            "        ...,\n",
            "        [ -3.0539,  -6.5739,   1.8174,  ...,  -3.5851, -10.8240,  -9.1191],\n",
            "        [-12.9808,   9.7892, -12.7672,  ..., -12.8863, -11.2206,  22.9790],\n",
            "        [ -6.3771, -10.8303,  -8.7373,  ...,  18.0004, -11.1578,  -6.8097]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([8, 0, 8, 2, 8, 5, 7, 7, 2, 2, 0, 0, 0, 7, 4, 1, 6, 6, 0, 8, 9, 0, 9, 0,\n",
            "        1, 2, 3, 0, 9, 6, 6, 2, 6, 3, 0, 8, 8, 4, 1, 4, 0, 6, 5, 9, 9, 9, 9, 9,\n",
            "        1, 2, 3, 5, 4, 2, 9, 6, 8, 9, 6, 6, 8, 0, 6, 9, 4, 6, 8, 0, 5, 4, 9, 2,\n",
            "        0, 9, 6, 4, 2, 4, 6, 5, 9, 7, 7, 4, 6, 5, 0, 1, 9, 0, 3, 1, 9, 0, 9, 7,\n",
            "        8, 6, 7, 6, 8, 2, 4, 5, 3, 0, 3, 2, 1, 7, 5, 9, 3, 4, 5, 3, 1, 5, 0, 1,\n",
            "        1, 1, 9, 7, 5, 4, 9, 7], device='cuda:0')\n",
            "Target: tensor([8, 0, 8, 2, 8, 5, 7, 7, 2, 2, 0, 0, 0, 7, 4, 1, 6, 6, 8, 8, 9, 0, 9, 0,\n",
            "        1, 3, 3, 0, 9, 6, 6, 2, 6, 3, 4, 0, 8, 4, 1, 4, 0, 6, 5, 0, 9, 9, 9, 9,\n",
            "        1, 2, 3, 5, 4, 2, 9, 6, 0, 9, 6, 6, 8, 0, 6, 1, 4, 6, 8, 0, 5, 4, 1, 2,\n",
            "        0, 9, 6, 4, 2, 4, 6, 5, 9, 7, 7, 4, 6, 5, 0, 1, 9, 0, 3, 1, 9, 0, 9, 7,\n",
            "        8, 6, 7, 6, 8, 2, 4, 5, 3, 0, 3, 2, 1, 7, 5, 9, 3, 4, 5, 7, 1, 5, 0, 1,\n",
            "        1, 1, 9, 7, 5, 4, 9, 7], device='cuda:0')\n",
            "Output: tensor([[  0.5150,  -6.6269,   1.0469,  ...,  -8.6818,  15.2851,  -4.8255],\n",
            "        [ -1.4986,   2.0082,  -4.2155,  ...,  -6.1643,  -0.1577,  -3.8454],\n",
            "        [ 13.1843,  -4.2636,  -2.7342,  ...,  -8.7202,  -1.2606,  -1.6091],\n",
            "        ...,\n",
            "        [  0.0192,   3.8682,  -4.1640,  ...,  -4.3214,   4.4247,  -3.6621],\n",
            "        [ -5.1442,   3.4242, -10.9767,  ..., -10.2679,   0.1326,  17.1314],\n",
            "        [ -2.9507,  -2.2392,  -1.8314,  ...,  -7.3261,  -2.2730,  -1.9873]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([8, 1, 0, 2, 8, 5, 6, 7, 0, 1, 4, 8, 4, 4, 6, 6, 3, 8, 1, 8, 4, 6, 5, 9,\n",
            "        2, 0, 1, 4, 9, 1, 6, 7, 2, 3, 1, 7, 6, 5, 2, 2, 5, 3, 0, 0, 0, 1, 7, 3,\n",
            "        3, 5, 8, 7, 5, 6, 5, 8, 0, 5, 9, 4, 6, 5, 1, 1, 0, 6, 3, 9, 4, 8, 1, 7,\n",
            "        7, 9, 9, 4, 5, 6, 0, 2, 8, 2, 7, 6, 7, 0, 2, 1, 2, 1, 4, 6, 9, 6, 1, 0,\n",
            "        1, 8, 7, 0, 0, 4, 7, 4, 3, 6, 9, 5, 9, 9, 7, 4, 5, 8, 1, 4, 7, 9, 9, 8,\n",
            "        8, 6, 3, 7, 0, 8, 9, 6], device='cuda:0')\n",
            "Target: tensor([8, 1, 0, 2, 8, 5, 6, 7, 0, 1, 4, 8, 4, 4, 6, 6, 5, 8, 1, 8, 4, 6, 5, 9,\n",
            "        2, 2, 1, 4, 9, 1, 6, 7, 2, 0, 1, 7, 6, 5, 2, 2, 5, 6, 0, 9, 0, 1, 5, 3,\n",
            "        3, 5, 8, 7, 5, 6, 5, 8, 0, 5, 9, 4, 6, 5, 1, 1, 0, 3, 3, 9, 4, 8, 1, 7,\n",
            "        7, 9, 9, 4, 3, 6, 3, 2, 8, 2, 7, 6, 7, 0, 2, 1, 2, 9, 4, 6, 9, 6, 1, 0,\n",
            "        1, 8, 7, 0, 0, 4, 7, 4, 2, 6, 9, 5, 9, 0, 7, 4, 5, 8, 1, 4, 7, 9, 9, 8,\n",
            "        8, 6, 3, 7, 0, 8, 9, 6], device='cuda:0')\n",
            "Output: tensor([[ -0.8023,  -7.0002,  12.5362,  ...,  -4.1346,  -4.4958,  -4.9168],\n",
            "        [ -7.5262,  -6.3755,  -1.0349,  ...,  -5.6634,  -7.4743,  -6.5870],\n",
            "        [ -9.9230,  -6.6350,  -6.0446,  ...,  -7.1393,  -9.4932,  -4.5592],\n",
            "        ...,\n",
            "        [ -9.0105,  -4.8775,   0.6663,  ...,  -3.6002,   0.2436,  -5.3628],\n",
            "        [ -6.4296,   0.4399, -10.5623,  ...,  -6.5623,  -5.1978,  17.9997],\n",
            "        [ -5.0796,  -7.3291,  -6.1571,  ...,  12.1112,  -7.7767,  -7.2282]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([2, 4, 3, 2, 9, 7, 7, 6, 8, 5, 6, 1, 6, 5, 9, 9, 9, 3, 0, 0, 3, 0, 2, 0,\n",
            "        0, 3, 9, 3, 5, 7, 6, 5, 9, 5, 6, 1, 2, 4, 0, 2, 7, 8, 4, 4, 0, 9, 5, 9,\n",
            "        4, 9, 3, 2, 6, 3, 2, 2, 8, 8, 6, 0, 1, 6, 8, 9, 8, 6, 0, 6, 9, 1, 5, 7,\n",
            "        1, 6, 0, 1, 5, 2, 8, 1, 1, 3, 8, 1, 8, 1, 8, 5, 1, 9, 0, 3, 0, 4, 0, 5,\n",
            "        1, 2, 0, 4, 9, 5, 1, 2, 2, 7, 8, 7, 2, 4, 5, 3, 7, 0, 6, 1, 5, 9, 8, 0,\n",
            "        2, 0, 7, 6, 0, 6, 9, 7], device='cuda:0')\n",
            "Target: tensor([2, 4, 6, 2, 9, 7, 4, 6, 8, 5, 6, 1, 3, 5, 9, 9, 1, 3, 2, 0, 3, 0, 2, 0,\n",
            "        7, 3, 9, 3, 5, 7, 6, 5, 9, 5, 6, 1, 2, 4, 0, 2, 7, 8, 4, 4, 0, 9, 5, 9,\n",
            "        2, 9, 3, 2, 4, 3, 2, 2, 8, 8, 6, 8, 1, 6, 8, 9, 8, 2, 1, 4, 9, 1, 5, 7,\n",
            "        1, 6, 0, 1, 5, 2, 8, 1, 1, 3, 8, 1, 3, 1, 8, 5, 1, 9, 0, 3, 0, 4, 0, 5,\n",
            "        1, 2, 0, 4, 9, 5, 1, 2, 2, 7, 8, 7, 2, 4, 5, 3, 4, 0, 6, 1, 5, 9, 8, 0,\n",
            "        2, 0, 7, 6, 0, 5, 9, 7], device='cuda:0')\n",
            "Output: tensor([[ -9.4280,  -5.8714,  -6.5161,  ...,  -7.0835,  -8.3004,  -8.3148],\n",
            "        [  4.5528,   2.4403,  -5.0978,  ...,  -7.7759,   4.8951,   4.0163],\n",
            "        [ -8.4868,  -7.8124,   0.1885,  ...,  -3.4908, -10.5581,  -7.0313],\n",
            "        ...,\n",
            "        [-11.0375,  -7.2992,  -3.3487,  ...,   5.0669, -11.4872,  -7.2268],\n",
            "        [ -7.9351,  -4.6707,   1.3824,  ...,  -6.0151, -10.6842,  -5.9560],\n",
            "        [ 12.4398,  -0.7557,   2.5746,  ...,  -6.4024,  -7.0013,  -6.3198]],\n",
            "       device='cuda:0')\n",
            "Prediction: tensor([5, 8, 4, 6, 2, 6, 7, 6, 8, 1, 1, 8, 9, 7, 2, 9, 0, 2, 0, 9, 0, 6, 5, 7,\n",
            "        7, 9, 1, 7, 9, 8, 4, 5, 0, 8, 0, 2, 5, 2, 0, 4, 4, 8, 9, 7, 8, 3, 6, 6,\n",
            "        0, 1, 1, 1, 8, 1, 7, 4, 0, 7, 8, 2, 1, 2, 5, 4, 6, 0, 5, 7, 4, 4, 3, 9,\n",
            "        5, 8, 8, 0, 8, 7, 4, 1, 8, 4, 9, 5, 3, 1, 7, 7, 7, 7, 0, 3, 8, 3, 3, 0,\n",
            "        5, 7, 0, 8, 0, 0, 9, 3, 0, 3, 4, 8, 2, 2, 6, 3, 5, 6, 2, 9, 4, 0, 1, 0,\n",
            "        5, 5, 7, 3, 0, 4, 6, 0], device='cuda:0')\n",
            "Target: tensor([5, 0, 4, 6, 0, 6, 3, 6, 8, 1, 1, 8, 9, 7, 2, 9, 0, 2, 2, 9, 0, 6, 5, 7,\n",
            "        7, 9, 1, 7, 9, 8, 4, 5, 0, 8, 0, 2, 5, 2, 1, 4, 4, 8, 9, 7, 8, 3, 6, 6,\n",
            "        0, 1, 1, 1, 8, 1, 4, 4, 0, 7, 8, 2, 1, 2, 5, 4, 6, 0, 5, 7, 4, 4, 3, 9,\n",
            "        5, 8, 8, 0, 8, 7, 4, 1, 8, 4, 9, 5, 4, 1, 7, 7, 7, 7, 0, 3, 8, 3, 3, 0,\n",
            "        5, 7, 0, 8, 0, 0, 9, 2, 2, 3, 4, 8, 2, 2, 6, 3, 3, 6, 2, 9, 4, 0, 1, 7,\n",
            "        5, 5, 7, 3, 0, 4, 2, 0], device='cuda:0')\n",
            "Output: tensor([[ -9.8707, -14.4738, -11.9305,  -5.2071,  -6.6855,   8.2389, -10.7358,\n",
            "          21.8518, -14.0494, -10.7203],\n",
            "        [ -6.3638,  -4.8231,  -6.9664,   4.8809,  -6.3635,   7.2869,  -5.8849,\n",
            "          -5.5063,  -6.4954,  -4.8728],\n",
            "        [  2.6732,  -3.0633,  -1.8020,  -7.5896,  -7.3334,  -9.4877,  -7.7109,\n",
            "          -7.0177,  13.5979,  -5.9111],\n",
            "        [ 13.6963,  -5.0966,  -4.8327, -10.7448,  -6.8215, -12.4772,  -6.4456,\n",
            "          -8.1911,   3.2022,  -2.6051],\n",
            "        [ -2.8993,  -5.5117,  -5.1953,  -5.1998,  -5.5045,  -9.5781,  -7.5095,\n",
            "          -8.6006,  17.7191,  -3.2704],\n",
            "        [ -4.5115,  -6.4110,  11.4592,  -4.3963,   2.5040,  -4.8805,  -4.6536,\n",
            "          -2.4364,  -8.1059,  -6.6946],\n",
            "        [ -8.6097,  -9.6891,  -6.9697,  -0.6743,  -4.3647,   0.4693, -11.1171,\n",
            "          13.3742,  -7.1588,  -5.1093],\n",
            "        [ 10.2435,  -0.1125,  -4.6971,  -6.3260,  -6.9121,  -9.9404, -10.6590,\n",
            "          -4.0293,  -2.1337,  -0.7035],\n",
            "        [ -5.1720,  -3.4052,  -3.7832,   8.1237,  -5.2473,  -2.1161,  -3.7173,\n",
            "          -7.6448,  -3.3211,  -8.3133],\n",
            "        [ -4.6612,  -5.9200,  -9.8927,   6.8399,  -7.2177,   9.1518,  -9.2259,\n",
            "          -6.1606,  -9.2618,  -3.5603],\n",
            "        [ -5.4319,  -4.6517,  -0.2143,   9.2118,   1.0381,  -4.0903,  -2.4740,\n",
            "         -12.3019,  -8.2552,  -8.1037],\n",
            "        [  7.6052,  -3.7775,  -4.4369,  -5.8845,  -8.0355, -10.1612,  -7.2768,\n",
            "          -7.4121,  10.0462,  -7.9953],\n",
            "        [-12.8783,  -8.3233,   0.4642,   9.6906,   1.7276,   1.3477,   1.7236,\n",
            "          -8.6893, -14.3158, -10.6047],\n",
            "        [ -7.1666,  -6.9707,  -3.1150,  -0.2826,  -7.0876,  10.8503,  -4.8530,\n",
            "          -4.2535,  -6.0503,  -5.6465],\n",
            "        [ -1.0011,   7.5180,   1.6067,  -5.7907,  -8.5916,  -8.8170,  -5.4665,\n",
            "          -6.4738, -10.2230,  -4.8039],\n",
            "        [ -7.1196,  -5.4532,  -6.7255,  -4.8769,   0.7747,   1.6808, -10.0460,\n",
            "          12.5957,  -9.5107,  -5.2416]], device='cuda:0')\n",
            "Prediction: tensor([7, 5, 8, 0, 8, 2, 7, 0, 3, 5, 3, 8, 3, 5, 1, 7], device='cuda:0')\n",
            "Target: tensor([7, 5, 8, 0, 8, 2, 7, 0, 3, 5, 3, 8, 3, 5, 1, 7], device='cuda:0')\n",
            "Test Loss: 0.3949, Test Accuracy: 0.9023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More Modification on ResNet Structure and Evaluate Model"
      ],
      "metadata": {
        "id": "OTDq9s6MiS9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "MdasjPdxgin5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#parameters\n",
        "batch_size = 128\n",
        "num_classes = 10  #CIFAR-10 has 10 classes\n",
        "epochs = 5\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "3GeERLQEifN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Transformation Pipeline for CIFAR-10\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),     #Resnet expect 224*224 Image\n",
        "    transforms.ToTensor(),      #Convert PIL Image to input tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])      #Normalization for pre-trained models\n",
        "])"
      ],
      "metadata": {
        "id": "xH-xor0biijn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading CIFAR-10 dataset\n",
        "train_dataset = datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "test_dataset = datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "#DataLoader for Batch Processing\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pECrfGuDilbo",
        "outputId": "209cc108-418a-4aab-f9d2-ba861ac263b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load a pretrained ResNet18 model\n",
        "model = models.resnet18(pretrained=True)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fh4MFTZOiqFP",
        "outputId": "40db305a-3867-4e66-ed38-9e8d0cca7dad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCNuEGUzitSq",
        "outputId": "66c9ebaf-28f6-4d46-f636-81b8f7910b84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object Module.parameters at 0x7e1e2e44b140>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "  print(param)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnfODHf0ixof",
        "outputId": "6bbdf605-a47c-4eba-ad71-979318566032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[[[-1.0419e-02, -6.1356e-03, -1.8098e-03,  ...,  5.6615e-02,\n",
            "            1.7083e-02, -1.2694e-02],\n",
            "          [ 1.1083e-02,  9.5276e-03, -1.0993e-01,  ..., -2.7124e-01,\n",
            "           -1.2907e-01,  3.7424e-03],\n",
            "          [-6.9434e-03,  5.9089e-02,  2.9548e-01,  ...,  5.1972e-01,\n",
            "            2.5632e-01,  6.3573e-02],\n",
            "          ...,\n",
            "          [-2.7535e-02,  1.6045e-02,  7.2595e-02,  ..., -3.3285e-01,\n",
            "           -4.2058e-01, -2.5781e-01],\n",
            "          [ 3.0613e-02,  4.0960e-02,  6.2850e-02,  ...,  4.1384e-01,\n",
            "            3.9359e-01,  1.6606e-01],\n",
            "          [-1.3736e-02, -3.6746e-03, -2.4084e-02,  ..., -1.5070e-01,\n",
            "           -8.2230e-02, -5.7828e-03]],\n",
            "\n",
            "         [[-1.1397e-02, -2.6619e-02, -3.4641e-02,  ...,  3.2521e-02,\n",
            "            6.6221e-04, -2.5743e-02],\n",
            "          [ 4.5687e-02,  3.3603e-02, -1.0453e-01,  ..., -3.1253e-01,\n",
            "           -1.6051e-01, -1.2826e-03],\n",
            "          [-8.3730e-04,  9.8420e-02,  4.0210e-01,  ...,  7.0789e-01,\n",
            "            3.6887e-01,  1.2455e-01],\n",
            "          ...,\n",
            "          [-5.5926e-02, -5.2239e-03,  2.7081e-02,  ..., -4.6178e-01,\n",
            "           -5.7080e-01, -3.6552e-01],\n",
            "          [ 3.2860e-02,  5.5574e-02,  9.9670e-02,  ...,  5.4636e-01,\n",
            "            4.8276e-01,  1.9867e-01],\n",
            "          [ 5.3051e-03,  6.6938e-03, -1.7254e-02,  ..., -1.4822e-01,\n",
            "           -7.7248e-02,  7.2183e-04]],\n",
            "\n",
            "         [[-2.0315e-03, -9.1617e-03,  2.1209e-02,  ...,  8.9177e-02,\n",
            "            3.3655e-02, -2.0102e-02],\n",
            "          [ 1.5398e-02, -1.8648e-02, -1.2591e-01,  ..., -2.5342e-01,\n",
            "           -1.2980e-01, -2.7975e-02],\n",
            "          [ 9.8454e-03,  4.9047e-02,  2.1699e-01,  ...,  3.4872e-01,\n",
            "            1.0433e-01,  1.8413e-02],\n",
            "          ...,\n",
            "          [-2.8356e-02,  1.8404e-02,  9.8647e-02,  ..., -1.1740e-01,\n",
            "           -2.5760e-01, -1.5451e-01],\n",
            "          [ 2.0766e-02, -2.6286e-03, -3.7825e-02,  ...,  2.4141e-01,\n",
            "            2.4345e-01,  1.1796e-01],\n",
            "          [ 7.4684e-04,  7.7677e-04, -1.0050e-02,  ..., -1.4865e-01,\n",
            "           -1.1754e-01, -3.8350e-02]]],\n",
            "\n",
            "\n",
            "        [[[-4.4154e-03, -4.0645e-03,  3.1589e-03,  ..., -3.7026e-02,\n",
            "           -2.5158e-02, -4.7945e-02],\n",
            "          [ 5.1310e-02,  5.3402e-02,  8.0436e-02,  ...,  1.4480e-01,\n",
            "            1.4287e-01,  1.2312e-01],\n",
            "          [-7.3337e-03,  2.1755e-03,  3.7580e-02,  ...,  6.1517e-02,\n",
            "            8.0324e-02,  1.1715e-01],\n",
            "          ...,\n",
            "          [-2.6754e-02, -1.2297e-01, -1.3653e-01,  ..., -1.4068e-01,\n",
            "           -1.1155e-01, -4.9556e-02],\n",
            "          [ 2.3524e-02, -1.7288e-02, -1.1122e-02,  ..., -1.8826e-02,\n",
            "           -2.3320e-02, -2.9474e-02],\n",
            "          [ 2.8689e-02,  2.1659e-02,  4.7888e-02,  ...,  2.5498e-02,\n",
            "            3.5346e-02,  1.1280e-02]],\n",
            "\n",
            "         [[ 4.6919e-04,  1.2153e-02,  4.2035e-02,  ...,  4.6403e-02,\n",
            "            4.0423e-02, -1.4439e-02],\n",
            "          [ 4.3463e-02,  6.8779e-02,  1.3268e-01,  ...,  2.8606e-01,\n",
            "            2.6905e-01,  2.0935e-01],\n",
            "          [-5.7621e-02, -2.2642e-02,  3.0547e-02,  ...,  1.3763e-01,\n",
            "            1.6538e-01,  1.7946e-01],\n",
            "          ...,\n",
            "          [-1.0816e-01, -2.5227e-01, -2.9742e-01,  ..., -2.8503e-01,\n",
            "           -2.1493e-01, -1.0320e-01],\n",
            "          [ 4.0709e-02, -3.2771e-02, -6.3450e-02,  ..., -9.2360e-02,\n",
            "           -6.9876e-02, -4.9841e-02],\n",
            "          [ 8.2942e-02,  8.7580e-02,  1.0111e-01,  ...,  5.2714e-02,\n",
            "            6.0968e-02,  4.1198e-02]],\n",
            "\n",
            "         [[-1.6391e-02, -1.3870e-02,  5.2810e-03,  ...,  4.3698e-02,\n",
            "            2.2707e-02, -4.5983e-02],\n",
            "          [ 3.3202e-02,  4.2014e-02,  9.3500e-02,  ...,  2.6162e-01,\n",
            "            2.2970e-01,  1.6694e-01],\n",
            "          [-4.5987e-02, -1.6365e-02,  2.6811e-02,  ...,  1.4951e-01,\n",
            "            1.3216e-01,  1.3579e-01],\n",
            "          ...,\n",
            "          [-7.2129e-02, -1.8902e-01, -2.3389e-01,  ..., -1.9038e-01,\n",
            "           -1.5609e-01, -7.5974e-02],\n",
            "          [ 5.1161e-02, -2.5815e-02, -6.9357e-02,  ..., -5.8999e-02,\n",
            "           -6.1550e-02, -4.4555e-02],\n",
            "          [ 1.1174e-01,  7.8979e-02,  6.5849e-02,  ...,  3.1617e-02,\n",
            "            2.5221e-02,  7.4257e-03]]],\n",
            "\n",
            "\n",
            "        [[[-7.0826e-08, -6.4306e-08, -7.3806e-08,  ..., -9.8000e-08,\n",
            "           -1.0905e-07, -8.3421e-08],\n",
            "          [-6.1125e-09,  2.0613e-09, -8.0922e-09,  ..., -4.9840e-08,\n",
            "           -4.3836e-08, -3.0538e-09],\n",
            "          [ 7.1953e-08,  7.5616e-08,  5.9282e-08,  ..., -9.7509e-09,\n",
            "           -1.0951e-09,  4.2442e-08],\n",
            "          ...,\n",
            "          [ 9.5889e-08,  1.0039e-07,  7.9817e-08,  ..., -1.7491e-08,\n",
            "           -4.7666e-08, -1.3265e-08],\n",
            "          [ 1.2904e-07,  1.4762e-07,  1.7477e-07,  ...,  1.3233e-07,\n",
            "            1.0628e-07,  9.3316e-08],\n",
            "          [ 1.2558e-07,  1.3644e-07,  1.8431e-07,  ...,  2.1399e-07,\n",
            "            1.7710e-07,  1.7166e-07]],\n",
            "\n",
            "         [[-1.2690e-07, -9.6139e-08, -1.0372e-07,  ..., -1.1808e-07,\n",
            "           -1.3309e-07, -1.0820e-07],\n",
            "          [-5.7412e-08, -2.5055e-08, -3.0115e-08,  ..., -7.2922e-08,\n",
            "           -6.7022e-08, -2.2574e-08],\n",
            "          [ 2.1813e-08,  4.8608e-08,  3.1222e-08,  ..., -1.8694e-08,\n",
            "           -7.9591e-09,  3.9750e-08],\n",
            "          ...,\n",
            "          [ 5.6013e-08,  7.5526e-08,  4.4496e-08,  ..., -4.4128e-08,\n",
            "           -5.9930e-08, -1.8247e-08],\n",
            "          [ 7.7614e-08,  9.8348e-08,  1.0455e-07,  ...,  6.3272e-08,\n",
            "            4.1781e-08,  4.5901e-08],\n",
            "          [ 5.9834e-08,  7.1006e-08,  9.0437e-08,  ...,  1.1654e-07,\n",
            "            8.7550e-08,  9.8837e-08]],\n",
            "\n",
            "         [[-4.3810e-08,  1.3270e-08,  7.8275e-09,  ..., -5.8804e-09,\n",
            "           -2.6217e-08, -1.5649e-08],\n",
            "          [ 4.1700e-08,  1.0778e-07,  1.0946e-07,  ...,  7.6403e-08,\n",
            "            7.1450e-08,  9.7615e-08],\n",
            "          [ 1.0436e-07,  1.6586e-07,  1.5933e-07,  ...,  1.3517e-07,\n",
            "            1.3487e-07,  1.6449e-07],\n",
            "          ...,\n",
            "          [ 9.8763e-08,  1.5072e-07,  1.2547e-07,  ...,  6.8316e-08,\n",
            "            6.8382e-08,  1.1367e-07],\n",
            "          [ 9.1435e-08,  1.3576e-07,  1.3793e-07,  ...,  1.1678e-07,\n",
            "            1.1723e-07,  1.4394e-07],\n",
            "          [ 6.2183e-08,  8.8184e-08,  1.0456e-07,  ...,  1.3941e-07,\n",
            "            1.3333e-07,  1.5844e-07]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-6.1896e-02, -3.0206e-02,  1.9225e-02,  ...,  4.3665e-02,\n",
            "           -2.2114e-02, -4.2214e-02],\n",
            "          [-3.8061e-02,  6.0774e-03,  4.5797e-02,  ...,  9.6029e-02,\n",
            "            5.9254e-02,  2.9958e-02],\n",
            "          [-2.9672e-02,  2.7766e-03,  2.0457e-02,  ...,  5.9828e-02,\n",
            "            4.1422e-02,  2.3134e-02],\n",
            "          ...,\n",
            "          [ 1.1916e-02,  4.5701e-02,  4.4892e-02,  ...,  4.7419e-02,\n",
            "            2.2274e-02, -5.4993e-03],\n",
            "          [-3.2468e-02, -1.2210e-02,  2.2023e-02,  ...,  5.8061e-02,\n",
            "           -7.5033e-03, -5.9736e-02],\n",
            "          [-4.3314e-02, -2.8162e-02, -5.9126e-03,  ...,  8.8460e-02,\n",
            "            8.4406e-03, -5.0019e-02]],\n",
            "\n",
            "         [[-6.1292e-02, -1.4004e-02,  1.7229e-02,  ...,  1.8349e-02,\n",
            "           -3.2708e-02, -4.1060e-02],\n",
            "          [-3.1506e-02,  2.4460e-02,  4.5516e-02,  ...,  6.6806e-02,\n",
            "            4.6687e-02,  3.3248e-02],\n",
            "          [-3.2216e-02,  2.0718e-02,  2.3343e-02,  ...,  3.5265e-02,\n",
            "            3.6478e-02,  3.1291e-02],\n",
            "          ...,\n",
            "          [ 1.7739e-02,  6.1040e-02,  4.8247e-02,  ...,  3.7785e-02,\n",
            "            2.8894e-02,  1.3984e-02],\n",
            "          [-1.0890e-02,  2.2079e-02,  4.2737e-02,  ...,  6.0247e-02,\n",
            "            1.6197e-02, -1.2493e-02],\n",
            "          [-2.2284e-02,  1.3220e-02,  3.0897e-02,  ...,  1.0403e-01,\n",
            "            4.0119e-02, -5.3310e-03]],\n",
            "\n",
            "         [[-8.5322e-02, -4.2603e-02,  6.8145e-03,  ...,  3.0751e-02,\n",
            "           -3.4818e-02, -4.9945e-02],\n",
            "          [-2.9215e-02,  1.8165e-02,  5.1092e-02,  ...,  9.0200e-02,\n",
            "            5.3438e-02,  4.0169e-02],\n",
            "          [-3.9932e-02, -1.1100e-03,  9.6176e-03,  ...,  2.4114e-02,\n",
            "            2.6298e-02,  2.5489e-02],\n",
            "          ...,\n",
            "          [-3.1890e-03,  3.0454e-02,  1.6316e-02,  ...,  5.5054e-03,\n",
            "           -6.2689e-03, -8.4638e-03],\n",
            "          [-2.2995e-02, -2.8211e-03,  2.3203e-02,  ...,  3.5888e-02,\n",
            "           -1.4296e-02, -3.2419e-02],\n",
            "          [-9.8894e-03,  7.0542e-03,  1.0659e-02,  ...,  7.0495e-02,\n",
            "            1.2996e-02, -8.3417e-03]]],\n",
            "\n",
            "\n",
            "        [[[-7.8699e-03,  1.9911e-02,  3.4208e-02,  ...,  2.8694e-02,\n",
            "            1.2820e-02,  1.8142e-02],\n",
            "          [ 8.7942e-03, -3.2875e-02, -3.5713e-02,  ...,  7.2533e-02,\n",
            "            4.5889e-02,  5.2383e-02],\n",
            "          [-3.6122e-02, -1.1878e-01, -1.3767e-01,  ...,  3.3811e-02,\n",
            "            3.7806e-02,  2.6944e-02],\n",
            "          ...,\n",
            "          [ 1.7322e-02,  3.9589e-03, -8.2269e-03,  ...,  2.7543e-03,\n",
            "            1.8313e-02,  1.6057e-02],\n",
            "          [-9.5007e-04,  1.6428e-02,  1.7156e-02,  ...,  3.3672e-03,\n",
            "            2.2857e-02,  6.5783e-04],\n",
            "          [ 6.1727e-03,  2.7145e-02,  1.4340e-02,  ...,  7.5867e-03,\n",
            "            1.8770e-02,  1.5624e-02]],\n",
            "\n",
            "         [[-1.3423e-02, -5.0696e-04,  8.0959e-03,  ..., -6.0963e-03,\n",
            "            9.2341e-03,  1.5751e-02],\n",
            "          [-1.8343e-02, -6.7982e-02, -7.0685e-02,  ...,  2.9855e-02,\n",
            "            2.6264e-02,  2.3773e-02],\n",
            "          [-5.4359e-02, -1.4663e-01, -1.6211e-01,  ...,  1.1781e-02,\n",
            "            3.2477e-02,  1.1980e-02],\n",
            "          ...,\n",
            "          [ 8.3686e-04, -1.7564e-02, -1.9535e-02,  ..., -4.1382e-03,\n",
            "            2.4658e-02,  1.2893e-02],\n",
            "          [-6.3183e-04,  1.1788e-02,  2.4810e-02,  ...,  6.1105e-03,\n",
            "            3.9210e-02,  9.6696e-03],\n",
            "          [-7.1831e-03,  6.6918e-03,  5.2723e-03,  ..., -7.6077e-03,\n",
            "            2.7253e-02,  1.7735e-02]],\n",
            "\n",
            "         [[-2.3753e-04, -4.9343e-03,  2.2991e-03,  ..., -4.7958e-02,\n",
            "           -2.6154e-02, -2.3525e-02],\n",
            "          [-3.3053e-04, -5.1502e-02, -5.9977e-02,  ..., -1.7369e-02,\n",
            "           -2.3337e-02, -3.7312e-02],\n",
            "          [-2.2674e-02, -9.9412e-02, -1.1176e-01,  ..., -1.1725e-02,\n",
            "           -8.3744e-03, -4.0615e-02],\n",
            "          ...,\n",
            "          [ 1.1437e-02, -8.0313e-03, -1.4955e-03,  ..., -3.4133e-02,\n",
            "           -8.7267e-03, -2.3526e-02],\n",
            "          [ 2.9522e-03,  6.7770e-04,  1.9933e-02,  ..., -2.2002e-02,\n",
            "            1.4814e-02, -1.4487e-02],\n",
            "          [-1.9085e-02, -2.9430e-02, -2.3284e-02,  ..., -4.8587e-02,\n",
            "           -1.3049e-02, -2.4368e-02]]],\n",
            "\n",
            "\n",
            "        [[[-3.6296e-02,  7.1996e-03,  1.9100e-02,  ...,  1.9602e-02,\n",
            "            1.4870e-02, -1.7298e-02],\n",
            "          [-1.1061e-02,  8.5665e-02,  1.2667e-01,  ...,  1.3744e-02,\n",
            "           -5.5036e-05, -3.0162e-02],\n",
            "          [ 1.1322e-01,  1.8634e-01,  5.0658e-02,  ..., -1.7333e-01,\n",
            "           -7.2041e-02, -6.2474e-02],\n",
            "          ...,\n",
            "          [-5.3062e-02, -2.5781e-01, -2.6747e-01,  ...,  2.6781e-01,\n",
            "            1.4344e-01,  5.5145e-02],\n",
            "          [-2.1009e-02, -2.9969e-02,  1.0245e-01,  ...,  2.0843e-01,\n",
            "           -4.1518e-03, -3.8118e-02],\n",
            "          [-2.2155e-02,  1.2380e-02,  8.4302e-02,  ..., -4.4992e-02,\n",
            "           -1.4687e-01, -9.0890e-02]],\n",
            "\n",
            "         [[-5.3969e-03,  3.2799e-02,  1.5486e-02,  ..., -7.7451e-03,\n",
            "            3.0229e-03,  1.1216e-03],\n",
            "          [ 6.1723e-02,  1.4899e-01,  1.4645e-01,  ..., -2.8897e-02,\n",
            "           -2.0227e-02, -9.1878e-03],\n",
            "          [ 1.6146e-01,  2.0886e-01, -2.5589e-02,  ..., -2.7278e-01,\n",
            "           -1.0735e-01, -6.2971e-02],\n",
            "          ...,\n",
            "          [-1.3723e-01, -4.0863e-01, -3.8551e-01,  ...,  4.0846e-01,\n",
            "            2.6202e-01,  1.3491e-01],\n",
            "          [-5.9388e-02, -6.1187e-02,  1.4197e-01,  ...,  3.5780e-01,\n",
            "            9.0893e-02, -1.7392e-03],\n",
            "          [ 7.8613e-03,  5.8403e-02,  1.5339e-01,  ...,  4.7045e-02,\n",
            "           -1.0095e-01, -9.7920e-02]],\n",
            "\n",
            "         [[-5.6799e-03,  1.3425e-02, -2.6461e-02,  ...,  4.4881e-03,\n",
            "            2.0666e-03,  1.3902e-02],\n",
            "          [ 6.5943e-03,  4.5181e-02,  6.0260e-02,  ...,  1.4368e-02,\n",
            "           -5.0725e-03,  4.0505e-03],\n",
            "          [ 5.5257e-02,  1.2397e-01,  4.3193e-02,  ..., -1.4486e-01,\n",
            "           -7.4489e-02, -5.7533e-02],\n",
            "          ...,\n",
            "          [-3.1513e-02, -1.6334e-01, -1.5795e-01,  ...,  2.2904e-01,\n",
            "            1.2017e-01,  7.1998e-02],\n",
            "          [-1.0456e-02, -1.1248e-03,  8.4582e-02,  ...,  1.5748e-01,\n",
            "            2.2142e-02, -1.0083e-02],\n",
            "          [-4.8639e-03, -5.0065e-03,  3.6341e-02,  ..., -2.4361e-02,\n",
            "           -7.1195e-02, -6.6788e-02]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 2.3487e-01,  2.6626e-01, -5.1096e-08,  5.1870e-01,  3.4404e-09,\n",
            "         2.2239e-01,  4.2289e-01,  1.3153e-07,  2.5093e-01,  1.5152e-06,\n",
            "         3.1687e-01,  2.5049e-01,  3.7893e-01,  1.0862e-05,  2.7526e-01,\n",
            "         2.3674e-01,  2.4202e-01,  3.9531e-01,  4.6935e-01,  2.9090e-01,\n",
            "         2.7268e-01,  2.7803e-01,  2.9069e-01,  2.0693e-01,  2.5899e-01,\n",
            "         2.7871e-01,  2.9115e-01,  3.1601e-01,  3.8889e-01,  3.0411e-01,\n",
            "         2.6776e-01,  2.1093e-01,  2.8708e-01,  3.3243e-01,  4.2673e-01,\n",
            "         3.7326e-01,  7.4804e-08,  1.9068e-01,  1.4740e-08,  2.2303e-01,\n",
            "         1.7908e-01,  2.4860e-01,  2.7400e-01,  2.5923e-01,  2.9420e-01,\n",
            "         2.9924e-01,  2.2369e-01,  2.6280e-01,  2.2001e-08,  2.6610e-01,\n",
            "         2.2089e-01,  2.8429e-01,  3.3072e-01,  2.2681e-01,  3.6538e-01,\n",
            "         2.1230e-01,  2.3965e-01,  2.4950e-01,  5.2583e-01,  2.4825e-01,\n",
            "         2.9565e-01,  2.5878e-01,  4.8326e-01,  2.6670e-01],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 2.3072e-01,  2.5382e-01, -1.0543e-06, -6.6439e-01, -1.6571e-08,\n",
            "         1.6152e-01,  4.5450e-01, -4.3020e-07,  3.0051e-01, -8.0052e-06,\n",
            "         3.4942e-01,  3.1148e-01, -2.4953e-01, -3.4749e-05,  1.0773e-01,\n",
            "         2.1897e-01,  3.8141e-01, -5.2988e-01, -6.2864e-01,  5.7140e-01,\n",
            "         2.9985e-01,  5.8430e-01,  4.8202e-01,  3.2853e-01,  1.9672e-01,\n",
            "         1.9496e-01,  1.5215e-01,  8.5522e-02,  5.1314e-01,  1.5237e-02,\n",
            "         1.6644e-01,  3.3239e-01,  2.4921e-01,  4.4337e-01, -2.8017e-01,\n",
            "        -2.0385e-02, -2.4507e-07,  3.2134e-01, -4.9152e-08,  2.3777e-01,\n",
            "         2.3291e-01,  3.1527e-01,  4.2776e-01,  2.9313e-01,  2.6379e-01,\n",
            "         6.7598e-01,  4.2910e-01,  3.4566e-01, -8.6909e-08,  2.4729e-01,\n",
            "         3.0316e-01,  6.1577e-01,  3.9835e-01,  3.3207e-01, -4.1219e-01,\n",
            "         3.7807e-01,  1.7895e-01,  2.5748e-01, -4.4908e-01,  2.1306e-01,\n",
            "         5.6934e-01,  5.7274e-01, -4.0238e-01,  2.3406e-01],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 5.7593e-02, -9.5114e-02, -2.0272e-02],\n",
            "          [-7.4556e-02, -7.9931e-01, -2.1284e-01],\n",
            "          [ 6.5571e-02, -9.6534e-02, -1.2111e-02]],\n",
            "\n",
            "         [[-6.9944e-03,  1.4266e-02,  5.5824e-04],\n",
            "          [ 4.1238e-02, -1.6125e-01, -2.3208e-02],\n",
            "          [ 3.2887e-03,  7.1779e-03,  7.1686e-02]],\n",
            "\n",
            "         [[-2.3627e-09, -3.9270e-08, -3.2971e-08],\n",
            "          [ 2.1737e-08,  8.3299e-09,  1.2543e-08],\n",
            "          [ 1.1382e-08,  8.8096e-09,  1.5506e-08]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.6921e-02,  1.8294e-02, -2.9358e-02],\n",
            "          [-9.8615e-02, -4.3645e-02, -5.2717e-02],\n",
            "          [-7.9635e-02,  2.9396e-02,  4.1479e-03]],\n",
            "\n",
            "         [[ 1.6948e-02,  1.3978e-02,  9.6727e-03],\n",
            "          [ 1.4297e-02, -6.6985e-04, -2.2077e-02],\n",
            "          [ 1.2398e-02,  3.5454e-02, -2.2320e-02]],\n",
            "\n",
            "         [[-2.2600e-02, -2.5331e-02, -2.3548e-02],\n",
            "          [ 6.0860e-02, -9.6779e-02,  2.4057e-02],\n",
            "          [-1.2750e-02,  9.2237e-02,  4.0152e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 2.2160e-02,  4.2177e-02, -1.6428e-02],\n",
            "          [-2.9667e-02,  5.6865e-02,  2.5486e-02],\n",
            "          [ 4.3847e-03,  5.1188e-02,  1.0436e-02]],\n",
            "\n",
            "         [[ 2.5342e-02,  5.4374e-02,  5.3888e-02],\n",
            "          [-2.8334e-02, -2.0139e-01, -5.6358e-02],\n",
            "          [ 5.6774e-02,  7.4188e-02,  2.1585e-02]],\n",
            "\n",
            "         [[-3.1458e-08,  3.5335e-08,  5.3791e-08],\n",
            "          [-2.6896e-08,  5.1530e-08,  5.4480e-08],\n",
            "          [-3.8487e-08, -1.1234e-08, -7.5787e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.2754e-01,  4.3552e-02, -6.5607e-02],\n",
            "          [-6.0462e-02,  1.5989e-01, -7.7070e-03],\n",
            "          [-9.4202e-02,  5.0750e-02, -7.8154e-02]],\n",
            "\n",
            "         [[-3.3309e-02,  1.6631e-03, -8.8497e-03],\n",
            "          [ 1.5553e-02, -5.8277e-02, -2.7437e-02],\n",
            "          [ 1.3126e-02, -3.0268e-02, -2.1661e-03]],\n",
            "\n",
            "         [[-4.2313e-03,  3.4517e-02,  3.8193e-03],\n",
            "          [ 5.4317e-02, -1.2457e-02,  3.2900e-02],\n",
            "          [ 2.2000e-04,  1.6040e-02,  1.2764e-01]]],\n",
            "\n",
            "\n",
            "        [[[-3.5247e-02,  8.0748e-03,  2.0353e-02],\n",
            "          [ 1.7344e-02, -2.4320e-02, -1.5511e-04],\n",
            "          [-2.7634e-04,  2.8024e-02, -2.3777e-03]],\n",
            "\n",
            "         [[-2.3741e-02, -3.2057e-03, -5.7059e-03],\n",
            "          [-1.1582e-02,  1.7200e-03,  2.1067e-02],\n",
            "          [ 4.3606e-03, -4.6459e-02, -7.2954e-02]],\n",
            "\n",
            "         [[ 3.1002e-08,  5.3568e-08,  3.1873e-08],\n",
            "          [-1.6063e-08, -1.8072e-08, -1.9508e-09],\n",
            "          [-5.8339e-08, -4.5366e-08, -1.2395e-08]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.9689e-03, -2.6809e-02, -4.3760e-02],\n",
            "          [ 2.4518e-02, -2.8396e-02, -3.5896e-02],\n",
            "          [-1.7883e-04, -2.4661e-02, -2.0085e-02]],\n",
            "\n",
            "         [[ 2.1551e-02,  2.2789e-03, -2.5823e-02],\n",
            "          [ 2.3272e-02, -7.9333e-03, -2.0814e-03],\n",
            "          [-5.7062e-03, -2.6934e-02, -1.4421e-02]],\n",
            "\n",
            "         [[-1.9674e-02,  2.7914e-02, -2.0025e-02],\n",
            "          [ 6.3222e-02, -3.9077e-02, -3.3220e-03],\n",
            "          [-2.7434e-02,  1.1390e-02, -3.1608e-03]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 4.3440e-03, -7.6970e-03, -6.4950e-02],\n",
            "          [ 1.3846e-02, -2.2803e-02, -4.6478e-02],\n",
            "          [ 2.7776e-02,  1.6080e-02, -1.3363e-02]],\n",
            "\n",
            "         [[ 4.7379e-02, -2.4982e-02, -2.7605e-02],\n",
            "          [ 7.0091e-02,  4.2084e-03, -1.0805e-01],\n",
            "          [ 1.7526e-02,  4.5647e-02,  7.8810e-03]],\n",
            "\n",
            "         [[ 2.6680e-09,  2.7671e-08,  2.4702e-08],\n",
            "          [ 6.3905e-09,  4.1020e-08,  3.3631e-08],\n",
            "          [ 5.8335e-09,  1.3334e-08,  9.6604e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.5900e-03,  4.7084e-02, -8.6949e-03],\n",
            "          [-6.3011e-03,  5.9585e-02,  5.8667e-03],\n",
            "          [-2.0255e-02,  4.3285e-02,  4.5094e-03]],\n",
            "\n",
            "         [[ 1.1253e-03, -5.7461e-03, -6.8411e-03],\n",
            "          [ 6.0616e-03,  7.3295e-03, -1.1784e-02],\n",
            "          [-1.1455e-03,  5.1868e-03, -1.9867e-02]],\n",
            "\n",
            "         [[ 1.7529e-02,  4.4606e-02, -2.6595e-02],\n",
            "          [ 2.2102e-02,  4.5857e-02,  2.3347e-02],\n",
            "          [ 1.8052e-02,  5.9689e-02,  1.7129e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.9112e-02,  3.4242e-03, -1.7523e-02],\n",
            "          [-2.3682e-02,  2.2716e-02, -3.8301e-02],\n",
            "          [-1.0308e-02, -4.3802e-03, -2.3582e-02]],\n",
            "\n",
            "         [[-4.9607e-02, -3.2724e-03, -1.5345e-02],\n",
            "          [-1.3524e-02,  5.4842e-02,  1.1187e-02],\n",
            "          [-2.3549e-02, -2.8495e-02, -6.6371e-02]],\n",
            "\n",
            "         [[-4.9804e-08, -2.8211e-08, -2.0583e-08],\n",
            "          [-5.2389e-08, -2.8522e-08, -3.5099e-08],\n",
            "          [-3.2171e-08, -3.4110e-08, -4.3153e-08]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.4487e-03,  2.6532e-02, -1.1202e-02],\n",
            "          [ 7.0925e-03,  3.7903e-02, -3.2481e-02],\n",
            "          [ 4.1381e-02,  3.2329e-02,  2.8309e-03]],\n",
            "\n",
            "         [[-6.5955e-03,  1.6476e-02,  2.1810e-02],\n",
            "          [-1.2293e-02,  2.2310e-02,  1.2645e-02],\n",
            "          [-8.9897e-03,  1.1948e-03, -5.2390e-03]],\n",
            "\n",
            "         [[-2.5295e-03,  7.2689e-02, -7.8046e-03],\n",
            "          [-4.2221e-02,  7.9756e-02, -2.7738e-02],\n",
            "          [ 4.6716e-03, -5.6596e-02, -8.2261e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 5.2235e-02,  3.5231e-03, -3.3131e-02],\n",
            "          [ 3.1048e-02,  1.6193e-02,  1.7283e-02],\n",
            "          [ 1.4446e-02,  2.4302e-02, -1.9689e-03]],\n",
            "\n",
            "         [[-2.4717e-02,  8.3009e-03, -6.1336e-02],\n",
            "          [-1.6134e-02,  5.5323e-02, -6.5029e-02],\n",
            "          [-2.4715e-02,  1.0030e-03,  3.2437e-02]],\n",
            "\n",
            "         [[ 1.8496e-08,  5.2798e-09,  4.1820e-08],\n",
            "          [ 3.7489e-08,  2.5450e-08,  3.0419e-08],\n",
            "          [ 1.1246e-08, -5.6956e-09, -2.0008e-08]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.1194e-03, -4.1052e-02, -1.0002e-02],\n",
            "          [ 2.5924e-02, -6.3819e-02,  1.3366e-02],\n",
            "          [ 2.9751e-02, -7.9476e-03,  1.4007e-02]],\n",
            "\n",
            "         [[-2.5166e-03,  2.2051e-02, -1.9967e-02],\n",
            "          [-5.9436e-02,  4.3872e-02,  2.6832e-02],\n",
            "          [-1.7509e-02,  2.4625e-02,  2.4822e-02]],\n",
            "\n",
            "         [[ 3.5832e-02, -7.0357e-02,  3.9452e-03],\n",
            "          [-2.9835e-02,  9.2727e-02,  1.9336e-02],\n",
            "          [-2.9145e-02, -9.7087e-03, -7.3388e-02]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.3090, 0.2147, 0.2366, 0.4259, 0.5137, 0.2181, 0.2204, 0.2300, 0.2640,\n",
            "        0.2695, 0.2138, 0.4602, 0.2661, 0.2319, 0.3900, 0.2389, 0.2660, 0.3634,\n",
            "        0.3474, 0.2477, 0.3285, 0.5349, 0.6440, 0.2275, 0.4482, 0.3078, 0.2604,\n",
            "        0.4651, 0.2179, 0.2858, 0.3426, 0.4420, 0.4450, 0.4500, 0.5516, 0.5092,\n",
            "        0.2564, 0.2634, 0.5664, 0.6410, 0.2228, 0.1986, 0.2460, 0.2242, 0.2143,\n",
            "        0.1982, 0.6368, 0.3106, 0.5049, 0.2403, 0.3065, 0.3760, 0.3794, 0.4281,\n",
            "        0.2991, 0.3326, 0.2596, 0.3345, 0.2006, 0.4351, 0.1683, 0.5149, 0.2629,\n",
            "        0.3254], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.1657,  0.2420,  0.1780, -0.0431, -0.2053,  0.1598,  0.2929,  0.0912,\n",
            "         0.1116,  0.0884,  0.1104, -0.2035,  0.1539,  0.0857, -0.1094,  0.0654,\n",
            "         0.0766, -0.2067, -0.0212,  0.1396,  0.0401, -0.2827, -0.3257, -0.0035,\n",
            "        -0.4373, -0.1248,  0.1282, -0.0874,  0.1199, -0.0829, -0.5315, -0.0780,\n",
            "        -0.3876, -0.0547, -0.1816, -0.1888,  0.1320,  0.0031, -0.2697, -0.2984,\n",
            "         0.1394,  0.2597,  0.1372,  0.0053,  0.0132,  0.3295, -0.2715, -0.0187,\n",
            "        -0.2467,  0.1579,  0.0165, -0.0890, -0.1903, -0.0787,  0.1700, -0.4832,\n",
            "         0.0619, -0.0677,  0.3125, -0.5064,  0.3138, -0.2617, -0.1545,  0.0063],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 2.5947e-02, -1.0458e-01, -4.7712e-03],\n",
            "          [-8.6223e-02, -3.3021e-01, -1.0275e-01],\n",
            "          [-5.7426e-02, -1.9074e-01, -5.4646e-02]],\n",
            "\n",
            "         [[-1.6951e-02,  2.1384e-02, -2.1074e-03],\n",
            "          [-3.2983e-03,  4.5014e-02, -1.1510e-02],\n",
            "          [-5.9602e-02,  6.4942e-03,  2.9080e-03]],\n",
            "\n",
            "         [[-4.4903e-03,  1.9637e-02,  1.3167e-02],\n",
            "          [ 1.3050e-02, -7.7471e-03,  1.1931e-02],\n",
            "          [ 1.3454e-02,  1.1103e-02,  5.5145e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.2706e-03, -7.7438e-03,  2.0753e-02],\n",
            "          [-4.0024e-02, -4.0383e-02, -3.4821e-02],\n",
            "          [-2.0251e-02, -9.5164e-03,  1.3954e-02]],\n",
            "\n",
            "         [[-2.3430e-03,  3.2303e-02, -4.3342e-03],\n",
            "          [ 8.6194e-03,  1.0553e-02,  1.8074e-03],\n",
            "          [-1.2760e-02, -1.0232e-02,  4.5711e-03]],\n",
            "\n",
            "         [[ 1.5302e-02,  2.1361e-02, -7.0908e-03],\n",
            "          [-1.4221e-02,  4.5979e-02,  2.1369e-02],\n",
            "          [ 3.1312e-02,  6.6428e-02,  2.1465e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 5.3422e-02,  4.0515e-02,  9.6680e-03],\n",
            "          [ 3.2884e-02, -2.3474e-02,  3.4642e-02],\n",
            "          [-1.2861e-02,  5.0066e-02,  5.4579e-02]],\n",
            "\n",
            "         [[ 2.8764e-02,  4.3431e-02,  2.8258e-02],\n",
            "          [ 2.8734e-02, -3.5459e-02, -5.2788e-02],\n",
            "          [-5.5119e-02, -7.1813e-02, -8.2970e-02]],\n",
            "\n",
            "         [[ 9.5293e-02,  1.2549e-01, -6.4001e-02],\n",
            "          [-4.1166e-02, -9.0480e-04,  5.1387e-02],\n",
            "          [-1.1311e-01, -7.9823e-02,  1.4373e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-7.6924e-03,  2.0647e-02,  1.9521e-02],\n",
            "          [-6.7352e-03,  1.2601e-04,  4.8309e-03],\n",
            "          [-6.2405e-03, -9.2119e-03, -2.5806e-04]],\n",
            "\n",
            "         [[-2.6153e-02, -2.4641e-02,  4.0970e-02],\n",
            "          [-1.9164e-02, -1.0160e-02,  3.3163e-02],\n",
            "          [ 5.4200e-03,  9.0485e-04,  6.7799e-04]],\n",
            "\n",
            "         [[ 7.7762e-03,  2.6447e-02,  6.3650e-02],\n",
            "          [-3.0608e-02,  2.4959e-02,  1.2951e-02],\n",
            "          [-2.0938e-02, -7.7342e-03, -3.8790e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.0893e-02, -1.4409e-02,  1.5730e-02],\n",
            "          [ 1.6655e-02,  4.4535e-02,  6.3212e-02],\n",
            "          [ 3.4121e-02,  7.3135e-02,  5.9203e-02]],\n",
            "\n",
            "         [[ 2.3195e-03,  7.7598e-03,  2.0308e-02],\n",
            "          [ 2.0457e-02,  4.0029e-02,  3.4744e-02],\n",
            "          [-4.7356e-02, -3.7286e-02,  1.4542e-02]],\n",
            "\n",
            "         [[-2.2742e-02, -1.9000e-02, -8.4317e-03],\n",
            "          [-9.8759e-04,  2.1510e-02,  6.3959e-03],\n",
            "          [-9.4558e-03,  2.6833e-03, -3.1136e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-7.5787e-03, -1.6056e-02, -6.4204e-04],\n",
            "          [-5.5104e-03,  1.4252e-02,  4.5000e-02],\n",
            "          [-9.2800e-03,  2.2351e-02,  4.1728e-02]],\n",
            "\n",
            "         [[ 2.5705e-02,  4.8207e-02,  7.9145e-02],\n",
            "          [-4.4350e-03,  3.8872e-03,  4.1694e-02],\n",
            "          [ 8.0536e-04, -1.0601e-02,  9.2706e-03]],\n",
            "\n",
            "         [[-3.3892e-02,  9.3543e-03,  4.1746e-02],\n",
            "          [-1.6470e-02,  3.9542e-03,  6.2438e-02],\n",
            "          [-3.1055e-02, -3.6302e-03,  7.0817e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-7.1044e-05, -9.0020e-03, -2.6998e-03],\n",
            "          [ 3.0072e-03,  1.1579e-02,  1.5214e-02],\n",
            "          [ 3.4832e-03,  1.1353e-05,  1.6320e-02]],\n",
            "\n",
            "         [[-2.6334e-02,  2.1967e-02, -6.0039e-02],\n",
            "          [ 4.4519e-02,  1.3203e-01, -9.1163e-03],\n",
            "          [ 5.4242e-02,  1.3726e-01,  2.7454e-02]],\n",
            "\n",
            "         [[ 1.7122e-02,  3.7646e-03,  1.4872e-02],\n",
            "          [ 1.2092e-02,  1.1319e-02,  3.4667e-02],\n",
            "          [ 8.1790e-03, -2.0805e-02,  2.7143e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.0111e-02, -1.0526e-02,  2.8394e-02],\n",
            "          [-2.5112e-02, -2.2196e-02,  3.7229e-02],\n",
            "          [-3.8220e-02, -4.6644e-02,  1.5660e-02]],\n",
            "\n",
            "         [[-2.5913e-03, -2.4307e-02,  1.0611e-02],\n",
            "          [-2.1730e-02, -4.3938e-02, -7.1536e-03],\n",
            "          [-2.5171e-02, -5.9467e-02, -2.5577e-02]],\n",
            "\n",
            "         [[ 2.8652e-02,  2.5850e-04,  1.1416e-03],\n",
            "          [ 3.7812e-02, -1.1271e-03,  9.6027e-03],\n",
            "          [ 3.9350e-02,  1.0134e-02,  1.0449e-02]]],\n",
            "\n",
            "\n",
            "        [[[-7.9305e-03,  7.0872e-03,  2.1412e-02],\n",
            "          [-6.0065e-02,  1.4147e-03,  9.7281e-02],\n",
            "          [-6.0130e-02, -2.1725e-02,  3.6863e-02]],\n",
            "\n",
            "         [[ 2.8024e-02,  2.6183e-02, -2.3027e-02],\n",
            "          [ 5.1900e-02, -2.0588e-03, -1.0940e-01],\n",
            "          [-3.2729e-02, -6.2752e-03,  8.0630e-03]],\n",
            "\n",
            "         [[-1.8062e-02, -1.9510e-02,  4.3163e-02],\n",
            "          [ 4.6080e-02,  2.9494e-02,  4.0844e-02],\n",
            "          [ 5.9607e-03, -6.5891e-03, -6.4623e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.2193e-02,  8.4653e-03,  3.6764e-03],\n",
            "          [ 1.7549e-02,  2.1971e-02, -4.5108e-03],\n",
            "          [ 2.1124e-02,  3.4591e-02, -1.6310e-02]],\n",
            "\n",
            "         [[ 3.8144e-02,  4.8395e-02, -9.5556e-02],\n",
            "          [ 1.8923e-02,  1.1341e-02, -7.6311e-02],\n",
            "          [ 4.7358e-03,  3.2138e-02, -7.4777e-02]],\n",
            "\n",
            "         [[-1.9031e-02, -3.2568e-02, -3.8251e-02],\n",
            "          [ 1.0705e-02,  2.3121e-03, -7.5078e-02],\n",
            "          [ 3.3316e-02,  3.5515e-02, -2.1023e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.3330e-01,  7.4683e-02, -3.8624e-03],\n",
            "          [ 9.1377e-02,  8.2415e-02,  3.9469e-02],\n",
            "          [-1.8265e-02, -5.9943e-02,  8.9354e-02]],\n",
            "\n",
            "         [[ 1.5566e-02, -4.1716e-02,  1.0633e-02],\n",
            "          [ 7.2644e-03,  3.1934e-02,  1.2732e-03],\n",
            "          [-2.0851e-02, -3.7593e-03, -7.0170e-02]],\n",
            "\n",
            "         [[-6.6139e-02,  1.0627e-01,  1.9590e-02],\n",
            "          [ 5.4987e-02, -1.5552e-01, -1.8819e-02],\n",
            "          [-4.2554e-03,  4.4964e-02, -2.4632e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-6.1691e-02, -4.5531e-02, -9.1721e-03],\n",
            "          [ 4.3995e-02,  4.5703e-02, -7.0108e-02],\n",
            "          [ 1.1388e-02,  4.4678e-02, -4.5953e-02]],\n",
            "\n",
            "         [[ 4.3432e-03,  2.3194e-02, -2.1895e-02],\n",
            "          [-8.0216e-02, -5.7606e-02, -9.8455e-03],\n",
            "          [-3.3285e-02, -1.1468e-01, -2.3779e-02]],\n",
            "\n",
            "         [[-6.3785e-02, -2.4485e-02, -4.9061e-02],\n",
            "          [-6.1594e-02,  1.0328e-01,  5.9685e-03],\n",
            "          [ 8.1863e-02, -3.0314e-02, -4.6373e-03]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.2496, 0.2198, 0.2756, 0.6073, 0.2654, 0.2942, 0.1136, 0.4425, 0.2868,\n",
            "        0.2974, 0.2506, 0.4103, 0.4855, 0.3383, 0.4670, 0.1772, 0.2171, 0.5025,\n",
            "        0.2263, 0.3667, 0.4867, 0.4586, 0.4652, 0.2200, 0.1510, 0.2761, 0.3813,\n",
            "        0.2803, 0.2382, 0.3953, 0.3032, 0.3163, 0.2025, 0.2323, 0.2003, 0.1661,\n",
            "        0.4690, 0.3476, 0.3414, 0.2274, 0.2485, 0.2356, 0.2726, 0.4657, 0.3429,\n",
            "        0.2465, 0.4674, 0.2812, 0.6241, 0.4152, 0.3403, 0.4218, 0.1152, 0.2985,\n",
            "        0.5802, 0.2795, 0.4706, 0.4517, 0.4303, 0.2749, 0.3427, 0.1137, 0.5069,\n",
            "        0.4370], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 2.2752e-01,  8.6747e-03, -6.7346e-02, -6.8779e-02,  3.5977e-01,\n",
            "        -2.0167e-01, -4.8431e-05,  2.3735e-02,  3.9549e-01,  3.7079e-02,\n",
            "         6.8793e-03,  2.7578e-01, -7.0272e-02, -2.3970e-01, -8.1753e-02,\n",
            "        -9.4132e-02, -1.4544e-01,  3.7301e-02, -3.6174e-01, -3.9561e-01,\n",
            "        -4.0789e-01,  3.5559e-03, -2.7878e-01, -3.5299e-02, -7.0281e-02,\n",
            "         2.1005e-01, -4.6362e-03, -1.9665e-01, -2.8066e-01, -1.6540e-02,\n",
            "         2.6452e-01, -8.9359e-02, -2.1046e-01, -1.3026e-01,  1.7215e-01,\n",
            "         5.3403e-02, -2.2295e-01, -4.8033e-02,  2.4572e-01,  2.0950e-01,\n",
            "         1.6220e-01,  1.1370e-01,  1.1457e-01, -1.4870e-01, -3.2150e-02,\n",
            "        -3.0549e-01,  4.9125e-01,  1.0873e-01,  1.2779e-02,  1.0044e-01,\n",
            "         4.1553e-01, -1.4710e-02,  2.3922e-02,  9.9812e-02, -1.7273e-01,\n",
            "         1.0078e-01, -1.4564e-01, -2.2735e-01,  1.3637e-01,  2.0127e-01,\n",
            "        -5.7430e-02,  2.3530e-01, -1.1299e-01,  3.0933e-01],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 1.9712e-02, -5.2562e-03, -3.7619e-03],\n",
            "          [-1.9635e-02, -1.2336e-02, -3.5196e-02],\n",
            "          [ 5.0761e-02,  7.5668e-02,  4.3344e-02]],\n",
            "\n",
            "         [[ 1.4160e-02, -8.6094e-03, -1.0541e-02],\n",
            "          [-4.2586e-02, -2.3814e-02, -5.4694e-02],\n",
            "          [-1.4018e-03,  4.6720e-02,  5.0898e-02]],\n",
            "\n",
            "         [[ 2.1559e-02,  4.1633e-03, -9.7118e-03],\n",
            "          [-9.3201e-03, -2.5432e-02, -2.8274e-02],\n",
            "          [-3.0107e-02, -4.8230e-02, -2.6001e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.4300e-03,  9.1875e-02,  3.1938e-03],\n",
            "          [-1.7945e-02,  5.7266e-02, -8.4098e-03],\n",
            "          [-3.4961e-02, -2.3296e-02, -3.5089e-02]],\n",
            "\n",
            "         [[ 2.5603e-02, -3.1689e-02, -5.4160e-02],\n",
            "          [ 6.9736e-02, -1.0716e-02, -6.8034e-02],\n",
            "          [ 3.5578e-02,  3.4749e-02, -1.9334e-02]],\n",
            "\n",
            "         [[-6.5420e-02, -4.6427e-03, -2.3362e-02],\n",
            "          [ 7.5833e-02,  9.1174e-03, -4.9701e-02],\n",
            "          [ 6.2944e-02, -9.8735e-02,  3.3158e-02]]],\n",
            "\n",
            "\n",
            "        [[[-9.0557e-03, -3.0753e-02,  1.1953e-02],\n",
            "          [-3.2539e-02, -6.2846e-03, -2.0235e-02],\n",
            "          [ 4.7996e-03, -2.1462e-02, -4.1557e-03]],\n",
            "\n",
            "         [[ 1.7163e-02, -2.3303e-03,  7.3972e-02],\n",
            "          [-3.2105e-02, -7.7536e-02, -1.2648e-02],\n",
            "          [ 3.8985e-02, -4.3170e-02,  1.0904e-02]],\n",
            "\n",
            "         [[-2.9643e-02, -5.8534e-02, -5.9736e-02],\n",
            "          [-2.9437e-02, -3.6441e-02, -1.2380e-02],\n",
            "          [-2.2775e-02, -2.4485e-03, -1.6124e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.6830e-02,  1.4267e-02,  6.2658e-02],\n",
            "          [ 3.0585e-04, -5.3241e-03,  3.2786e-03],\n",
            "          [ 2.1097e-02, -2.3189e-02,  1.2102e-02]],\n",
            "\n",
            "         [[-6.1182e-02, -2.9227e-02,  2.0036e-02],\n",
            "          [-7.6089e-02, -7.7057e-02,  8.6544e-02],\n",
            "          [-3.9228e-02, -3.2361e-02, -8.8970e-02]],\n",
            "\n",
            "         [[-1.3372e-01,  8.8362e-02,  8.3836e-02],\n",
            "          [-1.1688e-02,  4.3156e-01, -3.3629e-03],\n",
            "          [-2.3925e-02, -1.0092e-01, -1.0184e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 8.0165e-02,  4.3042e-02,  2.7325e-03],\n",
            "          [ 3.5269e-02, -1.5504e-02, -3.5011e-02],\n",
            "          [-1.7164e-02, -2.6827e-02, -3.3946e-02]],\n",
            "\n",
            "         [[ 4.5439e-02,  5.1585e-02,  1.8321e-02],\n",
            "          [-3.9647e-02,  2.3956e-02, -2.6609e-02],\n",
            "          [-3.0358e-02, -6.4729e-02,  2.5834e-02]],\n",
            "\n",
            "         [[ 3.8105e-02,  4.0986e-02,  4.1005e-02],\n",
            "          [ 1.7584e-02, -1.6494e-02, -3.2716e-02],\n",
            "          [ 5.5886e-03, -1.7068e-02, -3.0605e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.3694e-01, -1.4074e-01,  5.1423e-02],\n",
            "          [-1.2521e-01, -1.3128e-01,  7.5733e-02],\n",
            "          [-4.5032e-02, -1.7081e-02,  7.1252e-02]],\n",
            "\n",
            "         [[ 6.3381e-02,  1.5874e-02, -2.7322e-02],\n",
            "          [ 8.0356e-02,  3.6104e-02, -2.8506e-02],\n",
            "          [ 2.6638e-02,  2.2021e-02,  3.2345e-02]],\n",
            "\n",
            "         [[-1.2068e-03, -4.6179e-02, -1.5351e-02],\n",
            "          [-1.1276e-02,  1.9200e-02,  3.4336e-02],\n",
            "          [ 1.6540e-02, -7.8592e-03, -2.5392e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 3.3384e-02,  6.9963e-02,  1.0745e-02],\n",
            "          [-1.7518e-02, -5.3524e-02, -6.4960e-02],\n",
            "          [ 3.4248e-04, -4.5557e-02, -4.7336e-02]],\n",
            "\n",
            "         [[-5.1031e-03,  7.9784e-03, -8.6553e-04],\n",
            "          [-1.6557e-03,  1.4661e-02,  5.3365e-03],\n",
            "          [-3.1784e-02, -6.6940e-02, -4.6889e-02]],\n",
            "\n",
            "         [[-1.1775e-02,  7.2759e-03,  7.6622e-03],\n",
            "          [-6.1288e-02, -5.2078e-02, -4.5152e-02],\n",
            "          [-8.6584e-02, -9.7381e-02, -1.0405e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.1243e-02,  6.2456e-02,  2.5188e-02],\n",
            "          [-2.2911e-02, -2.1100e-03, -2.7573e-02],\n",
            "          [ 4.6557e-02,  6.4980e-02,  3.1879e-02]],\n",
            "\n",
            "         [[ 6.2867e-03,  2.4255e-02,  8.9674e-02],\n",
            "          [-7.7718e-03, -5.4311e-02, -4.6843e-02],\n",
            "          [-6.7499e-03, -6.6857e-02, -4.9842e-02]],\n",
            "\n",
            "         [[ 4.7326e-03, -3.9533e-02,  1.1500e-03],\n",
            "          [-2.7957e-02, -1.3466e-01, -6.0753e-02],\n",
            "          [-3.2010e-03,  7.2213e-02,  1.1009e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 2.3763e-02, -1.7876e-02, -7.4843e-03],\n",
            "          [ 1.6239e-02,  5.4479e-04, -3.3735e-02],\n",
            "          [-2.2854e-02, -1.4316e-03,  1.1010e-02]],\n",
            "\n",
            "         [[ 5.2277e-03, -2.5941e-03,  5.9594e-03],\n",
            "          [-2.9058e-03, -7.3409e-03,  3.0652e-02],\n",
            "          [ 7.5540e-02,  6.6445e-03,  2.5518e-03]],\n",
            "\n",
            "         [[-6.5970e-02, -4.1286e-02, -3.0278e-02],\n",
            "          [-3.5108e-02, -3.9099e-02, -1.6818e-02],\n",
            "          [-1.0224e-02, -8.6995e-03, -5.9939e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.1233e-02, -2.4559e-02, -7.4436e-03],\n",
            "          [-4.3734e-03, -3.2864e-02, -3.3453e-02],\n",
            "          [ 8.9269e-03, -1.7646e-02,  3.8375e-04]],\n",
            "\n",
            "         [[-7.8930e-02, -7.2940e-02, -6.7911e-02],\n",
            "          [-8.4146e-02, -8.3657e-02,  5.3666e-02],\n",
            "          [-3.5577e-02, -3.6835e-02,  5.8987e-03]],\n",
            "\n",
            "         [[ 8.3767e-02,  8.0476e-05,  7.2164e-02],\n",
            "          [-6.4219e-02, -1.2661e-01,  4.6026e-02],\n",
            "          [ 9.3033e-02, -4.7521e-02,  3.6777e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 4.1012e-02,  1.3361e-03, -5.8616e-02],\n",
            "          [ 4.2461e-02,  2.9437e-03, -2.0445e-02],\n",
            "          [ 7.6097e-02,  5.2504e-02, -5.5636e-03]],\n",
            "\n",
            "         [[ 2.2046e-02,  4.0888e-03,  1.4645e-02],\n",
            "          [-7.7532e-02, -1.1912e-01, -7.0892e-02],\n",
            "          [-1.0618e-02, -3.2121e-02, -2.3969e-02]],\n",
            "\n",
            "         [[-2.1612e-02, -2.6110e-03, -3.1664e-02],\n",
            "          [-3.2892e-02, -3.9771e-02, -5.1463e-02],\n",
            "          [-2.6150e-02, -3.6554e-02, -2.3315e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.4600e-03,  8.4181e-02,  2.3199e-02],\n",
            "          [ 5.7595e-02,  1.3036e-01,  3.2172e-02],\n",
            "          [-2.2774e-03,  4.2065e-02, -4.8619e-02]],\n",
            "\n",
            "         [[ 3.1533e-02, -4.3655e-02,  2.0361e-02],\n",
            "          [ 3.9973e-03, -5.1430e-02, -6.3839e-02],\n",
            "          [ 6.4002e-03,  4.5347e-02,  4.7346e-02]],\n",
            "\n",
            "         [[-9.1818e-02,  1.0264e-02,  9.6565e-02],\n",
            "          [-2.1635e-03, -2.3452e-02, -5.9038e-02],\n",
            "          [ 1.9402e-02,  2.8854e-02, -9.6113e-02]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.3910, 0.4375, 0.3746, 0.3990, 0.3404, 0.3503, 0.2618, 0.2707, 0.2865,\n",
            "        0.4308, 0.1895, 0.3041, 0.3837, 0.2944, 0.2105, 0.3304, 0.2943, 0.2887,\n",
            "        0.2060, 0.4627, 0.2335, 0.1831, 0.4489, 0.2830, 0.3389, 0.2997, 0.3503,\n",
            "        0.2735, 0.3908, 0.2817, 0.2636, 0.4462, 0.3282, 0.3776, 0.4471, 0.3878,\n",
            "        0.2516, 0.3172, 0.3661, 0.3166, 0.3818, 0.3128, 0.2274, 0.3627, 0.2902,\n",
            "        0.2381, 0.2988, 0.2469, 0.3840, 0.2886, 0.3197, 0.2879, 0.3218, 0.4559,\n",
            "        0.3500, 0.2420, 0.3396, 0.3519, 0.3839, 0.3806, 0.4039, 0.2826, 0.4594,\n",
            "        0.3342], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0997, -0.4755, -0.0474, -0.2698, -0.0834, -0.0072,  0.0474,  0.1022,\n",
            "        -0.0170, -0.1471,  0.2307,  0.1447, -0.1775,  0.0273,  0.1559, -0.1836,\n",
            "         0.1238, -0.1522,  0.0554, -0.2881, -0.2606,  0.2316, -0.3242, -0.0219,\n",
            "        -0.2645,  0.0576, -0.2465,  0.0481, -0.3530,  0.0950, -0.1862, -0.1707,\n",
            "        -0.0161, -0.2604, -0.3145, -0.1083,  0.0659, -0.1427, -0.0570, -0.0076,\n",
            "        -0.3006, -0.0744, -0.0683, -0.1104,  0.0253,  0.0489, -0.2515,  0.1150,\n",
            "        -0.3783,  0.0846, -0.0368,  0.1439, -0.0468, -0.3087, -0.0240,  0.1397,\n",
            "        -0.0908, -0.1795, -0.1129, -0.0793, -0.1491,  0.0594, -0.4433, -0.0138],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-2.1574e-02, -4.5688e-03,  4.5483e-03],\n",
            "          [-8.1870e-03,  4.1740e-02,  2.3010e-02],\n",
            "          [-8.9283e-03,  5.7352e-02,  2.9818e-02]],\n",
            "\n",
            "         [[ 5.8627e-02,  4.2864e-02,  4.4912e-02],\n",
            "          [ 2.2281e-02, -1.2969e-02,  7.6099e-03],\n",
            "          [ 4.5373e-02,  3.0712e-02,  3.7700e-02]],\n",
            "\n",
            "         [[-1.5456e-02, -3.8692e-02, -4.6010e-02],\n",
            "          [-2.3123e-02,  2.8293e-02,  4.7790e-03],\n",
            "          [-2.0328e-02,  1.3756e-02,  2.5883e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.1302e-02,  4.2291e-02,  5.7833e-02],\n",
            "          [ 4.5210e-02,  5.5850e-02,  1.4318e-02],\n",
            "          [ 1.4241e-02,  1.7968e-02,  1.4344e-02]],\n",
            "\n",
            "         [[ 4.6012e-03,  1.2566e-02,  4.8931e-02],\n",
            "          [-6.5754e-03, -2.6431e-02,  1.5855e-02],\n",
            "          [ 1.3192e-02,  1.9011e-02,  1.3842e-02]],\n",
            "\n",
            "         [[ 6.1983e-02,  6.9919e-02,  6.1035e-02],\n",
            "          [ 6.1253e-02,  9.9557e-02,  5.9060e-02],\n",
            "          [ 5.8298e-02,  8.1652e-02,  8.1499e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.0088e-02, -1.2959e-02,  9.7798e-03],\n",
            "          [ 5.5408e-02,  4.3501e-02,  5.6983e-02],\n",
            "          [ 5.3427e-02,  3.5118e-02,  3.6782e-02]],\n",
            "\n",
            "         [[ 2.4442e-03, -3.0207e-02, -1.0377e-02],\n",
            "          [-4.5297e-02, -4.5318e-02,  5.4623e-03],\n",
            "          [-4.4762e-02, -1.5508e-02,  6.9745e-03]],\n",
            "\n",
            "         [[ 3.9658e-02,  3.6838e-02,  5.8796e-03],\n",
            "          [ 2.3207e-02,  3.9240e-03, -2.0887e-02],\n",
            "          [-1.4829e-02,  5.3606e-03,  1.7404e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.2160e-02,  5.9042e-02,  4.8433e-02],\n",
            "          [-2.6464e-02, -8.0667e-03, -1.0359e-02],\n",
            "          [-2.6699e-02, -9.5411e-03, -2.8902e-02]],\n",
            "\n",
            "         [[-2.9235e-02, -3.9078e-02, -4.4955e-02],\n",
            "          [-2.0346e-02, -4.4891e-02, -3.7477e-02],\n",
            "          [ 1.9653e-02, -1.5562e-03, -5.8245e-03]],\n",
            "\n",
            "         [[-5.0696e-02, -4.8902e-02,  9.1631e-03],\n",
            "          [ 5.1668e-03,  2.0509e-02,  6.6874e-02],\n",
            "          [ 2.8934e-02,  4.6717e-02,  2.1371e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 2.1744e-02, -2.8354e-02, -3.2557e-02],\n",
            "          [ 3.0519e-02,  1.8536e-02,  1.5244e-02],\n",
            "          [ 1.3832e-03,  1.7051e-02,  3.2020e-02]],\n",
            "\n",
            "         [[-3.6293e-02,  1.0914e-02,  4.5371e-02],\n",
            "          [ 1.3399e-02,  6.4272e-02,  8.8210e-02],\n",
            "          [ 4.6697e-02,  9.9653e-02,  8.7606e-02]],\n",
            "\n",
            "         [[-2.4336e-02, -2.9627e-02,  1.9537e-02],\n",
            "          [-3.3412e-02, -2.2290e-02, -2.8879e-02],\n",
            "          [ 1.4765e-02,  1.7234e-02, -1.8185e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.9859e-02, -7.1075e-02, -5.8546e-02],\n",
            "          [ 2.2902e-02,  1.1184e-02, -2.3654e-02],\n",
            "          [ 8.1897e-02,  1.1996e-01,  9.3242e-02]],\n",
            "\n",
            "         [[ 3.1984e-02,  7.4931e-02,  6.6020e-02],\n",
            "          [ 2.8490e-02,  1.1931e-01,  1.2100e-01],\n",
            "          [ 7.9259e-04,  4.3812e-02,  4.4648e-02]],\n",
            "\n",
            "         [[ 3.2748e-02,  4.1444e-02, -8.1932e-03],\n",
            "          [ 4.5541e-02,  2.9426e-02, -8.5440e-03],\n",
            "          [ 1.1634e-04,  1.8045e-03,  1.4826e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-4.4144e-02, -8.3106e-02, -5.3073e-02],\n",
            "          [ 3.2124e-02,  1.0286e-02,  2.4409e-02],\n",
            "          [ 6.1606e-03, -1.9455e-02,  4.0534e-02]],\n",
            "\n",
            "         [[ 5.6026e-04,  9.6961e-03,  2.5010e-03],\n",
            "          [ 7.1679e-03, -1.7535e-02, -2.3857e-02],\n",
            "          [-9.8745e-03, -1.8550e-02,  1.7301e-03]],\n",
            "\n",
            "         [[ 4.3882e-03,  4.2049e-02,  7.5950e-02],\n",
            "          [-6.5610e-02, -3.6130e-02, -1.9404e-02],\n",
            "          [-3.8091e-02, -2.6749e-02, -1.3865e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.5593e-02, -4.6050e-02, -2.2809e-02],\n",
            "          [-9.7648e-03,  2.4910e-03,  2.4503e-02],\n",
            "          [ 2.0381e-02,  5.2393e-02,  6.9019e-02]],\n",
            "\n",
            "         [[ 9.3306e-04,  1.2483e-02, -1.1817e-02],\n",
            "          [-1.2627e-02, -1.8756e-02, -1.4144e-03],\n",
            "          [-5.2490e-03, -4.6126e-03, -1.3224e-02]],\n",
            "\n",
            "         [[ 7.4689e-04, -1.0135e-02, -7.8264e-03],\n",
            "          [ 1.2491e-02, -2.5865e-02,  4.0514e-02],\n",
            "          [ 5.8855e-03,  4.5990e-02,  1.0651e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.2262e-02, -1.5378e-02,  1.3862e-03],\n",
            "          [ 4.1166e-02, -2.4944e-02, -2.6686e-02],\n",
            "          [-1.7423e-02,  5.2690e-03, -2.1861e-02]],\n",
            "\n",
            "         [[-3.1207e-02, -3.3025e-02,  2.2114e-02],\n",
            "          [-2.4009e-02,  1.2988e-02,  2.2430e-02],\n",
            "          [ 1.0332e-02,  4.3601e-03,  4.7321e-03]],\n",
            "\n",
            "         [[ 2.0182e-02,  6.1569e-02, -2.8771e-02],\n",
            "          [ 5.8231e-02,  4.6767e-02, -2.8417e-05],\n",
            "          [ 3.7545e-02, -4.5886e-02,  1.5849e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.0431e-03, -3.6082e-03,  7.1986e-03],\n",
            "          [ 2.4895e-02,  6.1671e-03, -3.2427e-02],\n",
            "          [ 7.2338e-03,  2.2406e-03, -5.3330e-02]],\n",
            "\n",
            "         [[ 2.8072e-02, -1.0571e-02, -1.3854e-02],\n",
            "          [-1.0879e-02,  6.1929e-03, -5.6713e-03],\n",
            "          [-2.6083e-02,  8.1861e-03, -3.2873e-02]],\n",
            "\n",
            "         [[-3.1032e-02, -6.0485e-02, -2.5583e-02],\n",
            "          [-4.6239e-02, -2.2805e-02, -7.7678e-03],\n",
            "          [-9.4698e-03,  4.0247e-03, -4.8637e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 2.3128e-02, -5.6038e-02, -3.4572e-02],\n",
            "          [ 1.0638e-03,  5.7929e-02, -7.6970e-03],\n",
            "          [-3.0103e-02,  3.5573e-02, -1.8143e-02]],\n",
            "\n",
            "         [[ 9.6840e-02, -1.1186e-01, -7.8766e-02],\n",
            "          [-1.0444e-01, -1.0851e-01, -1.9553e-01],\n",
            "          [-1.1986e-01, -7.1474e-02,  3.6750e-02]],\n",
            "\n",
            "         [[-2.2194e-02,  6.0298e-03,  5.6914e-02],\n",
            "          [-4.8342e-02,  7.8893e-02, -5.1026e-02],\n",
            "          [-5.1294e-02, -5.7434e-02, -1.9178e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.4896e-02, -8.1267e-02,  5.1794e-02],\n",
            "          [-8.3985e-02, -5.7778e-02,  6.7891e-02],\n",
            "          [ 2.3837e-02,  3.8954e-02,  4.1141e-02]],\n",
            "\n",
            "         [[ 4.6446e-03,  2.7367e-02, -2.3154e-02],\n",
            "          [ 2.0675e-02,  2.3429e-02,  6.4380e-04],\n",
            "          [-5.2222e-02, -1.4854e-02, -2.5150e-02]],\n",
            "\n",
            "         [[ 2.1291e-02,  1.2736e-02,  8.4553e-03],\n",
            "          [-8.2932e-02,  7.2067e-02,  1.3107e-01],\n",
            "          [ 8.5491e-03,  1.3677e-01,  3.9867e-02]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.2560, 0.5690, 0.4042, 0.5130, 0.2178, 0.4940, 0.3315, 0.5510, 0.4354,\n",
            "        0.5291, 0.2081, 0.4735, 0.5945, 0.5645, 0.2761, 0.2571, 0.4853, 0.6240,\n",
            "        0.4370, 0.2308, 0.4970, 0.3157, 0.5706, 0.2162, 0.1932, 0.1448, 0.2218,\n",
            "        0.2389, 0.5871, 0.3501, 0.4109, 0.3199, 0.5808, 0.3281, 0.2723, 0.1971,\n",
            "        0.6139, 0.4075, 0.6304, 0.3874, 0.7605, 0.2111, 0.3071, 0.4603, 0.3099,\n",
            "        0.1914, 0.4431, 0.2537, 0.5745, 0.6459, 0.3914, 0.3090, 0.6782, 0.1937,\n",
            "        0.5814, 0.2570, 0.3514, 0.2124, 0.5794, 0.3415, 0.2051, 0.0715, 0.4090,\n",
            "        0.4416], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1778, -0.1287,  0.0349, -0.1452,  0.1864, -0.1413, -0.4201, -0.1334,\n",
            "         0.2183, -0.1912,  0.0311, -0.0235, -0.1724, -0.0274, -0.0295, -0.1031,\n",
            "         0.0047,  0.0828, -0.1521,  0.0183, -0.2418, -0.0831, -0.0491, -0.0688,\n",
            "        -0.2560,  0.1381, -0.0165,  0.2092, -0.0028, -0.0265, -0.0225,  0.0286,\n",
            "        -0.1065, -0.3698,  0.2862, -0.1036,  0.3080, -0.0894,  0.2772,  0.1136,\n",
            "        -0.3157,  0.0423,  0.0567,  0.2369, -0.0727,  0.0465, -0.0536,  0.1309,\n",
            "         0.0282, -0.1371,  0.1464, -0.0717, -0.3237, -0.1583, -0.0424, -0.1278,\n",
            "        -0.1703,  0.0413,  0.0891,  0.0770, -0.0730,  0.0683, -0.0391,  0.0476],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-7.1555e-02, -1.1031e-01, -1.3711e-01],\n",
            "          [ 7.0593e-02, -1.4782e-02, -1.0053e-01],\n",
            "          [ 1.1938e-01,  8.7330e-02, -8.2206e-03]],\n",
            "\n",
            "         [[-2.3999e-02, -6.3682e-03,  2.4303e-03],\n",
            "          [ 6.1831e-03,  1.8781e-02,  2.5324e-02],\n",
            "          [ 2.3656e-03, -4.0037e-03, -1.1949e-02]],\n",
            "\n",
            "         [[ 6.0344e-03,  6.3784e-03, -1.2247e-02],\n",
            "          [ 7.8854e-03, -1.3464e-02, -4.2702e-02],\n",
            "          [ 1.7380e-02, -1.3862e-02, -4.7145e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.4324e-02,  3.2257e-02,  2.5819e-02],\n",
            "          [ 8.4676e-03, -4.5413e-04, -1.0832e-02],\n",
            "          [-6.7166e-03, -1.5052e-02, -2.6939e-02]],\n",
            "\n",
            "         [[-1.2089e-02, -2.3588e-02, -2.2689e-02],\n",
            "          [ 1.0135e-02,  1.8285e-02, -1.5695e-02],\n",
            "          [ 2.1352e-02,  5.8568e-02,  4.2873e-02]],\n",
            "\n",
            "         [[ 1.4421e-02, -2.8298e-02, -7.0770e-03],\n",
            "          [ 3.0260e-02, -6.6294e-03, -1.6901e-02],\n",
            "          [ 3.9085e-02,  1.4222e-02,  2.2294e-02]]],\n",
            "\n",
            "\n",
            "        [[[-7.7911e-02, -7.3929e-02, -3.6671e-02],\n",
            "          [-3.4903e-02, -6.2355e-02, -3.7793e-02],\n",
            "          [-2.8379e-02, -5.4291e-02, -4.9411e-02]],\n",
            "\n",
            "         [[-1.2970e-02, -2.1825e-02, -2.8767e-04],\n",
            "          [ 7.6444e-03,  1.7653e-02,  1.6660e-02],\n",
            "          [ 3.8337e-02,  2.3006e-02, -1.6620e-03]],\n",
            "\n",
            "         [[-8.7592e-02, -8.4735e-02, -5.5818e-02],\n",
            "          [-7.7731e-02, -8.0311e-02, -3.2554e-02],\n",
            "          [-5.6313e-02, -4.2047e-02,  1.5247e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.2377e-02, -4.0018e-02, -2.9523e-02],\n",
            "          [-1.5294e-02, -1.4165e-02,  2.7086e-03],\n",
            "          [ 1.1652e-02,  2.3886e-02,  2.4413e-02]],\n",
            "\n",
            "         [[ 2.0891e-03, -3.0475e-02, -3.3818e-02],\n",
            "          [ 6.7829e-03,  3.8681e-04, -1.4540e-02],\n",
            "          [-3.1306e-03,  6.7689e-03,  8.4524e-03]],\n",
            "\n",
            "         [[ 3.0586e-02,  4.6281e-02,  3.8359e-04],\n",
            "          [ 5.3079e-02,  6.7488e-02,  3.0547e-02],\n",
            "          [ 2.3374e-02,  4.3993e-02, -3.8713e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 1.3878e-02,  3.2724e-02,  4.6584e-02],\n",
            "          [-8.0647e-03,  1.6209e-03,  1.5153e-02],\n",
            "          [-7.0342e-02, -5.3299e-02, -4.5920e-02]],\n",
            "\n",
            "         [[ 4.6035e-02,  3.5400e-02,  3.4941e-02],\n",
            "          [ 5.8351e-02,  5.4640e-02,  2.7162e-02],\n",
            "          [ 2.6799e-02,  4.5056e-02,  6.6886e-03]],\n",
            "\n",
            "         [[-3.3766e-02, -3.8605e-02, -2.4172e-02],\n",
            "          [-1.8285e-03,  1.0888e-02,  1.1425e-02],\n",
            "          [ 2.2282e-02,  1.4024e-02,  3.6332e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.6330e-02, -6.9552e-02, -8.9737e-02],\n",
            "          [ 3.9766e-02,  1.5501e-02, -2.2695e-02],\n",
            "          [ 1.0290e-01,  1.2294e-01,  6.3867e-02]],\n",
            "\n",
            "         [[-4.2318e-03,  4.9511e-02, -7.6289e-03],\n",
            "          [-2.7720e-02,  7.0398e-03, -9.4052e-03],\n",
            "          [-6.7008e-02, -6.0542e-02, -2.5967e-02]],\n",
            "\n",
            "         [[-5.8560e-03, -1.7573e-02, -3.8016e-02],\n",
            "          [ 2.8579e-03, -4.1603e-03,  1.0113e-02],\n",
            "          [ 2.6243e-02,  3.5200e-02,  3.1143e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-4.4193e-02, -6.5322e-02, -1.7594e-02],\n",
            "          [-9.3970e-02, -5.8291e-02,  1.2093e-02],\n",
            "          [-2.2998e-02,  3.2463e-02,  7.1731e-02]],\n",
            "\n",
            "         [[-4.7220e-03, -3.0125e-03, -1.8075e-02],\n",
            "          [ 1.2667e-02, -8.0509e-03, -1.4605e-02],\n",
            "          [ 7.8220e-03, -1.0720e-02, -2.6515e-02]],\n",
            "\n",
            "         [[-2.5299e-02, -4.9383e-02, -1.2720e-02],\n",
            "          [-5.2206e-02, -4.7233e-02, -4.2470e-03],\n",
            "          [-4.8697e-02, -2.5320e-02,  8.6178e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.7617e-03,  7.8398e-03, -5.9525e-03],\n",
            "          [ 4.0277e-03,  7.3575e-03, -1.1667e-02],\n",
            "          [-3.9997e-02, -3.8038e-02, -5.0469e-02]],\n",
            "\n",
            "         [[-3.8949e-03, -6.8965e-03,  3.4102e-02],\n",
            "          [-6.9814e-03, -4.9762e-02,  5.8711e-02],\n",
            "          [ 1.8361e-02,  2.5874e-02,  8.0028e-02]],\n",
            "\n",
            "         [[-3.3014e-02, -2.1510e-02, -2.1509e-03],\n",
            "          [-4.3894e-02, -3.2009e-02, -1.6265e-02],\n",
            "          [-1.1037e-02,  2.8872e-04,  3.0937e-02]]],\n",
            "\n",
            "\n",
            "        [[[-4.9907e-02, -5.0222e-02, -5.0985e-02],\n",
            "          [ 2.2644e-02, -1.4098e-02, -2.4426e-02],\n",
            "          [ 1.9960e-02,  9.6426e-02,  1.0580e-01]],\n",
            "\n",
            "         [[-3.6873e-02,  2.1413e-03,  8.3469e-03],\n",
            "          [-4.0796e-02, -3.3767e-02, -3.4955e-02],\n",
            "          [ 3.9466e-02,  7.0508e-02,  8.6065e-02]],\n",
            "\n",
            "         [[ 1.4842e-02,  6.6914e-03,  1.4324e-02],\n",
            "          [-3.2621e-02, -4.4027e-02, -2.2269e-02],\n",
            "          [ 7.1982e-03, -1.9187e-02, -4.9348e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.9938e-03,  1.6018e-02,  1.1242e-02],\n",
            "          [-4.7668e-03,  2.1921e-02,  2.2660e-02],\n",
            "          [-2.6753e-02,  2.6917e-04, -5.6827e-03]],\n",
            "\n",
            "         [[-8.7725e-03,  1.0761e-02,  7.3603e-03],\n",
            "          [-1.8010e-05, -1.7926e-02,  4.8229e-03],\n",
            "          [ 4.2431e-02, -1.5764e-02,  2.3554e-02]],\n",
            "\n",
            "         [[-1.3830e-02, -3.0793e-03, -4.0854e-03],\n",
            "          [ 3.3363e-02,  4.2952e-02,  3.5867e-02],\n",
            "          [-3.9653e-02, -3.0855e-02, -4.3189e-02]]],\n",
            "\n",
            "\n",
            "        [[[-3.8617e-02, -3.1549e-03,  2.5739e-03],\n",
            "          [-1.1592e-02,  9.8761e-03,  7.5235e-03],\n",
            "          [-1.9339e-02, -9.8779e-03,  2.1755e-03]],\n",
            "\n",
            "         [[ 1.6889e-04,  1.8302e-03, -8.9537e-03],\n",
            "          [ 5.8343e-03,  1.7360e-02, -1.9029e-02],\n",
            "          [ 5.8642e-03, -7.4307e-04,  1.4667e-03]],\n",
            "\n",
            "         [[-1.6506e-02, -2.8401e-02,  1.3986e-02],\n",
            "          [-2.2922e-02, -4.3484e-02,  1.0471e-02],\n",
            "          [-2.5801e-03, -4.5258e-02,  7.9791e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.5260e-03, -7.6469e-03,  1.3597e-02],\n",
            "          [ 5.5301e-04, -2.9176e-03,  2.2147e-02],\n",
            "          [ 3.2763e-03, -1.0775e-05,  1.3163e-02]],\n",
            "\n",
            "         [[ 5.1756e-03,  1.8495e-02, -8.0268e-03],\n",
            "          [-3.5030e-02,  2.6403e-02, -7.1220e-03],\n",
            "          [-5.2325e-02, -1.1185e-02,  1.9146e-02]],\n",
            "\n",
            "         [[-6.8805e-02,  5.1618e-02,  1.9787e-02],\n",
            "          [ 2.5533e-02, -6.1926e-02,  4.9924e-02],\n",
            "          [ 1.0532e-01, -4.4136e-02,  4.9907e-02]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.3248, 0.3613, 0.2960, 0.2913, 0.3407, 0.3435, 0.3049, 0.3308, 0.3447,\n",
            "        0.3860, 0.3196, 0.2622, 0.2994, 0.2189, 0.2397, 0.3744, 0.3555, 0.1948,\n",
            "        0.3349, 0.2159, 0.3349, 0.3454, 0.3094, 0.3769, 0.3546, 0.3267, 0.3178,\n",
            "        0.3272, 0.3832, 0.2585, 0.2973, 0.3481, 0.2827, 0.2995, 0.3451, 0.3471,\n",
            "        0.3440, 0.3344, 0.3211, 0.3180, 0.2940, 0.3353, 0.3253, 0.3733, 0.3198,\n",
            "        0.2987, 0.1620, 0.3262, 0.3271, 0.3410, 0.3693, 0.3320, 0.3357, 0.2951,\n",
            "        0.3115, 0.3185, 0.3139, 0.2633, 0.3089, 0.3601, 0.2734, 0.3433, 0.3335,\n",
            "        0.3288, 0.2706, 0.2879, 0.3318, 0.3310, 0.3170, 0.2977, 0.3300, 0.3216,\n",
            "        0.3205, 0.3231, 0.3481, 0.3130, 0.2826, 0.2856, 0.3279, 0.3666, 0.3288,\n",
            "        0.3575, 0.3377, 0.2904, 0.3273, 0.3214, 0.3332, 0.3452, 0.1842, 0.3916,\n",
            "        0.3337, 0.2325, 0.3285, 0.3358, 0.2885, 0.3149, 0.3288, 0.2236, 0.3159,\n",
            "        0.2993, 0.3403, 0.3220, 0.3171, 0.2950, 0.2847, 0.3224, 0.3119, 0.2613,\n",
            "        0.3374, 0.3333, 0.3330, 0.2959, 0.4087, 0.2192, 0.2982, 0.4006, 0.3081,\n",
            "        0.3171, 0.2862, 0.2952, 0.3070, 0.3583, 0.3232, 0.3345, 0.3453, 0.3043,\n",
            "        0.3327, 0.3337], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0589, -0.1686, -0.0206,  0.0027, -0.0955, -0.1048,  0.0349, -0.0885,\n",
            "        -0.2053, -0.1764, -0.1224, -0.0364, -0.0785,  0.2088, -0.0403, -0.1820,\n",
            "        -0.1076,  0.2989, -0.0570,  0.2064, -0.0921, -0.1376, -0.1304, -0.1193,\n",
            "        -0.1006, -0.0380, -0.1108, -0.0477, -0.1087,  0.1581, -0.1123, -0.1584,\n",
            "         0.0976, -0.0430, -0.1349, -0.1189, -0.0986, -0.0479, -0.0837, -0.0720,\n",
            "        -0.0836, -0.2442, -0.3376, -0.2124, -0.0693, -0.0651,  0.4979, -0.0811,\n",
            "        -0.1021, -0.0788, -0.1802, -0.1011, -0.1090, -0.0617, -0.0856, -0.0495,\n",
            "        -0.0370,  0.0023, -0.0508, -0.2430,  0.0009, -0.1525, -0.0963, -0.0516,\n",
            "        -0.0473,  0.0884, -0.1028, -0.0907, -0.1086, -0.0379, -0.1030, -0.1609,\n",
            "        -0.0903, -0.0898, -0.1282, -0.0830, -0.0186, -0.0232, -0.0045, -0.2131,\n",
            "        -0.1431, -0.1391, -0.1303, -0.0568, -0.1862, -0.1209, -0.0340, -0.1181,\n",
            "         0.2298, -0.2085, -0.1335,  0.1418, -0.0891, -0.1273,  0.0107, -0.1029,\n",
            "        -0.1025,  0.1562, -0.0937, -0.0657, -0.1245, -0.0451, -0.0707, -0.0447,\n",
            "         0.0715, -0.0484, -0.0312, -0.0437, -0.0927, -0.1465, -0.1151, -0.0183,\n",
            "        -0.1927,  0.2491,  0.0300, -0.1310, -0.0468, -0.0851, -0.0421, -0.0413,\n",
            "        -0.0457, -0.1433, -0.0981, -0.1046, -0.1315, -0.1249, -0.0982, -0.0961],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-0.0074, -0.0098,  0.0028],\n",
            "          [-0.0108,  0.0258,  0.0455],\n",
            "          [-0.0272,  0.0053,  0.0132]],\n",
            "\n",
            "         [[ 0.0354,  0.0251,  0.0078],\n",
            "          [ 0.0040,  0.0199,  0.0274],\n",
            "          [ 0.0353,  0.0355,  0.0133]],\n",
            "\n",
            "         [[ 0.0193, -0.0213, -0.0362],\n",
            "          [-0.0196, -0.0189, -0.0595],\n",
            "          [-0.0218, -0.0077,  0.0039]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0068,  0.0108, -0.0037],\n",
            "          [ 0.0135,  0.0114, -0.0013],\n",
            "          [ 0.0081,  0.0002,  0.0006]],\n",
            "\n",
            "         [[ 0.0077,  0.0077,  0.0044],\n",
            "          [-0.0102, -0.0117, -0.0096],\n",
            "          [-0.0039,  0.0181,  0.0133]],\n",
            "\n",
            "         [[ 0.0124, -0.0269, -0.0120],\n",
            "          [ 0.0268,  0.0264, -0.0215],\n",
            "          [ 0.0129,  0.0028, -0.0054]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0128,  0.0185, -0.0027],\n",
            "          [-0.0168, -0.0123,  0.0355],\n",
            "          [-0.0009,  0.0245,  0.0182]],\n",
            "\n",
            "         [[-0.0067, -0.0207, -0.0144],\n",
            "          [-0.0073,  0.0426,  0.0074],\n",
            "          [ 0.0276,  0.0160,  0.0159]],\n",
            "\n",
            "         [[-0.0229, -0.0206,  0.0236],\n",
            "          [-0.0275, -0.0561, -0.0699],\n",
            "          [ 0.0205,  0.0513,  0.0220]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0272, -0.0390, -0.0395],\n",
            "          [-0.0621, -0.0749, -0.0956],\n",
            "          [-0.0605, -0.0777, -0.0746]],\n",
            "\n",
            "         [[ 0.0287,  0.0293,  0.0287],\n",
            "          [ 0.0079,  0.0471,  0.0146],\n",
            "          [-0.0018,  0.0220,  0.0074]],\n",
            "\n",
            "         [[ 0.0016, -0.0168, -0.0046],\n",
            "          [-0.0081, -0.0255, -0.0524],\n",
            "          [-0.0093, -0.0010, -0.0378]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0047,  0.0061, -0.0071],\n",
            "          [ 0.0236, -0.0931, -0.0793],\n",
            "          [-0.0079, -0.0505, -0.0105]],\n",
            "\n",
            "         [[ 0.0148,  0.0162, -0.0515],\n",
            "          [ 0.0086,  0.0081, -0.0429],\n",
            "          [ 0.0908,  0.0654,  0.0435]],\n",
            "\n",
            "         [[-0.0138, -0.0064,  0.0085],\n",
            "          [ 0.0138, -0.0124,  0.0054],\n",
            "          [ 0.0202, -0.0035,  0.0080]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0009,  0.0185, -0.0306],\n",
            "          [-0.0170,  0.0057, -0.0491],\n",
            "          [-0.0328, -0.0374, -0.0459]],\n",
            "\n",
            "         [[-0.0046,  0.0069, -0.0011],\n",
            "          [-0.0079, -0.0499,  0.0421],\n",
            "          [-0.0752, -0.0048, -0.0058]],\n",
            "\n",
            "         [[ 0.0115, -0.0146,  0.0379],\n",
            "          [ 0.0141,  0.0486,  0.0232],\n",
            "          [ 0.0215, -0.0101,  0.0338]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0264,  0.0040,  0.0075],\n",
            "          [ 0.0636, -0.0320, -0.0018],\n",
            "          [ 0.0262,  0.0076,  0.0495]],\n",
            "\n",
            "         [[-0.0287, -0.0227, -0.0513],\n",
            "          [-0.0260, -0.0487, -0.0140],\n",
            "          [-0.0173, -0.0416, -0.0117]],\n",
            "\n",
            "         [[-0.0350,  0.0356,  0.0347],\n",
            "          [ 0.0183,  0.0436, -0.0263],\n",
            "          [ 0.0178,  0.0356,  0.0113]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0097, -0.0173, -0.0002],\n",
            "          [-0.0199,  0.0820,  0.0129],\n",
            "          [-0.0238, -0.0048,  0.0486]],\n",
            "\n",
            "         [[-0.0244, -0.0258, -0.0353],\n",
            "          [-0.0296, -0.0966, -0.0535],\n",
            "          [-0.0150,  0.0059, -0.0197]],\n",
            "\n",
            "         [[ 0.0068, -0.0368, -0.0255],\n",
            "          [-0.0116, -0.0236, -0.0078],\n",
            "          [ 0.0086,  0.0079, -0.0189]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0329,  0.0297,  0.0111],\n",
            "          [ 0.0390, -0.0090,  0.0226],\n",
            "          [ 0.0078, -0.0280, -0.0230]],\n",
            "\n",
            "         [[ 0.0137,  0.0229, -0.0190],\n",
            "          [ 0.0027,  0.0112,  0.0074],\n",
            "          [ 0.0211,  0.0436,  0.0108]],\n",
            "\n",
            "         [[-0.0237,  0.0221,  0.0004],\n",
            "          [-0.0309,  0.0609,  0.0167],\n",
            "          [-0.0885, -0.0834, -0.0342]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0073, -0.0197,  0.0018],\n",
            "          [ 0.0073, -0.0343, -0.0243],\n",
            "          [-0.0115, -0.0605, -0.0551]],\n",
            "\n",
            "         [[ 0.0030,  0.0026,  0.0171],\n",
            "          [-0.0134, -0.0086,  0.0090],\n",
            "          [ 0.0195,  0.0094,  0.0045]],\n",
            "\n",
            "         [[ 0.0014,  0.0008,  0.0119],\n",
            "          [-0.0024, -0.0107,  0.0126],\n",
            "          [-0.0051, -0.0058, -0.0146]]],\n",
            "\n",
            "\n",
            "        [[[-0.0074,  0.0118, -0.0427],\n",
            "          [ 0.0016, -0.0457, -0.1398],\n",
            "          [-0.0065, -0.0021, -0.0484]],\n",
            "\n",
            "         [[ 0.0075,  0.0527,  0.0388],\n",
            "          [-0.0125,  0.0847,  0.0062],\n",
            "          [ 0.0013, -0.0197, -0.0822]],\n",
            "\n",
            "         [[-0.0249,  0.0166,  0.0169],\n",
            "          [ 0.0087,  0.0214,  0.0117],\n",
            "          [-0.0009,  0.0306,  0.0136]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0078,  0.0220, -0.0175],\n",
            "          [-0.0076, -0.0211, -0.0037],\n",
            "          [ 0.0126, -0.0207, -0.0054]],\n",
            "\n",
            "         [[ 0.0302, -0.0082, -0.0649],\n",
            "          [-0.0238, -0.0954, -0.0530],\n",
            "          [-0.0168, -0.0111,  0.0010]],\n",
            "\n",
            "         [[-0.0245, -0.0847,  0.0251],\n",
            "          [ 0.0106,  0.0387,  0.1400],\n",
            "          [ 0.0155, -0.0095,  0.0041]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.1454, 0.3270, 0.3113, 0.2538, 0.4086, 0.3937, 0.4400, 0.3108, 0.3406,\n",
            "        0.2168, 0.2170, 0.3857, 0.1971, 0.2692, 0.1663, 0.2454, 0.3232, 0.3686,\n",
            "        0.3893, 0.3264, 0.3875, 0.4707, 0.1958, 0.4717, 0.1673, 0.3938, 0.3044,\n",
            "        0.1929, 0.2175, 0.2119, 0.4230, 0.3683, 0.2455, 0.2229, 0.3370, 0.3229,\n",
            "        0.2688, 0.3557, 0.2581, 0.4031, 0.4492, 0.3642, 0.2599, 0.1881, 0.1359,\n",
            "        0.2958, 0.1913, 0.3065, 0.3981, 0.4102, 0.1874, 0.4516, 0.3340, 0.1628,\n",
            "        0.3599, 0.1624, 0.2886, 0.1358, 0.4491, 0.2694, 0.4823, 0.3393, 0.4764,\n",
            "        0.3155, 0.6005, 0.4654, 0.5264, 0.2991, 0.2992, 0.4621, 0.2614, 0.4247,\n",
            "        0.4662, 0.4249, 0.3345, 0.2655, 0.4048, 0.3605, 0.1782, 0.3833, 0.2823,\n",
            "        0.3843, 0.3307, 0.2151, 0.3317, 0.1458, 0.2771, 0.4917, 0.3199, 0.4222,\n",
            "        0.1559, 0.4884, 0.3267, 0.3440, 0.1608, 0.4855, 0.2677, 0.1616, 0.3221,\n",
            "        0.4243, 0.3661, 0.1893, 0.3400, 0.3648, 0.1779, 0.3544, 0.2852, 0.2437,\n",
            "        0.4472, 0.3011, 0.3997, 0.6173, 0.2794, 0.4867, 0.1502, 0.6021, 0.3604,\n",
            "        0.4696, 0.3711, 0.2388, 0.5347, 0.1509, 0.3213, 0.4394, 0.3229, 0.4329,\n",
            "        0.1489, 0.3702], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0246,  0.0593,  0.1347, -0.1089, -0.0470, -0.1359, -0.0550,  0.0509,\n",
            "        -0.0613,  0.0916,  0.0031, -0.0274, -0.0539,  0.0177,  0.0432,  0.0074,\n",
            "         0.0548, -0.0321, -0.0224,  0.0142, -0.2150, -0.1160,  0.0486, -0.1141,\n",
            "         0.1066,  0.0355,  0.0140,  0.0177,  0.0781,  0.1331,  0.0139,  0.0447,\n",
            "         0.1063,  0.0528, -0.0539, -0.1160,  0.1055, -0.1591,  0.0100,  0.1197,\n",
            "         0.0170,  0.0929, -0.0675,  0.0987,  0.1034,  0.0501,  0.0297,  0.0281,\n",
            "        -0.0075, -0.0577, -0.0144, -0.1640,  0.1255,  0.0817,  0.0635,  0.0936,\n",
            "         0.0213,  0.0486, -0.1174,  0.0237, -0.2177,  0.0099, -0.1883,  0.0467,\n",
            "        -0.0829,  0.0585, -0.0306,  0.0509,  0.0541, -0.1671,  0.0115, -0.0302,\n",
            "        -0.1393,  0.0115,  0.0428,  0.1189, -0.1289,  0.0479,  0.0474, -0.0625,\n",
            "         0.0009, -0.0144,  0.0909,  0.1342, -0.0338,  0.0560,  0.0848, -0.0467,\n",
            "         0.0228, -0.0097,  0.1360, -0.2625,  0.0088, -0.0553,  0.0383, -0.0720,\n",
            "         0.0907,  0.1612, -0.1076,  0.1011, -0.0519,  0.0838, -0.0704, -0.0806,\n",
            "        -0.0243,  0.0533,  0.1277,  0.1403, -0.0593, -0.0639, -0.0766, -0.1163,\n",
            "         0.0661, -0.1644,  0.0422, -0.2786, -0.1006, -0.0696, -0.0761,  0.0371,\n",
            "        -0.0247,  0.0916, -0.0200, -0.0176,  0.0298, -0.0373,  0.0466, -0.1371],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 0.0159]],\n",
            "\n",
            "         [[-0.3109]],\n",
            "\n",
            "         [[ 0.0126]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1672]],\n",
            "\n",
            "         [[ 0.0127]],\n",
            "\n",
            "         [[ 0.0132]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0036]],\n",
            "\n",
            "         [[-0.0011]],\n",
            "\n",
            "         [[-0.0083]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0234]],\n",
            "\n",
            "         [[-0.0756]],\n",
            "\n",
            "         [[-0.0126]]],\n",
            "\n",
            "\n",
            "        [[[-0.0419]],\n",
            "\n",
            "         [[ 0.0079]],\n",
            "\n",
            "         [[-0.1662]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0319]],\n",
            "\n",
            "         [[-0.0188]],\n",
            "\n",
            "         [[ 0.0645]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.0287]],\n",
            "\n",
            "         [[ 0.0470]],\n",
            "\n",
            "         [[-0.0523]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0474]],\n",
            "\n",
            "         [[ 0.0586]],\n",
            "\n",
            "         [[ 0.0588]]],\n",
            "\n",
            "\n",
            "        [[[-0.0078]],\n",
            "\n",
            "         [[-0.0203]],\n",
            "\n",
            "         [[ 0.0564]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.7802]],\n",
            "\n",
            "         [[-0.0023]],\n",
            "\n",
            "         [[-0.0259]]],\n",
            "\n",
            "\n",
            "        [[[-0.0283]],\n",
            "\n",
            "         [[-0.0132]],\n",
            "\n",
            "         [[-0.0514]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0236]],\n",
            "\n",
            "         [[-0.0677]],\n",
            "\n",
            "         [[ 0.0268]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.3334,  0.0581,  0.0715,  0.3442,  0.1756,  0.1509,  0.1568,  0.3100,\n",
            "         0.1927,  0.1516,  0.3044,  0.2238,  0.3706,  0.1739,  0.3051,  0.2610,\n",
            "         0.1575,  0.2015,  0.2933,  0.1010,  0.5871,  0.0676,  0.2499,  0.0929,\n",
            "         0.2443,  0.0495,  0.2449,  0.2750,  0.3071,  0.3025,  0.1818,  0.0688,\n",
            "         0.2223,  0.3766,  0.4661,  0.3284,  0.1035,  0.3400,  0.2325,  0.1514,\n",
            "         0.1753,  0.2269,  0.2606,  0.1831,  0.2894,  0.2590,  0.2208,  0.1399,\n",
            "         0.0643,  0.2833,  0.3451,  0.2017,  0.0696,  0.2722,  0.1127,  0.2917,\n",
            "         0.2358,  0.2703,  0.0911,  0.2591,  0.1302,  0.2261,  0.1967,  0.0539,\n",
            "         0.0697,  0.0524,  0.1050,  0.0861,  0.1173,  0.0957,  0.1862,  0.1642,\n",
            "         0.1336,  0.1065,  0.1312,  0.0888,  0.0793,  0.0475,  0.3049,  0.2325,\n",
            "         0.2908,  0.1292,  0.0778,  0.2263,  0.2379,  0.3405,  0.0914,  0.1936,\n",
            "         0.1223,  0.1400,  0.2953,  0.2360,  0.1681,  0.1338,  0.2666,  0.1495,\n",
            "         0.0761,  0.1674,  0.1784,  0.1720,  0.2318,  0.3753,  0.2103,  0.1922,\n",
            "         0.4002,  0.1718,  0.0593,  0.0742,  0.0686,  0.1931,  0.1386,  0.1111,\n",
            "         0.3055,  0.1205,  0.3443,  0.1633,  0.3673,  0.1534,  0.0742,  0.2088,\n",
            "         0.0394,  0.2594,  0.1385, -0.0051,  0.1905,  0.1275,  0.3071,  0.1682],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0246,  0.0593,  0.1347, -0.1089, -0.0470, -0.1359, -0.0550,  0.0509,\n",
            "        -0.0613,  0.0916,  0.0031, -0.0274, -0.0539,  0.0177,  0.0432,  0.0074,\n",
            "         0.0548, -0.0321, -0.0224,  0.0142, -0.2150, -0.1160,  0.0486, -0.1141,\n",
            "         0.1066,  0.0355,  0.0140,  0.0177,  0.0781,  0.1331,  0.0139,  0.0447,\n",
            "         0.1063,  0.0528, -0.0539, -0.1160,  0.1055, -0.1591,  0.0100,  0.1197,\n",
            "         0.0170,  0.0929, -0.0675,  0.0987,  0.1034,  0.0501,  0.0297,  0.0281,\n",
            "        -0.0075, -0.0577, -0.0144, -0.1640,  0.1255,  0.0817,  0.0635,  0.0936,\n",
            "         0.0213,  0.0486, -0.1174,  0.0237, -0.2177,  0.0099, -0.1883,  0.0467,\n",
            "        -0.0829,  0.0585, -0.0306,  0.0509,  0.0541, -0.1671,  0.0115, -0.0302,\n",
            "        -0.1393,  0.0115,  0.0428,  0.1189, -0.1289,  0.0479,  0.0474, -0.0625,\n",
            "         0.0009, -0.0144,  0.0909,  0.1342, -0.0338,  0.0560,  0.0848, -0.0467,\n",
            "         0.0228, -0.0097,  0.1360, -0.2625,  0.0088, -0.0553,  0.0383, -0.0720,\n",
            "         0.0907,  0.1612, -0.1076,  0.1011, -0.0519,  0.0838, -0.0704, -0.0806,\n",
            "        -0.0243,  0.0533,  0.1277,  0.1403, -0.0593, -0.0639, -0.0766, -0.1163,\n",
            "         0.0661, -0.1644,  0.0422, -0.2786, -0.1006, -0.0696, -0.0761,  0.0371,\n",
            "        -0.0247,  0.0916, -0.0200, -0.0176,  0.0298, -0.0373,  0.0466, -0.1371],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-9.9023e-04, -7.7429e-03, -7.9740e-03],\n",
            "          [ 2.4844e-02,  1.8642e-03,  5.8352e-03],\n",
            "          [ 9.5089e-03, -1.6476e-02,  3.9157e-03]],\n",
            "\n",
            "         [[-2.1488e-02, -1.2330e-03, -1.4281e-02],\n",
            "          [-1.7044e-02,  9.5922e-03,  7.0445e-03],\n",
            "          [ 1.0790e-02, -7.2350e-03, -1.1357e-02]],\n",
            "\n",
            "         [[-1.1126e-03,  3.0388e-02,  2.2247e-02],\n",
            "          [-6.1184e-02, -2.3797e-02,  2.3747e-03],\n",
            "          [ 4.0678e-02, -1.0356e-01, -6.0011e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-8.5833e-03,  1.1438e-02,  2.0800e-02],\n",
            "          [-1.6565e-02, -3.9587e-02,  1.2594e-02],\n",
            "          [-1.4314e-03, -5.4257e-03,  3.6794e-02]],\n",
            "\n",
            "         [[-1.3687e-02, -2.9514e-02, -1.4745e-02],\n",
            "          [ 2.8299e-02,  2.2096e-02,  3.4839e-03],\n",
            "          [-4.3521e-03, -2.6706e-03,  1.2258e-04]],\n",
            "\n",
            "         [[ 7.6403e-03,  2.0666e-02,  3.7429e-02],\n",
            "          [ 6.9478e-03,  4.3983e-02,  1.7538e-02],\n",
            "          [-9.7797e-03, -2.4789e-02, -1.1349e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 8.4439e-02,  8.4827e-02, -5.1478e-02],\n",
            "          [ 3.5253e-02, -1.1375e-03, -1.0331e-01],\n",
            "          [-6.4078e-02, -1.2660e-01, -1.2952e-01]],\n",
            "\n",
            "         [[ 1.0628e-03, -1.4083e-02,  4.7109e-03],\n",
            "          [-2.1059e-02, -2.8778e-02,  9.9708e-03],\n",
            "          [ 1.4074e-02,  1.8691e-02,  5.8192e-02]],\n",
            "\n",
            "         [[ 2.2139e-02,  8.9027e-03,  1.4790e-02],\n",
            "          [-1.7497e-02, -5.3924e-03,  2.7834e-02],\n",
            "          [-1.3855e-02, -1.3346e-02,  1.7668e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.8032e-02, -2.3097e-02, -7.1775e-03],\n",
            "          [-3.5089e-02,  1.0861e-02,  1.3640e-02],\n",
            "          [ 6.3449e-04,  9.7476e-03,  7.3670e-03]],\n",
            "\n",
            "         [[-4.4184e-02, -1.6190e-02,  1.2243e-02],\n",
            "          [-4.0349e-02, -1.7894e-02,  2.8911e-02],\n",
            "          [-6.5176e-03, -1.0490e-02,  9.1658e-03]],\n",
            "\n",
            "         [[ 4.3621e-03,  1.3119e-02,  1.8442e-03],\n",
            "          [ 1.1555e-02, -1.3031e-02, -9.5657e-03],\n",
            "          [-2.3314e-02,  1.1609e-03,  2.6771e-03]]],\n",
            "\n",
            "\n",
            "        [[[-2.1180e-02, -6.2213e-03,  1.7609e-03],\n",
            "          [-4.7424e-03,  1.1101e-02,  1.1296e-02],\n",
            "          [-1.4529e-02,  2.9843e-02,  2.4383e-03]],\n",
            "\n",
            "         [[ 6.9183e-03,  9.2937e-03,  3.0078e-02],\n",
            "          [-4.2612e-03,  4.9560e-03, -4.7338e-03],\n",
            "          [ 3.1360e-02,  1.9035e-03, -4.7242e-03]],\n",
            "\n",
            "         [[-3.6726e-02,  5.7285e-03,  1.3919e-01],\n",
            "          [-4.2992e-02,  9.4023e-04,  7.7141e-02],\n",
            "          [-5.0050e-02, -4.9479e-03,  2.4693e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.7203e-02,  7.4712e-03, -4.2659e-02],\n",
            "          [-8.1729e-03, -9.2536e-02, -5.4934e-03],\n",
            "          [-2.5927e-02,  8.3993e-04,  7.4632e-02]],\n",
            "\n",
            "         [[ 1.8076e-02,  4.5272e-03, -1.3757e-02],\n",
            "          [-1.8939e-02, -3.2739e-02, -2.9666e-02],\n",
            "          [-2.0608e-02, -4.6167e-03,  1.3080e-03]],\n",
            "\n",
            "         [[-1.2078e-02, -2.0285e-03, -1.6998e-02],\n",
            "          [-3.4805e-02, -4.9195e-02, -3.1973e-02],\n",
            "          [-2.1021e-02, -5.1164e-03, -4.8522e-03]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 3.1791e-02,  2.2948e-02,  1.0390e-02],\n",
            "          [-1.2628e-02, -2.9320e-03,  4.2645e-03],\n",
            "          [-2.1707e-02, -1.0856e-02,  1.6094e-02]],\n",
            "\n",
            "         [[-1.4525e-03, -1.0131e-02, -4.6862e-04],\n",
            "          [ 2.2130e-02,  2.2736e-02,  5.0183e-03],\n",
            "          [-6.0125e-02, -4.3150e-02, -4.4480e-02]],\n",
            "\n",
            "         [[ 3.0761e-03,  3.4396e-03,  6.0877e-03],\n",
            "          [-1.3683e-02,  4.0576e-03, -2.6544e-02],\n",
            "          [ 6.8231e-02,  6.3474e-02, -9.3660e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.8752e-02,  1.9400e-02,  4.1691e-02],\n",
            "          [ 8.7770e-03,  8.2394e-04,  1.8619e-02],\n",
            "          [ 1.8796e-02,  6.2238e-02, -2.3801e-02]],\n",
            "\n",
            "         [[-2.9788e-02, -3.4598e-02, -2.5225e-02],\n",
            "          [ 8.4234e-03, -2.3222e-02, -9.4612e-03],\n",
            "          [ 6.9035e-03,  6.9737e-02, -1.3359e-02]],\n",
            "\n",
            "         [[ 2.6981e-03, -4.3182e-02, -1.6731e-02],\n",
            "          [ 2.5812e-02, -7.2025e-02, -6.5399e-02],\n",
            "          [ 4.6257e-02,  2.9469e-02, -1.5811e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.1079e-02,  3.8220e-02,  8.3305e-03],\n",
            "          [-5.9912e-03,  3.5584e-02, -1.7534e-03],\n",
            "          [ 1.8735e-02,  7.0859e-03, -3.5151e-03]],\n",
            "\n",
            "         [[-4.5937e-02, -7.4695e-02, -5.3608e-02],\n",
            "          [-8.6266e-03,  9.0894e-03, -3.0345e-02],\n",
            "          [-2.8158e-02, -2.1204e-02, -8.4730e-03]],\n",
            "\n",
            "         [[-7.1772e-02, -6.8582e-02,  2.5544e-02],\n",
            "          [ 5.0363e-02,  2.5269e-02,  5.6668e-02],\n",
            "          [ 2.6238e-03,  1.3871e-03, -8.4692e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.9644e-02,  1.0896e-02, -3.0402e-02],\n",
            "          [ 1.5095e-03,  5.0455e-02,  1.5597e-02],\n",
            "          [-2.1015e-02, -1.0757e-02, -3.4942e-02]],\n",
            "\n",
            "         [[-2.7573e-02,  2.9707e-02, -2.9490e-02],\n",
            "          [ 2.3301e-03, -3.9011e-02,  6.8010e-03],\n",
            "          [ 4.4006e-02,  3.5397e-02,  7.9087e-02]],\n",
            "\n",
            "         [[-2.7480e-02,  5.0337e-02,  1.4290e-02],\n",
            "          [-5.2482e-02, -4.7748e-03,  1.2988e-02],\n",
            "          [-1.8935e-02, -3.0808e-02, -1.7583e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.2280e-02,  4.7408e-02,  3.4054e-02],\n",
            "          [ 2.1445e-02,  3.8987e-03,  4.6985e-04],\n",
            "          [ 1.5159e-02,  8.2067e-03,  3.2426e-02]],\n",
            "\n",
            "         [[ 9.2653e-03,  2.3661e-02,  4.2089e-02],\n",
            "          [ 2.1976e-02,  4.6128e-02,  1.1402e-02],\n",
            "          [ 7.2843e-03,  5.2285e-02,  8.6340e-03]],\n",
            "\n",
            "         [[ 1.4022e-02,  1.2800e-02,  3.5398e-02],\n",
            "          [-4.4398e-02,  1.7399e-02, -1.5838e-02],\n",
            "          [ 3.1712e-02,  5.8679e-02, -9.3244e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.8399e-03,  7.8628e-03, -5.6169e-04],\n",
            "          [ 8.0402e-03,  1.7392e-02,  7.8734e-03],\n",
            "          [-1.7713e-02, -4.5957e-02, -9.8762e-03]],\n",
            "\n",
            "         [[-9.7569e-03, -7.5795e-03, -2.4627e-02],\n",
            "          [-8.2454e-03,  6.3065e-02, -3.2954e-03],\n",
            "          [-7.7549e-03, -1.3404e-04, -8.1337e-03]],\n",
            "\n",
            "         [[ 1.7664e-02,  1.0114e-02,  4.2687e-03],\n",
            "          [-3.7950e-03,  2.6715e-02,  2.0121e-02],\n",
            "          [ 1.6868e-02, -6.6515e-03, -1.1107e-02]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.3323, 0.2908, 0.3246, 0.3435, 0.3011, 0.3054, 0.3041, 0.3539, 0.2862,\n",
            "        0.3601, 0.2970, 0.3381, 0.2565, 0.3276, 0.3030, 0.4085, 0.3519, 0.4218,\n",
            "        0.3055, 0.2551, 0.3425, 0.3215, 0.3366, 0.2700, 0.2849, 0.3954, 0.3166,\n",
            "        0.3286, 0.3515, 0.3953, 0.2768, 0.3625, 0.1988, 0.2717, 0.3355, 0.2797,\n",
            "        0.2510, 0.3832, 0.3266, 0.3263, 0.3681, 0.3401, 0.3651, 0.3391, 0.3071,\n",
            "        0.3231, 0.3691, 0.2410, 0.3536, 0.3189, 0.3238, 0.3611, 0.3086, 0.3309,\n",
            "        0.3886, 0.4362, 0.4550, 0.2962, 0.3071, 0.3386, 0.3317, 0.3228, 0.2393,\n",
            "        0.3147, 0.2738, 0.3218, 0.3198, 0.3411, 0.3611, 0.2833, 0.3035, 0.3183,\n",
            "        0.3146, 0.3890, 0.2607, 0.3479, 0.3236, 0.3709, 0.2592, 0.3742, 0.2555,\n",
            "        0.2966, 0.3505, 0.3165, 0.2808, 0.2660, 0.2817, 0.4795, 0.3372, 0.2723,\n",
            "        0.2955, 0.3225, 0.2470, 0.3160, 0.3515, 0.3131, 0.3372, 0.2837, 0.3540,\n",
            "        0.2897, 0.2490, 0.3019, 0.3114, 0.3510, 0.3022, 0.3617, 0.2859, 0.2831,\n",
            "        0.3243, 0.2769, 0.3314, 0.2394, 0.2932, 0.2788, 0.2686, 0.3194, 0.3542,\n",
            "        0.2683, 0.2955, 0.2924, 0.3538, 0.4256, 0.3603, 0.3013, 0.2763, 0.4354,\n",
            "        0.3991, 0.2694], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1735, -0.2337, -0.3383, -0.0806, -0.1920, -0.0621, -0.1885, -0.2830,\n",
            "        -0.1680, -0.1796, -0.2645, -0.1983, -0.1183, -0.2432, -0.1706, -0.3090,\n",
            "        -0.2661, -0.4040, -0.1949, -0.1392, -0.2449, -0.1242, -0.2012, -0.1901,\n",
            "        -0.1014, -0.3468, -0.2245, -0.3272, -0.3057, -0.3289, -0.1532, -0.1967,\n",
            "        -0.0667, -0.3281, -0.1418, -0.1527, -0.0987, -0.3243, -0.2252, -0.3462,\n",
            "        -0.2284, -0.2263, -0.1810, -0.1564, -0.1730, -0.1507, -0.2913, -0.1643,\n",
            "        -0.1998, -0.1532, -0.2211, -0.2247, -0.0913, -0.1563, -0.2453, -0.4854,\n",
            "        -0.4428, -0.1021, -0.1615, -0.2125, -0.2239, -0.1952, -0.0447, -0.1733,\n",
            "        -0.1178, -0.4775, -0.2110, -0.2305, -0.1795, -0.1582, -0.2008, -0.2041,\n",
            "        -0.1974, -0.2750, -0.0395, -0.2161, -0.2786, -0.2626, -0.0997, -0.2953,\n",
            "        -0.1431, -0.1448, -0.1894, -0.1283, -0.1807, -0.1144, -0.1308, -0.4154,\n",
            "        -0.2324, -0.1376, -0.1154, -0.2099, -0.0966, -0.1669, -0.3835, -0.2545,\n",
            "        -0.1603, -0.1904, -0.2420, -0.1658, -0.1133, -0.1498, -0.1213, -0.2318,\n",
            "        -0.2017, -0.3827, -0.1491, -0.1174, -0.1261, -0.2031, -0.1832, -0.2274,\n",
            "        -0.1281, -0.2557, -0.1400, -0.0723, -0.2212, -0.1486, -0.2914, -0.1116,\n",
            "        -0.2194, -0.4898, -0.3693, -0.1437, -0.1232, -0.3723, -0.6794, -0.1536],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-1.6153e-02,  5.0134e-03, -9.0186e-04],\n",
            "          [-8.8386e-03, -1.9390e-02, -2.4174e-02],\n",
            "          [ 6.3052e-03,  1.0245e-02, -1.3816e-02]],\n",
            "\n",
            "         [[-1.0979e-02,  2.6164e-03,  2.3656e-02],\n",
            "          [-1.7687e-02,  1.9861e-02,  6.4150e-02],\n",
            "          [ 6.0224e-03,  7.6342e-02,  1.0215e-01]],\n",
            "\n",
            "         [[-8.1113e-03,  6.8414e-03,  2.5436e-02],\n",
            "          [-8.0696e-03,  9.2929e-03,  8.2899e-03],\n",
            "          [ 7.7306e-03,  1.2159e-02,  7.1625e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.5175e-02,  6.2196e-03,  2.1798e-02],\n",
            "          [-1.5199e-02, -8.5439e-02, -2.4713e-02],\n",
            "          [-1.8460e-02, -4.9767e-02, -1.6818e-03]],\n",
            "\n",
            "         [[ 3.0728e-02,  3.9962e-02,  3.1253e-02],\n",
            "          [-1.8738e-02, -6.7510e-02, -2.7649e-02],\n",
            "          [ 2.8429e-02,  3.1854e-02,  1.0543e-02]],\n",
            "\n",
            "         [[-1.8320e-02, -1.5854e-02, -1.0685e-02],\n",
            "          [-2.7442e-02, -3.0616e-02, -1.0485e-02],\n",
            "          [-1.5122e-02, -1.0595e-02, -2.5322e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.6868e-03,  3.0996e-02,  4.2763e-02],\n",
            "          [ 4.6537e-02,  4.8606e-02,  2.3800e-03],\n",
            "          [ 1.6654e-02,  1.2900e-02, -1.8230e-02]],\n",
            "\n",
            "         [[-1.0441e-02, -1.5934e-03, -1.6128e-02],\n",
            "          [-1.2799e-02,  4.9570e-03, -1.4585e-02],\n",
            "          [-2.3553e-02, -3.7023e-03, -1.4399e-02]],\n",
            "\n",
            "         [[ 1.0338e-02, -1.7560e-02, -3.3046e-02],\n",
            "          [-3.2090e-02, -5.9258e-03,  2.0201e-03],\n",
            "          [-4.1428e-02,  4.9121e-03,  1.6906e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.9525e-02, -4.6498e-02, -5.9916e-02],\n",
            "          [-2.6670e-02, -1.9079e-02, -2.9419e-02],\n",
            "          [-3.9683e-03,  1.9405e-02,  7.3317e-03]],\n",
            "\n",
            "         [[ 1.4293e-02,  1.5643e-02,  5.8117e-04],\n",
            "          [ 5.1493e-03,  7.4332e-03, -3.6928e-03],\n",
            "          [-1.3522e-02, -8.5536e-03, -2.1259e-03]],\n",
            "\n",
            "         [[-3.0908e-02, -1.9839e-02, -1.9375e-02],\n",
            "          [-1.0368e-02, -2.4294e-02,  2.4103e-04],\n",
            "          [-1.9275e-02, -2.9707e-02, -1.5623e-02]]],\n",
            "\n",
            "\n",
            "        [[[-4.9212e-02, -2.9588e-02,  8.8023e-02],\n",
            "          [ 4.7453e-03,  4.3564e-02,  9.3115e-02],\n",
            "          [ 7.4083e-02,  4.2868e-02, -5.1033e-02]],\n",
            "\n",
            "         [[ 6.6992e-03,  2.1676e-02, -5.4254e-04],\n",
            "          [ 1.9286e-02,  1.0920e-02, -4.5440e-03],\n",
            "          [ 3.1075e-02, -1.7168e-03, -2.7603e-02]],\n",
            "\n",
            "         [[ 6.0096e-02, -2.9359e-02, -5.8911e-02],\n",
            "          [-1.9133e-02, -8.1624e-02, -2.2553e-02],\n",
            "          [ 1.1597e-02,  2.5092e-02,  1.2130e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.4307e-03, -2.3130e-02,  9.6233e-03],\n",
            "          [-4.3785e-02, -2.6735e-02,  2.1993e-02],\n",
            "          [-3.5919e-02, -4.1009e-02, -2.1860e-02]],\n",
            "\n",
            "         [[ 3.3705e-02,  6.2938e-02,  4.3502e-02],\n",
            "          [ 1.1111e-03,  1.9243e-02, -1.9707e-03],\n",
            "          [-1.1493e-02, -5.3445e-02, -9.6676e-03]],\n",
            "\n",
            "         [[-2.6664e-03, -2.6954e-02, -1.7667e-02],\n",
            "          [-8.3382e-03,  8.9920e-03,  8.1260e-04],\n",
            "          [-2.6832e-02, -3.5991e-02, -4.2495e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1.8876e-03, -2.2728e-02, -4.2991e-03],\n",
            "          [-9.2231e-03, -3.4333e-02, -1.3392e-02],\n",
            "          [-1.2774e-02, -1.1435e-02,  1.5617e-02]],\n",
            "\n",
            "         [[ 1.0703e-02,  1.2792e-02,  2.2662e-02],\n",
            "          [ 7.3185e-03, -1.7847e-02,  1.0674e-02],\n",
            "          [-1.5936e-02, -1.9318e-02,  2.1768e-02]],\n",
            "\n",
            "         [[-7.3009e-03,  3.0234e-02, -1.1899e-02],\n",
            "          [-2.6099e-02,  3.7452e-03,  3.2776e-02],\n",
            "          [-3.3101e-02, -7.1923e-03,  1.6559e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.2818e-02, -1.0021e-01, -4.7012e-02],\n",
            "          [ 2.8293e-03,  4.1410e-02, -1.1391e-02],\n",
            "          [-1.1152e-02, -5.5861e-03,  1.9968e-02]],\n",
            "\n",
            "         [[-2.3932e-02, -3.0687e-02, -1.1756e-03],\n",
            "          [ 1.5311e-03, -3.5002e-02, -2.4414e-02],\n",
            "          [-8.7575e-03, -7.7842e-02, -3.8842e-02]],\n",
            "\n",
            "         [[ 2.6107e-02,  1.5406e-02,  1.7569e-02],\n",
            "          [-1.5130e-02, -4.8687e-03,  3.0773e-03],\n",
            "          [-1.3470e-02, -9.3201e-03, -4.8982e-03]]],\n",
            "\n",
            "\n",
            "        [[[-2.0228e-02, -3.0006e-02, -9.8419e-03],\n",
            "          [-3.8676e-02, -3.3481e-02, -7.4265e-03],\n",
            "          [-2.8935e-02, -3.2037e-02,  2.9245e-03]],\n",
            "\n",
            "         [[-1.2900e-02,  3.8046e-03,  1.5940e-02],\n",
            "          [-2.4030e-02,  2.0666e-03,  5.7250e-03],\n",
            "          [ 6.9989e-03,  1.2192e-02,  1.5406e-02]],\n",
            "\n",
            "         [[-1.5018e-02, -9.0988e-03,  2.4450e-02],\n",
            "          [ 1.0039e-02,  1.2561e-02,  2.6997e-02],\n",
            "          [ 2.9556e-02,  1.9463e-02, -2.6584e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.8481e-02,  3.9417e-04,  9.9768e-03],\n",
            "          [-4.5447e-03,  1.2307e-02,  3.5507e-02],\n",
            "          [-1.1873e-03, -2.6185e-03,  1.1547e-02]],\n",
            "\n",
            "         [[ 4.6292e-03, -1.3690e-02, -1.0171e-02],\n",
            "          [ 1.2104e-02,  1.6793e-02,  1.3003e-02],\n",
            "          [ 1.3328e-03,  3.4701e-03,  1.7323e-02]],\n",
            "\n",
            "         [[-8.7332e-05,  5.8646e-03, -3.5117e-03],\n",
            "          [ 3.8112e-03, -7.1828e-03, -1.1407e-02],\n",
            "          [ 1.9705e-02,  2.0556e-02,  5.7084e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 3.6998e-02,  3.2616e-02, -9.4535e-04],\n",
            "          [-2.9484e-02, -2.3441e-02, -2.8085e-02],\n",
            "          [-2.5451e-02,  3.9048e-02,  3.6686e-02]],\n",
            "\n",
            "         [[-1.8732e-02, -1.5352e-02,  1.1149e-02],\n",
            "          [-2.1324e-03, -2.3177e-02,  1.7628e-02],\n",
            "          [-4.0012e-03,  1.5463e-02,  9.2496e-03]],\n",
            "\n",
            "         [[-2.9346e-02,  7.7071e-03, -5.6520e-03],\n",
            "          [-2.3611e-02, -1.9390e-03,  2.0221e-02],\n",
            "          [ 8.0955e-03, -2.3268e-02, -2.8827e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.3532e-02, -2.9092e-02, -4.0045e-02],\n",
            "          [ 2.6530e-03, -2.0568e-02,  1.3075e-02],\n",
            "          [ 1.6061e-02, -5.5725e-02, -4.9167e-02]],\n",
            "\n",
            "         [[-7.9132e-03,  2.1466e-02,  2.0913e-02],\n",
            "          [-1.7259e-02, -2.5851e-02,  2.7177e-03],\n",
            "          [-4.6532e-02, -2.4846e-02, -1.9911e-02]],\n",
            "\n",
            "         [[-5.0350e-02, -2.5574e-02,  1.7763e-02],\n",
            "          [-3.4474e-02,  5.5247e-03, -2.7754e-02],\n",
            "          [-2.0743e-02, -2.2332e-02, -4.3512e-02]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.1194, 0.1625, 0.3084, 0.2931, 0.2957, 0.5263, 0.4038, 0.2024, 0.3401,\n",
            "        0.1982, 0.2559, 0.2311, 0.1630, 0.2891, 0.2248, 0.2311, 0.2417, 0.2187,\n",
            "        0.1922, 0.3103, 0.2015, 0.4802, 0.2481, 0.3898, 0.3204, 0.4035, 0.2617,\n",
            "        0.1551, 0.2256, 0.2117, 0.2708, 0.3537, 0.2505, 0.1843, 0.2465, 0.6501,\n",
            "        0.3898, 0.4289, 0.1799, 0.1604, 0.1775, 0.3600, 0.2694, 0.1283, 0.1662,\n",
            "        0.1716, 0.1837, 0.1710, 0.4178, 0.3249, 0.1759, 0.4717, 0.4115, 0.1995,\n",
            "        0.2025, 0.1492, 0.2860, 0.1072, 0.3649, 0.1906, 0.5369, 0.2400, 0.4411,\n",
            "        0.1702, 0.1993, 0.2045, 0.1972, 0.4041, 0.3034, 0.6168, 0.2284, 0.3228,\n",
            "        0.4547, 0.4370, 0.1570, 0.4057, 0.5791, 0.2338, 0.1586, 0.3130, 0.2201,\n",
            "        0.3195, 0.1166, 0.2517, 0.2184, 0.0989, 0.3116, 0.2613, 0.3277, 0.1778,\n",
            "        0.2718, 0.4174, 0.5140, 0.2136, 0.1905, 0.2898, 0.2472, 0.1341, 0.6212,\n",
            "        0.1810, 0.2394, 0.1417, 0.1759, 0.2827, 0.1987, 0.3775, 0.3749, 0.1274,\n",
            "        0.3656, 0.4305, 0.4212, 0.2673, 0.2016, 0.5098, 0.1449, 0.4408, 0.3583,\n",
            "        0.2503, 0.5682, 0.2518, 0.1392, 0.0617, 0.3406, 0.1313, 0.4586, 0.2914,\n",
            "        0.1326, 0.3915], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1403, -0.0889, -0.4147, -0.2264, -0.0737, -0.3534, -0.3379, -0.0752,\n",
            "        -0.1791,  0.0448, -0.2842, -0.1765, -0.1591, -0.0675, -0.1543, -0.1061,\n",
            "        -0.2334, -0.0981, -0.0908, -0.0567, -0.1908, -0.2055, -0.2704, -0.1883,\n",
            "        -0.3570, -0.1125, -0.1632, -0.0211, -0.1687, -0.2124, -0.1713, -0.0872,\n",
            "        -0.2194, -0.1888, -0.2954, -0.4570, -0.0226, -0.0527,  0.0406, -0.0609,\n",
            "        -0.0456, -0.1176, -0.0145,  0.0318, -0.2046, -0.0953, -0.0496, -0.1051,\n",
            "        -0.0793, -0.1933, -0.1467, -0.3215, -0.3257, -0.2287, -0.0356, -0.1869,\n",
            "        -0.1932, -0.0771,  0.2768, -0.0656, -0.0895, -0.2548, -0.2365,  0.0021,\n",
            "        -0.0987, -0.3178,  0.1613,  0.0006, -0.2347, -0.4150, -0.1310, -0.3142,\n",
            "        -0.2582, -0.5400,  0.0772, -0.2546, -0.4454, -0.0262, -0.0937, -0.2201,\n",
            "        -0.2044, -0.0155, -0.0893, -0.2167,  0.1112, -0.0619, -0.1217, -0.1593,\n",
            "        -0.1317, -0.1717, -0.3729, -0.3354, -0.3414,  0.0358, -0.2067, -0.1087,\n",
            "         0.0141, -0.0338, -0.2129, -0.1122, -0.1627, -0.2000,  0.0908, -0.0041,\n",
            "        -0.1313, -0.2942,  0.0160, -0.1065, -0.1289, -0.1699, -0.1721, -0.1809,\n",
            "        -0.2295, -0.3611, -0.1746, -0.3540, -0.1554, -0.2709, -0.2607,  0.0084,\n",
            "        -0.0311, -0.0022, -0.0831,  0.0380, -0.4893, -0.2749,  0.1245, -0.1272],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-0.0159, -0.0166, -0.0159],\n",
            "          [-0.0053,  0.0151,  0.0099],\n",
            "          [-0.0149,  0.0004, -0.0114]],\n",
            "\n",
            "         [[-0.0095, -0.0186, -0.0061],\n",
            "          [ 0.0098, -0.0123, -0.0053],\n",
            "          [ 0.0071, -0.0161, -0.0071]],\n",
            "\n",
            "         [[-0.0227, -0.0377, -0.0337],\n",
            "          [-0.0316, -0.0580, -0.0391],\n",
            "          [-0.0346, -0.0388, -0.0157]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0049,  0.0080,  0.0144],\n",
            "          [-0.0015,  0.0242,  0.0056],\n",
            "          [-0.0044,  0.0062,  0.0069]],\n",
            "\n",
            "         [[ 0.0160, -0.0120, -0.0013],\n",
            "          [ 0.0096,  0.0057,  0.0016],\n",
            "          [-0.0099, -0.0136, -0.0064]],\n",
            "\n",
            "         [[ 0.0534,  0.0464,  0.0248],\n",
            "          [ 0.0341, -0.0029, -0.0041],\n",
            "          [-0.0140, -0.0046, -0.0142]]],\n",
            "\n",
            "\n",
            "        [[[-0.0012, -0.0186, -0.0345],\n",
            "          [ 0.0050, -0.0117, -0.0333],\n",
            "          [ 0.0060, -0.0162, -0.0175]],\n",
            "\n",
            "         [[ 0.0107,  0.0140, -0.0192],\n",
            "          [ 0.0029,  0.0126,  0.0073],\n",
            "          [-0.0168, -0.0187, -0.0014]],\n",
            "\n",
            "         [[-0.0219,  0.0063,  0.0159],\n",
            "          [-0.0035,  0.0057,  0.0311],\n",
            "          [ 0.0023,  0.0032,  0.0175]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0162, -0.0078,  0.0077],\n",
            "          [-0.0015, -0.0122, -0.0093],\n",
            "          [ 0.0009, -0.0022,  0.0074]],\n",
            "\n",
            "         [[ 0.0029, -0.0236,  0.0058],\n",
            "          [ 0.0143, -0.0169, -0.0062],\n",
            "          [-0.0077, -0.0323, -0.0337]],\n",
            "\n",
            "         [[ 0.0087,  0.0140,  0.0081],\n",
            "          [-0.0034,  0.0105,  0.0150],\n",
            "          [ 0.0189,  0.0309,  0.0256]]],\n",
            "\n",
            "\n",
            "        [[[-0.0358, -0.0226, -0.0144],\n",
            "          [-0.0075, -0.0221,  0.0111],\n",
            "          [-0.0036, -0.0148, -0.0164]],\n",
            "\n",
            "         [[-0.0146, -0.0307, -0.0204],\n",
            "          [-0.0285, -0.0453, -0.0579],\n",
            "          [ 0.0288, -0.0152, -0.0245]],\n",
            "\n",
            "         [[ 0.0174,  0.0199, -0.0046],\n",
            "          [ 0.0178,  0.0236,  0.0136],\n",
            "          [ 0.0293,  0.0434,  0.0186]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0079, -0.0122, -0.0158],\n",
            "          [-0.0107, -0.0314, -0.0075],\n",
            "          [-0.0070, -0.0173, -0.0308]],\n",
            "\n",
            "         [[-0.0103, -0.0118, -0.0171],\n",
            "          [-0.0264, -0.0015,  0.0280],\n",
            "          [-0.0089,  0.0054,  0.0097]],\n",
            "\n",
            "         [[ 0.0145, -0.0315, -0.0197],\n",
            "          [-0.0149, -0.0177,  0.0077],\n",
            "          [ 0.0125,  0.0071, -0.0062]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0053,  0.0255,  0.0071],\n",
            "          [ 0.0197,  0.0270,  0.0420],\n",
            "          [ 0.0412,  0.0223,  0.0350]],\n",
            "\n",
            "         [[ 0.0026,  0.0041,  0.0106],\n",
            "          [ 0.0046,  0.0203,  0.0081],\n",
            "          [ 0.0145, -0.0030, -0.0197]],\n",
            "\n",
            "         [[-0.0040,  0.0255,  0.0023],\n",
            "          [-0.0152,  0.0260,  0.0083],\n",
            "          [-0.0021,  0.0269,  0.0032]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0055, -0.0049,  0.0183],\n",
            "          [ 0.0085,  0.0023,  0.0077],\n",
            "          [ 0.0082,  0.0096,  0.0215]],\n",
            "\n",
            "         [[-0.0177,  0.0099, -0.0129],\n",
            "          [-0.0127,  0.0096, -0.0124],\n",
            "          [ 0.0090,  0.0493,  0.0362]],\n",
            "\n",
            "         [[ 0.0123,  0.0184, -0.0178],\n",
            "          [ 0.0058, -0.0058, -0.0117],\n",
            "          [ 0.0034, -0.0103, -0.0425]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0316,  0.0765,  0.0443],\n",
            "          [ 0.0940,  0.1480,  0.1510],\n",
            "          [ 0.0665,  0.1386,  0.1132]],\n",
            "\n",
            "         [[-0.0282, -0.0041, -0.0200],\n",
            "          [-0.0193, -0.0012,  0.0107],\n",
            "          [-0.0165, -0.0028,  0.0008]],\n",
            "\n",
            "         [[-0.0122, -0.0322, -0.0153],\n",
            "          [-0.0009, -0.0123, -0.0039],\n",
            "          [-0.0193, -0.0107, -0.0211]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0030,  0.0066, -0.0003],\n",
            "          [ 0.0146, -0.0167,  0.0142],\n",
            "          [ 0.0227,  0.0120,  0.0072]],\n",
            "\n",
            "         [[ 0.0039, -0.0197,  0.0111],\n",
            "          [-0.0251, -0.0384, -0.0447],\n",
            "          [-0.0359, -0.0981, -0.0684]],\n",
            "\n",
            "         [[-0.0085,  0.0023,  0.0031],\n",
            "          [ 0.0038,  0.0182,  0.0069],\n",
            "          [ 0.0089,  0.0080,  0.0126]]],\n",
            "\n",
            "\n",
            "        [[[-0.0130, -0.0090,  0.0011],\n",
            "          [-0.0260, -0.0195, -0.0094],\n",
            "          [ 0.0048,  0.0024,  0.0106]],\n",
            "\n",
            "         [[-0.0025, -0.0140, -0.0286],\n",
            "          [-0.0024,  0.0011, -0.0233],\n",
            "          [ 0.0123,  0.0008,  0.0014]],\n",
            "\n",
            "         [[-0.0490, -0.0439, -0.0579],\n",
            "          [-0.0359, -0.0365, -0.0386],\n",
            "          [-0.0410, -0.0333, -0.0137]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0118, -0.0081, -0.0158],\n",
            "          [-0.0272, -0.0285,  0.0075],\n",
            "          [-0.0244,  0.0139,  0.0061]],\n",
            "\n",
            "         [[-0.0273,  0.0197,  0.0222],\n",
            "          [-0.0376,  0.0208,  0.0187],\n",
            "          [-0.0446, -0.0044, -0.0168]],\n",
            "\n",
            "         [[ 0.0199,  0.0268,  0.0120],\n",
            "          [ 0.0203,  0.0215,  0.0025],\n",
            "          [ 0.0002, -0.0076, -0.0196]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.2856, 0.2425, 0.3032, 0.3168, 0.3011, 0.3475, 0.3076, 0.3105, 0.3646,\n",
            "        0.3255, 0.2195, 0.3167, 0.2674, 0.3104, 0.3026, 0.3443, 0.2915, 0.3379,\n",
            "        0.2887, 0.2996, 0.3588, 0.3164, 0.2882, 0.2917, 0.3492, 0.3749, 0.3587,\n",
            "        0.3166, 0.2756, 0.2978, 0.3364, 0.2893, 0.3106, 0.2506, 0.3460, 0.3621,\n",
            "        0.2570, 0.3695, 0.2935, 0.3286, 0.3243, 0.3188, 0.3093, 0.3314, 0.3550,\n",
            "        0.2978, 0.2737, 0.3023, 0.3179, 0.2831, 0.3065, 0.3390, 0.3053, 0.3099,\n",
            "        0.3017, 0.3472, 0.3034, 0.2935, 0.3352, 0.3676, 0.3163, 0.3404, 0.3078,\n",
            "        0.2819, 0.3794, 0.3083, 0.2778, 0.3363, 0.2284, 0.3259, 0.2790, 0.3072,\n",
            "        0.2975, 0.3847, 0.3372, 0.2253, 0.2827, 0.3737, 0.2796, 0.3485, 0.3879,\n",
            "        0.3288, 0.3340, 0.3335, 0.2756, 0.3500, 0.2897, 0.2798, 0.2907, 0.3220,\n",
            "        0.3824, 0.3522, 0.3278, 0.3689, 0.3147, 0.3600, 0.3123, 0.2519, 0.2355,\n",
            "        0.3211, 0.3203, 0.3345, 0.2768, 0.3341, 0.3153, 0.3175, 0.2224, 0.2956,\n",
            "        0.3206, 0.2658, 0.3662, 0.2715, 0.3655, 0.3427, 0.2820, 0.2754, 0.4669,\n",
            "        0.3090, 0.3468, 0.3144, 0.3220, 0.2765, 0.3301, 0.3219, 0.3152, 0.2813,\n",
            "        0.2497, 0.3514, 0.3264, 0.3014, 0.2734, 0.3522, 0.3831, 0.3028, 0.2940,\n",
            "        0.2825, 0.3099, 0.2373, 0.2705, 0.4189, 0.2985, 0.3841, 0.2754, 0.3091,\n",
            "        0.3169, 0.2824, 0.2749, 0.3493, 0.4018, 0.3108, 0.2176, 0.2821, 0.3199,\n",
            "        0.3358, 0.2468, 0.3332, 0.2876, 0.2964, 0.2385, 0.3451, 0.3081, 0.2760,\n",
            "        0.2533, 0.2576, 0.3092, 0.2950, 0.3089, 0.3113, 0.3475, 0.3172, 0.2474,\n",
            "        0.3371, 0.3450, 0.3189, 0.3150, 0.3008, 0.2694, 0.3730, 0.3235, 0.2988,\n",
            "        0.2812, 0.3245, 0.3630, 0.2843, 0.3533, 0.3451, 0.3244, 0.3524, 0.3118,\n",
            "        0.3429, 0.3215, 0.2748, 0.3287, 0.3656, 0.2901, 0.2523, 0.3284, 0.2523,\n",
            "        0.3426, 0.2851, 0.2918, 0.2497, 0.5159, 0.3026, 0.2743, 0.2379, 0.3524,\n",
            "        0.3394, 0.2264, 0.2652, 0.3759, 0.3777, 0.2459, 0.3046, 0.3067, 0.3775,\n",
            "        0.2976, 0.3552, 0.2696, 0.2649, 0.2872, 0.2985, 0.2867, 0.3676, 0.3494,\n",
            "        0.3823, 0.3246, 0.3567, 0.2662, 0.3357, 0.2935, 0.2987, 0.2664, 0.3019,\n",
            "        0.3175, 0.2436, 0.3274, 0.2764, 0.2466, 0.2876, 0.3060, 0.3157, 0.3329,\n",
            "        0.2984, 0.2961, 0.3309, 0.3729, 0.3238, 0.3491, 0.3342, 0.3037, 0.3578,\n",
            "        0.2849, 0.2827, 0.2809, 0.3249], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0915,  0.0189, -0.1235, -0.0613, -0.1003, -0.1306, -0.1473, -0.1079,\n",
            "        -0.2438, -0.1113,  0.1361, -0.1477,  0.0387, -0.0907,  0.0352, -0.1851,\n",
            "        -0.1319, -0.1746, -0.0815, -0.1004, -0.3394, -0.1712, -0.0807, -0.1228,\n",
            "        -0.2263, -0.1503, -0.2314, -0.2327, -0.0854, -0.0802, -0.0716, -0.0839,\n",
            "        -0.0592,  0.0358, -0.0322, -0.2197,  0.0027, -0.1471, -0.0264, -0.1886,\n",
            "        -0.2417, -0.1494, -0.1904, -0.1089, -0.2657, -0.1362, -0.0487, -0.1340,\n",
            "        -0.0930, -0.0064, -0.1721, -0.1476, -0.1714,  0.0336, -0.1011, -0.1761,\n",
            "        -0.1184, -0.0482, -0.3260, -0.1555, -0.0169, -0.2373, -0.1015, -0.1051,\n",
            "        -0.2738, -0.1917, -0.0503, -0.1098,  0.1484, -0.2282, -0.0700, -0.1427,\n",
            "        -0.1417, -0.3096, -0.2043,  0.0269, -0.0779, -0.0842, -0.0464, -0.1429,\n",
            "        -0.3917,  0.0257, -0.1779, -0.0993, -0.0507, -0.2222, -0.0951, -0.0861,\n",
            "        -0.0743, -0.1666, -0.2054, -0.1782, -0.1150, -0.2525, -0.0694, -0.0536,\n",
            "        -0.0499, -0.0311,  0.1212, -0.0988, -0.1570, -0.3093, -0.0797, -0.0994,\n",
            "        -0.1774, -0.0505,  0.0766, -0.0480, -0.1278, -0.0651, -0.1737,  0.0303,\n",
            "        -0.1334, -0.2435, -0.0746, -0.0365, -0.1843, -0.0887, -0.1924, -0.1110,\n",
            "        -0.1458, -0.0895, -0.0956, -0.2042, -0.1338, -0.0637, -0.0699, -0.1656,\n",
            "        -0.1521, -0.1317, -0.0826, -0.2470, -0.1174, -0.1475, -0.0840, -0.0681,\n",
            "        -0.1789,  0.0288, -0.0362, -0.3005, -0.1441, -0.0812, -0.0492, -0.0657,\n",
            "        -0.1249, -0.1104,  0.0187, -0.1351, -0.1944, -0.0909,  0.2067, -0.1081,\n",
            "        -0.2499, -0.0999,  0.0507, -0.1899, -0.0369, -0.1432,  0.1279, -0.1782,\n",
            "        -0.1172, -0.0099,  0.0785, -0.0681, -0.0365, -0.1596, -0.1606, -0.0922,\n",
            "        -0.1773, -0.1788,  0.0306, -0.1101, -0.1355, -0.2244, -0.0860, -0.1232,\n",
            "        -0.0927, -0.1666, -0.1393, -0.0898, -0.0614, -0.1740, -0.2503, -0.0593,\n",
            "        -0.1272, -0.1422, -0.0743, -0.2208, -0.2207, -0.2742, -0.1302, -0.0916,\n",
            "        -0.1696, -0.2481, -0.1524,  0.0410, -0.1077,  0.0408, -0.1915, -0.0697,\n",
            "        -0.1049, -0.0110, -0.3257, -0.1336, -0.1021,  0.0128, -0.2717, -0.1245,\n",
            "         0.0288, -0.1025, -0.2405, -0.1476,  0.1008, -0.0220, -0.0983, -0.4417,\n",
            "        -0.0774, -0.3207, -0.0272, -0.0726, -0.0608, -0.0430, -0.0872, -0.1280,\n",
            "        -0.1608, -0.1529, -0.1745, -0.1702, -0.0486, -0.1459, -0.0552, -0.0808,\n",
            "        -0.0264, -0.0952, -0.1126, -0.0452, -0.0837, -0.0331,  0.0127, -0.0865,\n",
            "        -0.1446, -0.0732, -0.2160, -0.0952, -0.1297, -0.2008, -0.2135, -0.2204,\n",
            "        -0.2381, -0.1787, -0.1386, -0.1901, -0.0981, -0.0850, -0.0761, -0.0586],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-0.0093, -0.0339, -0.0119],\n",
            "          [-0.0246, -0.0798, -0.0487],\n",
            "          [-0.0435, -0.0801, -0.0653]],\n",
            "\n",
            "         [[-0.0289,  0.0002, -0.0286],\n",
            "          [ 0.0099,  0.0103, -0.0177],\n",
            "          [-0.0107,  0.0028, -0.0125]],\n",
            "\n",
            "         [[-0.0147,  0.0226,  0.0044],\n",
            "          [ 0.0155,  0.0109, -0.0040],\n",
            "          [-0.0208, -0.0180, -0.0173]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0003, -0.0041, -0.0008],\n",
            "          [-0.0217, -0.0223, -0.0299],\n",
            "          [ 0.0105,  0.0035, -0.0114]],\n",
            "\n",
            "         [[ 0.0097,  0.0184,  0.0370],\n",
            "          [ 0.0037,  0.0104,  0.0152],\n",
            "          [ 0.0084,  0.0183,  0.0302]],\n",
            "\n",
            "         [[ 0.0014,  0.0084,  0.0097],\n",
            "          [ 0.0265,  0.0415,  0.0553],\n",
            "          [ 0.0169,  0.0610,  0.0563]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0117,  0.0165,  0.0051],\n",
            "          [ 0.0294,  0.0204,  0.0216],\n",
            "          [ 0.0079,  0.0133,  0.0117]],\n",
            "\n",
            "         [[-0.0153, -0.0213, -0.0090],\n",
            "          [-0.0292, -0.0516, -0.0436],\n",
            "          [-0.0045, -0.0372, -0.0420]],\n",
            "\n",
            "         [[ 0.0003,  0.0398, -0.0001],\n",
            "          [ 0.0129,  0.0332,  0.0163],\n",
            "          [-0.0096, -0.0057, -0.0170]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0447,  0.0082,  0.0169],\n",
            "          [-0.0066, -0.0371, -0.0059],\n",
            "          [-0.0239, -0.0607, -0.0289]],\n",
            "\n",
            "         [[-0.0057, -0.0423, -0.0219],\n",
            "          [-0.0228, -0.0314, -0.0583],\n",
            "          [-0.0196, -0.0530, -0.0485]],\n",
            "\n",
            "         [[ 0.0065,  0.0033,  0.0093],\n",
            "          [ 0.0010, -0.0049, -0.0110],\n",
            "          [-0.0160, -0.0101, -0.0146]]],\n",
            "\n",
            "\n",
            "        [[[-0.0061, -0.0067, -0.0069],\n",
            "          [-0.0052, -0.0089, -0.0143],\n",
            "          [-0.0115, -0.0171, -0.0237]],\n",
            "\n",
            "         [[ 0.0399,  0.0167,  0.0210],\n",
            "          [ 0.0165, -0.0262, -0.0116],\n",
            "          [ 0.0059, -0.0206, -0.0153]],\n",
            "\n",
            "         [[ 0.0060,  0.0242,  0.0207],\n",
            "          [ 0.0050, -0.0062,  0.0148],\n",
            "          [ 0.0099, -0.0279, -0.0054]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0097, -0.0017, -0.0101],\n",
            "          [-0.0006,  0.0434,  0.0381],\n",
            "          [ 0.0038,  0.0450,  0.0392]],\n",
            "\n",
            "         [[ 0.0134,  0.0069,  0.0196],\n",
            "          [ 0.0068,  0.0250,  0.0152],\n",
            "          [ 0.0018, -0.0044,  0.0037]],\n",
            "\n",
            "         [[-0.0174, -0.0163, -0.0240],\n",
            "          [-0.0197, -0.0174, -0.0178],\n",
            "          [-0.0300, -0.0137, -0.0211]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0066,  0.0061,  0.0284],\n",
            "          [-0.0130, -0.0509, -0.0204],\n",
            "          [-0.0154, -0.0149, -0.0181]],\n",
            "\n",
            "         [[ 0.0123,  0.0325,  0.0227],\n",
            "          [-0.0042, -0.0181,  0.0022],\n",
            "          [ 0.0029, -0.0174,  0.0032]],\n",
            "\n",
            "         [[-0.0110,  0.0111, -0.0142],\n",
            "          [ 0.0082,  0.0301,  0.0416],\n",
            "          [ 0.0006,  0.0002,  0.0213]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0008,  0.0266,  0.0118],\n",
            "          [-0.0159, -0.0074, -0.0051],\n",
            "          [ 0.0042, -0.0069, -0.0073]],\n",
            "\n",
            "         [[-0.0070,  0.0010,  0.0019],\n",
            "          [ 0.0061,  0.0510, -0.0035],\n",
            "          [-0.0081, -0.0303, -0.0174]],\n",
            "\n",
            "         [[-0.0100,  0.0003, -0.0011],\n",
            "          [-0.0015, -0.0297, -0.0195],\n",
            "          [-0.0010,  0.0049,  0.0151]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0161,  0.0126,  0.0055],\n",
            "          [-0.0132, -0.0196, -0.0208],\n",
            "          [-0.0077, -0.0230, -0.0202]],\n",
            "\n",
            "         [[-0.0019,  0.0530,  0.0134],\n",
            "          [ 0.0027,  0.0249,  0.0167],\n",
            "          [-0.0192, -0.0188, -0.0200]],\n",
            "\n",
            "         [[ 0.0081,  0.0300,  0.0006],\n",
            "          [ 0.0036,  0.0254, -0.0104],\n",
            "          [-0.0010, -0.0193, -0.0121]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0163,  0.0251,  0.0290],\n",
            "          [-0.0073, -0.0014,  0.0124],\n",
            "          [-0.0211, -0.0347, -0.0195]],\n",
            "\n",
            "         [[ 0.0165,  0.0519,  0.0494],\n",
            "          [ 0.0058,  0.0199,  0.0363],\n",
            "          [-0.0049, -0.0165, -0.0130]],\n",
            "\n",
            "         [[-0.0102, -0.0308, -0.0340],\n",
            "          [ 0.0055, -0.0109,  0.0005],\n",
            "          [ 0.0382,  0.0209,  0.0326]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0184, -0.0031,  0.0103],\n",
            "          [-0.0072, -0.0013, -0.0070],\n",
            "          [ 0.0217,  0.0011, -0.0033]],\n",
            "\n",
            "         [[-0.0166,  0.0002, -0.0092],\n",
            "          [ 0.0150,  0.0584,  0.0218],\n",
            "          [-0.0136,  0.0181,  0.0164]],\n",
            "\n",
            "         [[ 0.0219,  0.0316,  0.0183],\n",
            "          [-0.0021, -0.0156,  0.0203],\n",
            "          [ 0.0053,  0.0005,  0.0157]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0045, -0.0025,  0.0017],\n",
            "          [-0.0052, -0.0088,  0.0137],\n",
            "          [-0.0107,  0.0010,  0.0170]],\n",
            "\n",
            "         [[-0.0060, -0.0368,  0.0031],\n",
            "          [ 0.0102,  0.0291, -0.0007],\n",
            "          [ 0.0099,  0.0600,  0.0272]],\n",
            "\n",
            "         [[-0.0092, -0.0183,  0.0062],\n",
            "          [-0.0319, -0.0294, -0.0149],\n",
            "          [-0.0148, -0.0123, -0.0236]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.3212, 0.2124, 0.2661, 0.3594, 0.2785, 0.2582, 0.3108, 0.3096, 0.3348,\n",
            "        0.2992, 0.2545, 0.2458, 0.3133, 0.4159, 0.2997, 0.3070, 0.3135, 0.4418,\n",
            "        0.3743, 0.2570, 0.2943, 0.3078, 0.2738, 0.3948, 0.2928, 0.3572, 0.3435,\n",
            "        0.5379, 0.4243, 0.3908, 0.2745, 0.2798, 0.3217, 0.1956, 0.2751, 0.3187,\n",
            "        0.3507, 0.2751, 0.1919, 0.3307, 0.2850, 0.3038, 0.2179, 0.2652, 0.2944,\n",
            "        0.2138, 0.2184, 0.2948, 0.3262, 0.3759, 0.2557, 0.3796, 0.2950, 0.3386,\n",
            "        0.3243, 0.3070, 0.3331, 0.2302, 0.3036, 0.3377, 0.2922, 0.2204, 0.3267,\n",
            "        0.3198, 0.4023, 0.2987, 0.4860, 0.2854, 0.2716, 0.4341, 0.2834, 0.2296,\n",
            "        0.2507, 0.3120, 0.3673, 0.3244, 0.3380, 0.3272, 0.2868, 0.2877, 0.3210,\n",
            "        0.2332, 0.3379, 0.2767, 0.2942, 0.2672, 0.4401, 0.2908, 0.3771, 0.2789,\n",
            "        0.3056, 0.3276, 0.3871, 0.2453, 0.2559, 0.2783, 0.3168, 0.3410, 0.2318,\n",
            "        0.3577, 0.5036, 0.3557, 0.2475, 0.1852, 0.2273, 0.3602, 0.2919, 0.3928,\n",
            "        0.4423, 0.2052, 0.2524, 0.2189, 0.4113, 0.3611, 0.4284, 0.2333, 0.3504,\n",
            "        0.7001, 0.3754, 0.2874, 0.3702, 0.3174, 0.3640, 0.2889, 0.4155, 0.2479,\n",
            "        0.2898, 0.3740, 0.4926, 0.2808, 0.2388, 0.3473, 0.1868, 0.2837, 0.3090,\n",
            "        0.3614, 0.2797, 0.6871, 0.2854, 0.2937, 0.3128, 0.4863, 0.2193, 0.2871,\n",
            "        0.2554, 0.4175, 0.3044, 0.3230, 0.3343, 0.4947, 0.3924, 0.2264, 0.2657,\n",
            "        0.4193, 0.3483, 0.3551, 0.2877, 0.2559, 0.2459, 0.2775, 0.3842, 0.2949,\n",
            "        0.3510, 0.1926, 0.3101, 0.3417, 0.3931, 0.3918, 0.3239, 0.2851, 0.4583,\n",
            "        0.2669, 0.2663, 0.4433, 0.3221, 0.3655, 0.3336, 0.4393, 0.3970, 0.3727,\n",
            "        0.3523, 0.3586, 0.3286, 0.4181, 0.2955, 0.3050, 0.2988, 0.4320, 0.2309,\n",
            "        0.3826, 0.2270, 0.2228, 0.3206, 0.3273, 0.2627, 0.3087, 0.2920, 0.2328,\n",
            "        0.4144, 0.4075, 0.3264, 0.3583, 0.3014, 0.3150, 0.4438, 0.4042, 0.2028,\n",
            "        0.3855, 0.2570, 0.2361, 0.2343, 0.3312, 0.2303, 0.3744, 0.4727, 0.3601,\n",
            "        0.2754, 0.1987, 0.3027, 0.3427, 0.2994, 0.2533, 0.2639, 0.3460, 0.3847,\n",
            "        0.4368, 0.3786, 0.3123, 0.2591, 0.3979, 0.2577, 0.3131, 0.2934, 0.3027,\n",
            "        0.2942, 0.2266, 0.2806, 0.2977, 0.1858, 0.2788, 0.2504, 0.3948, 0.3496,\n",
            "        0.2429, 0.2155, 0.2683, 0.4100, 0.3495, 0.4243, 0.2627, 0.3329, 0.2849,\n",
            "        0.3924, 0.3728, 0.2655, 0.3338], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-2.6385e-02,  9.9545e-02, -6.8362e-03, -8.7678e-02,  7.8322e-03,\n",
            "         4.0653e-02, -3.0738e-02,  5.9561e-03,  1.6938e-03,  4.7769e-02,\n",
            "         6.3004e-02,  3.5760e-02, -5.0405e-02,  2.1372e-02, -9.0327e-03,\n",
            "        -3.3702e-02, -4.5529e-02, -1.9238e-01, -6.7608e-02,  7.7464e-02,\n",
            "        -3.3957e-02, -7.9947e-02,  1.3138e-01, -1.2730e-01, -6.2825e-02,\n",
            "        -5.4973e-03, -9.1520e-02, -1.7570e-01, -8.2958e-03, -9.4468e-02,\n",
            "         2.4915e-03, -3.1894e-02, -1.5755e-02,  1.4372e-01, -3.4770e-03,\n",
            "         1.0756e-02, -5.1074e-02,  3.5848e-02,  8.7819e-02, -4.5240e-02,\n",
            "        -4.5785e-02,  1.4690e-02,  6.8685e-02,  1.6806e-02, -4.7715e-02,\n",
            "         5.6814e-02,  4.5985e-02, -5.0668e-02,  5.8914e-03, -1.0335e-01,\n",
            "         1.0281e-02, -1.0520e-01, -1.6557e-02, -1.9213e-02, -3.4522e-02,\n",
            "         2.0138e-02, -1.3621e-01,  3.9602e-02, -8.8277e-03, -1.0837e-02,\n",
            "        -2.9803e-02,  7.2119e-02, -6.6893e-02, -9.4287e-03, -3.0956e-02,\n",
            "        -2.6700e-02, -1.4184e-01,  1.1900e-01,  6.6929e-02, -2.1375e-01,\n",
            "         4.2720e-02,  4.7782e-02,  3.3905e-02,  1.3857e-04, -1.4822e-01,\n",
            "        -2.3684e-02, -7.4337e-02, -6.8427e-02, -2.0076e-02,  1.4697e-02,\n",
            "        -3.9620e-02,  1.9393e-02, -6.9640e-02, -5.5779e-02,  7.9664e-03,\n",
            "         2.3639e-02, -2.5781e-01,  6.3564e-03, -1.0041e-01,  2.8047e-02,\n",
            "         1.5223e-02, -4.8413e-02, -1.5355e-01,  1.0493e-01,  4.9892e-02,\n",
            "         6.5689e-02, -5.4139e-02,  7.7265e-03,  9.4090e-02, -1.9978e-02,\n",
            "        -2.3561e-01, -6.2284e-02,  3.3363e-02,  1.1017e-01,  7.6999e-02,\n",
            "        -3.2478e-02,  4.8089e-02, -1.4995e-01, -1.6503e-01,  1.2304e-01,\n",
            "         7.1157e-02,  5.8895e-02, -4.8155e-02, -9.7220e-02, -1.8604e-01,\n",
            "         8.5326e-02, -5.1602e-02, -3.0799e-01, -6.0410e-02, -7.7131e-02,\n",
            "        -2.7282e-01,  2.8950e-02, -1.3280e-01,  1.7264e-02, -3.9184e-02,\n",
            "         5.4153e-02, -3.7219e-02, -1.5283e-01, -1.7663e-01,  8.3937e-02,\n",
            "         6.9329e-02, -8.2645e-02,  1.1178e-01, -5.0772e-02, -4.4772e-02,\n",
            "        -3.7455e-02,  3.0361e-02, -3.7824e-01,  1.4857e-02,  6.8245e-03,\n",
            "        -5.2090e-02, -2.9501e-01,  8.9860e-02,  2.9646e-02,  1.9930e-02,\n",
            "        -8.3483e-02, -9.6416e-02, -2.3830e-02,  3.4911e-02, -2.6626e-01,\n",
            "        -1.6184e-01,  7.3552e-02,  2.7568e-02, -1.1087e-01, -1.0322e-02,\n",
            "        -9.7499e-02,  1.3995e-02,  1.0802e-02,  7.8444e-02,  1.3057e-02,\n",
            "        -3.9476e-02,  2.4805e-02, -7.7445e-02, -2.8419e-02,  1.0412e-02,\n",
            "        -4.2330e-02, -1.6634e-01, -9.4908e-02, -3.4328e-02,  4.5544e-02,\n",
            "        -3.0001e-01, -6.9376e-03,  1.4103e-02, -2.6155e-01, -7.3598e-02,\n",
            "        -1.0627e-01, -1.0544e-02, -7.1160e-02, -1.0339e-01, -2.9787e-02,\n",
            "        -1.4277e-01, -5.1710e-02, -5.7126e-02, -5.4392e-02, -4.2260e-02,\n",
            "        -8.5120e-03,  1.5887e-02, -6.5358e-02, -6.1346e-02, -1.4502e-01,\n",
            "         3.9939e-02,  8.1640e-02, -7.7597e-03, -3.4062e-02,  3.1969e-02,\n",
            "        -4.4756e-02, -7.0306e-02,  1.0213e-01, -1.7993e-01, -2.1173e-01,\n",
            "        -5.9797e-02, -1.1596e-01,  3.9271e-02, -4.5443e-02, -1.8446e-01,\n",
            "        -1.0848e-01,  5.5781e-02, -6.3649e-02,  1.6825e-02,  1.6623e-04,\n",
            "         7.9866e-02, -6.7240e-02,  7.9827e-02, -3.9905e-03, -1.9016e-01,\n",
            "         2.0026e-02,  7.3181e-02,  1.0323e-01, -2.6431e-02,  2.3963e-02,\n",
            "        -4.4247e-02,  2.2896e-02,  2.3444e-02, -2.3481e-02,  1.0516e-02,\n",
            "        -2.1494e-01, -1.2810e-01, -1.8338e-02, -6.0221e-04, -5.1624e-02,\n",
            "         5.6575e-02, -5.4297e-02,  1.4146e-02, -4.9852e-02,  6.7255e-02,\n",
            "         5.1713e-02, -4.0266e-03,  3.5057e-02,  8.2781e-02,  1.0034e-02,\n",
            "         5.9230e-02, -2.0429e-01, -7.6169e-02,  4.1432e-02,  7.7517e-02,\n",
            "         7.6005e-02, -1.5919e-01, -8.3613e-02, -1.6625e-01,  2.2968e-03,\n",
            "        -6.8489e-02,  3.8119e-02, -9.8731e-02, -2.0290e-02,  1.5394e-02,\n",
            "        -1.0548e-01], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 0.0081]],\n",
            "\n",
            "         [[-0.0192]],\n",
            "\n",
            "         [[-0.0173]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0128]],\n",
            "\n",
            "         [[ 0.0025]],\n",
            "\n",
            "         [[ 0.0054]]],\n",
            "\n",
            "\n",
            "        [[[-0.0143]],\n",
            "\n",
            "         [[-0.0554]],\n",
            "\n",
            "         [[-0.0346]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0275]],\n",
            "\n",
            "         [[ 0.0360]],\n",
            "\n",
            "         [[ 0.0240]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0076]],\n",
            "\n",
            "         [[ 0.0207]],\n",
            "\n",
            "         [[-0.0101]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0278]],\n",
            "\n",
            "         [[ 0.0064]],\n",
            "\n",
            "         [[-0.0022]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0336]],\n",
            "\n",
            "         [[-0.0424]],\n",
            "\n",
            "         [[ 0.0226]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0330]],\n",
            "\n",
            "         [[-0.0009]],\n",
            "\n",
            "         [[-0.0177]]],\n",
            "\n",
            "\n",
            "        [[[-0.0114]],\n",
            "\n",
            "         [[-0.0183]],\n",
            "\n",
            "         [[ 0.0076]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0151]],\n",
            "\n",
            "         [[ 0.0332]],\n",
            "\n",
            "         [[ 0.0002]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0063]],\n",
            "\n",
            "         [[-0.0200]],\n",
            "\n",
            "         [[ 0.0010]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0191]],\n",
            "\n",
            "         [[ 0.0455]],\n",
            "\n",
            "         [[ 0.0078]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.0674, 0.0514, 0.0385, 0.1692, 0.0604, 0.0460, 0.1209, 0.1110, 0.0418,\n",
            "        0.0387, 0.0442, 0.0707, 0.0790, 0.1094, 0.0959, 0.0544, 0.1032, 0.2190,\n",
            "        0.0459, 0.0372, 0.1410, 0.0587, 0.0360, 0.0955, 0.1657, 0.1024, 0.1417,\n",
            "        0.0580, 0.0536, 0.0716, 0.0865, 0.1110, 0.0511, 0.0515, 0.0809, 0.1154,\n",
            "        0.0777, 0.0449, 0.0490, 0.1056, 0.1457, 0.0744, 0.0530, 0.0600, 0.1026,\n",
            "        0.0486, 0.0408, 0.1312, 0.0639, 0.1062, 0.0915, 0.1476, 0.0900, 0.0742,\n",
            "        0.1069, 0.0776, 0.1423, 0.0495, 0.0974, 0.0661, 0.1292, 0.0548, 0.1145,\n",
            "        0.0950, 0.0921, 0.1579, 0.0496, 0.0236, 0.0398, 0.0935, 0.0291, 0.0653,\n",
            "        0.0885, 0.1190, 0.1692, 0.0692, 0.1316, 0.0606, 0.0480, 0.0654, 0.1082,\n",
            "        0.0624, 0.1103, 0.1106, 0.1076, 0.0400, 0.0723, 0.0947, 0.0662, 0.0464,\n",
            "        0.0444, 0.1727, 0.0921, 0.0345, 0.0451, 0.0374, 0.0940, 0.0818, 0.0397,\n",
            "        0.0452, 0.0985, 0.1095, 0.1072, 0.0506, 0.0444, 0.0755, 0.0420, 0.1046,\n",
            "        0.1172, 0.0447, 0.0459, 0.0409, 0.0539, 0.1036, 0.0741, 0.0311, 0.1086,\n",
            "        0.1746, 0.0777, 0.0689, 0.1100, 0.0489, 0.1048, 0.1097, 0.1025, 0.0448,\n",
            "        0.0675, 0.0707, 0.1364, 0.0438, 0.0346, 0.1769, 0.0667, 0.1155, 0.0628,\n",
            "        0.0873, 0.0406, 0.2890, 0.0703, 0.0428, 0.1173, 0.1049, 0.0611, 0.0469,\n",
            "        0.0400, 0.0744, 0.1003, 0.1012, 0.0599, 0.1078, 0.1512, 0.0322, 0.0430,\n",
            "        0.0977, 0.0951, 0.0838, 0.0958, 0.0448, 0.0263, 0.0425, 0.1154, 0.0771,\n",
            "        0.1781, 0.0300, 0.0699, 0.0724, 0.1600, 0.0893, 0.1130, 0.0534, 0.1359,\n",
            "        0.0375, 0.0809, 0.1145, 0.1232, 0.0942, 0.0880, 0.0346, 0.0996, 0.0461,\n",
            "        0.0694, 0.0630, 0.1590, 0.0509, 0.1254, 0.0590, 0.0744, 0.1084, 0.0514,\n",
            "        0.0931, 0.0848, 0.0240, 0.0279, 0.0993, 0.0612, 0.0599, 0.1095, 0.0508,\n",
            "        0.0658, 0.1162, 0.0833, 0.1651, 0.0505, 0.1231, 0.1228, 0.1038, 0.0369,\n",
            "        0.0756, 0.0415, 0.1192, 0.0292, 0.0839, 0.0577, 0.0951, 0.0944, 0.0309,\n",
            "        0.0390, 0.0604, 0.0672, 0.0501, 0.0383, 0.0946, 0.0958, 0.0501, 0.0243,\n",
            "        0.1074, 0.1908, 0.0693, 0.1376, 0.1151, 0.0329, 0.0647, 0.0616, 0.1106,\n",
            "        0.0358, 0.0721, 0.0851, 0.0375, 0.0368, 0.0947, 0.0464, 0.1666, 0.1049,\n",
            "        0.0755, 0.0398, 0.0249, 0.1528, 0.1167, 0.0886, 0.0540, 0.0726, 0.0736,\n",
            "        0.0797, 0.0854, 0.0609, 0.1263], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-2.6385e-02,  9.9545e-02, -6.8362e-03, -8.7678e-02,  7.8322e-03,\n",
            "         4.0653e-02, -3.0738e-02,  5.9561e-03,  1.6938e-03,  4.7769e-02,\n",
            "         6.3004e-02,  3.5760e-02, -5.0405e-02,  2.1372e-02, -9.0327e-03,\n",
            "        -3.3702e-02, -4.5529e-02, -1.9238e-01, -6.7608e-02,  7.7464e-02,\n",
            "        -3.3957e-02, -7.9947e-02,  1.3138e-01, -1.2730e-01, -6.2825e-02,\n",
            "        -5.4973e-03, -9.1520e-02, -1.7570e-01, -8.2958e-03, -9.4468e-02,\n",
            "         2.4915e-03, -3.1894e-02, -1.5755e-02,  1.4372e-01, -3.4770e-03,\n",
            "         1.0756e-02, -5.1074e-02,  3.5848e-02,  8.7819e-02, -4.5240e-02,\n",
            "        -4.5785e-02,  1.4690e-02,  6.8685e-02,  1.6806e-02, -4.7715e-02,\n",
            "         5.6814e-02,  4.5985e-02, -5.0668e-02,  5.8914e-03, -1.0335e-01,\n",
            "         1.0281e-02, -1.0520e-01, -1.6557e-02, -1.9213e-02, -3.4522e-02,\n",
            "         2.0138e-02, -1.3621e-01,  3.9602e-02, -8.8277e-03, -1.0837e-02,\n",
            "        -2.9803e-02,  7.2119e-02, -6.6893e-02, -9.4287e-03, -3.0956e-02,\n",
            "        -2.6700e-02, -1.4184e-01,  1.1900e-01,  6.6929e-02, -2.1375e-01,\n",
            "         4.2720e-02,  4.7782e-02,  3.3905e-02,  1.3857e-04, -1.4822e-01,\n",
            "        -2.3684e-02, -7.4337e-02, -6.8427e-02, -2.0076e-02,  1.4697e-02,\n",
            "        -3.9620e-02,  1.9393e-02, -6.9640e-02, -5.5779e-02,  7.9664e-03,\n",
            "         2.3639e-02, -2.5781e-01,  6.3564e-03, -1.0041e-01,  2.8047e-02,\n",
            "         1.5223e-02, -4.8413e-02, -1.5355e-01,  1.0493e-01,  4.9892e-02,\n",
            "         6.5689e-02, -5.4139e-02,  7.7265e-03,  9.4090e-02, -1.9978e-02,\n",
            "        -2.3561e-01, -6.2284e-02,  3.3363e-02,  1.1017e-01,  7.6999e-02,\n",
            "        -3.2478e-02,  4.8089e-02, -1.4995e-01, -1.6503e-01,  1.2304e-01,\n",
            "         7.1157e-02,  5.8895e-02, -4.8155e-02, -9.7220e-02, -1.8604e-01,\n",
            "         8.5326e-02, -5.1602e-02, -3.0799e-01, -6.0410e-02, -7.7131e-02,\n",
            "        -2.7282e-01,  2.8950e-02, -1.3280e-01,  1.7264e-02, -3.9184e-02,\n",
            "         5.4153e-02, -3.7219e-02, -1.5283e-01, -1.7663e-01,  8.3937e-02,\n",
            "         6.9329e-02, -8.2645e-02,  1.1178e-01, -5.0772e-02, -4.4772e-02,\n",
            "        -3.7455e-02,  3.0361e-02, -3.7824e-01,  1.4857e-02,  6.8245e-03,\n",
            "        -5.2090e-02, -2.9501e-01,  8.9860e-02,  2.9646e-02,  1.9930e-02,\n",
            "        -8.3483e-02, -9.6416e-02, -2.3830e-02,  3.4911e-02, -2.6626e-01,\n",
            "        -1.6184e-01,  7.3552e-02,  2.7568e-02, -1.1087e-01, -1.0322e-02,\n",
            "        -9.7499e-02,  1.3995e-02,  1.0802e-02,  7.8444e-02,  1.3057e-02,\n",
            "        -3.9476e-02,  2.4805e-02, -7.7445e-02, -2.8419e-02,  1.0412e-02,\n",
            "        -4.2330e-02, -1.6634e-01, -9.4908e-02, -3.4328e-02,  4.5544e-02,\n",
            "        -3.0001e-01, -6.9376e-03,  1.4103e-02, -2.6155e-01, -7.3598e-02,\n",
            "        -1.0627e-01, -1.0544e-02, -7.1160e-02, -1.0339e-01, -2.9787e-02,\n",
            "        -1.4277e-01, -5.1710e-02, -5.7126e-02, -5.4392e-02, -4.2260e-02,\n",
            "        -8.5120e-03,  1.5887e-02, -6.5358e-02, -6.1346e-02, -1.4502e-01,\n",
            "         3.9939e-02,  8.1640e-02, -7.7597e-03, -3.4062e-02,  3.1969e-02,\n",
            "        -4.4756e-02, -7.0306e-02,  1.0213e-01, -1.7993e-01, -2.1173e-01,\n",
            "        -5.9797e-02, -1.1596e-01,  3.9271e-02, -4.5443e-02, -1.8446e-01,\n",
            "        -1.0848e-01,  5.5781e-02, -6.3649e-02,  1.6825e-02,  1.6623e-04,\n",
            "         7.9866e-02, -6.7240e-02,  7.9827e-02, -3.9905e-03, -1.9016e-01,\n",
            "         2.0026e-02,  7.3181e-02,  1.0323e-01, -2.6431e-02,  2.3963e-02,\n",
            "        -4.4247e-02,  2.2896e-02,  2.3444e-02, -2.3481e-02,  1.0516e-02,\n",
            "        -2.1494e-01, -1.2810e-01, -1.8338e-02, -6.0221e-04, -5.1624e-02,\n",
            "         5.6575e-02, -5.4297e-02,  1.4146e-02, -4.9852e-02,  6.7255e-02,\n",
            "         5.1713e-02, -4.0266e-03,  3.5057e-02,  8.2781e-02,  1.0034e-02,\n",
            "         5.9230e-02, -2.0429e-01, -7.6169e-02,  4.1432e-02,  7.7517e-02,\n",
            "         7.6005e-02, -1.5919e-01, -8.3613e-02, -1.6625e-01,  2.2968e-03,\n",
            "        -6.8489e-02,  3.8119e-02, -9.8731e-02, -2.0290e-02,  1.5394e-02,\n",
            "        -1.0548e-01], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 4.8367e-02,  4.8045e-02,  3.8471e-02],\n",
            "          [ 4.9888e-02,  5.5208e-02,  5.6701e-02],\n",
            "          [ 2.4192e-02,  1.3436e-02,  2.4655e-02]],\n",
            "\n",
            "         [[-3.6542e-03, -3.1100e-03,  4.9227e-03],\n",
            "          [-1.2114e-03,  3.4020e-03,  1.9846e-02],\n",
            "          [-2.1704e-02, -2.1158e-02, -2.8686e-03]],\n",
            "\n",
            "         [[-1.2536e-02, -2.0486e-02, -2.3154e-02],\n",
            "          [-1.3515e-02, -2.3781e-02, -2.5515e-02],\n",
            "          [ 1.0584e-02,  7.2999e-03, -5.2329e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.3596e-02, -1.8328e-02, -5.0577e-02],\n",
            "          [ 1.6590e-02,  5.0719e-02,  2.1919e-02],\n",
            "          [-1.9203e-02, -8.8315e-03, -2.0335e-02]],\n",
            "\n",
            "         [[-7.6949e-03, -1.5848e-02,  1.5841e-03],\n",
            "          [-6.2470e-03, -1.3135e-02,  6.9092e-03],\n",
            "          [-3.3791e-03,  1.7889e-03,  3.7373e-03]],\n",
            "\n",
            "         [[-6.6310e-03,  5.8503e-03, -5.8571e-04],\n",
            "          [-2.4600e-02, -8.9747e-03, -7.2466e-03],\n",
            "          [-1.7566e-02, -8.5829e-03, -7.5220e-03]]],\n",
            "\n",
            "\n",
            "        [[[-2.3679e-02, -9.4399e-03, -1.1688e-02],\n",
            "          [-2.4777e-02, -1.7326e-02, -3.1489e-02],\n",
            "          [-3.3683e-03,  9.7571e-03, -5.1527e-03]],\n",
            "\n",
            "         [[-3.0809e-02, -4.0685e-02, -2.2731e-02],\n",
            "          [-5.1065e-03, -1.6457e-02, -1.8804e-02],\n",
            "          [ 5.0382e-02,  5.2054e-02,  3.9185e-02]],\n",
            "\n",
            "         [[-3.7790e-02, -4.2234e-02, -2.9703e-02],\n",
            "          [-6.4766e-03,  2.6967e-03, -8.1736e-03],\n",
            "          [ 3.7747e-02,  5.5416e-02,  2.5806e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.7275e-02, -4.5364e-02, -3.9567e-02],\n",
            "          [ 8.9827e-03,  1.6150e-02,  1.1675e-02],\n",
            "          [-9.7209e-03, -3.6449e-02, -1.6842e-02]],\n",
            "\n",
            "         [[ 1.7824e-02,  1.5013e-02,  1.0225e-02],\n",
            "          [ 5.4044e-03,  1.1664e-02,  6.4623e-03],\n",
            "          [ 2.1803e-02,  4.1795e-02,  1.9234e-02]],\n",
            "\n",
            "         [[-2.6730e-04,  1.5218e-03, -5.0352e-03],\n",
            "          [ 2.5761e-02,  2.7110e-02, -9.3395e-04],\n",
            "          [-1.1949e-02, -7.5204e-03, -3.9370e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.7447e-02, -1.8358e-02, -2.6020e-02],\n",
            "          [-1.4074e-02, -1.1302e-02, -1.4814e-02],\n",
            "          [-3.1460e-03, -1.8674e-02, -9.3350e-03]],\n",
            "\n",
            "         [[-5.1125e-03, -4.8036e-03,  1.8139e-02],\n",
            "          [-1.0524e-02, -1.5152e-02,  2.3904e-03],\n",
            "          [ 8.7093e-03,  9.3810e-03,  2.4203e-03]],\n",
            "\n",
            "         [[-7.6392e-03, -8.1496e-03, -1.5331e-02],\n",
            "          [-8.0622e-03, -1.3383e-02, -1.3938e-02],\n",
            "          [-1.6904e-02, -3.0059e-02, -1.8659e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.8390e-02, -2.6080e-03,  9.3782e-03],\n",
            "          [-6.4662e-04, -1.3146e-02,  1.0045e-02],\n",
            "          [-2.2293e-03, -1.4097e-02,  1.7385e-02]],\n",
            "\n",
            "         [[ 3.0293e-04,  2.9622e-03,  1.0030e-02],\n",
            "          [-5.7588e-03, -1.6943e-03,  6.9988e-03],\n",
            "          [ 9.8134e-03,  1.4197e-02,  5.9742e-03]],\n",
            "\n",
            "         [[ 2.8753e-03, -1.7814e-03,  1.0873e-02],\n",
            "          [ 1.5230e-02,  4.5867e-03,  1.6860e-02],\n",
            "          [ 1.9536e-03,  1.9503e-02,  1.2168e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 1.3983e-02,  2.4598e-03, -7.4604e-03],\n",
            "          [-2.2250e-02, -1.2757e-02, -2.8846e-03],\n",
            "          [-1.0911e-02,  7.5499e-03,  8.6910e-03]],\n",
            "\n",
            "         [[-4.8463e-03, -8.3250e-03,  1.3420e-02],\n",
            "          [-6.2502e-03, -7.3982e-03,  1.1153e-02],\n",
            "          [ 4.0391e-03, -9.0354e-03, -7.5441e-03]],\n",
            "\n",
            "         [[-5.1627e-03, -8.9529e-03, -1.2414e-02],\n",
            "          [-4.9261e-03, -3.5488e-03,  2.1501e-03],\n",
            "          [-1.1709e-02, -1.4984e-02, -1.9216e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.5428e-02, -7.6036e-04, -1.3522e-03],\n",
            "          [-3.4856e-02, -7.4478e-04, -6.5064e-03],\n",
            "          [-9.1655e-03, -2.8467e-02, -4.8924e-02]],\n",
            "\n",
            "         [[ 1.2207e-02,  1.0519e-02, -8.4421e-03],\n",
            "          [-2.5495e-02,  2.8140e-03,  1.6165e-03],\n",
            "          [-1.8831e-02,  1.2268e-02,  1.5439e-02]],\n",
            "\n",
            "         [[-1.3684e-02, -4.1732e-03,  1.2609e-02],\n",
            "          [-6.8834e-04,  5.9757e-03, -1.0183e-02],\n",
            "          [ 2.1559e-04, -1.3462e-02, -3.0114e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.6186e-02, -6.4926e-02, -4.3146e-02],\n",
            "          [-2.1790e-02, -4.9106e-02, -3.4568e-02],\n",
            "          [ 4.0506e-02,  4.2449e-02,  6.1562e-02]],\n",
            "\n",
            "         [[ 3.5715e-03, -1.0916e-02, -2.2922e-02],\n",
            "          [-2.4831e-03,  6.4555e-03, -1.1316e-02],\n",
            "          [ 1.6662e-03, -1.9145e-02, -2.3007e-02]],\n",
            "\n",
            "         [[-7.1243e-03, -4.2783e-05,  4.9363e-03],\n",
            "          [-1.5832e-02,  4.0474e-03,  4.5135e-04],\n",
            "          [-4.7967e-03, -7.2164e-04, -1.7230e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.1589e-02,  7.7814e-04,  6.3205e-03],\n",
            "          [ 1.1360e-02, -6.2076e-03, -2.7689e-02],\n",
            "          [ 2.6392e-02,  2.3775e-03, -1.4937e-02]],\n",
            "\n",
            "         [[-1.1237e-02, -2.6285e-03,  9.1537e-03],\n",
            "          [-8.2120e-03, -2.2236e-02,  3.2917e-04],\n",
            "          [ 5.5909e-03, -1.3858e-03,  6.8947e-03]],\n",
            "\n",
            "         [[-1.4783e-02, -1.0367e-02, -2.7472e-02],\n",
            "          [-4.1090e-02, -3.8532e-02, -3.9202e-02],\n",
            "          [-2.1614e-02, -3.4340e-02, -1.8542e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.9492e-02, -1.6098e-02, -3.1792e-02],\n",
            "          [ 2.5374e-02,  4.6815e-02,  2.7513e-02],\n",
            "          [ 3.5903e-02,  3.1892e-02,  2.6156e-02]],\n",
            "\n",
            "         [[ 1.6856e-02,  1.5645e-02,  1.4189e-02],\n",
            "          [ 2.2550e-02,  3.0456e-02,  1.6739e-02],\n",
            "          [-2.3615e-04, -7.9501e-03, -1.9666e-03]],\n",
            "\n",
            "         [[-7.9060e-03, -4.7390e-03,  1.6030e-03],\n",
            "          [ 1.3802e-03, -8.5837e-03,  6.9451e-03],\n",
            "          [ 1.1407e-02, -5.9877e-03,  1.3759e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.0124e-03,  2.9951e-02,  1.1915e-02],\n",
            "          [-4.3412e-02, -3.1776e-03, -2.7705e-02],\n",
            "          [-1.6183e-02, -1.1247e-02, -3.5084e-02]],\n",
            "\n",
            "         [[ 2.9837e-02,  5.9935e-02,  2.4631e-02],\n",
            "          [-1.9571e-03,  2.2415e-02, -1.5499e-02],\n",
            "          [ 1.6075e-02,  1.7850e-02, -1.8412e-02]],\n",
            "\n",
            "         [[-4.3712e-03, -4.9032e-02, -2.1335e-02],\n",
            "          [-5.2598e-03, -2.8579e-02, -2.2090e-02],\n",
            "          [ 8.5126e-03,  2.0862e-03,  2.3301e-02]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.2480, 0.1972, 0.2279, 0.2709, 0.3296, 0.2640, 0.2710, 0.3475, 0.2388,\n",
            "        0.2904, 0.2769, 0.3045, 0.2268, 0.2634, 0.2999, 0.2397, 0.2724, 0.2723,\n",
            "        0.2133, 0.3806, 0.2767, 0.2403, 0.2406, 0.2917, 0.2675, 0.2305, 0.2394,\n",
            "        0.3123, 0.2984, 0.3353, 0.2234, 0.1919, 0.3168, 0.2626, 0.2901, 0.2918,\n",
            "        0.3455, 0.2561, 0.2434, 0.2298, 0.3318, 0.3481, 0.2032, 0.2478, 0.2478,\n",
            "        0.2483, 0.3252, 0.2567, 0.2685, 0.1977, 0.2541, 0.4079, 0.2480, 0.2076,\n",
            "        0.2276, 0.2683, 0.2098, 0.2056, 0.2010, 0.3560, 0.2384, 0.3284, 0.1952,\n",
            "        0.2445, 0.2848, 0.3742, 0.2746, 0.2117, 0.3859, 0.4785, 0.3005, 0.2848,\n",
            "        0.3762, 0.2903, 0.2126, 0.1776, 0.2778, 0.3878, 0.3123, 0.1974, 0.2679,\n",
            "        0.2300, 0.2474, 0.2320, 0.2635, 0.2819, 0.2296, 0.3194, 0.3814, 0.2503,\n",
            "        0.2269, 0.2676, 0.3431, 0.3799, 0.3787, 0.2968, 0.3021, 0.2575, 0.3007,\n",
            "        0.1939, 0.1950, 0.3217, 0.3623, 0.2171, 0.2486, 0.2266, 0.2133, 0.2851,\n",
            "        0.2715, 0.2720, 0.3107, 0.2174, 0.2675, 0.2387, 0.3434, 0.2761, 0.2084,\n",
            "        0.2975, 0.3178, 0.2818, 0.2858, 0.3498, 0.2675, 0.2638, 0.3159, 0.2879,\n",
            "        0.1873, 0.2986, 0.3584, 0.2570, 0.1815, 0.2758, 0.2640, 0.2486, 0.2567,\n",
            "        0.2252, 0.3420, 0.2910, 0.2898, 0.2902, 0.2404, 0.2381, 0.3633, 0.2690,\n",
            "        0.3810, 0.2947, 0.2743, 0.4644, 0.3133, 0.2444, 0.3477, 0.3001, 0.1977,\n",
            "        0.2301, 0.2513, 0.2660, 0.3271, 0.1622, 0.2274, 0.2225, 0.3596, 0.3215,\n",
            "        0.1997, 0.2215, 0.2706, 0.2831, 0.2621, 0.3710, 0.2730, 0.2903, 0.1893,\n",
            "        0.2140, 0.2460, 0.3141, 0.2424, 0.3699, 0.2364, 0.2420, 0.2948, 0.2497,\n",
            "        0.2760, 0.2686, 0.2895, 0.3857, 0.1398, 0.2832, 0.3362, 0.2522, 0.2823,\n",
            "        0.2381, 0.2311, 0.3274, 0.4078, 0.2648, 0.2525, 0.3388, 0.3251, 0.2420,\n",
            "        0.2856, 0.3605, 0.2603, 0.2294, 0.2483, 0.2171, 0.2353, 0.4117, 0.2588,\n",
            "        0.2888, 0.1972, 0.2408, 0.2755, 0.3031, 0.2457, 0.2744, 0.3564, 0.2546,\n",
            "        0.3673, 0.2883, 0.2590, 0.3021, 0.2890, 0.3505, 0.2092, 0.2953, 0.3222,\n",
            "        0.2925, 0.2574, 0.3012, 0.3893, 0.2211, 0.2226, 0.3258, 0.3205, 0.2975,\n",
            "        0.2323, 0.3323, 0.2812, 0.2702, 0.2300, 0.2846, 0.3318, 0.2292, 0.3498,\n",
            "        0.2622, 0.3581, 0.4003, 0.2924, 0.3049, 0.3478, 0.2845, 0.2742, 0.2019,\n",
            "        0.2466, 0.2988, 0.2044, 0.2691], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1332, -0.0644, -0.3239, -0.2390, -0.3262, -0.1796, -0.2087, -0.3208,\n",
            "        -0.1874, -0.2988, -0.2099, -0.2283, -0.2141, -0.2460, -0.2768, -0.1351,\n",
            "        -0.2498, -0.2393, -0.1223, -0.4590, -0.2172, -0.1220, -0.2101, -0.1779,\n",
            "        -0.2426, -0.1546, -0.1549, -0.3716, -0.2817, -0.3886, -0.1545, -0.0687,\n",
            "        -0.3412, -0.2261, -0.1961, -0.2242, -0.2984, -0.1381, -0.2251, -0.1658,\n",
            "        -0.4534, -0.3226, -0.0977, -0.1349, -0.2619, -0.1428, -0.3960, -0.1633,\n",
            "        -0.2101, -0.1161, -0.1448, -0.5502, -0.2179, -0.1246,  0.0502, -0.1902,\n",
            "        -0.1047, -0.1000, -0.1411, -0.3124, -0.2190, -0.3062, -0.1247, -0.1557,\n",
            "        -0.2973, -0.3825, -0.1951, -0.1381, -0.5761, -0.3879, -0.2808, -0.2542,\n",
            "        -0.3470, -0.2460, -0.1091, -0.0562, -0.1833, -0.4956, -0.3059, -0.0988,\n",
            "        -0.2255, -0.1958, -0.1320, -0.1738, -0.2287, -0.1926, -0.0924, -0.3427,\n",
            "        -0.5489, -0.2431, -0.1935, -0.1641, -0.2503, -0.3274, -0.4008, -0.2824,\n",
            "        -0.2694, -0.1939, -0.2413, -0.0309, -0.0880, -0.3421, -0.3104, -0.1102,\n",
            "        -0.1539, -0.1233, -0.1780, -0.2715, -0.2005, -0.1846, -0.2843, -0.1117,\n",
            "        -0.1816, -0.2119, -0.3304, -0.2267, -0.1413, -0.3376, -0.2674, -0.2524,\n",
            "        -0.2554, -0.4735, -0.2342, -0.2130, -0.3282, -0.1966, -0.1063, -0.2615,\n",
            "        -0.4234, -0.1374, -0.0811, -0.3069, -0.1538, -0.1453, -0.1612, -0.1631,\n",
            "        -0.3759, -0.2608, -0.2382, -0.2499, -0.1485, -0.1487, -0.4328, -0.1377,\n",
            "        -0.2781, -0.2259, -0.2072, -0.4165, -0.3582, -0.1382, -0.3598, -0.2672,\n",
            "        -0.2090, -0.0177, -0.1279, -0.2812, -0.3621,  0.0476, -0.2232, -0.1272,\n",
            "        -0.3237, -0.3008, -0.1119, -0.0839, -0.2426, -0.2000, -0.1873, -0.4685,\n",
            "        -0.2000, -0.3462, -0.0706, -0.1973, -0.3548, -0.1975, -0.3537, -0.3546,\n",
            "        -0.1433, -0.2052, -0.2722, -0.1528, -0.2798, -0.1945, -0.2474, -0.4910,\n",
            "         0.1322, -0.2378, -0.5166, -0.3959, -0.2354, -0.1266, -0.0810, -0.4132,\n",
            "        -0.5576, -0.2238, -0.1563, -0.3950, -0.3283, -0.0846, -0.3103, -0.3130,\n",
            "        -0.1498, -0.1396, -0.0972, -0.1620, -0.1631, -0.6364, -0.1350, -0.2345,\n",
            "        -0.1049, -0.1625, -0.2878, -0.2450, -0.1468, -0.2035, -0.5358, -0.1683,\n",
            "        -0.5524, -0.2511, -0.1230, -0.2305, -0.1925, -0.3759, -0.1014, -0.1697,\n",
            "        -0.4002, -0.2980, -0.3035, -0.1563, -0.4660, -0.1155, -0.1665, -0.3382,\n",
            "        -0.2935, -0.3122, -0.3015, -0.3261, -0.2542, -0.2037, -0.0955, -0.2070,\n",
            "        -0.4370, -0.2051, -0.4205, -0.3125, -0.4845, -0.3528, -0.2624, -0.2894,\n",
            "        -0.3976, -0.2107, -0.1791, -0.1075, -0.1213, -0.3022,  0.0516, -0.1928],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-4.2568e-02, -2.6148e-02, -2.2019e-02],\n",
            "          [-1.7334e-02, -7.5950e-03, -7.2384e-03],\n",
            "          [-1.7876e-03,  2.3800e-02,  1.4873e-02]],\n",
            "\n",
            "         [[-2.8277e-03, -5.0644e-03, -4.9442e-03],\n",
            "          [ 1.2117e-03,  1.4908e-02,  1.6013e-02],\n",
            "          [ 1.4391e-02,  3.3109e-02,  5.0061e-02]],\n",
            "\n",
            "         [[-3.4891e-03, -4.4437e-03,  2.6589e-03],\n",
            "          [ 1.5105e-02,  2.6303e-02,  2.6802e-02],\n",
            "          [ 3.9232e-02,  5.0057e-02,  4.6637e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.2877e-02,  1.5454e-02, -2.4483e-02],\n",
            "          [ 3.1145e-02,  3.4944e-02,  1.3296e-02],\n",
            "          [-1.7674e-04,  7.3297e-03, -5.7174e-03]],\n",
            "\n",
            "         [[-2.1781e-02, -3.7379e-02, -1.3382e-02],\n",
            "          [ 1.8976e-02,  1.4155e-02, -6.5395e-03],\n",
            "          [ 2.6831e-02,  3.6354e-02,  1.1450e-02]],\n",
            "\n",
            "         [[ 3.1603e-02,  3.3933e-02,  3.1575e-02],\n",
            "          [-1.0098e-02, -1.2657e-02,  1.1674e-02],\n",
            "          [ 1.0325e-02,  7.9424e-05,  1.5911e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 2.5937e-02,  6.2590e-03,  6.0798e-03],\n",
            "          [-4.5745e-03, -3.5188e-02, -2.9249e-02],\n",
            "          [ 2.1366e-02,  2.0480e-03,  6.2699e-03]],\n",
            "\n",
            "         [[-2.9549e-03, -1.3679e-03, -8.6876e-03],\n",
            "          [ 7.9988e-03,  1.2888e-03, -5.9629e-03],\n",
            "          [-9.1481e-03, -2.1914e-02, -4.1572e-02]],\n",
            "\n",
            "         [[ 7.6390e-03,  3.0253e-03,  2.7817e-04],\n",
            "          [ 7.0329e-03,  1.1914e-02, -2.4419e-03],\n",
            "          [-8.2131e-03, -9.7848e-05, -1.9223e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.4498e-03,  5.1611e-03,  3.7416e-03],\n",
            "          [ 3.2110e-04,  8.3762e-03,  3.6612e-03],\n",
            "          [ 9.3343e-03,  8.1829e-03,  1.1234e-03]],\n",
            "\n",
            "         [[-6.6849e-02, -5.9871e-02, -3.3931e-02],\n",
            "          [ 2.2337e-02,  3.1932e-02,  3.7244e-02],\n",
            "          [ 9.3296e-03,  3.7222e-02,  1.4052e-02]],\n",
            "\n",
            "         [[-2.0643e-03,  1.2408e-02, -3.1072e-03],\n",
            "          [-8.2882e-03,  1.3917e-02, -2.0680e-02],\n",
            "          [-1.9329e-02,  1.1953e-02, -2.3436e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.7788e-03, -3.5982e-03, -1.2592e-03],\n",
            "          [-1.5320e-02, -1.0690e-02, -2.0311e-02],\n",
            "          [-3.4649e-04, -2.2188e-03, -1.5021e-02]],\n",
            "\n",
            "         [[-2.8952e-02, -3.3958e-02, -2.5437e-02],\n",
            "          [-1.5919e-04,  1.5204e-02,  3.4554e-02],\n",
            "          [ 3.6892e-02,  7.0144e-02,  7.3610e-02]],\n",
            "\n",
            "         [[ 1.0721e-02,  2.1531e-03, -5.6155e-03],\n",
            "          [ 1.1754e-02, -4.8546e-03, -5.5013e-03],\n",
            "          [-3.7388e-04, -9.7639e-03, -1.5029e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.5622e-02,  9.8976e-03,  3.4725e-03],\n",
            "          [ 1.4711e-02,  7.0707e-03, -9.1826e-03],\n",
            "          [ 7.0986e-03,  6.3087e-03, -3.5893e-03]],\n",
            "\n",
            "         [[-6.4518e-03, -6.7673e-03,  1.1635e-02],\n",
            "          [ 1.4707e-02,  2.3831e-02,  4.9396e-02],\n",
            "          [ 1.8897e-02,  3.4981e-02,  4.5488e-02]],\n",
            "\n",
            "         [[ 1.5900e-02,  3.3369e-02,  2.6194e-02],\n",
            "          [ 1.0616e-02,  1.8515e-02,  3.0190e-03],\n",
            "          [ 1.1004e-02,  2.5503e-02,  1.3654e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-2.1231e-02, -1.2804e-02, -1.5498e-02],\n",
            "          [ 7.6750e-03,  1.2120e-02,  1.5099e-02],\n",
            "          [ 1.8536e-02,  2.5110e-02,  2.5283e-02]],\n",
            "\n",
            "         [[ 7.4059e-03, -3.0540e-03, -1.5475e-03],\n",
            "          [-8.4415e-03, -2.2002e-02, -3.4099e-03],\n",
            "          [ 9.1918e-03,  2.2617e-03, -1.4260e-02]],\n",
            "\n",
            "         [[-5.2568e-03, -5.3507e-03, -3.2230e-03],\n",
            "          [-1.5805e-02,  6.0508e-03, -1.5917e-03],\n",
            "          [-8.9323e-03,  2.6483e-03,  5.0508e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.9826e-02, -2.1209e-03,  1.4889e-02],\n",
            "          [ 5.7275e-02,  3.5549e-02,  6.0175e-03],\n",
            "          [ 2.3347e-02, -2.2153e-02, -2.5497e-02]],\n",
            "\n",
            "         [[-1.3985e-02, -6.4766e-02, -1.7286e-02],\n",
            "          [ 1.1704e-02,  1.0714e-02,  4.6278e-02],\n",
            "          [-1.0038e-02, -3.5707e-03,  2.2691e-02]],\n",
            "\n",
            "         [[-8.3342e-03, -1.3070e-03, -1.0049e-02],\n",
            "          [ 3.2605e-02,  5.3259e-02,  2.2172e-02],\n",
            "          [ 3.7339e-02,  6.1155e-02,  4.4555e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.6584e-02, -1.3850e-02, -1.4604e-02],\n",
            "          [-1.7604e-02, -2.1268e-02, -1.6734e-02],\n",
            "          [-6.0039e-04,  3.8569e-03,  1.2837e-02]],\n",
            "\n",
            "         [[ 1.7623e-02,  2.3706e-02,  2.7633e-02],\n",
            "          [-2.2841e-02, -1.9576e-02, -1.6551e-02],\n",
            "          [-8.0822e-03,  4.3779e-03, -5.3622e-03]],\n",
            "\n",
            "         [[ 1.5582e-02,  3.7879e-02,  2.3555e-02],\n",
            "          [-6.4632e-03,  9.8620e-03,  1.2121e-02],\n",
            "          [-1.3743e-02, -6.1246e-03, -2.7332e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.5037e-03, -1.2064e-02, -9.0989e-03],\n",
            "          [-4.7911e-04, -2.8339e-03,  2.1365e-03],\n",
            "          [-6.2077e-03, -2.6615e-03,  1.1215e-02]],\n",
            "\n",
            "         [[-8.1794e-03, -2.2417e-02, -3.4012e-02],\n",
            "          [-2.8553e-02, -2.9546e-02, -4.4372e-02],\n",
            "          [-5.0348e-02, -3.4973e-02, -5.2028e-02]],\n",
            "\n",
            "         [[ 1.4728e-02,  3.2834e-02,  2.6312e-02],\n",
            "          [ 1.3449e-02,  2.6407e-02,  2.6924e-02],\n",
            "          [ 2.5572e-02,  3.4316e-02,  2.6184e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 7.2026e-03, -2.3931e-03,  2.2182e-03],\n",
            "          [ 4.2555e-03, -6.4084e-03,  7.8548e-03],\n",
            "          [ 2.0510e-02,  1.8644e-02,  2.3280e-02]],\n",
            "\n",
            "         [[-1.2471e-02,  1.3008e-02,  1.0010e-02],\n",
            "          [-1.7496e-03,  6.1331e-03,  4.3366e-03],\n",
            "          [ 5.2269e-03,  1.5111e-02, -8.1881e-03]],\n",
            "\n",
            "         [[-3.7337e-02,  1.9923e-02, -2.4149e-02],\n",
            "          [-4.9487e-02, -1.0510e-02, -4.2107e-02],\n",
            "          [-5.7684e-03, -4.8632e-03, -1.8332e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.2013e-03, -1.5208e-02, -1.6507e-02],\n",
            "          [-8.8276e-03, -1.8698e-02, -1.6637e-03],\n",
            "          [-1.2015e-02,  2.9667e-03,  6.2300e-03]],\n",
            "\n",
            "         [[-1.8341e-02, -9.0521e-03,  2.6030e-02],\n",
            "          [ 3.5930e-02,  5.3049e-02,  5.8487e-02],\n",
            "          [-1.3661e-02, -3.6888e-03, -7.1606e-03]],\n",
            "\n",
            "         [[-1.2594e-02, -4.0898e-02,  1.7162e-03],\n",
            "          [-1.7420e-02, -4.3435e-02, -1.3183e-02],\n",
            "          [-3.7506e-02, -5.5707e-02, -3.0051e-02]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.1971, 0.1771, 0.1303, 0.1995, 0.1839, 0.0934, 0.2333, 0.2236, 0.1654,\n",
            "        0.1280, 0.0842, 0.1085, 0.3168, 0.2032, 0.3246, 0.2184, 0.3208, 0.2824,\n",
            "        0.3408, 0.3339, 0.3307, 0.5571, 0.2821, 0.3081, 0.2114, 0.2971, 0.2361,\n",
            "        0.5500, 0.1221, 0.3381, 0.1528, 0.1544, 0.1982, 0.0582, 0.1812, 0.2489,\n",
            "        0.1954, 0.0705, 0.0918, 0.1328, 0.2616, 0.2013, 0.0720, 0.1573, 0.1919,\n",
            "        0.0813, 0.1170, 0.2504, 0.2863, 0.3032, 0.1476, 0.3696, 0.1870, 0.2097,\n",
            "        0.1907, 0.2364, 0.1642, 0.1079, 0.2531, 0.1703, 0.1266, 0.0814, 0.2407,\n",
            "        0.2609, 0.2705, 0.2128, 0.5007, 0.2375, 0.0802, 0.2896, 0.1776, 0.0887,\n",
            "        0.1094, 0.1834, 0.2812, 0.1971, 0.2021, 0.3443, 0.1411, 0.1362, 0.2676,\n",
            "        0.1618, 0.2723, 0.2727, 0.2528, 0.0982, 0.4707, 0.2239, 0.3649, 0.1987,\n",
            "        0.0815, 0.2543, 0.3322, 0.1561, 0.2336, 0.1294, 0.2570, 0.1700, 0.1374,\n",
            "        0.2215, 0.5015, 0.3132, 0.1487, 0.1174, 0.0916, 0.2130, 0.1393, 0.3057,\n",
            "        0.5634, 0.1018, 0.0994, 0.0492, 0.4427, 0.3142, 0.4002, 0.1334, 0.2174,\n",
            "        0.5522, 0.2806, 0.2784, 0.4333, 0.2602, 0.3788, 0.1827, 0.2664, 0.1077,\n",
            "        0.3001, 0.2428, 0.5130, 0.0829, 0.1254, 0.1996, 0.1451, 0.2253, 0.1467,\n",
            "        0.3712, 0.0794, 0.5425, 0.2058, 0.2103, 0.1288, 0.4993, 0.1815, 0.1845,\n",
            "        0.4154, 0.3817, 0.2054, 0.2205, 0.1471, 0.4964, 0.4202, 0.0801, 0.0623,\n",
            "        0.3536, 0.2760, 0.3840, 0.1632, 0.1402, 0.2674, 0.0844, 0.2305, 0.2259,\n",
            "        0.2146, 0.4181, 0.2821, 0.2926, 0.3416, 0.4640, 0.3025, 0.3732, 0.5871,\n",
            "        0.0616, 0.2797, 0.3042, 0.2173, 0.3550, 0.2096, 0.2449, 0.3428, 0.2868,\n",
            "        0.3543, 0.4667, 0.3220, 0.3805, 0.2632, 0.2160, 0.1924, 0.4074, 0.4966,\n",
            "        0.3623, 0.1670, 0.1321, 0.2374, 0.2118, 0.1522, 0.1668, 0.3836, 0.0983,\n",
            "        0.3729, 0.3943, 0.4353, 0.2270, 0.1508, 0.3133, 0.3850, 0.5774, 0.1892,\n",
            "        0.2822, 0.0907, 0.2364, 0.0964, 0.2360, 0.0699, 0.2938, 0.5100, 0.3348,\n",
            "        0.2339, 0.1145, 0.2155, 0.2266, 0.2829, 0.2341, 0.1891, 0.2906, 0.2681,\n",
            "        0.3876, 0.3915, 0.1844, 0.1889, 0.4405, 0.1405, 0.3460, 0.2724, 0.2567,\n",
            "        0.2785, 0.1148, 0.1607, 0.1754, 0.0883, 0.1649, 0.1268, 0.2356, 0.2811,\n",
            "        0.0766, 0.1424, 0.1683, 0.3979, 0.2685, 0.6383, 0.1087, 0.3180, 0.1760,\n",
            "        0.3634, 0.2615, 0.1999, 0.2541], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0162, -0.2033,  0.0294, -0.1697, -0.1840, -0.0309, -0.2039, -0.1426,\n",
            "        -0.0443, -0.0886, -0.0647, -0.0968, -0.0380, -0.2073, -0.3061,  0.1443,\n",
            "        -0.3079, -0.1232, -0.1627, -0.0980, -0.2471, -0.2837, -0.1201, -0.2893,\n",
            "        -0.2303, -0.3562, -0.0825, -0.3483,  0.0707, -0.1321, -0.1074, -0.1451,\n",
            "         0.0235,  0.0225, -0.1885, -0.2507, -0.2461,  0.0631, -0.0023, -0.1209,\n",
            "        -0.2581, -0.1640, -0.0172, -0.1143, -0.2096, -0.0158,  0.0128, -0.1332,\n",
            "        -0.3139, -0.2294, -0.1527, -0.3503,  0.2086,  0.0785, -0.1597, -0.1990,\n",
            "         0.0346,  0.0388, -0.1269,  0.1019,  0.0981, -0.0390, -0.2537, -0.1356,\n",
            "        -0.1796, -0.2422, -0.4517, -0.3124, -0.0177, -0.2615, -0.1567,  0.0212,\n",
            "        -0.0753, -0.1426, -0.2788,  0.0062, -0.1895, -0.2327, -0.1298, -0.1200,\n",
            "        -0.1917, -0.0987, -0.1916, -0.1666, -0.2729,  0.1287, -0.4620, -0.2259,\n",
            "        -0.2270,  0.1939,  0.0230, -0.3303, -0.3202, -0.1292, -0.0716,  0.0048,\n",
            "        -0.2579, -0.0116, -0.0557, -0.1229, -0.4804, -0.2351, -0.1367, -0.0578,\n",
            "        -0.0537, -0.2743, -0.0827, -0.1922, -0.3481, -0.0358, -0.1094,  0.0138,\n",
            "        -0.1888, -0.2592, -0.3293, -0.0820, -0.1839, -0.1636, -0.3163, -0.0246,\n",
            "        -0.1667, -0.1653, -0.3076, -0.2229, -0.1834, -0.0536, -0.0621, -0.1752,\n",
            "        -0.5243, -0.1933, -0.1119, -0.2283, -0.0437, -0.1777, -0.1300, -0.2519,\n",
            "        -0.0456, -0.6305, -0.1364, -0.2138,  0.0406, -0.5287, -0.2014, -0.1442,\n",
            "        -0.1930, -0.3033,  0.1030, -0.1499, -0.2297, -0.5301, -0.2543, -0.0417,\n",
            "         0.0429, -0.3218, -0.1611, -0.2562, -0.1187, -0.1001,  0.0225,  0.0996,\n",
            "        -0.2138, -0.2019,  0.0808, -0.0121, -0.2364, -0.3247, -0.1482, -0.4846,\n",
            "        -0.3449, -0.1365, -0.6664,  0.0418, -0.2807, -0.0961, -0.2378, -0.1834,\n",
            "        -0.1890, -0.0377, -0.3056, -0.1843, -0.1357, -0.3038, -0.2680, -0.4143,\n",
            "        -0.2633, -0.1750, -0.1856, -0.2405, -0.1082, -0.2250, -0.1268, -0.1094,\n",
            "         0.0594, -0.1419, -0.1178, -0.1602, -0.0328, -0.0194, -0.1985,  0.0470,\n",
            "        -0.1887, -0.2776, -0.0930, -0.4092, -0.3378, -0.7252,  0.0260, -0.1829,\n",
            "         0.0561, -0.2227, -0.0026, -0.3218, -0.0093, -0.2843, -0.5121, -0.2337,\n",
            "        -0.0836, -0.0818, -0.1296, -0.2090,  0.0169, -0.1899, -0.1892, -0.3075,\n",
            "        -0.3108, -0.2986, -0.4712, -0.1823, -0.1893, -0.3131, -0.0876, -0.1166,\n",
            "        -0.2995, -0.0831, -0.3427, -0.0772, -0.1460, -0.1611,  0.0203, -0.0627,\n",
            "        -0.0610, -0.2574, -0.1383,  0.0470, -0.0302, -0.1638, -0.3323, -0.1741,\n",
            "        -0.6307, -0.0772, -0.2123, -0.1559, -0.0459, -0.2416, -0.0143, -0.2079],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-1.1645e-02, -1.9010e-02, -2.1876e-02],\n",
            "          [ 2.0482e-02,  2.3962e-02,  2.9161e-02],\n",
            "          [ 4.3672e-02,  3.3278e-02,  4.9908e-02]],\n",
            "\n",
            "         [[-7.4040e-03,  2.8083e-03, -4.7339e-03],\n",
            "          [ 6.9030e-03,  1.4271e-02, -3.6954e-03],\n",
            "          [-3.1341e-03,  1.3736e-02,  1.6127e-03]],\n",
            "\n",
            "         [[ 1.8676e-02, -1.0553e-02, -1.4233e-02],\n",
            "          [ 8.9944e-03, -2.5068e-03, -1.2145e-02],\n",
            "          [-4.9455e-03, -2.9206e-02, -9.6385e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.2655e-02,  1.7691e-02,  9.8264e-04],\n",
            "          [ 7.4271e-03,  7.6115e-03,  1.1135e-02],\n",
            "          [ 2.3242e-02,  1.1058e-02,  4.0498e-03]],\n",
            "\n",
            "         [[ 1.8557e-02,  1.2472e-02,  1.7220e-02],\n",
            "          [-4.8544e-03,  8.3627e-03,  2.2811e-02],\n",
            "          [-5.1675e-03,  2.3264e-02,  3.4068e-02]],\n",
            "\n",
            "         [[ 2.4934e-02,  2.2373e-02,  4.2614e-02],\n",
            "          [ 1.3486e-02,  1.6760e-03,  1.3019e-02],\n",
            "          [-6.2821e-03, -1.5112e-03, -8.9229e-03]]],\n",
            "\n",
            "\n",
            "        [[[-9.8089e-04, -6.3011e-03,  5.9932e-03],\n",
            "          [ 1.5936e-02,  1.3394e-02,  2.9934e-02],\n",
            "          [ 2.3149e-02,  2.0709e-02,  2.5485e-02]],\n",
            "\n",
            "         [[-2.0015e-02, -3.3349e-02, -8.0396e-03],\n",
            "          [-7.2800e-03, -1.2187e-02, -2.0389e-04],\n",
            "          [-1.3138e-02, -2.0427e-02, -1.6286e-02]],\n",
            "\n",
            "         [[-6.7681e-03,  5.0045e-03, -2.6683e-03],\n",
            "          [-2.1073e-02,  2.8275e-04, -1.8205e-02],\n",
            "          [-1.7382e-02, -5.0244e-03, -3.0386e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.1035e-02, -2.2964e-02, -1.1028e-02],\n",
            "          [-6.3256e-03, -4.1667e-03, -1.7323e-02],\n",
            "          [-1.3611e-02, -2.3468e-02, -1.6436e-02]],\n",
            "\n",
            "         [[ 7.3663e-03,  6.6219e-03,  5.2776e-03],\n",
            "          [-3.5464e-03,  3.2750e-03, -9.1126e-03],\n",
            "          [ 3.5593e-04, -1.0151e-02, -1.9123e-02]],\n",
            "\n",
            "         [[ 1.8193e-03,  8.8087e-03,  5.1361e-03],\n",
            "          [ 3.1915e-03,  2.5287e-02,  2.4939e-02],\n",
            "          [ 1.3968e-02,  1.9613e-02,  2.2382e-02]]],\n",
            "\n",
            "\n",
            "        [[[-4.1548e-03,  8.8964e-03,  2.0143e-03],\n",
            "          [ 1.1327e-02,  1.3251e-02,  1.4014e-02],\n",
            "          [ 7.2196e-03,  1.3045e-02,  2.4827e-02]],\n",
            "\n",
            "         [[-1.5025e-02,  5.0530e-03,  7.4766e-03],\n",
            "          [-2.4685e-02, -1.6732e-02, -1.0888e-02],\n",
            "          [-2.8064e-02, -1.1875e-02, -3.4120e-03]],\n",
            "\n",
            "         [[ 2.8449e-02,  1.4594e-02,  6.9441e-03],\n",
            "          [ 2.4799e-02,  1.9453e-02,  1.1294e-02],\n",
            "          [-1.0787e-02, -2.1006e-02, -1.0372e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.4967e-02,  8.2449e-03,  2.0244e-03],\n",
            "          [ 1.4287e-02, -6.3867e-03, -8.0757e-03],\n",
            "          [ 2.7547e-02,  1.0791e-02,  1.6567e-02]],\n",
            "\n",
            "         [[ 3.6191e-02,  3.8918e-02,  3.9028e-02],\n",
            "          [-8.3489e-04,  1.3273e-02,  2.0172e-02],\n",
            "          [-2.0652e-02, -5.4010e-03,  1.7147e-03]],\n",
            "\n",
            "         [[ 2.0373e-04,  3.5919e-03,  8.5592e-03],\n",
            "          [ 6.2363e-03, -9.3086e-05,  1.2940e-02],\n",
            "          [ 1.3152e-02,  1.0732e-02,  1.9896e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1.7400e-02, -6.7019e-03, -9.1787e-03],\n",
            "          [-9.9672e-03,  2.6298e-04,  3.3439e-03],\n",
            "          [ 1.5721e-02,  1.4216e-02,  2.0509e-02]],\n",
            "\n",
            "         [[ 2.1410e-02,  3.6914e-02,  2.8239e-02],\n",
            "          [ 3.8158e-02,  4.8944e-02,  3.4652e-02],\n",
            "          [ 3.1723e-02,  4.4208e-02,  4.0035e-02]],\n",
            "\n",
            "         [[-3.3437e-03, -1.0482e-02, -5.3990e-03],\n",
            "          [-5.3186e-03,  1.1394e-02,  1.7593e-03],\n",
            "          [-5.6652e-03, -6.6373e-03, -1.3492e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.7099e-02, -1.8145e-03, -1.3040e-02],\n",
            "          [-2.2750e-02, -3.6062e-03, -8.0294e-03],\n",
            "          [-1.6087e-02, -1.0175e-02, -1.3529e-02]],\n",
            "\n",
            "         [[ 4.1701e-04, -5.1785e-03, -2.1884e-02],\n",
            "          [ 2.6919e-03,  8.9139e-03, -1.4217e-04],\n",
            "          [-7.3746e-03, -6.6853e-03, -2.3725e-02]],\n",
            "\n",
            "         [[ 1.9425e-02,  1.3175e-02,  1.7511e-02],\n",
            "          [ 1.8235e-02,  4.4286e-02,  2.3767e-02],\n",
            "          [ 2.6504e-02,  3.3104e-02,  1.9696e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.0177e-02, -1.0701e-02, -2.0428e-02],\n",
            "          [-1.7986e-02,  5.9928e-03, -1.0584e-03],\n",
            "          [-1.8794e-02, -1.8773e-03, -6.9449e-03]],\n",
            "\n",
            "         [[-2.8498e-03,  1.6427e-03,  1.4575e-04],\n",
            "          [-5.4403e-03,  8.3667e-03, -9.4164e-03],\n",
            "          [-4.4999e-03,  5.4902e-03,  2.4863e-03]],\n",
            "\n",
            "         [[-1.3356e-02, -2.1525e-02,  5.3421e-04],\n",
            "          [-1.9160e-02, -2.4645e-02, -1.3791e-02],\n",
            "          [-6.1991e-03, -1.3174e-02, -3.6783e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.3993e-03, -2.7823e-03,  7.6715e-03],\n",
            "          [-2.0649e-02, -1.2731e-02, -9.4138e-03],\n",
            "          [-1.3678e-03, -3.4410e-02, -2.6984e-02]],\n",
            "\n",
            "         [[-3.5651e-04,  2.0102e-03,  1.4130e-02],\n",
            "          [-1.3073e-02, -1.6616e-02, -1.2690e-02],\n",
            "          [-3.5934e-02, -4.1700e-02, -3.3968e-02]],\n",
            "\n",
            "         [[ 2.0470e-02,  8.0159e-04, -1.1607e-03],\n",
            "          [ 9.5101e-03,  3.0336e-02,  2.7362e-02],\n",
            "          [ 1.5588e-02,  3.2851e-02,  1.3015e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.5574e-02, -3.2971e-02, -3.1939e-02],\n",
            "          [-2.2502e-02, -5.7187e-03, -5.6729e-03],\n",
            "          [-2.7309e-02, -1.6981e-02,  1.2832e-04]],\n",
            "\n",
            "         [[-1.1925e-02, -2.9479e-02, -2.0437e-02],\n",
            "          [-2.4408e-02, -2.2069e-02, -1.9965e-03],\n",
            "          [-2.3279e-02, -5.5140e-03,  2.5630e-02]],\n",
            "\n",
            "         [[-1.6100e-02, -8.2417e-03,  1.5266e-04],\n",
            "          [-2.6195e-03, -8.2754e-03, -2.9435e-02],\n",
            "          [-2.7493e-03, -2.4889e-02, -2.3583e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.7985e-02,  1.8594e-02,  8.9198e-04],\n",
            "          [-1.7319e-02,  7.8735e-03, -2.8659e-03],\n",
            "          [ 3.8596e-03,  2.9061e-02,  2.4188e-02]],\n",
            "\n",
            "         [[-2.6735e-02, -1.4391e-02, -4.0148e-02],\n",
            "          [-2.6728e-02, -2.4455e-02, -6.9176e-03],\n",
            "          [-5.7244e-02, -2.1995e-04,  5.5438e-02]],\n",
            "\n",
            "         [[ 2.3487e-02,  2.7157e-03, -8.4719e-04],\n",
            "          [ 1.7886e-02,  5.4860e-03,  2.8059e-02],\n",
            "          [ 4.6468e-03,  1.8598e-02,  1.3761e-03]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.2427, 0.2232, 0.2511, 0.2288, 0.2074, 0.2905, 0.2482, 0.3102, 0.2749,\n",
            "        0.2892, 0.2448, 0.1759, 0.2426, 0.2780, 0.2315, 0.2631, 0.3383, 0.2785,\n",
            "        0.2536, 0.2989, 0.2335, 0.2812, 0.3486, 0.2778, 0.2280, 0.2547, 0.3032,\n",
            "        0.2468, 0.2512, 0.2973, 0.2577, 0.3200, 0.2385, 0.2714, 0.2532, 0.2625,\n",
            "        0.3344, 0.2626, 0.1838, 0.2839, 0.2187, 0.2666, 0.2858, 0.2471, 0.2915,\n",
            "        0.2332, 0.2637, 0.2691, 0.2432, 0.2384, 0.2356, 0.2525, 0.2564, 0.2451,\n",
            "        0.2529, 0.2522, 0.2800, 0.3165, 0.2340, 0.2634, 0.2569, 0.1942, 0.2621,\n",
            "        0.2205, 0.2301, 0.2323, 0.2811, 0.1897, 0.2280, 0.3472, 0.2717, 0.3191,\n",
            "        0.2440, 0.2719, 0.2781, 0.2262, 0.3444, 0.2648, 0.2725, 0.2851, 0.2039,\n",
            "        0.2935, 0.2742, 0.2774, 0.2654, 0.2430, 0.2721, 0.2708, 0.3085, 0.2895,\n",
            "        0.2596, 0.2147, 0.3119, 0.3449, 0.2262, 0.2814, 0.2326, 0.2712, 0.2637,\n",
            "        0.2323, 0.3333, 0.2714, 0.2991, 0.2747, 0.2515, 0.2394, 0.2709, 0.2836,\n",
            "        0.2866, 0.2408, 0.2560, 0.2048, 0.2394, 0.2813, 0.3267, 0.2761, 0.2123,\n",
            "        0.2715, 0.2540, 0.2771, 0.3209, 0.1905, 0.3989, 0.2676, 0.2357, 0.2169,\n",
            "        0.3216, 0.3596, 0.2838, 0.2648, 0.2702, 0.2469, 0.2442, 0.2553, 0.2599,\n",
            "        0.2693, 0.2399, 0.2700, 0.2063, 0.2711, 0.2834, 0.2781, 0.2529, 0.2013,\n",
            "        0.2343, 0.2082, 0.3063, 0.1635, 0.2673, 0.2197, 0.2787, 0.2724, 0.2744,\n",
            "        0.2287, 0.2969, 0.2662, 0.2982, 0.2396, 0.3039, 0.2319, 0.2773, 0.2661,\n",
            "        0.2898, 0.2489, 0.3060, 0.2612, 0.2937, 0.3045, 0.2999, 0.2580, 0.2093,\n",
            "        0.2714, 0.2993, 0.2679, 0.2963, 0.2754, 0.2580, 0.2566, 0.2634, 0.2325,\n",
            "        0.2442, 0.2934, 0.2398, 0.2631, 0.2851, 0.2870, 0.2239, 0.2410, 0.2676,\n",
            "        0.2681, 0.2638, 0.2732, 0.2812, 0.2203, 0.2670, 0.2764, 0.2550, 0.3160,\n",
            "        0.2888, 0.2615, 0.2178, 0.2485, 0.2414, 0.2798, 0.2872, 0.2767, 0.2551,\n",
            "        0.2429, 0.2459, 0.3288, 0.3024, 0.2912, 0.2625, 0.3019, 0.2643, 0.2721,\n",
            "        0.2108, 0.2368, 0.2269, 0.1988, 0.2830, 0.2569, 0.2349, 0.2755, 0.2442,\n",
            "        0.2717, 0.2747, 0.2785, 0.2516, 0.2227, 0.2783, 0.2465, 0.2652, 0.2641,\n",
            "        0.2960, 0.2671, 0.2679, 0.2537, 0.2847, 0.2507, 0.2525, 0.2024, 0.2311,\n",
            "        0.2618, 0.2764, 0.3031, 0.2452, 0.2716, 0.2273, 0.2295, 0.2611, 0.2329,\n",
            "        0.2690, 0.2753, 0.2737, 0.2590, 0.2421, 0.2685, 0.3392, 0.3073, 0.1371,\n",
            "        0.3650, 0.2980, 0.2460, 0.2487, 0.2912, 0.2704, 0.2560, 0.2213, 0.2569,\n",
            "        0.2661, 0.2367, 0.2742, 0.2847, 0.3055, 0.2671, 0.2819, 0.2791, 0.2401,\n",
            "        0.2549, 0.2210, 0.3507, 0.2852, 0.2162, 0.2821, 0.2369, 0.2905, 0.2826,\n",
            "        0.2300, 0.2745, 0.2437, 0.2522, 0.2489, 0.2395, 0.2851, 0.2887, 0.2621,\n",
            "        0.2500, 0.2689, 0.2427, 0.3010, 0.3067, 0.2861, 0.2387, 0.2462, 0.2859,\n",
            "        0.2550, 0.2630, 0.2442, 0.2145, 0.2898, 0.2282, 0.2327, 0.2242, 0.2738,\n",
            "        0.2485, 0.2379, 0.3058, 0.2798, 0.2761, 0.2252, 0.2866, 0.2660, 0.3250,\n",
            "        0.2612, 0.2767, 0.3205, 0.2932, 0.3183, 0.2939, 0.3103, 0.2553, 0.2981,\n",
            "        0.3667, 0.3086, 0.2254, 0.2352, 0.2348, 0.2555, 0.2597, 0.2369, 0.3017,\n",
            "        0.2776, 0.2728, 0.3174, 0.2785, 0.2721, 0.2637, 0.2702, 0.3633, 0.2869,\n",
            "        0.2675, 0.3405, 0.2587, 0.2732, 0.2747, 0.2821, 0.2750, 0.2630, 0.2018,\n",
            "        0.2358, 0.3034, 0.3155, 0.3013, 0.2775, 0.2511, 0.2945, 0.1605, 0.2825,\n",
            "        0.2964, 0.2194, 0.2061, 0.2332, 0.2348, 0.2663, 0.2543, 0.2927, 0.2215,\n",
            "        0.2521, 0.2827, 0.1993, 0.2453, 0.2597, 0.2654, 0.2757, 0.2650, 0.2444,\n",
            "        0.2949, 0.2308, 0.3071, 0.1904, 0.3024, 0.2786, 0.3659, 0.2966, 0.2746,\n",
            "        0.2449, 0.2201, 0.2564, 0.2853, 0.2392, 0.2457, 0.2467, 0.2374, 0.2664,\n",
            "        0.2460, 0.3182, 0.1793, 0.2379, 0.2596, 0.2847, 0.2452, 0.1974, 0.2388,\n",
            "        0.2949, 0.2879, 0.2786, 0.2765, 0.3296, 0.2530, 0.2690, 0.2547, 0.2333,\n",
            "        0.2348, 0.2690, 0.2718, 0.2679, 0.2516, 0.2710, 0.2366, 0.2601, 0.2764,\n",
            "        0.2880, 0.2008, 0.2637, 0.2263, 0.2511, 0.2604, 0.2805, 0.2989, 0.2965,\n",
            "        0.2597, 0.2767, 0.2553, 0.2959, 0.2512, 0.2925, 0.3008, 0.2423, 0.2394,\n",
            "        0.2708, 0.3704, 0.2879, 0.2532, 0.2248, 0.2023, 0.2279, 0.2366, 0.3082,\n",
            "        0.2980, 0.2909, 0.2777, 0.4293, 0.2658, 0.2940, 0.2418, 0.2816, 0.3247,\n",
            "        0.2647, 0.2216, 0.2758, 0.2421, 0.2078, 0.2332, 0.2271, 0.2611, 0.3650,\n",
            "        0.2017, 0.2598, 0.2160, 0.2641, 0.1408, 0.2664, 0.2502, 0.2553, 0.2227,\n",
            "        0.2417, 0.2696, 0.2388, 0.2833, 0.2333, 0.2667, 0.2224, 0.2691, 0.2710,\n",
            "        0.2459, 0.2674, 0.2430, 0.2593, 0.1851, 0.2950, 0.3664, 0.2212, 0.3026,\n",
            "        0.1840, 0.3443, 0.2140, 0.3717, 0.2360, 0.3081, 0.2638, 0.2233],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1986, -0.1593, -0.2054, -0.1598, -0.1268, -0.3226, -0.1597, -0.3477,\n",
            "        -0.2497, -0.2730, -0.2319, -0.0286, -0.1899, -0.2813, -0.1733, -0.2412,\n",
            "        -0.3712, -0.2747, -0.2053, -0.2585, -0.1535, -0.2748, -0.3241, -0.2525,\n",
            "        -0.1906, -0.2252, -0.3436, -0.2202, -0.1664, -0.2716, -0.1920, -0.3399,\n",
            "        -0.2026, -0.2972, -0.2616, -0.2238, -0.2486, -0.2606, -0.0893, -0.3572,\n",
            "        -0.1283, -0.2583, -0.2450, -0.1523, -0.3165, -0.1445, -0.2522, -0.1963,\n",
            "        -0.1794, -0.1071, -0.1662, -0.2053, -0.2530, -0.1447, -0.2517, -0.2062,\n",
            "        -0.2817, -0.3376, -0.1382, -0.2389, -0.2557, -0.0156, -0.2169, -0.1763,\n",
            "        -0.1486, -0.2122, -0.2002, -0.0716, -0.2089, -0.3580, -0.2588, -0.3599,\n",
            "        -0.1528, -0.2107, -0.2925, -0.1855, -0.3970, -0.1257, -0.2574, -0.2412,\n",
            "        -0.0863, -0.3065, -0.2701, -0.3380, -0.2485, -0.1935, -0.2987, -0.2279,\n",
            "        -0.3600, -0.2764, -0.2480, -0.1208, -0.3378, -0.2661, -0.1677, -0.2470,\n",
            "        -0.2152, -0.2591, -0.1936, -0.1543, -0.4117, -0.1570, -0.2372, -0.2997,\n",
            "        -0.2124, -0.2034, -0.1848, -0.3070, -0.3438, -0.1839, -0.1937, -0.0916,\n",
            "        -0.2338, -0.3558, -0.1967, -0.3303, -0.1398, -0.2177, -0.1665, -0.1857,\n",
            "        -0.3115, -0.1049, -0.4229, -0.2408, -0.1320, -0.1631, -0.3378, -0.3300,\n",
            "        -0.3183, -0.2268, -0.2787, -0.1950, -0.1950, -0.1463, -0.2437, -0.2297,\n",
            "        -0.1282, -0.2164, -0.1179, -0.2437, -0.2611, -0.2656, -0.1948, -0.1208,\n",
            "        -0.1668, -0.1351, -0.2713, -0.0560, -0.2243, -0.1318, -0.2356, -0.2720,\n",
            "        -0.2051, -0.1736, -0.2891, -0.2627, -0.3358, -0.1779, -0.2309, -0.1477,\n",
            "        -0.2685, -0.1882, -0.2629, -0.1983, -0.3522, -0.1905, -0.2778, -0.3395,\n",
            "        -0.2895, -0.2240, -0.1150, -0.2462, -0.2426, -0.2581, -0.3133, -0.2315,\n",
            "        -0.2271, -0.2077, -0.2109, -0.1371, -0.1323, -0.2529, -0.1716, -0.2532,\n",
            "        -0.2277, -0.2084, -0.1803, -0.1868, -0.2404, -0.2166, -0.2197, -0.2870,\n",
            "        -0.3062, -0.1507, -0.1054, -0.2199, -0.2415, -0.3310, -0.2700, -0.1568,\n",
            "        -0.1449, -0.2610, -0.1828, -0.2648, -0.3134, -0.2937, -0.2687, -0.2115,\n",
            "        -0.2164, -0.4522, -0.2999, -0.3032, -0.2292, -0.3099, -0.2642, -0.2695,\n",
            "        -0.1441, -0.1671, -0.1570, -0.1415, -0.2222, -0.1736, -0.1481, -0.2573,\n",
            "        -0.2060, -0.1703, -0.2360, -0.1770, -0.2132, -0.2016, -0.3001, -0.1518,\n",
            "        -0.2086, -0.2805, -0.2698, -0.2292, -0.1293, -0.2514, -0.2600, -0.2454,\n",
            "        -0.1744, -0.1029, -0.1679, -0.2353, -0.2007, -0.3363, -0.1640, -0.2430,\n",
            "        -0.1699, -0.1697, -0.1837, -0.1625, -0.2415, -0.2687, -0.2305, -0.2029,\n",
            "        -0.2209, -0.2240, -0.2675, -0.3233,  0.1462, -0.4777, -0.2376, -0.1489,\n",
            "        -0.1462, -0.3055, -0.2234, -0.1697, -0.1952, -0.2131, -0.2340, -0.2039,\n",
            "        -0.3054, -0.2596, -0.3470, -0.2176, -0.2706, -0.2897, -0.1729, -0.2300,\n",
            "        -0.1066, -0.3556, -0.2912, -0.1777, -0.2007, -0.1699, -0.3009, -0.3046,\n",
            "        -0.1693, -0.2602, -0.2053, -0.1810, -0.1808, -0.1730, -0.3757, -0.1808,\n",
            "        -0.1805, -0.1895, -0.2643, -0.2075, -0.2365, -0.1975, -0.3064, -0.1984,\n",
            "        -0.1811, -0.3676, -0.1198, -0.1485, -0.1770, -0.0781, -0.2052, -0.1360,\n",
            "        -0.1417, -0.1691, -0.2395, -0.1785, -0.1747, -0.2484, -0.2717, -0.3096,\n",
            "        -0.1465, -0.2239, -0.2584, -0.3572, -0.2311, -0.2878, -0.3841, -0.3475,\n",
            "        -0.3896, -0.1891, -0.2861, -0.2431, -0.2837, -0.4365, -0.3353, -0.1802,\n",
            "        -0.1976, -0.1529, -0.1978, -0.2535, -0.1954, -0.2667, -0.2813, -0.2487,\n",
            "        -0.3070, -0.2339, -0.2212, -0.1925, -0.2224, -0.4178, -0.3151, -0.2663,\n",
            "        -0.3581, -0.1935, -0.2385, -0.2424, -0.1850, -0.2265, -0.1803, -0.0777,\n",
            "        -0.1492, -0.3361, -0.4133, -0.3123, -0.2745, -0.1247, -0.3102,  0.0041,\n",
            "        -0.1981, -0.3301, -0.2047, -0.1053, -0.1653, -0.1634, -0.1116, -0.2314,\n",
            "        -0.3191, -0.1818, -0.2657, -0.2220, -0.1029, -0.1999, -0.2702, -0.2139,\n",
            "        -0.2256, -0.2653, -0.1630, -0.3322, -0.1617, -0.3446,  0.0288, -0.2456,\n",
            "        -0.3171, -0.3580, -0.2857, -0.2520, -0.2031, -0.1522, -0.2203, -0.3490,\n",
            "        -0.1685, -0.1424, -0.1602, -0.1553, -0.3057, -0.2420, -0.3536, -0.0551,\n",
            "        -0.0987, -0.2272, -0.2619, -0.2035, -0.0906, -0.1976, -0.3040, -0.2732,\n",
            "        -0.3161, -0.2102, -0.3384, -0.1740, -0.1475, -0.1842, -0.1823, -0.1151,\n",
            "        -0.2183, -0.2010, -0.2659, -0.2205, -0.2567, -0.1633, -0.2213, -0.2658,\n",
            "        -0.2938, -0.1069, -0.2522, -0.1103, -0.2216, -0.2244, -0.2908, -0.2176,\n",
            "        -0.3605, -0.2374, -0.2391, -0.2251, -0.2256, -0.1339, -0.1970, -0.2970,\n",
            "        -0.2206, -0.2051, -0.2229, -0.3602, -0.2923, -0.2498, -0.1466, -0.0979,\n",
            "        -0.1686, -0.2158, -0.2881, -0.3002, -0.2760, -0.2496, -0.3536, -0.2868,\n",
            "        -0.3251, -0.1847, -0.3062, -0.3861, -0.2650, -0.1339, -0.1846, -0.1630,\n",
            "        -0.0630, -0.1717, -0.1415, -0.1906, -0.4611, -0.1391, -0.1920, -0.1369,\n",
            "        -0.1647, -0.0055, -0.2598, -0.2653, -0.2319, -0.1780, -0.1913, -0.2055,\n",
            "        -0.1891, -0.2625, -0.1633, -0.2497, -0.1696, -0.1907, -0.2431, -0.1825,\n",
            "        -0.2607, -0.1943, -0.2361, -0.0581, -0.2758, -0.2593, -0.1466, -0.3589,\n",
            "        -0.0439, -0.3440, -0.1089, -0.4219, -0.1503, -0.2792, -0.3035, -0.1156],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 1.6218e-04, -1.4720e-02, -1.7000e-02],\n",
            "          [-1.2850e-02, -3.3085e-02, -3.6656e-02],\n",
            "          [ 2.7812e-02,  1.7691e-02, -1.8369e-02]],\n",
            "\n",
            "         [[ 1.0528e-02,  3.1379e-02,  2.4801e-02],\n",
            "          [-1.2698e-02, -2.9453e-02, -1.1834e-02],\n",
            "          [-9.4094e-03, -8.9462e-03, -3.1349e-02]],\n",
            "\n",
            "         [[-7.8447e-03, -2.9256e-02,  5.3590e-03],\n",
            "          [-1.3791e-02, -1.1116e-02,  5.0388e-03],\n",
            "          [-2.4919e-03,  7.3514e-03,  5.4013e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.0276e-03, -1.0275e-02, -2.9986e-02],\n",
            "          [-3.8465e-03,  1.9549e-03, -1.6291e-02],\n",
            "          [-1.8100e-03,  8.3778e-03, -8.5481e-03]],\n",
            "\n",
            "         [[-1.8196e-02, -1.3533e-02, -1.7457e-02],\n",
            "          [ 2.2457e-02,  5.7402e-02,  1.9325e-02],\n",
            "          [-2.4977e-02, -3.2113e-02, -8.1780e-03]],\n",
            "\n",
            "         [[ 3.6550e-03,  4.9358e-03, -5.7597e-03],\n",
            "          [-1.6875e-02,  1.3999e-04,  3.7629e-04],\n",
            "          [-2.6272e-03,  1.0947e-03,  1.1145e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 1.4018e-02,  3.9198e-03, -1.7189e-03],\n",
            "          [-1.3175e-03,  4.3503e-04, -1.1798e-02],\n",
            "          [-9.8003e-03, -1.7693e-02, -1.9910e-02]],\n",
            "\n",
            "         [[-1.4957e-02, -1.9796e-02, -2.8724e-02],\n",
            "          [ 5.8908e-03, -1.5228e-02, -5.6715e-03],\n",
            "          [ 2.9284e-03, -1.8028e-02, -7.1433e-03]],\n",
            "\n",
            "         [[-1.1625e-02, -3.3804e-02, -1.0025e-02],\n",
            "          [-1.6606e-02, -5.5716e-02, -2.3204e-02],\n",
            "          [-2.5758e-02, -4.3135e-02, -2.5901e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.5007e-02, -1.4333e-02, -2.5937e-03],\n",
            "          [-2.3078e-02, -1.5820e-02, -2.2818e-03],\n",
            "          [-4.1318e-03, -8.0353e-03, -2.3236e-03]],\n",
            "\n",
            "         [[-1.8531e-02, -1.8004e-02, -2.8084e-02],\n",
            "          [-3.6680e-02, -6.8641e-02, -5.2469e-02],\n",
            "          [-1.1712e-02, -2.4334e-02, -1.6733e-02]],\n",
            "\n",
            "         [[-2.2078e-02, -2.9163e-02, -3.8717e-03],\n",
            "          [-7.0301e-03,  1.6718e-02,  5.4339e-03],\n",
            "          [-1.3131e-02,  1.1999e-02, -1.7480e-02]]],\n",
            "\n",
            "\n",
            "        [[[-5.2378e-03, -3.4890e-03, -2.0851e-03],\n",
            "          [ 1.5306e-02, -2.1752e-02, -8.7682e-03],\n",
            "          [ 2.2460e-02,  9.9175e-03, -3.3635e-03]],\n",
            "\n",
            "         [[ 7.4677e-03, -9.1762e-03, -9.2569e-05],\n",
            "          [ 1.9441e-04,  1.2344e-03, -8.9978e-03],\n",
            "          [-5.1243e-04,  2.1850e-04, -4.8828e-03]],\n",
            "\n",
            "         [[ 1.7078e-02,  3.3955e-03,  9.3503e-03],\n",
            "          [ 2.0334e-02, -1.0621e-04, -8.2017e-05],\n",
            "          [ 1.0706e-02, -1.8414e-03,  1.0828e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.2008e-02,  2.3494e-02,  2.5386e-02],\n",
            "          [ 1.9307e-02,  2.3924e-02,  2.8972e-02],\n",
            "          [ 9.9003e-03,  2.0158e-02,  2.2655e-02]],\n",
            "\n",
            "         [[-9.8395e-03, -1.1114e-02, -3.7696e-03],\n",
            "          [-2.9508e-02, -3.6956e-02, -1.8228e-02],\n",
            "          [-1.3663e-03, -2.5845e-03,  1.0352e-02]],\n",
            "\n",
            "         [[-7.3867e-03, -2.5413e-02, -2.1942e-02],\n",
            "          [-1.6699e-02, -1.5133e-02, -1.3030e-02],\n",
            "          [-2.0090e-02,  3.7970e-03, -1.0341e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1.6157e-02, -1.6883e-02, -2.8328e-04],\n",
            "          [-7.7759e-03, -2.4465e-03, -1.4641e-02],\n",
            "          [ 2.4639e-02,  3.9862e-02,  2.1048e-02]],\n",
            "\n",
            "         [[ 2.4491e-03, -9.3885e-03, -1.1786e-02],\n",
            "          [ 2.5301e-02,  2.5625e-04,  7.1335e-03],\n",
            "          [ 2.2342e-02,  1.9042e-02,  7.2526e-03]],\n",
            "\n",
            "         [[-1.4652e-02, -2.7802e-02, -4.3564e-03],\n",
            "          [-1.7961e-02, -4.3846e-02,  2.7409e-03],\n",
            "          [-4.7968e-03, -8.4231e-03,  1.2070e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.0171e-02, -3.3546e-02, -1.6728e-02],\n",
            "          [-1.7847e-02, -5.1713e-02, -2.6780e-02],\n",
            "          [-1.3145e-03, -4.3181e-03, -9.6373e-03]],\n",
            "\n",
            "         [[-5.3917e-03, -2.0410e-04,  2.7798e-03],\n",
            "          [-9.6882e-04, -2.5141e-02,  1.4804e-02],\n",
            "          [ 2.8748e-02,  9.0832e-03,  4.2548e-02]],\n",
            "\n",
            "         [[-1.5698e-02, -1.9303e-02, -9.1469e-03],\n",
            "          [-2.0025e-02, -1.1131e-02, -3.3902e-02],\n",
            "          [-5.7436e-03, -7.3640e-03, -1.0044e-02]]],\n",
            "\n",
            "\n",
            "        [[[-8.8612e-03, -4.5370e-03, -1.2354e-02],\n",
            "          [-5.9245e-03, -1.7058e-02, -2.8041e-02],\n",
            "          [-1.0435e-02,  7.6695e-04, -1.0578e-02]],\n",
            "\n",
            "         [[ 9.5200e-03, -5.1975e-03,  1.2947e-02],\n",
            "          [ 4.4305e-03, -2.3992e-02, -8.4569e-04],\n",
            "          [ 4.6608e-03,  9.6787e-03,  8.2174e-03]],\n",
            "\n",
            "         [[ 5.1559e-03,  4.4635e-04, -7.9934e-03],\n",
            "          [ 3.3069e-03,  1.4450e-02,  8.9234e-03],\n",
            "          [ 6.3402e-03,  1.9043e-02,  1.9021e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.6964e-03, -1.3777e-02,  6.0539e-03],\n",
            "          [-1.5745e-03, -2.3391e-02, -1.0052e-02],\n",
            "          [ 9.5183e-03, -1.2251e-02,  2.2436e-03]],\n",
            "\n",
            "         [[ 1.0375e-02,  3.5875e-03, -5.7940e-04],\n",
            "          [ 7.0412e-03, -1.0673e-02, -4.9120e-03],\n",
            "          [-2.6034e-03,  1.1306e-02,  7.0696e-03]],\n",
            "\n",
            "         [[-1.7509e-02, -2.3182e-02, -1.7897e-02],\n",
            "          [-1.7769e-03,  1.9672e-03, -7.3220e-03],\n",
            "          [-6.6833e-03,  9.8286e-03,  2.0653e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 2.8375e-02, -8.1936e-03,  1.8009e-02],\n",
            "          [ 1.5829e-02, -1.3571e-02, -1.9335e-02],\n",
            "          [ 4.0766e-03, -1.5722e-02, -5.0620e-02]],\n",
            "\n",
            "         [[-5.5310e-03, -1.8996e-02, -7.9436e-03],\n",
            "          [ 1.3825e-03, -4.9608e-02,  1.7256e-03],\n",
            "          [ 7.6629e-03, -7.6101e-03,  1.2541e-02]],\n",
            "\n",
            "         [[ 1.8052e-02,  3.1718e-02,  4.2556e-03],\n",
            "          [-3.6760e-03,  3.0490e-03, -1.2264e-02],\n",
            "          [-8.9404e-03, -1.6604e-02,  1.6348e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.3192e-03,  1.8204e-02,  1.8114e-02],\n",
            "          [-6.1202e-03,  1.5905e-03,  2.0264e-02],\n",
            "          [-1.1471e-02, -1.5697e-02,  9.0871e-03]],\n",
            "\n",
            "         [[ 3.7707e-03,  8.0599e-03,  1.8290e-02],\n",
            "          [ 1.7257e-02,  6.9638e-03,  1.8746e-02],\n",
            "          [ 1.0751e-02,  1.3663e-02, -1.0081e-03]],\n",
            "\n",
            "         [[ 1.9711e-02, -1.4569e-02, -2.4663e-02],\n",
            "          [ 2.5966e-03, -2.4807e-02,  9.3861e-03],\n",
            "          [-1.2876e-03,  1.3974e-03,  1.3434e-02]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.4474, 0.5138, 0.4335, 0.3421, 0.3855, 0.3495, 0.3741, 0.5836, 0.4327,\n",
            "        0.5043, 0.4618, 0.3866, 0.3498, 0.4798, 0.3310, 0.3913, 0.3880, 0.5225,\n",
            "        0.3975, 0.3292, 0.4151, 0.4458, 0.3970, 0.3614, 0.3914, 0.4633, 0.3463,\n",
            "        0.3644, 0.3272, 0.4584, 0.4280, 0.4538, 0.4030, 0.4673, 0.4209, 0.3987,\n",
            "        0.4233, 0.3876, 0.4212, 0.3460, 0.3522, 0.3744, 0.4550, 0.2888, 0.4590,\n",
            "        0.4817, 0.4450, 0.5110, 0.4052, 0.4247, 0.3558, 0.3075, 0.4462, 0.4724,\n",
            "        0.4253, 0.3884, 0.4492, 0.3727, 0.4630, 0.3985, 0.3512, 0.3665, 0.3860,\n",
            "        0.5082, 0.4022, 0.3458, 0.4805, 0.5390, 0.4223, 0.4275, 0.4590, 0.4736,\n",
            "        0.3673, 0.5405, 0.3243, 0.5178, 0.4743, 0.3506, 0.3759, 0.4328, 0.3867,\n",
            "        0.4591, 0.3843, 0.4982, 0.5288, 0.3946, 0.4589, 0.3197, 0.4676, 0.4806,\n",
            "        0.4308, 0.4235, 0.3284, 0.3877, 0.4140, 0.4469, 0.4041, 0.4407, 0.4356,\n",
            "        0.5120, 0.5059, 0.4628, 0.4585, 0.3311, 0.3424, 0.4150, 0.5170, 0.4593,\n",
            "        0.5228, 0.4252, 0.4214, 0.4995, 0.4098, 0.5380, 0.4874, 0.3719, 0.4649,\n",
            "        0.4320, 0.3277, 0.3743, 0.4360, 0.4838, 0.4399, 0.3763, 0.4150, 0.5147,\n",
            "        0.5012, 0.4382, 0.3655, 0.4037, 0.4498, 0.4720, 0.3914, 0.3237, 0.3208,\n",
            "        0.3224, 0.4291, 0.4009, 0.3947, 0.3779, 0.4349, 0.4120, 0.3274, 0.4334,\n",
            "        0.3740, 0.4189, 0.4288, 0.3071, 0.4260, 0.3410, 0.4375, 0.4407, 0.3750,\n",
            "        0.5853, 0.4518, 0.5045, 0.3005, 0.4968, 0.4155, 0.3755, 0.5514, 0.4146,\n",
            "        0.4677, 0.1404, 0.5001, 0.4193, 0.4246, 0.4452, 0.5109, 0.4488, 0.4574,\n",
            "        0.3896, 0.4145, 0.4497, 0.4245, 0.3971, 0.3957, 0.4072, 0.5305, 0.4986,\n",
            "        0.3733, 0.4280, 0.3469, 0.4178, 0.3766, 0.4029, 0.3814, 0.4493, 0.5132,\n",
            "        0.4080, 0.4155, 0.3635, 0.4391, 0.3489, 0.4228, 0.4833, 0.3494, 0.4406,\n",
            "        0.3795, 0.4298, 0.4910, 0.3878, 0.6299, 0.4322, 0.5436, 0.4140, 0.4312,\n",
            "        0.3161, 0.3612, 0.3597, 0.4281, 0.4506, 0.4294, 0.3646, 0.4110, 0.4038,\n",
            "        0.4098, 0.3901, 0.3928, 0.5421, 0.3629, 0.4078, 0.4586, 0.4217, 0.3953,\n",
            "        0.3997, 0.3838, 0.4374, 0.3576, 0.4217, 0.4128, 0.3904, 0.4137, 0.5145,\n",
            "        0.4039, 0.3577, 0.4429, 0.5639, 0.3848, 0.6104, 0.4482, 0.6203, 0.5336,\n",
            "        0.3480, 0.5401, 0.6044, 0.4077, 0.3469, 0.4281, 0.4631, 0.5948, 0.3479,\n",
            "        0.3689, 0.3658, 0.3191, 0.5492, 0.3410, 0.5386, 0.4041, 0.3373, 0.4186,\n",
            "        0.5187, 0.3933, 0.3188, 0.3502, 0.3736, 0.4238, 0.4752, 0.3322, 0.5078,\n",
            "        0.4317, 0.5318, 0.4413, 0.5510, 0.5648, 0.4130, 0.4017, 0.4304, 0.4077,\n",
            "        0.4285, 0.4360, 0.3749, 0.4261, 0.3905, 0.3030, 0.3412, 0.3768, 0.4507,\n",
            "        0.3127, 0.4592, 0.4298, 0.3936, 0.3106, 0.3869, 0.3594, 0.4046, 0.4722,\n",
            "        0.4373, 0.3902, 0.3515, 0.4448, 0.4299, 0.4347, 0.4693, 0.4807, 0.2549,\n",
            "        0.4171, 0.4387, 0.4156, 0.3976, 0.4092, 0.4953, 0.4824, 0.3468, 0.4382,\n",
            "        0.4179, 0.4668, 0.3299, 0.5986, 0.4949, 0.4167, 0.4996, 0.4528, 0.4550,\n",
            "        0.4945, 0.3415, 0.4658, 0.4356, 0.3976, 0.5439, 0.4643, 0.5122, 0.4669,\n",
            "        0.4463, 0.4810, 0.3492, 0.3961, 0.3593, 0.4053, 0.3878, 0.3959, 0.5001,\n",
            "        0.2808, 0.5470, 0.4448, 0.4894, 0.4621, 0.3417, 0.3485, 0.5060, 0.3637,\n",
            "        0.3774, 0.3248, 0.4520, 0.3936, 0.3403, 0.4660, 0.4114, 0.3643, 0.4196,\n",
            "        0.3903, 0.5128, 0.4221, 0.4115, 0.4240, 0.3610, 0.4999, 0.3672, 0.4721,\n",
            "        0.4252, 0.5590, 0.4694, 0.7322, 0.5849, 0.4749, 0.4426, 0.3934, 0.3909,\n",
            "        0.4576, 0.3636, 0.4146, 0.4129, 0.5081, 0.3681, 0.3652, 0.4254, 0.2945,\n",
            "        0.4142, 0.3145, 0.4304, 0.4252, 0.3493, 0.4257, 0.5133, 0.3261, 0.4367,\n",
            "        0.3637, 0.3712, 0.4183, 0.3772, 0.4418, 0.4231, 0.4133, 0.4731, 0.4955,\n",
            "        0.4046, 0.4079, 0.4719, 0.3875, 0.4673, 0.4129, 0.4569, 0.3530, 0.4793,\n",
            "        0.3844, 0.3785, 0.3343, 0.4351, 0.6512, 0.4295, 0.4122, 0.3788, 0.3692,\n",
            "        0.4343, 0.4214, 0.3873, 0.4566, 0.4456, 0.4107, 0.4596, 0.7082, 0.4452,\n",
            "        0.3515, 0.4785, 0.4217, 0.5756, 0.4312, 0.4047, 0.4043, 0.4764, 0.5489,\n",
            "        0.4430, 0.5559, 0.3744, 0.3951, 0.4376, 0.4752, 0.4340, 0.4399, 0.3586,\n",
            "        0.4161, 0.3930, 0.4599, 0.4354, 0.3448, 0.4649, 0.4442, 0.4275, 0.3881,\n",
            "        0.3247, 0.4909, 0.3426, 0.3989, 0.4320, 0.3363, 0.3991, 0.4732, 0.3514,\n",
            "        0.4736, 0.4244, 0.4603, 0.3298, 0.4357, 0.4353, 0.3742, 0.4191, 0.3880,\n",
            "        0.4212, 0.4527, 0.7213, 0.3969, 0.5217, 0.3786, 0.3512, 0.5318, 0.4138,\n",
            "        0.3243, 0.3244, 0.3652, 0.4774, 0.3997, 0.2800, 0.4562, 0.4463, 0.4816,\n",
            "        0.4290, 0.4399, 0.4633, 0.3575, 0.4774, 0.3105, 0.4356, 0.3797, 0.4304,\n",
            "        0.4261, 0.3740, 0.3370, 0.3917, 0.3637, 0.4347, 0.5235, 0.3845],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1759, -0.2156, -0.2047, -0.1695, -0.1628, -0.1473, -0.2158, -0.2905,\n",
            "        -0.1112, -0.2196, -0.1020, -0.1549, -0.1989, -0.0445, -0.1508, -0.1920,\n",
            "        -0.2114, -0.1655, -0.1854, -0.1733, -0.1289, -0.2376, -0.1965, -0.1965,\n",
            "        -0.1776, -0.1774, -0.1760, -0.1546, -0.1648, -0.2599, -0.1752, -0.2498,\n",
            "        -0.1741, -0.2410, -0.2498, -0.2938, -0.1496, -0.1578, -0.1800, -0.1851,\n",
            "        -0.1516, -0.1345, -0.2746, -0.1248, -0.2246, -0.2531, -0.2398, -0.1859,\n",
            "        -0.1739, -0.2393, -0.1214, -0.1803, -0.2729, -0.2617, -0.1855, -0.2316,\n",
            "        -0.2333, -0.1860, -0.2097, -0.0692, -0.1912, -0.2078, -0.1084, -0.2810,\n",
            "        -0.1303, -0.1654, -0.2119, -0.3641, -0.2951, -0.2384, -0.1632, -0.1892,\n",
            "        -0.1792, -0.2031, -0.1770, -0.2738, -0.3324, -0.1725, -0.1793, -0.2638,\n",
            "        -0.2207, -0.1609, -0.1534, -0.1414, -0.2992, -0.1450, -0.1838, -0.1779,\n",
            "        -0.1422, -0.2198, -0.1900, -0.1580, -0.1666, -0.2490, -0.1569, -0.1718,\n",
            "        -0.1660, -0.1972, -0.2287, -0.2366, -0.2230, -0.1543, -0.2030, -0.1431,\n",
            "        -0.1363, -0.2015, -0.1804, -0.2093, -0.2964, -0.1984, -0.2683, -0.2216,\n",
            "        -0.2147, -0.3404, -0.2668, -0.1890, -0.1733, -0.2226, -0.1772, -0.1698,\n",
            "        -0.1095, -0.2180, -0.1154, -0.1654, -0.1910, -0.3535, -0.3112, -0.2161,\n",
            "        -0.1496, -0.1667, -0.2849, -0.2207, -0.1529, -0.1807, -0.2118, -0.1869,\n",
            "        -0.1376, -0.1770, -0.1861, -0.1969, -0.1741, -0.3011, -0.0787, -0.2017,\n",
            "        -0.1947, -0.2247, -0.2459, -0.1058, -0.1401, -0.1213, -0.1199, -0.1760,\n",
            "        -0.2156, -0.3307, -0.3515, -0.2366, -0.1185, -0.2155, -0.1751, -0.1892,\n",
            "        -0.3365, -0.1598, -0.2554,  0.0644, -0.2856, -0.1198, -0.1583, -0.2297,\n",
            "        -0.3352, -0.1987, -0.2686, -0.1632, -0.2461, -0.2900, -0.2428, -0.1449,\n",
            "        -0.1900, -0.2149, -0.1541, -0.2917, -0.2504, -0.2213, -0.0463, -0.1547,\n",
            "        -0.1511, -0.1527, -0.1735, -0.1931, -0.1987, -0.2239, -0.2086, -0.2688,\n",
            "        -0.1845, -0.1797, -0.1833, -0.3880, -0.1539, -0.1553, -0.1567, -0.2238,\n",
            "        -0.1511, -0.2540, -0.2849, -0.1826, -0.2687, -0.2328, -0.2108, -0.2410,\n",
            "        -0.1022, -0.1507, -0.1978, -0.1734, -0.2282, -0.0985, -0.1847, -0.1770,\n",
            "        -0.1576, -0.1937, -0.1643, -0.2822, -0.1866, -0.2754, -0.2266, -0.2169,\n",
            "        -0.1352, -0.2194, -0.1060, -0.2139, -0.1322, -0.1889, -0.2130, -0.1913,\n",
            "        -0.2364, -0.1402, -0.2228, -0.2354, -0.1632, -0.1905, -0.1428, -0.1177,\n",
            "        -0.2419, -0.2733, -0.2963, -0.1600, -0.3558, -0.3673, -0.2201, -0.1505,\n",
            "        -0.2084, -0.0870, -0.2052, -0.2070, -0.1986, -0.2299, -0.0745, -0.1765,\n",
            "        -0.1412, -0.2180, -0.1450, -0.1426, -0.1452, -0.2916, -0.0871, -0.1359,\n",
            "        -0.2003, -0.1125, -0.2588, -0.1988, -0.2028, -0.2443, -0.0864, -0.3415,\n",
            "        -0.2579, -0.2343, -0.3552, -0.1859, -0.1153, -0.1732, -0.1780, -0.1909,\n",
            "        -0.2018, -0.1886, -0.2751, -0.1501,  0.1165, -0.1891, -0.1845, -0.2037,\n",
            "        -0.0339, -0.3464, -0.1956, -0.1962, -0.1537, -0.1902, -0.1431, -0.3022,\n",
            "        -0.1780, -0.1971, -0.2118, -0.0952, -0.1711, -0.2409, -0.2184, -0.2114,\n",
            "        -0.2042, -0.0566, -0.0700, -0.2081, -0.1872, -0.2079, -0.1540, -0.2266,\n",
            "        -0.1981, -0.1679, -0.2022, -0.2010, -0.1051, -0.1705, -0.2139,  0.0396,\n",
            "        -0.1077, -0.2745, -0.2690, -0.2603, -0.2819, -0.1917, -0.1940, -0.2944,\n",
            "        -0.1822, -0.2903, -0.1064, -0.2076, -0.2648, -0.3032, -0.2878, -0.1579,\n",
            "        -0.0071, -0.2142, -0.2022, -0.1516, -0.1123,  0.0246, -0.0978, -0.1382,\n",
            "        -0.1800, -0.3214, -0.2179, -0.1369, -0.0800,  0.0117, -0.1839, -0.1926,\n",
            "        -0.1614, -0.2769, -0.1909, -0.2101, -0.2305, -0.2055, -0.2017, -0.2741,\n",
            "        -0.1005, -0.3152, -0.1121, -0.1700, -0.1364, -0.2157, -0.2673, -0.1584,\n",
            "        -0.1997, -0.1745, -0.1886, -0.2307, -0.2024, -0.3376, -0.2266, -0.2355,\n",
            "        -0.2133, -0.2346, -0.2412, -0.2358, -0.1265, -0.2341, -0.1887, -0.1646,\n",
            "        -0.1417, -0.1882, -0.1076, -0.3048, -0.1162, -0.1651, -0.2046, -0.1833,\n",
            "        -0.3102, -0.1778, -0.1575, -0.2676, -0.1777, -0.1569, -0.1741, -0.1892,\n",
            "        -0.3028, -0.1457, -0.2179, -0.2226, -0.1609, -0.1423, -0.2683, -0.2920,\n",
            "        -0.1740, -0.2079, -0.1940, -0.2679, -0.1973, -0.1951, -0.1665, -0.2286,\n",
            "        -0.1903, -0.2667, -0.4010, -0.2550, -0.1817, -0.2025, -0.1589, -0.2476,\n",
            "        -0.0573, -0.2203, -0.2084, -0.1587, -0.1212, -0.1795, -0.3449, -0.1662,\n",
            "        -0.2523, -0.2435, -0.2878, -0.2797, -0.1897, -0.2113, -0.1943, -0.2050,\n",
            "        -0.1694, -0.2243, -0.2987, -0.1328, -0.1428, -0.2399, -0.1593, -0.1999,\n",
            "        -0.3225, -0.1860, -0.1763, -0.2691, -0.2097, -0.2396, -0.1140, -0.1897,\n",
            "        -0.1870, -0.1829, -0.2615, -0.2073, -0.1858, -0.0598, -0.1915, -0.2183,\n",
            "        -0.2088, -0.1742, -0.2715, -0.1999, -0.2117, -0.2492, -0.1717, -0.1566,\n",
            "        -0.1669, -0.3015, -0.1685, -0.2434, -0.2297, -0.1947, -0.2860, -0.3288,\n",
            "        -0.2197, -0.1862, -0.1755, -0.0987, -0.1756, -0.1304, -0.1555, -0.1679,\n",
            "        -0.2222, -0.2819, -0.2652, -0.0947, -0.2412, -0.2731, -0.2572, -0.2604,\n",
            "        -0.2934, -0.2470, -0.1820, -0.2740, -0.1336, -0.1698, -0.1919, -0.1796,\n",
            "        -0.2325, -0.1352, -0.1077, -0.2184, -0.1539, -0.2015, -0.3243, -0.1713],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 0.0057]],\n",
            "\n",
            "         [[ 0.0020]],\n",
            "\n",
            "         [[ 0.0167]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0085]],\n",
            "\n",
            "         [[-0.0274]],\n",
            "\n",
            "         [[ 0.0097]]],\n",
            "\n",
            "\n",
            "        [[[-0.0271]],\n",
            "\n",
            "         [[-0.0157]],\n",
            "\n",
            "         [[ 0.0543]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0206]],\n",
            "\n",
            "         [[-0.0308]],\n",
            "\n",
            "         [[ 0.0013]]],\n",
            "\n",
            "\n",
            "        [[[-0.0523]],\n",
            "\n",
            "         [[-0.0353]],\n",
            "\n",
            "         [[ 0.0394]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0382]],\n",
            "\n",
            "         [[-0.0264]],\n",
            "\n",
            "         [[-0.0443]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0494]],\n",
            "\n",
            "         [[ 0.0436]],\n",
            "\n",
            "         [[ 0.0103]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0072]],\n",
            "\n",
            "         [[ 0.0014]],\n",
            "\n",
            "         [[-0.0669]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0533]],\n",
            "\n",
            "         [[-0.0148]],\n",
            "\n",
            "         [[-0.0480]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0055]],\n",
            "\n",
            "         [[ 0.0429]],\n",
            "\n",
            "         [[ 0.0129]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0300]],\n",
            "\n",
            "         [[-0.0092]],\n",
            "\n",
            "         [[ 0.0090]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0101]],\n",
            "\n",
            "         [[-0.0111]],\n",
            "\n",
            "         [[-0.0080]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.1694,  0.3368,  0.2993,  0.3745,  0.1513,  0.1781,  0.3167,  0.3947,\n",
            "         0.1858,  0.2068,  0.1090,  0.2042,  0.2955,  0.0765,  0.2023,  0.2487,\n",
            "         0.3295,  0.3349,  0.2532,  0.2739,  0.1661,  0.3432,  0.3424,  0.2969,\n",
            "         0.2226,  0.0993,  0.3328,  0.2349,  0.2894,  0.2296,  0.2719,  0.3945,\n",
            "         0.1990,  0.2564,  0.2557,  0.3541,  0.1848,  0.2513,  0.3101,  0.2782,\n",
            "         0.2109,  0.2441,  0.3282,  0.3248,  0.2499,  0.1873,  0.2643,  0.3949,\n",
            "         0.1962,  0.2587,  0.1708,  0.3381,  0.2238,  0.2498,  0.2787,  0.3783,\n",
            "         0.3445,  0.2681,  0.2956,  0.1146,  0.2688,  0.3479,  0.1295,  0.2843,\n",
            "         0.1552,  0.3026,  0.2738,  0.1891,  0.3568,  0.2302,  0.2199,  0.2070,\n",
            "         0.2119,  0.0971,  0.2482,  0.2264,  0.3555,  0.3113,  0.2386,  0.2654,\n",
            "         0.2975,  0.2666,  0.2180,  0.1451,  0.2460,  0.1734,  0.2358,  0.2891,\n",
            "         0.2091,  0.1971,  0.2185,  0.2008,  0.2461,  0.3726,  0.2028,  0.1993,\n",
            "         0.3652,  0.2258,  0.2606,  0.1900,  0.2764,  0.2011,  0.1973,  0.2958,\n",
            "         0.3222,  0.4117,  0.1475,  0.2674,  0.1928,  0.3615,  0.2774,  0.2143,\n",
            "         0.2688,  0.4286,  0.2560,  0.2777,  0.1339,  0.5103,  0.3238,  0.2417,\n",
            "         0.1529,  0.1843,  0.0579,  0.2288,  0.1797,  0.2803,  0.2279,  0.1579,\n",
            "         0.3196,  0.1842,  0.3378,  0.1688,  0.1654,  0.3049,  0.3533,  0.2948,\n",
            "         0.1140,  0.2503,  0.1892,  0.2647,  0.2405,  0.3880,  0.1933,  0.1918,\n",
            "         0.2511,  0.2901,  0.3151,  0.3252,  0.1296,  0.2491,  0.1417,  0.1295,\n",
            "         0.3062,  0.2836,  0.3483,  0.2306,  0.2741,  0.2700,  0.1873,  0.2431,\n",
            "         0.3526,  0.3546,  0.2721,  0.2708,  0.3065,  0.0832,  0.2968,  0.2286,\n",
            "         0.3276,  0.2695,  0.2452,  0.2444,  0.2857,  0.3365,  0.2784,  0.2933,\n",
            "         0.3397,  0.2231,  0.2330,  0.1486,  0.3846,  0.3104,  0.1724,  0.1724,\n",
            "         0.3466,  0.2978,  0.2582,  0.1879,  0.2419,  0.2249,  0.2720,  0.3735,\n",
            "         0.4259,  0.3754,  0.1731,  0.3698,  0.2349,  0.2694,  0.3148,  0.1658,\n",
            "         0.1181,  0.2994,  0.4018,  0.2126,  0.3864,  0.2955,  0.1848,  0.3686,\n",
            "         0.1972,  0.3265,  0.2319,  0.1676,  0.1756,  0.2367,  0.2139,  0.1974,\n",
            "         0.2561,  0.2619,  0.2170,  0.2284,  0.3486,  0.4500,  0.2563,  0.2559,\n",
            "         0.2814,  0.1797,  0.1736,  0.2013,  0.3411,  0.2245,  0.1385,  0.2284,\n",
            "         0.2230,  0.2566,  0.2301,  0.3639,  0.1380,  0.2381,  0.2590,  0.0830,\n",
            "         0.1863,  0.1267,  0.4501,  0.2741,  0.2590,  0.2782,  0.2248,  0.2718,\n",
            "         0.1949,  0.1815,  0.2969,  0.3168,  0.3389,  0.2790,  0.1594,  0.2752,\n",
            "         0.2947,  0.2909,  0.1418,  0.3336,  0.1953,  0.2646,  0.0879,  0.2553,\n",
            "         0.3335,  0.1943,  0.2777,  0.2386,  0.3676,  0.3042,  0.1234,  0.2615,\n",
            "         0.2548,  0.3224,  0.3462,  0.2090,  0.2142,  0.2054,  0.2115,  0.2153,\n",
            "         0.2163,  0.2509,  0.2429,  0.3326, -0.0527,  0.2244,  0.2319,  0.2674,\n",
            "         0.1103,  0.2320,  0.2822,  0.3234,  0.2818,  0.2093,  0.2261,  0.2900,\n",
            "         0.3127,  0.3456,  0.2592,  0.1677,  0.3924,  0.2694,  0.1997,  0.2973,\n",
            "         0.3324,  0.2270,  0.0656,  0.2964,  0.1948,  0.2383,  0.3021,  0.2510,\n",
            "         0.3117,  0.3185,  0.1721,  0.1867,  0.1665,  0.2851,  0.3512, -0.0486,\n",
            "         0.1558,  0.2213,  0.3281,  0.3861,  0.2375,  0.3057,  0.1178,  0.2681,\n",
            "         0.1921,  0.2211,  0.1679,  0.2877,  0.2495,  0.2451,  0.2678,  0.2393,\n",
            "         0.0988,  0.2778,  0.2465,  0.1747,  0.1005,  0.0502,  0.2809,  0.2810,\n",
            "         0.1716,  0.2114,  0.2213,  0.2817,  0.1506,  0.0769,  0.2381,  0.2411,\n",
            "         0.2942,  0.2543,  0.2556,  0.3451,  0.2948,  0.3040,  0.3204,  0.2757,\n",
            "         0.1657,  0.2941,  0.1301,  0.1854,  0.2866,  0.3198,  0.2127,  0.3608,\n",
            "         0.3440,  0.0954,  0.2586,  0.1709,  0.2007,  0.1967,  0.1972,  0.1942,\n",
            "         0.3201,  0.3484,  0.3437,  0.3153,  0.2020,  0.3251,  0.3227,  0.3038,\n",
            "         0.2634,  0.2364,  0.2492,  0.3080,  0.2591,  0.2391,  0.2720,  0.2601,\n",
            "         0.3210,  0.1818,  0.3526,  0.3579,  0.2861,  0.2526,  0.1642,  0.2897,\n",
            "         0.3996,  0.2651,  0.2031,  0.2502,  0.3694,  0.2085,  0.2804,  0.2233,\n",
            "         0.2309,  0.1609,  0.2369,  0.2116,  0.3549,  0.1635,  0.1642,  0.3072,\n",
            "         0.3077,  0.2152,  0.2821,  0.2857,  0.1701,  0.2305,  0.2134,  0.3189,\n",
            "         0.1061,  0.2628,  0.2608,  0.1749,  0.0820,  0.1815,  0.3566,  0.1204,\n",
            "         0.3159,  0.1595,  0.3790,  0.3272,  0.2086,  0.3096,  0.2253,  0.1456,\n",
            "         0.1346,  0.2304,  0.2913,  0.2727,  0.2027,  0.2688,  0.1958,  0.2277,\n",
            "         0.3036,  0.3250,  0.3000,  0.3328,  0.2417,  0.2665,  0.2473,  0.0913,\n",
            "         0.2503,  0.2543,  0.3710,  0.3321,  0.3693,  0.1099,  0.1701,  0.1758,\n",
            "         0.3888,  0.2206,  0.2766,  0.2813,  0.1755,  0.2616,  0.1544,  0.2519,\n",
            "         0.1945,  0.2452,  0.3405,  0.2446,  0.2426,  0.1822,  0.3002,  0.3037,\n",
            "         0.3118,  0.2414,  0.2326,  0.1303,  0.3081,  0.0979,  0.2776,  0.2918,\n",
            "         0.3848,  0.1789,  0.3622,  0.3005,  0.1923,  0.2672,  0.1663,  0.2998,\n",
            "         0.2710,  0.2040,  0.2565,  0.2289,  0.2552,  0.2121,  0.3532,  0.2293,\n",
            "         0.2510,  0.3085,  0.2368,  0.3000,  0.2111,  0.3456,  0.3422,  0.1576],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1759, -0.2156, -0.2047, -0.1695, -0.1628, -0.1473, -0.2158, -0.2905,\n",
            "        -0.1112, -0.2196, -0.1020, -0.1549, -0.1989, -0.0445, -0.1508, -0.1920,\n",
            "        -0.2114, -0.1655, -0.1854, -0.1733, -0.1289, -0.2376, -0.1965, -0.1965,\n",
            "        -0.1776, -0.1774, -0.1760, -0.1546, -0.1648, -0.2599, -0.1752, -0.2498,\n",
            "        -0.1741, -0.2410, -0.2498, -0.2938, -0.1496, -0.1578, -0.1800, -0.1851,\n",
            "        -0.1516, -0.1345, -0.2746, -0.1248, -0.2246, -0.2531, -0.2398, -0.1859,\n",
            "        -0.1739, -0.2393, -0.1214, -0.1803, -0.2729, -0.2617, -0.1855, -0.2316,\n",
            "        -0.2333, -0.1860, -0.2097, -0.0692, -0.1912, -0.2078, -0.1084, -0.2810,\n",
            "        -0.1303, -0.1654, -0.2119, -0.3641, -0.2951, -0.2384, -0.1632, -0.1892,\n",
            "        -0.1792, -0.2031, -0.1770, -0.2738, -0.3324, -0.1725, -0.1793, -0.2638,\n",
            "        -0.2207, -0.1609, -0.1534, -0.1414, -0.2992, -0.1450, -0.1838, -0.1779,\n",
            "        -0.1422, -0.2198, -0.1900, -0.1580, -0.1666, -0.2490, -0.1569, -0.1718,\n",
            "        -0.1660, -0.1972, -0.2287, -0.2366, -0.2230, -0.1543, -0.2030, -0.1431,\n",
            "        -0.1363, -0.2015, -0.1804, -0.2093, -0.2964, -0.1984, -0.2683, -0.2216,\n",
            "        -0.2147, -0.3404, -0.2668, -0.1890, -0.1733, -0.2226, -0.1772, -0.1698,\n",
            "        -0.1095, -0.2180, -0.1154, -0.1654, -0.1910, -0.3535, -0.3112, -0.2161,\n",
            "        -0.1496, -0.1667, -0.2849, -0.2207, -0.1529, -0.1807, -0.2118, -0.1869,\n",
            "        -0.1376, -0.1770, -0.1861, -0.1969, -0.1741, -0.3011, -0.0787, -0.2017,\n",
            "        -0.1947, -0.2247, -0.2459, -0.1058, -0.1401, -0.1213, -0.1199, -0.1760,\n",
            "        -0.2156, -0.3307, -0.3515, -0.2366, -0.1185, -0.2155, -0.1751, -0.1892,\n",
            "        -0.3365, -0.1598, -0.2554,  0.0644, -0.2856, -0.1198, -0.1583, -0.2297,\n",
            "        -0.3352, -0.1987, -0.2686, -0.1632, -0.2461, -0.2900, -0.2428, -0.1449,\n",
            "        -0.1900, -0.2149, -0.1541, -0.2917, -0.2504, -0.2213, -0.0463, -0.1547,\n",
            "        -0.1511, -0.1527, -0.1735, -0.1931, -0.1987, -0.2239, -0.2086, -0.2688,\n",
            "        -0.1845, -0.1797, -0.1833, -0.3880, -0.1539, -0.1553, -0.1567, -0.2238,\n",
            "        -0.1511, -0.2540, -0.2849, -0.1826, -0.2687, -0.2328, -0.2108, -0.2410,\n",
            "        -0.1022, -0.1507, -0.1978, -0.1734, -0.2282, -0.0985, -0.1847, -0.1770,\n",
            "        -0.1576, -0.1937, -0.1643, -0.2822, -0.1866, -0.2754, -0.2266, -0.2169,\n",
            "        -0.1352, -0.2194, -0.1060, -0.2139, -0.1322, -0.1889, -0.2130, -0.1913,\n",
            "        -0.2364, -0.1402, -0.2228, -0.2354, -0.1632, -0.1905, -0.1428, -0.1177,\n",
            "        -0.2419, -0.2733, -0.2963, -0.1600, -0.3558, -0.3673, -0.2201, -0.1505,\n",
            "        -0.2084, -0.0870, -0.2052, -0.2070, -0.1986, -0.2299, -0.0745, -0.1765,\n",
            "        -0.1412, -0.2180, -0.1450, -0.1426, -0.1452, -0.2916, -0.0871, -0.1359,\n",
            "        -0.2003, -0.1125, -0.2588, -0.1988, -0.2028, -0.2443, -0.0864, -0.3415,\n",
            "        -0.2579, -0.2343, -0.3552, -0.1859, -0.1153, -0.1732, -0.1780, -0.1909,\n",
            "        -0.2018, -0.1886, -0.2751, -0.1501,  0.1165, -0.1891, -0.1845, -0.2037,\n",
            "        -0.0339, -0.3464, -0.1956, -0.1962, -0.1537, -0.1902, -0.1431, -0.3022,\n",
            "        -0.1780, -0.1971, -0.2118, -0.0952, -0.1711, -0.2409, -0.2184, -0.2114,\n",
            "        -0.2042, -0.0566, -0.0700, -0.2081, -0.1872, -0.2079, -0.1540, -0.2266,\n",
            "        -0.1981, -0.1679, -0.2022, -0.2010, -0.1051, -0.1705, -0.2139,  0.0396,\n",
            "        -0.1077, -0.2745, -0.2690, -0.2603, -0.2819, -0.1917, -0.1940, -0.2944,\n",
            "        -0.1822, -0.2903, -0.1064, -0.2076, -0.2648, -0.3032, -0.2878, -0.1579,\n",
            "        -0.0071, -0.2142, -0.2022, -0.1516, -0.1123,  0.0246, -0.0978, -0.1382,\n",
            "        -0.1800, -0.3214, -0.2179, -0.1369, -0.0800,  0.0117, -0.1839, -0.1926,\n",
            "        -0.1614, -0.2769, -0.1909, -0.2101, -0.2305, -0.2055, -0.2017, -0.2741,\n",
            "        -0.1005, -0.3152, -0.1121, -0.1700, -0.1364, -0.2157, -0.2673, -0.1584,\n",
            "        -0.1997, -0.1745, -0.1886, -0.2307, -0.2024, -0.3376, -0.2266, -0.2355,\n",
            "        -0.2133, -0.2346, -0.2412, -0.2358, -0.1265, -0.2341, -0.1887, -0.1646,\n",
            "        -0.1417, -0.1882, -0.1076, -0.3048, -0.1162, -0.1651, -0.2046, -0.1833,\n",
            "        -0.3102, -0.1778, -0.1575, -0.2676, -0.1777, -0.1569, -0.1741, -0.1892,\n",
            "        -0.3028, -0.1457, -0.2179, -0.2226, -0.1609, -0.1423, -0.2683, -0.2920,\n",
            "        -0.1740, -0.2079, -0.1940, -0.2679, -0.1973, -0.1951, -0.1665, -0.2286,\n",
            "        -0.1903, -0.2667, -0.4010, -0.2550, -0.1817, -0.2025, -0.1589, -0.2476,\n",
            "        -0.0573, -0.2203, -0.2084, -0.1587, -0.1212, -0.1795, -0.3449, -0.1662,\n",
            "        -0.2523, -0.2435, -0.2878, -0.2797, -0.1897, -0.2113, -0.1943, -0.2050,\n",
            "        -0.1694, -0.2243, -0.2987, -0.1328, -0.1428, -0.2399, -0.1593, -0.1999,\n",
            "        -0.3225, -0.1860, -0.1763, -0.2691, -0.2097, -0.2396, -0.1140, -0.1897,\n",
            "        -0.1870, -0.1829, -0.2615, -0.2073, -0.1858, -0.0598, -0.1915, -0.2183,\n",
            "        -0.2088, -0.1742, -0.2715, -0.1999, -0.2117, -0.2492, -0.1717, -0.1566,\n",
            "        -0.1669, -0.3015, -0.1685, -0.2434, -0.2297, -0.1947, -0.2860, -0.3288,\n",
            "        -0.2197, -0.1862, -0.1755, -0.0987, -0.1756, -0.1304, -0.1555, -0.1679,\n",
            "        -0.2222, -0.2819, -0.2652, -0.0947, -0.2412, -0.2731, -0.2572, -0.2604,\n",
            "        -0.2934, -0.2470, -0.1820, -0.2740, -0.1336, -0.1698, -0.1919, -0.1796,\n",
            "        -0.2325, -0.1352, -0.1077, -0.2184, -0.1539, -0.2015, -0.3243, -0.1713],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-8.0284e-03, -5.7776e-03,  6.4154e-03],\n",
            "          [ 5.0498e-03, -6.7796e-03,  1.2691e-02],\n",
            "          [ 1.3331e-02,  1.4523e-02,  2.4522e-02]],\n",
            "\n",
            "         [[-1.9876e-03,  1.2466e-02,  1.0494e-02],\n",
            "          [-1.9364e-02, -1.6696e-02, -1.1857e-02],\n",
            "          [-1.1569e-02, -3.7674e-03, -3.4679e-03]],\n",
            "\n",
            "         [[-1.1440e-02, -1.3884e-02,  1.1559e-03],\n",
            "          [-1.7906e-02, -2.9349e-02, -1.3876e-02],\n",
            "          [-1.4057e-02, -2.6989e-02, -2.3963e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-6.3040e-03, -3.1167e-03, -1.3304e-02],\n",
            "          [ 7.1623e-03,  6.4669e-03,  1.6063e-02],\n",
            "          [-1.0750e-02, -1.0480e-02, -6.1070e-03]],\n",
            "\n",
            "         [[ 7.4484e-03,  6.3878e-03, -1.2579e-02],\n",
            "          [-7.7356e-03,  1.8112e-03, -1.7890e-02],\n",
            "          [-2.9142e-03,  7.7705e-03, -9.7314e-03]],\n",
            "\n",
            "         [[ 2.1760e-02,  2.2364e-02,  2.2731e-02],\n",
            "          [ 2.6681e-02,  2.9127e-02,  3.3356e-02],\n",
            "          [ 1.2892e-02, -3.5818e-03,  5.3022e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.0597e-02, -9.1551e-03, -2.3418e-02],\n",
            "          [-1.0768e-02, -3.3171e-03, -1.8559e-02],\n",
            "          [-1.8607e-02, -4.2634e-03, -1.5591e-02]],\n",
            "\n",
            "         [[-2.6090e-02, -2.2517e-02, -3.0593e-02],\n",
            "          [-3.9406e-02, -2.6639e-02, -2.8202e-02],\n",
            "          [-2.6143e-02, -1.9647e-02, -2.1466e-02]],\n",
            "\n",
            "         [[-3.5259e-03,  1.6623e-03, -6.5624e-03],\n",
            "          [-5.0597e-03, -8.7162e-04, -5.3742e-03],\n",
            "          [-7.9651e-03, -9.7778e-03, -1.0736e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.8492e-02, -3.6799e-03,  1.0043e-02],\n",
            "          [-5.2974e-03, -2.0757e-02, -1.5120e-02],\n",
            "          [ 2.1435e-02,  6.4916e-03,  4.7660e-03]],\n",
            "\n",
            "         [[-1.8810e-02, -6.0469e-04, -7.6999e-03],\n",
            "          [-1.7697e-02, -7.8692e-03, -1.6543e-02],\n",
            "          [-1.7206e-02, -2.4746e-02, -3.0270e-02]],\n",
            "\n",
            "         [[-3.1191e-02, -1.4363e-02,  2.2032e-03],\n",
            "          [-1.2033e-02, -2.3699e-03, -1.6630e-02],\n",
            "          [-1.2905e-02, -1.5363e-02, -3.6297e-03]]],\n",
            "\n",
            "\n",
            "        [[[-3.2648e-02, -4.8158e-03, -2.0476e-02],\n",
            "          [-2.5846e-02, -1.4660e-03, -2.8170e-02],\n",
            "          [-2.6640e-02,  4.3022e-03, -2.7636e-02]],\n",
            "\n",
            "         [[-6.3289e-03, -1.5401e-02, -1.3096e-03],\n",
            "          [-1.7499e-02, -2.6212e-02, -2.3646e-02],\n",
            "          [-7.3207e-03, -1.5592e-02, -8.9578e-03]],\n",
            "\n",
            "         [[ 8.9701e-04, -6.6914e-03, -5.3129e-03],\n",
            "          [-1.1727e-03, -1.0726e-02, -9.0103e-03],\n",
            "          [ 3.2311e-03, -4.5854e-03,  4.3512e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.1822e-02, -3.6889e-02, -2.2588e-02],\n",
            "          [-1.3054e-02, -3.4191e-02, -2.7238e-02],\n",
            "          [-1.2383e-02, -2.3452e-02, -2.2486e-02]],\n",
            "\n",
            "         [[ 6.8177e-03,  2.1561e-02,  1.3674e-02],\n",
            "          [ 3.1192e-03,  1.0660e-02,  1.0409e-02],\n",
            "          [ 8.0477e-03, -4.6817e-03, -4.3912e-03]],\n",
            "\n",
            "         [[-1.1983e-02, -1.6201e-02, -2.2626e-02],\n",
            "          [-1.3461e-02, -7.0928e-03, -1.4384e-02],\n",
            "          [-2.4456e-02,  1.4885e-02,  1.2247e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-2.6347e-02, -2.9923e-02, -3.7810e-02],\n",
            "          [-1.5663e-02, -4.1126e-03, -1.1482e-02],\n",
            "          [-1.3415e-02, -1.5432e-02, -1.8204e-02]],\n",
            "\n",
            "         [[-3.8392e-03, -1.1093e-02, -8.0841e-04],\n",
            "          [-5.9634e-03, -5.9165e-03, -9.3332e-03],\n",
            "          [-2.2761e-03,  5.4781e-03, -5.6050e-03]],\n",
            "\n",
            "         [[-1.8406e-03, -2.8134e-03,  8.3246e-03],\n",
            "          [-1.2453e-03,  2.1453e-04,  7.4868e-03],\n",
            "          [ 1.3450e-02,  3.0599e-02,  2.6405e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.5268e-04,  2.3897e-03,  6.2558e-03],\n",
            "          [-1.4338e-02, -2.3146e-02, -1.9024e-02],\n",
            "          [-2.7306e-02, -3.0079e-02, -3.1762e-02]],\n",
            "\n",
            "         [[ 1.4584e-02,  4.3430e-03,  1.2053e-02],\n",
            "          [-6.1130e-03, -2.8539e-02, -1.8268e-02],\n",
            "          [-1.6844e-02, -4.7816e-02, -2.6274e-02]],\n",
            "\n",
            "         [[-1.8850e-02, -9.3396e-03,  7.8905e-03],\n",
            "          [-1.5322e-03,  8.3153e-03,  1.7783e-02],\n",
            "          [-8.3318e-03, -1.5759e-02, -1.2061e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 9.9578e-03,  7.4573e-03, -1.8738e-03],\n",
            "          [-1.7752e-03, -6.8015e-04, -7.4443e-03],\n",
            "          [-1.8319e-02, -1.4264e-02, -7.1446e-03]],\n",
            "\n",
            "         [[ 7.8524e-03, -2.6520e-03, -1.7556e-02],\n",
            "          [ 4.5240e-03, -4.8661e-03, -1.5215e-02],\n",
            "          [-5.0211e-03, -1.1864e-02, -1.4846e-02]],\n",
            "\n",
            "         [[ 2.9163e-02,  1.0344e-02,  2.4736e-02],\n",
            "          [ 1.2012e-02, -1.0346e-02,  3.5472e-03],\n",
            "          [ 8.2238e-03, -1.8237e-02, -5.4892e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-8.8434e-03, -4.3184e-03, -5.7536e-03],\n",
            "          [ 7.7230e-03, -4.1936e-04,  7.7260e-03],\n",
            "          [ 1.3536e-02,  1.5705e-02,  2.0893e-02]],\n",
            "\n",
            "         [[ 1.6743e-03,  1.9720e-03,  2.1567e-02],\n",
            "          [-8.0074e-03, -4.6606e-03,  4.0560e-03],\n",
            "          [-1.6688e-02, -1.3754e-02, -1.1708e-02]],\n",
            "\n",
            "         [[-9.7959e-03, -9.4502e-03, -9.3443e-03],\n",
            "          [ 6.9547e-03, -3.9134e-05,  6.2691e-03],\n",
            "          [-1.3193e-02,  9.3272e-04,  1.4579e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.4963e-03,  5.5133e-04,  1.1571e-02],\n",
            "          [ 1.0174e-02,  1.7889e-03,  1.1035e-02],\n",
            "          [ 7.0212e-03,  1.4651e-03,  1.2769e-03]],\n",
            "\n",
            "         [[-1.3021e-02,  6.4109e-03, -1.5199e-02],\n",
            "          [ 2.4775e-02,  2.1926e-02,  3.3679e-02],\n",
            "          [ 2.6471e-04, -3.0235e-03,  1.1690e-02]],\n",
            "\n",
            "         [[-2.9665e-02, -1.5314e-02, -1.7500e-02],\n",
            "          [-1.8339e-02, -2.0845e-02, -1.5494e-02],\n",
            "          [-1.6086e-03,  1.0831e-02, -1.4309e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-7.7044e-03, -2.1100e-02, -2.2816e-02],\n",
            "          [ 5.7688e-03,  1.9362e-04,  7.7105e-04],\n",
            "          [-6.1357e-03,  9.7275e-03, -2.5464e-03]],\n",
            "\n",
            "         [[ 1.1043e-02,  2.4205e-02,  3.4213e-02],\n",
            "          [ 2.9181e-02,  2.6904e-02,  4.5372e-02],\n",
            "          [-2.1594e-02, -1.1072e-03, -7.8312e-03]],\n",
            "\n",
            "         [[-8.3287e-03, -7.9521e-03, -5.3358e-03],\n",
            "          [-6.2527e-04, -5.3243e-03, -8.6296e-03],\n",
            "          [ 3.6094e-03, -1.2544e-03, -4.3801e-03]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.2587, 0.3073, 0.2595, 0.3223, 0.2662, 0.2652, 0.2575, 0.2660, 0.2766,\n",
            "        0.2414, 0.3045, 0.2853, 0.2821, 0.2880, 0.3094, 0.3444, 0.3155, 0.4129,\n",
            "        0.2110, 0.2903, 0.2496, 0.2601, 0.2967, 0.3033, 0.4152, 0.2719, 0.3661,\n",
            "        0.3251, 0.3898, 0.3346, 0.2753, 0.2712, 0.2414, 0.3351, 0.3394, 0.3167,\n",
            "        0.3360, 0.2666, 0.2109, 0.2705, 0.2587, 0.3070, 0.2720, 0.2316, 0.2885,\n",
            "        0.2884, 0.2955, 0.3057, 0.3043, 0.2596, 0.2673, 0.1929, 0.3136, 0.3593,\n",
            "        0.2622, 0.2931, 0.3295, 0.2514, 0.3208, 0.2798, 0.3259, 0.2939, 0.2390,\n",
            "        0.3105, 0.3471, 0.2812, 0.2148, 0.2997, 0.3061, 0.2740, 0.2791, 0.3790,\n",
            "        0.3592, 0.3247, 0.2995, 0.2735, 0.3356, 0.2703, 0.3255, 0.3127, 0.2783,\n",
            "        0.2702, 0.3900, 0.2942, 0.2899, 0.3461, 0.3432, 0.4685, 0.2634, 0.2553,\n",
            "        0.3019, 0.3961, 0.2742, 0.2995, 0.3858, 0.2785, 0.3212, 0.3109, 0.3642,\n",
            "        0.2193, 0.2643, 0.2333, 0.3151, 0.3102, 0.2936, 0.2374, 0.2419, 0.2976,\n",
            "        0.3335, 0.2619, 0.3984, 0.2721, 0.2718, 0.2678, 0.2757, 0.2445, 0.3508,\n",
            "        0.2174, 0.3309, 0.2653, 0.2564, 0.1748, 0.3177, 0.2751, 0.2067, 0.2905,\n",
            "        0.2762, 0.3329, 0.2738, 0.3224, 0.2199, 0.2997, 0.2206, 0.3213, 0.2760,\n",
            "        0.3927, 0.3174, 0.2698, 0.2988, 0.2610, 0.2550, 0.2788, 0.4445, 0.2862,\n",
            "        0.3606, 0.3279, 0.2869, 0.3294, 0.2244, 0.2338, 0.1754, 0.2318, 0.3186,\n",
            "        0.3322, 0.2255, 0.3041, 0.2837, 0.3276, 0.2392, 0.3668, 0.1971, 0.2946,\n",
            "        0.3613, 0.2736, 0.2554, 0.2860, 0.2511, 0.3490, 0.3253, 0.2934, 0.2027,\n",
            "        0.2580, 0.2200, 0.3089, 0.3074, 0.3332, 0.2943, 0.3375, 0.2330, 0.2611,\n",
            "        0.3383, 0.2837, 0.3546, 0.3093, 0.3791, 0.2197, 0.2648, 0.2830, 0.2587,\n",
            "        0.3588, 0.2830, 0.3971, 0.3194, 0.3066, 0.2754, 0.2647, 0.0970, 0.2182,\n",
            "        0.2334, 0.2624, 0.1829, 0.2933, 0.2747, 0.3001, 0.2996, 0.3107, 0.3256,\n",
            "        0.2940, 0.3901, 0.2790, 0.3030, 0.2838, 0.3010, 0.3044, 0.3479, 0.3087,\n",
            "        0.2611, 0.1958, 0.2941, 0.2558, 0.2889, 0.3148, 0.2516, 0.2664, 0.2862,\n",
            "        0.3940, 0.2933, 0.2781, 0.3796, 0.3022, 0.2583, 0.3021, 0.2784, 0.2967,\n",
            "        0.2994, 0.3856, 0.3277, 0.2587, 0.2539, 0.2824, 0.2634, 0.1489, 0.2205,\n",
            "        0.3929, 0.3401, 0.2717, 0.2789, 0.2917, 0.3177, 0.1992, 0.3684, 0.3120,\n",
            "        0.3201, 0.2810, 0.2302, 0.2779, 0.2865, 0.2858, 0.2713, 0.1601, 0.2496,\n",
            "        0.2895, 0.3154, 0.3443, 0.3285, 0.3444, 0.3251, 0.3235, 0.3375, 0.2282,\n",
            "        0.2128, 0.1795, 0.3077, 0.3005, 0.2775, 0.3054, 0.2914, 0.3535, 0.2871,\n",
            "        0.2669, 0.3961, 0.2674, 0.3898, 0.3183, 0.3242, 0.2789, 0.1911, 0.2569,\n",
            "        0.3427, 0.2464, 0.2778, 0.2098, 0.3019, 0.3145, 0.3271, 0.2914, 0.2619,\n",
            "        0.2643, 0.3039, 0.2520, 0.2099, 0.3643, 0.2915, 0.1957, 0.3286, 0.2355,\n",
            "        0.3210, 0.2982, 0.3388, 0.3450, 0.3716, 0.2898, 0.2846, 0.2805, 0.2219,\n",
            "        0.2910, 0.2681, 0.3163, 0.1964, 0.3176, 0.3092, 0.2706, 0.2505, 0.2508,\n",
            "        0.3166, 0.3583, 0.1563, 0.2608, 0.2892, 0.3401, 0.2891, 0.3126, 0.2172,\n",
            "        0.2459, 0.2651, 0.4052, 0.2986, 0.3026, 0.3773, 0.2262, 0.2675, 0.2900,\n",
            "        0.3759, 0.3201, 0.2567, 0.3443, 0.2348, 0.3057, 0.2347, 0.3277, 0.2938,\n",
            "        0.2746, 0.2805, 0.2421, 0.3590, 0.2622, 0.2773, 0.2396, 0.2134, 0.2727,\n",
            "        0.2984, 0.2744, 0.2591, 0.2628, 0.3568, 0.2009, 0.3220, 0.2868, 0.2561,\n",
            "        0.3113, 0.2138, 0.3136, 0.2745, 0.3046, 0.3042, 0.1972, 0.2815, 0.2542,\n",
            "        0.2983, 0.2613, 0.2668, 0.3142, 0.2930, 0.3800, 0.1966, 0.2948, 0.3363,\n",
            "        0.2713, 0.3625, 0.2909, 0.2695, 0.3111, 0.3242, 0.3009, 0.3231, 0.3051,\n",
            "        0.2012, 0.2716, 0.3692, 0.2694, 0.1481, 0.2858, 0.2819, 0.2391, 0.2867,\n",
            "        0.3466, 0.3431, 0.2365, 0.3357, 0.1685, 0.2925, 0.3092, 0.3127, 0.1883,\n",
            "        0.2561, 0.3086, 0.1732, 0.2989, 0.3235, 0.2693, 0.2630, 0.2913, 0.2786,\n",
            "        0.3124, 0.3098, 0.2695, 0.2403, 0.2906, 0.2784, 0.2654, 0.3485, 0.3939,\n",
            "        0.3033, 0.3145, 0.2622, 0.1540, 0.2790, 0.2967, 0.1954, 0.2632, 0.2957,\n",
            "        0.2581, 0.3231, 0.2795, 0.2859, 0.3139, 0.2488, 0.2404, 0.3714, 0.2649,\n",
            "        0.2267, 0.2878, 0.3462, 0.3063, 0.3180, 0.1726, 0.3153, 0.2625, 0.3020,\n",
            "        0.2996, 0.3632, 0.1541, 0.3192, 0.2200, 0.2894, 0.2622, 0.2534, 0.2935,\n",
            "        0.3208, 0.2231, 0.2743, 0.3023, 0.2829, 0.2394, 0.2506, 0.3512, 0.3366,\n",
            "        0.2666, 0.2930, 0.3049, 0.2321, 0.3397, 0.2727, 0.2900, 0.3146, 0.2682,\n",
            "        0.3094, 0.3718, 0.3387, 0.3202, 0.2423, 0.2745, 0.2966, 0.2500, 0.2329,\n",
            "        0.3419, 0.2928, 0.3536, 0.3739, 0.1935, 0.2670, 0.2846, 0.2583, 0.3783,\n",
            "        0.2826, 0.2929, 0.2728, 0.3645, 0.2770, 0.2756, 0.2523, 0.2500],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1668, -0.3019, -0.2187, -0.2917, -0.1971, -0.2325, -0.1869, -0.1857,\n",
            "        -0.2474, -0.1629, -0.2448, -0.2508, -0.1895, -0.2651, -0.3250, -0.3811,\n",
            "        -0.2953, -0.4963, -0.0294, -0.2724, -0.2007, -0.2220, -0.2945, -0.2579,\n",
            "        -0.5152, -0.1994, -0.5016, -0.2736, -0.4528, -0.3968, -0.2281, -0.1772,\n",
            "        -0.1293, -0.2655, -0.3252, -0.3232, -0.3337, -0.1901, -0.0692, -0.2196,\n",
            "        -0.2132, -0.2565, -0.1646, -0.1567, -0.2087, -0.2178, -0.2480, -0.2767,\n",
            "        -0.3071, -0.1988, -0.1985, -0.0235, -0.2458, -0.4156, -0.1660, -0.1923,\n",
            "        -0.3328, -0.1481, -0.3047, -0.2277, -0.3182, -0.2744, -0.1643, -0.3365,\n",
            "        -0.4050, -0.2082, -0.0621, -0.2671, -0.2809, -0.2185, -0.2148, -0.4465,\n",
            "        -0.3376, -0.3213, -0.2921, -0.1998, -0.3369, -0.2092, -0.2831, -0.2893,\n",
            "        -0.1719, -0.2189, -0.4016, -0.2484, -0.2070, -0.3849, -0.3753, -0.5874,\n",
            "        -0.1637, -0.1748, -0.2217, -0.5067, -0.2496, -0.2117, -0.4291, -0.1944,\n",
            "        -0.3089, -0.2621, -0.4096, -0.0602, -0.2009, -0.1316, -0.3336, -0.2627,\n",
            "        -0.2320, -0.0910, -0.1560, -0.2889, -0.3286, -0.1628, -0.5128, -0.2036,\n",
            "        -0.1726, -0.1844, -0.2285, -0.1925, -0.3432, -0.0929, -0.3138, -0.1912,\n",
            "        -0.1926, -0.0342, -0.3268, -0.1699, -0.0828, -0.2417, -0.2069, -0.3870,\n",
            "        -0.2210, -0.2867, -0.0526, -0.3092, -0.0655, -0.2594, -0.2160, -0.5062,\n",
            "        -0.2905, -0.2125, -0.3124, -0.2128, -0.1946, -0.2520, -0.5475, -0.2321,\n",
            "        -0.3350, -0.3473, -0.2158, -0.3603, -0.0759, -0.1472, -0.0327, -0.1404,\n",
            "        -0.3128, -0.3063, -0.1120, -0.2664, -0.2700, -0.3112, -0.1519, -0.3843,\n",
            "        -0.0645, -0.2373, -0.4227, -0.2546, -0.1611, -0.2350, -0.1524, -0.3494,\n",
            "        -0.3453, -0.2081, -0.0918, -0.2025, -0.1246, -0.2533, -0.2768, -0.3156,\n",
            "        -0.2530, -0.3957, -0.0981, -0.1257, -0.3697, -0.2333, -0.3664, -0.2829,\n",
            "        -0.4320, -0.0836, -0.1583, -0.2395, -0.1818, -0.4408, -0.2376, -0.4450,\n",
            "        -0.3232, -0.2787, -0.1858, -0.2137,  0.0481, -0.1058, -0.1093, -0.2035,\n",
            "        -0.0496, -0.2117, -0.1598, -0.2389, -0.2830, -0.2878, -0.3406, -0.2560,\n",
            "        -0.4468, -0.2444, -0.2492, -0.2222, -0.2792, -0.3005, -0.4180, -0.2568,\n",
            "        -0.1872, -0.0270, -0.2645, -0.1873, -0.3022, -0.3400, -0.1803, -0.1810,\n",
            "        -0.2079, -0.4775, -0.2047, -0.1878, -0.4504, -0.2516, -0.1657, -0.2765,\n",
            "        -0.2329, -0.2446, -0.2956, -0.4163, -0.2816, -0.1571, -0.2199, -0.2125,\n",
            "        -0.1684,  0.0356, -0.0914, -0.4484, -0.3535, -0.2212, -0.2550, -0.2509,\n",
            "        -0.2702, -0.0599, -0.3505, -0.2924, -0.2360, -0.2339, -0.1259, -0.2597,\n",
            "        -0.2267, -0.1978, -0.1371, -0.0129, -0.1175, -0.2527, -0.3099, -0.3231,\n",
            "        -0.3468, -0.3553, -0.3537, -0.3315, -0.3713, -0.1091, -0.0959, -0.0258,\n",
            "        -0.2756, -0.2808, -0.2012, -0.2812, -0.1991, -0.3948, -0.2257, -0.2469,\n",
            "        -0.4211, -0.2110, -0.4670, -0.3069, -0.3549, -0.2337, -0.0612, -0.1321,\n",
            "        -0.2968, -0.1870, -0.2316, -0.0686, -0.3113, -0.2895, -0.3149, -0.2686,\n",
            "        -0.2081, -0.2096, -0.3011, -0.1810, -0.0227, -0.3873, -0.2665, -0.0225,\n",
            "        -0.2973, -0.0973, -0.2980, -0.3219, -0.2926, -0.3196, -0.4332, -0.1980,\n",
            "        -0.2117, -0.2302, -0.0980, -0.2344, -0.2154, -0.2921, -0.0350, -0.3361,\n",
            "        -0.2620, -0.2188, -0.1566, -0.1795, -0.2726, -0.4103,  0.0413, -0.1507,\n",
            "        -0.2552, -0.3137, -0.2466, -0.2961, -0.0938, -0.1481, -0.2129, -0.5480,\n",
            "        -0.2915, -0.2802, -0.5077, -0.1306, -0.1862, -0.2400, -0.4362, -0.3017,\n",
            "        -0.1633, -0.3447, -0.1047, -0.2846, -0.1244, -0.3036, -0.2404, -0.2333,\n",
            "        -0.2494, -0.1866, -0.3294, -0.1677, -0.2540, -0.1295, -0.0512, -0.1966,\n",
            "        -0.2801, -0.1702, -0.1879, -0.1850, -0.3274, -0.0369, -0.2979, -0.2612,\n",
            "        -0.1889, -0.3270, -0.1377, -0.2787, -0.2201, -0.2417, -0.2834, -0.0555,\n",
            "        -0.2538, -0.1040, -0.2660, -0.1644, -0.1723, -0.2672, -0.2797, -0.4214,\n",
            "        -0.0378, -0.2386, -0.3498, -0.2435, -0.4348, -0.2554, -0.1719, -0.2836,\n",
            "        -0.3316, -0.2787, -0.2879, -0.2640, -0.0560, -0.1789, -0.4195, -0.2152,\n",
            "         0.0567, -0.2359, -0.2249, -0.0911, -0.2644, -0.3875, -0.3317, -0.1415,\n",
            "        -0.3425, -0.0020, -0.1941, -0.2821, -0.2809, -0.0965, -0.1841, -0.2971,\n",
            "        -0.0173, -0.3043, -0.3013, -0.1729, -0.1872, -0.2683, -0.2033, -0.3059,\n",
            "        -0.2939, -0.2163, -0.1889, -0.2581, -0.2296, -0.2066, -0.3462, -0.4298,\n",
            "        -0.2600, -0.3095, -0.1800, -0.0116, -0.2124, -0.2552, -0.0523, -0.2216,\n",
            "        -0.2605, -0.2134, -0.2867, -0.2556, -0.2275, -0.3437, -0.1698, -0.1560,\n",
            "        -0.4120, -0.2067, -0.1159, -0.2408, -0.3093, -0.2621, -0.2593, -0.0135,\n",
            "        -0.3099, -0.2179, -0.2766, -0.2400, -0.3934,  0.0072, -0.2982, -0.0930,\n",
            "        -0.2166, -0.1635, -0.1827, -0.2308, -0.2525, -0.0991, -0.2325, -0.2938,\n",
            "        -0.2480, -0.0934, -0.1911, -0.3772, -0.3369, -0.1606, -0.2752, -0.3005,\n",
            "        -0.1372, -0.2990, -0.2156, -0.2622, -0.3160, -0.1342, -0.2903, -0.3865,\n",
            "        -0.2916, -0.3243, -0.2051, -0.2656, -0.2359, -0.1508, -0.1063, -0.3595,\n",
            "        -0.2312, -0.3046, -0.4178, -0.0276, -0.2204, -0.2426, -0.1616, -0.4789,\n",
            "        -0.1713, -0.2802, -0.2305, -0.4327, -0.2413, -0.1862, -0.1486, -0.1507],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 2.8729e-04,  4.2632e-03, -2.0266e-03],\n",
            "          [ 1.9513e-04,  2.4381e-03, -5.8632e-03],\n",
            "          [ 4.4803e-03,  8.6577e-03,  8.5538e-04]],\n",
            "\n",
            "         [[-1.1335e-02, -1.3195e-02, -1.0305e-02],\n",
            "          [-4.9507e-03, -4.5898e-03, -3.1041e-03],\n",
            "          [-7.5883e-03, -8.3795e-03, -8.9239e-03]],\n",
            "\n",
            "         [[-1.1914e-02, -1.2104e-02, -1.0167e-02],\n",
            "          [-1.2093e-02, -1.1557e-02, -8.9600e-03],\n",
            "          [-1.2515e-02, -9.3296e-03, -6.4079e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-9.3573e-03, -1.0662e-02, -1.2672e-02],\n",
            "          [-8.0600e-03, -8.5423e-03, -1.2121e-02],\n",
            "          [-8.1498e-03, -8.8037e-03, -1.0611e-02]],\n",
            "\n",
            "         [[ 4.2632e-03,  5.6461e-03,  2.8460e-03],\n",
            "          [ 4.7070e-03,  6.2550e-03,  7.5862e-03],\n",
            "          [ 1.1504e-02,  1.1518e-02,  1.0728e-02]],\n",
            "\n",
            "         [[-6.2455e-03, -9.1693e-03, -9.6664e-03],\n",
            "          [-4.2935e-03, -6.5311e-03, -5.0513e-03],\n",
            "          [-3.1141e-03, -5.0124e-03, -5.8122e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 2.7483e-03,  3.7146e-04,  3.3262e-05],\n",
            "          [-4.5675e-03, -6.6689e-03, -6.4447e-03],\n",
            "          [-6.7610e-03, -7.3204e-03, -9.5855e-03]],\n",
            "\n",
            "         [[-1.4630e-02, -1.2320e-02, -1.4457e-02],\n",
            "          [-8.6197e-03, -5.8059e-03, -1.1075e-02],\n",
            "          [-6.2154e-03, -6.8218e-03, -9.3805e-03]],\n",
            "\n",
            "         [[ 1.0879e-03,  4.3850e-04, -1.9456e-03],\n",
            "          [-1.2517e-03,  3.2917e-04, -2.1435e-03],\n",
            "          [ 4.8136e-03,  2.5333e-03,  5.1504e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.4644e-02,  1.7434e-02,  2.0734e-02],\n",
            "          [ 2.3101e-02,  1.3487e-02,  2.0728e-02],\n",
            "          [ 1.9381e-02,  1.5243e-02,  1.7340e-02]],\n",
            "\n",
            "         [[ 1.2212e-02,  1.2448e-02,  1.5048e-02],\n",
            "          [ 5.2993e-03,  4.0090e-03,  9.3927e-03],\n",
            "          [ 6.6766e-03,  2.4941e-03,  8.3288e-03]],\n",
            "\n",
            "         [[ 3.1040e-02,  2.8243e-02,  3.2319e-02],\n",
            "          [ 3.8608e-02,  3.3099e-02,  3.8652e-02],\n",
            "          [ 2.5839e-02,  2.6524e-02,  2.4995e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.1761e-03,  4.5553e-03,  2.0612e-03],\n",
            "          [ 4.9747e-03,  1.1420e-02,  8.5734e-03],\n",
            "          [ 4.8583e-03,  1.1469e-02,  1.0039e-02]],\n",
            "\n",
            "         [[-6.2547e-05,  6.5336e-04,  9.4747e-04],\n",
            "          [ 5.0603e-03,  7.7136e-03,  6.5484e-03],\n",
            "          [-4.8432e-04,  2.3057e-03,  2.9219e-03]],\n",
            "\n",
            "         [[-3.2788e-02, -2.7615e-02, -3.2608e-02],\n",
            "          [-3.6296e-02, -2.8170e-02, -3.0277e-02],\n",
            "          [-3.6814e-02, -3.1547e-02, -3.0231e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.2998e-03, -2.8590e-04, -4.9266e-03],\n",
            "          [-7.0530e-03, -2.3684e-04, -1.5838e-03],\n",
            "          [-6.9291e-03,  4.8084e-04, -3.1548e-03]],\n",
            "\n",
            "         [[ 1.1854e-02,  8.4836e-03,  1.3839e-02],\n",
            "          [ 2.8741e-03, -9.7358e-05,  4.4888e-03],\n",
            "          [-2.5515e-03, -2.7788e-03, -3.2464e-03]],\n",
            "\n",
            "         [[-1.2408e-02, -1.5001e-02, -1.3377e-02],\n",
            "          [-1.4540e-02, -1.8537e-02, -1.7392e-02],\n",
            "          [-6.7315e-03, -9.5205e-03, -9.0692e-03]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 3.0369e-03,  1.9542e-03,  1.7140e-03],\n",
            "          [-7.6240e-03, -2.8765e-03, -5.1760e-03],\n",
            "          [-9.3019e-03, -4.8800e-03, -4.2932e-03]],\n",
            "\n",
            "         [[ 4.4836e-03,  2.4909e-03,  1.5746e-03],\n",
            "          [ 1.2065e-02,  1.2936e-02,  1.0344e-02],\n",
            "          [ 1.9010e-02,  1.7459e-02,  1.5988e-02]],\n",
            "\n",
            "         [[-1.4914e-03, -8.1727e-03, -8.0671e-03],\n",
            "          [-6.6247e-03, -6.2421e-03, -9.2717e-03],\n",
            "          [-8.7991e-03, -7.7528e-03, -8.6336e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.8040e-02, -1.5366e-02, -1.5334e-02],\n",
            "          [-1.3148e-02, -1.2180e-02, -1.0915e-02],\n",
            "          [-1.4545e-02, -1.4756e-02, -1.1787e-02]],\n",
            "\n",
            "         [[ 3.5762e-03,  6.6073e-03, -1.4055e-03],\n",
            "          [ 4.3975e-03,  7.8375e-03,  8.8085e-05],\n",
            "          [-5.0697e-03, -5.6633e-04, -5.9284e-03]],\n",
            "\n",
            "         [[-1.9234e-03, -8.8012e-03, -5.8821e-03],\n",
            "          [ 3.6685e-03, -1.3784e-03, -3.2117e-03],\n",
            "          [-4.7037e-04,  1.5340e-04, -3.4046e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.8305e-02, -1.7735e-02, -2.1683e-02],\n",
            "          [-1.6598e-02, -1.2508e-02, -2.0530e-02],\n",
            "          [-1.0800e-02, -9.8670e-03, -1.7195e-02]],\n",
            "\n",
            "         [[ 2.0721e-02,  2.2466e-02,  2.5049e-02],\n",
            "          [ 1.8682e-02,  1.3160e-02,  2.3696e-02],\n",
            "          [ 2.2104e-02,  1.7261e-02,  2.4877e-02]],\n",
            "\n",
            "         [[-5.7091e-03, -2.6876e-03, -9.2260e-04],\n",
            "          [-9.4530e-03, -7.0543e-03, -6.2770e-03],\n",
            "          [-4.5806e-03, -2.7182e-03, -2.5823e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.4150e-02,  1.4002e-02,  1.6559e-02],\n",
            "          [ 2.1363e-02,  1.4359e-02,  1.5854e-02],\n",
            "          [ 2.5786e-02,  2.7233e-02,  2.5104e-02]],\n",
            "\n",
            "         [[-4.6450e-03,  1.2419e-03, -1.8768e-03],\n",
            "          [ 1.3005e-03,  4.0888e-03, -6.5483e-04],\n",
            "          [-7.9783e-03, -6.6539e-03, -8.9957e-03]],\n",
            "\n",
            "         [[ 1.1494e-02,  2.6621e-02,  1.5649e-02],\n",
            "          [ 6.5960e-03,  1.7290e-02,  7.5466e-03],\n",
            "          [-8.0256e-03,  4.6246e-03, -5.7808e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 1.4232e-02,  1.1769e-02,  9.4342e-03],\n",
            "          [ 6.2592e-03,  5.1087e-03,  2.3311e-03],\n",
            "          [-1.9694e-03,  2.7110e-03, -2.8945e-03]],\n",
            "\n",
            "         [[-7.0772e-03,  1.0365e-03, -5.8451e-03],\n",
            "          [-9.1879e-03, -3.1388e-03, -8.1517e-03],\n",
            "          [-8.0300e-03, -5.1313e-03, -9.5734e-03]],\n",
            "\n",
            "         [[ 2.4314e-02,  1.8942e-02,  2.4256e-02],\n",
            "          [ 2.0090e-02,  1.1472e-02,  1.5993e-02],\n",
            "          [ 2.2910e-02,  2.0622e-02,  2.3820e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.6375e-02, -1.6928e-02, -1.9019e-02],\n",
            "          [-9.7367e-03, -1.1274e-02, -1.0261e-02],\n",
            "          [-1.2310e-02, -1.5931e-02, -1.4151e-02]],\n",
            "\n",
            "         [[ 4.7098e-03, -4.5205e-04,  2.8042e-03],\n",
            "          [ 2.1428e-03, -4.6175e-03, -1.6818e-03],\n",
            "          [-1.3336e-03, -5.5009e-03, -2.6237e-03]],\n",
            "\n",
            "         [[-1.4367e-02, -1.3520e-02, -1.1387e-02],\n",
            "          [-4.7420e-03, -1.7309e-03, -2.6426e-03],\n",
            "          [ 5.1448e-03,  7.0428e-03,  5.0202e-03]]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([1.8419, 1.8307, 1.7650, 1.8288, 1.9505, 1.8026, 1.9536, 2.2790, 1.7662,\n",
            "        1.8902, 1.7768, 1.7749, 1.9055, 1.7328, 1.8762, 1.8211, 1.7967, 2.3428,\n",
            "        1.7985, 1.7271, 1.7915, 1.9512, 1.8928, 1.9017, 1.8784, 1.9809, 1.8569,\n",
            "        1.7830, 1.8911, 1.8859, 1.7764, 1.9832, 1.8389, 1.7616, 1.8728, 1.8753,\n",
            "        1.9008, 1.8209, 1.7039, 1.7377, 1.7786, 1.6944, 1.7829, 1.7815, 1.7594,\n",
            "        1.8428, 1.9238, 2.0871, 1.8980, 1.8413, 1.8471, 1.8584, 1.7640, 1.8453,\n",
            "        1.7606, 1.9504, 1.9620, 1.8755, 1.9424, 1.8731, 1.8674, 1.9422, 1.8750,\n",
            "        1.9208, 1.7464, 1.8558, 1.6539, 2.0660, 2.0298, 1.9174, 1.8972, 1.7589,\n",
            "        1.7551, 1.9560, 1.7909, 1.7971, 1.7851, 1.7733, 1.8061, 1.7949, 1.8169,\n",
            "        1.8089, 1.8641, 2.1542, 1.7739, 1.7913, 1.8022, 1.7155, 1.7679, 1.7704,\n",
            "        1.6266, 1.8645, 1.9076, 1.8576, 1.6924, 1.8020, 1.7100, 1.7713, 1.8572,\n",
            "        1.7103, 2.0664, 1.9054, 1.9422, 1.8078, 1.7412, 1.6061, 1.9105, 1.8947,\n",
            "        1.7954, 1.8989, 1.8239, 1.7619, 1.7951, 1.8149, 1.8539, 1.8502, 1.7095,\n",
            "        2.1831, 1.8599, 1.8252, 1.8193, 1.8460, 1.7968, 1.6229, 1.8450, 1.8290,\n",
            "        1.8706, 1.9293, 1.6881, 1.9725, 1.8981, 1.8925, 1.8851, 1.8445, 1.9764,\n",
            "        2.0674, 1.8384, 1.8414, 1.8762, 1.7931, 1.7131, 1.9644, 1.7854, 1.9369,\n",
            "        1.8972, 1.8940, 1.8700, 1.7967, 1.8775, 1.9409, 1.7391, 1.7944, 1.9678,\n",
            "        1.7678, 1.6851, 1.9414, 1.9663, 1.9882, 1.7915, 1.8141, 1.8325, 2.1200,\n",
            "        1.9256, 2.3592, 2.0304, 1.9594, 1.7334, 1.9048, 1.8221, 1.7811, 1.9084,\n",
            "        1.8053, 1.9171, 1.9644, 1.8256, 1.6432, 1.9173, 1.9094, 1.9923, 1.7963,\n",
            "        1.9077, 1.7619, 2.1724, 1.7931, 1.7564, 1.8889, 1.9832, 1.9136, 1.8035,\n",
            "        1.8419, 1.8278, 1.8057, 1.9063, 1.8646, 1.7848, 1.8230, 1.7986, 1.7091,\n",
            "        1.7724, 1.7939, 1.7611, 1.9325, 2.0162, 1.7295, 2.0196, 1.8876, 1.8325,\n",
            "        1.8225, 1.7870, 1.9160, 1.7197, 1.7170, 1.9133, 1.7770, 1.9943, 1.8389,\n",
            "        1.8070, 1.8516, 1.7857, 1.9648, 1.9553, 1.9232, 1.8086, 1.8114, 1.7141,\n",
            "        1.8058, 1.8532, 1.9255, 1.7682, 1.8314, 1.8495, 1.8296, 1.8278, 1.8819,\n",
            "        1.7698, 1.7838, 1.7807, 1.9974, 1.6994, 1.9483, 1.7793, 1.8029, 2.2210,\n",
            "        1.6455, 1.8357, 2.1706, 1.9204, 1.7414, 1.7809, 1.8648, 1.9145, 1.8849,\n",
            "        1.8346, 1.9368, 1.8169, 2.2302, 1.8262, 2.0651, 1.9888, 1.8169, 1.8462,\n",
            "        1.9681, 1.8083, 1.8595, 1.8539, 1.7699, 1.9001, 1.7285, 1.7553, 1.8924,\n",
            "        1.7829, 1.9428, 1.8724, 1.7228, 2.0548, 1.7732, 1.8561, 1.7699, 1.9269,\n",
            "        1.8171, 2.4075, 1.7257, 1.7819, 1.7244, 1.8521, 1.8302, 1.8797, 1.7617,\n",
            "        1.9650, 1.9807, 1.7102, 1.7486, 1.8350, 1.9919, 1.8505, 1.9000, 1.8269,\n",
            "        1.9787, 1.7635, 1.6071, 1.7998, 1.9545, 1.7348, 1.7140, 1.8851, 1.7981,\n",
            "        1.9100, 1.8315, 1.7864, 1.9165, 1.8839, 1.9017, 1.9334, 1.7405, 1.7661,\n",
            "        1.8015, 1.9987, 1.7622, 1.9107, 1.8444, 1.7128, 1.8726, 1.8529, 1.9270,\n",
            "        1.8769, 1.7261, 1.8393, 1.9075, 1.7953, 1.8246, 1.7605, 2.0470, 1.9221,\n",
            "        1.9205, 1.8910, 1.7666, 1.6801, 1.8308, 1.8845, 1.8339, 1.8238, 1.7616,\n",
            "        1.6114, 1.8411, 1.7437, 1.8423, 1.9540, 1.7465, 1.7741, 1.8746, 1.8856,\n",
            "        1.7740, 1.7603, 1.7682, 1.8396, 1.6869, 1.8080, 1.8836, 1.8283, 1.8341,\n",
            "        1.8522, 1.9749, 1.8707, 1.7719, 1.8993, 1.8108, 1.8480, 1.8267, 1.8731,\n",
            "        1.9576, 1.8347, 1.9509, 1.9641, 1.7997, 1.7652, 1.9253, 1.7126, 1.7551,\n",
            "        1.9427, 1.8559, 1.9163, 1.7681, 1.7803, 1.8500, 1.8535, 1.8865, 1.7599,\n",
            "        2.0692, 1.8021, 1.7077, 1.8890, 1.9457, 1.8516, 1.7882, 1.8356, 1.8472,\n",
            "        1.6708, 1.7435, 1.9080, 1.9653, 2.0401, 1.8935, 1.8450, 1.7536, 1.7733,\n",
            "        1.8135, 1.8534, 1.9368, 1.7348, 1.8738, 1.9632, 1.9033, 1.7422, 1.7842,\n",
            "        1.8516, 2.0218, 1.7044, 1.8793, 1.8655, 1.8516, 1.8002, 1.8687, 1.8460,\n",
            "        1.7589, 1.8174, 1.9830, 1.9034, 2.1222, 1.8460, 1.9209, 1.8893, 1.9422,\n",
            "        1.8489, 1.8396, 1.9953, 2.0865, 1.8253, 1.7700, 1.8035, 1.7535, 1.8923,\n",
            "        1.8620, 1.8627, 1.7264, 1.8140, 1.9613, 1.8812, 1.8729, 2.0050, 1.7092,\n",
            "        1.7726, 1.9410, 1.8381, 1.8366, 1.7276, 1.8796, 1.7548, 1.9536, 1.8062,\n",
            "        1.8883, 2.0278, 1.8775, 1.9446, 1.8676, 1.8423, 1.7798, 1.9403, 1.8375,\n",
            "        2.0473, 1.9507, 1.8337, 1.8184, 1.7791, 1.8993, 1.8781, 1.8691, 1.8493,\n",
            "        1.7623, 1.9458, 1.7564, 1.7448, 1.8633, 1.6863, 1.8062, 1.8702, 2.0048,\n",
            "        1.8504, 1.8964, 1.9489, 1.8264, 1.9019, 1.8196, 1.9712, 1.8969, 1.8652,\n",
            "        1.8709, 1.6984, 1.8677, 1.8846, 1.9256, 1.8620, 1.6366, 1.8434, 1.7506,\n",
            "        1.8438, 1.5788, 1.9316, 1.9535, 1.7878, 1.7354, 2.0920, 1.9456],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.2371, 0.3433, 0.3279, 0.4642, 0.2233, 0.2370, 0.2176, 0.3793, 0.3140,\n",
            "        0.2803, 0.2434, 0.2116, 0.2478, 0.2435, 0.2298, 0.3172, 0.2725, 0.6511,\n",
            "        0.2925, 0.2281, 0.2279, 0.4254, 0.2342, 0.3328, 0.2632, 0.2176, 0.3180,\n",
            "        0.3893, 0.1387, 0.2274, 0.3379, 0.0767, 0.2253, 0.2504, 0.1990, 0.1951,\n",
            "        0.2566, 0.3253, 0.2797, 0.3149, 0.2373, 0.2533, 0.1956, 0.3236, 0.2093,\n",
            "        0.2333, 0.2300, 0.5019, 0.2830, 0.1885, 0.3264, 0.2722, 0.2369, 0.2430,\n",
            "        0.3625, 0.2165, 0.4700, 0.3047, 0.3675, 0.2641, 0.1979, 0.2664, 0.3448,\n",
            "        0.2005, 0.2450, 0.4351, 0.2689, 0.1632, 0.3087, 0.1209, 0.2153, 0.1592,\n",
            "        0.2960, 0.1423, 0.2951, 0.2706, 0.2007, 0.2939, 0.2210, 0.2243, 0.2465,\n",
            "        0.3910, 0.4599, 0.5417, 0.2147, 0.3469, 0.2703, 0.2229, 0.3645, 0.2647,\n",
            "        0.2421, 0.2492, 0.1666, 0.2763, 0.2560, 0.2151, 0.3363, 0.2767, 0.2516,\n",
            "        0.2988, 0.2622, 0.3499, 0.3001, 0.3907, 0.3184, 0.2233, 0.2649, 0.2110,\n",
            "        0.2034, 0.2752, 0.2314, 0.3480, 0.2238, 0.2892, 0.1991, 0.2923, 0.3259,\n",
            "        0.0722, 0.3039, 0.3041, 0.3803, 0.2568, 0.2382, 0.3057, 0.2652, 0.1532,\n",
            "        0.2110, 0.2567, 0.3148, 0.2746, 0.1833, 0.1950, 0.1116, 0.2279, 0.3705,\n",
            "        0.2477, 0.2000, 0.3060, 0.2548, 0.2468, 0.3028, 0.1921, 0.2952, 0.1980,\n",
            "        0.2135, 0.1583, 0.1586, 0.3944, 0.2352, 0.3947, 0.2740, 0.2861, 0.1856,\n",
            "        0.2702, 0.2986, 0.1728, 0.2658, 0.2696, 0.2028, 0.1838, 0.3176, 0.6246,\n",
            "        0.2631, 0.3855, 0.2074, 0.2317, 0.4171, 0.2044, 0.2926, 0.3506, 0.2305,\n",
            "        0.2400, 0.1420, 0.1093, 0.2757, 0.3253, 0.2334, 0.1650, 0.4026, 0.2066,\n",
            "        0.1790, 0.3032, 0.5658, 0.3246, 0.3834, 0.3254, 0.1772, 0.2909, 0.2350,\n",
            "        0.2519, 0.1968, 0.2003, 0.3213, 0.4802, 0.2543, 0.2578, 0.3280, 0.2270,\n",
            "        0.3044, 0.2273, 0.2447, 0.2527, 0.4136, 0.2588, 0.3589, 0.2688, 0.2115,\n",
            "        0.2022, 0.3186, 0.3740, 0.1785, 0.2074, 0.2346, 0.3566, 0.2623, 0.2620,\n",
            "        0.2880, 0.1462, 0.1896, 0.2777, 0.1852, 0.3240, 0.2748, 0.2164, 0.3066,\n",
            "        0.1845, 0.3992, 0.1695, 0.4411, 0.2812, 0.2730, 0.2784, 0.1861, 0.3589,\n",
            "        0.1934, 0.3320, 0.3350, 0.2655, 0.2740, 0.3185, 0.2633, 0.2458, 0.2003,\n",
            "        0.2809, 0.3049, 0.2050, 0.2904, 0.2381, 0.3278, 0.3484, 0.4293, 0.2422,\n",
            "        0.2859, 0.1864, 0.2954, 0.5634, 0.2081, 0.3743, 0.2902, 0.3820, 0.3069,\n",
            "        0.2101, 0.2750, 0.2878, 0.1870, 0.3015, 0.1661, 0.2998, 0.3101, 0.2522,\n",
            "        0.2419, 0.1758, 0.2681, 0.2812, 0.1495, 0.2868, 0.3157, 0.2587, 0.2437,\n",
            "        0.1467, 0.5416, 0.2490, 0.2831, 0.2783, 0.1614, 0.1963, 0.2034, 0.2364,\n",
            "        0.2527, 0.1573, 0.3184, 0.2841, 0.1613, 0.1489, 0.2850, 0.1625, 0.3277,\n",
            "        0.4936, 0.2780, 0.3178, 0.1743, 0.2158, 0.2222, 0.2821, 0.4267, 0.2713,\n",
            "        0.1778, 0.3067, 0.2270, 0.1772, 0.3897, 0.2923, 0.4843, 0.2345, 0.2327,\n",
            "        0.2740, 0.2700, 0.2804, 0.4035, 0.1501, 0.3329, 0.3286, 0.2803, 0.2309,\n",
            "        0.1738, 0.3270, 0.3097, 0.1808, 0.2384, 0.2107, 0.3240, 0.3346, 0.2236,\n",
            "        0.2061, 0.2687, 0.2360, 0.3338, 0.2694, 0.3203, 0.2895, 0.1884, 0.1491,\n",
            "        0.3957, 0.5167, 0.3407, 0.1854, 0.1816, 0.2626, 0.1855, 0.2219, 0.1482,\n",
            "        0.2584, 0.2458, 0.2616, 0.2396, 0.2402, 0.2423, 0.3463, 0.2731, 0.1524,\n",
            "        0.2514, 0.2760, 0.1734, 0.2715, 0.4052, 0.2252, 0.3676, 0.3070, 0.3127,\n",
            "        0.1836, 0.4330, 0.2203, 0.2073, 0.2803, 0.2984, 0.2191, 0.3272, 0.2267,\n",
            "        0.2749, 0.3056, 0.4566, 0.2962, 0.3528, 0.3236, 0.4220, 0.2715, 0.2256,\n",
            "        0.2903, 0.1829, 0.3994, 0.2820, 0.2471, 0.1647, 0.3654, 0.4504, 0.2685,\n",
            "        0.2992, 0.2825, 0.2435, 0.2212, 0.4300, 0.4342, 0.1988, 0.2863, 0.3398,\n",
            "        0.2444, 0.2905, 0.2559, 0.2586, 0.1702, 0.1906, 0.2536, 0.2978, 0.2498,\n",
            "        0.3777, 0.2252, 0.2472, 0.2243, 0.1732, 0.2194, 0.2091, 0.2820, 0.2898,\n",
            "        0.2887, 0.3292, 0.1644, 0.2962, 0.3279, 0.2535, 0.2795, 0.2238, 0.2607,\n",
            "        0.1937, 0.2680, 0.2418, 0.5193, 0.2502, 0.3147, 0.2166, 0.2313, 0.2027,\n",
            "        0.1880, 0.2180, 0.3826, 0.3871, 0.2358, 0.3556, 0.2272, 0.3272, 0.3442,\n",
            "        0.3154, 0.1993, 0.3135, 0.2254, 0.3048, 0.2658, 0.3337, 0.2679, 0.2670,\n",
            "        0.2363, 0.4347, 0.1931, 0.1995, 0.2072, 0.3202, 0.2667, 0.2305, 0.2383,\n",
            "        0.2246, 0.2562, 0.2837, 0.4046, 0.2786, 0.2243, 0.1591, 0.1923, 0.1894,\n",
            "        0.2496, 0.1140, 0.3128, 0.3197, 0.3530, 0.2999, 0.2115, 0.4718, 0.2979,\n",
            "        0.3472, 0.2890, 0.4740, 0.2230, 0.3630, 0.4015, 0.2446, 0.1897, 0.1460,\n",
            "        0.1874, 0.2734, 0.2366, 0.3001, 0.2359, 0.2688, 0.3256, 0.2749, 0.2848,\n",
            "        0.2299, 0.3001, 0.4818, 0.3074, 0.3164, 0.3114, 0.3549, 0.2859],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.0185, -0.0705, -0.0518,  ..., -0.0390,  0.1735, -0.0410],\n",
            "        [-0.0818, -0.0944,  0.0174,  ...,  0.2028, -0.0248,  0.0372],\n",
            "        [-0.0332, -0.0566, -0.0242,  ..., -0.0344, -0.0227,  0.0197],\n",
            "        ...,\n",
            "        [-0.0103,  0.0033, -0.0359,  ..., -0.0279, -0.0115,  0.0128],\n",
            "        [-0.0359, -0.0353, -0.0296,  ..., -0.0330, -0.0110, -0.0513],\n",
            "        [ 0.0021, -0.0248, -0.0829,  ...,  0.0417, -0.0500,  0.0663]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-2.6341e-03,  3.0005e-03,  6.5581e-04, -2.6909e-02,  6.3637e-03,\n",
            "         1.3260e-02, -1.1178e-02,  2.0639e-02, -3.6373e-03, -1.2325e-02,\n",
            "        -1.2629e-02, -7.2057e-03, -1.9321e-02, -2.4960e-02, -1.1885e-02,\n",
            "        -8.3259e-03, -9.5745e-03, -1.6658e-02,  9.1804e-03, -1.5354e-02,\n",
            "         7.1358e-03,  3.0737e-02,  1.3239e-02, -7.7528e-03,  4.7448e-03,\n",
            "         1.1175e-02,  1.5949e-02, -1.6712e-02, -1.0130e-03, -3.7167e-03,\n",
            "         6.5269e-03, -1.2041e-02,  9.0427e-03, -8.3279e-04,  8.8647e-03,\n",
            "        -2.6307e-02, -1.4588e-02,  2.9433e-03,  2.9718e-03, -1.9125e-02,\n",
            "        -4.7922e-03,  1.3828e-02,  9.8802e-03, -1.8417e-02,  1.9734e-02,\n",
            "         1.6941e-03,  1.2420e-02, -5.5842e-03, -1.0612e-02,  3.9847e-04,\n",
            "         4.2733e-03, -1.3298e-02,  2.0661e-02,  1.6963e-02,  2.7952e-03,\n",
            "         7.4214e-04,  1.3168e-02,  3.2213e-03,  1.0458e-02,  1.6511e-02,\n",
            "         9.1717e-04,  3.9388e-03, -5.6534e-03,  1.9372e-02,  7.5238e-03,\n",
            "         1.3437e-02, -1.3185e-02, -1.0026e-02,  7.1920e-03, -2.3166e-03,\n",
            "        -1.8895e-02,  1.2519e-02,  1.9583e-03,  7.3836e-03, -9.6664e-03,\n",
            "         2.0189e-02,  7.6652e-03,  1.8529e-02,  1.5710e-02,  1.8582e-02,\n",
            "        -6.9314e-03,  1.7090e-02,  9.1268e-03, -3.8876e-02, -2.4116e-02,\n",
            "        -6.8715e-03, -1.1648e-02,  7.8817e-03,  1.8046e-03,  2.8480e-02,\n",
            "        -1.9379e-02, -1.6295e-02,  1.0468e-02, -1.3027e-02, -8.4211e-03,\n",
            "        -2.9210e-02, -2.4856e-03, -8.7141e-03, -1.6397e-02, -9.3054e-03,\n",
            "        -1.5931e-02, -2.6346e-02, -1.1091e-03,  2.2589e-02,  2.1387e-03,\n",
            "        -2.3212e-02, -1.4085e-02, -5.6224e-03, -2.0090e-02, -3.0284e-02,\n",
            "        -4.9574e-02,  2.3283e-02,  1.4954e-02, -7.7501e-03, -3.9482e-03,\n",
            "        -3.7629e-02, -2.4220e-02, -1.0194e-02, -7.7038e-03, -4.1312e-03,\n",
            "        -2.9553e-03, -6.2174e-03, -1.2076e-02, -7.0168e-03, -3.8948e-03,\n",
            "        -1.6953e-02, -2.4585e-02,  5.5353e-03, -8.3370e-03, -7.0759e-03,\n",
            "        -2.4023e-02, -6.3686e-03,  7.3420e-04,  5.2883e-03, -2.2181e-02,\n",
            "        -2.6972e-02, -1.7990e-02, -1.6393e-02,  2.1485e-03, -1.6122e-02,\n",
            "        -1.6112e-02,  6.5931e-03, -2.0045e-02,  6.4149e-03, -1.2601e-02,\n",
            "        -7.6238e-03,  1.1411e-02, -4.5084e-02, -9.2018e-03, -1.5563e-02,\n",
            "        -1.3590e-02, -1.4374e-03, -1.9466e-02,  2.0737e-02, -1.0476e-02,\n",
            "         6.3229e-03,  8.3229e-03, -1.0791e-02, -1.8903e-02,  5.8624e-03,\n",
            "        -2.0189e-03,  3.2436e-02,  4.0581e-02, -4.0820e-05,  1.0886e-02,\n",
            "        -1.6544e-02, -5.3365e-04, -2.2903e-02,  4.6295e-03, -4.8402e-03,\n",
            "         1.0187e-02,  1.7954e-02,  4.8211e-03,  6.1831e-03,  1.4419e-02,\n",
            "        -1.2094e-02, -8.7460e-03,  1.9488e-03,  1.4685e-02,  1.2464e-02,\n",
            "         7.0523e-03, -4.1783e-03,  1.2048e-02, -2.0199e-02,  9.9144e-03,\n",
            "         1.3978e-02, -1.0321e-03,  5.7394e-03,  1.4019e-03,  6.0113e-04,\n",
            "        -5.5790e-04,  2.4424e-02,  2.3076e-02, -1.4610e-02,  1.1185e-02,\n",
            "         3.4608e-02,  1.6944e-02,  4.3295e-03, -2.5606e-02,  1.2279e-02,\n",
            "        -2.5810e-02,  8.5365e-03,  2.0437e-02,  2.2557e-02,  2.2966e-02,\n",
            "         8.8420e-03, -1.3894e-02,  3.8719e-03, -9.3046e-03,  2.3220e-02,\n",
            "        -1.4949e-02,  6.9258e-03,  5.0070e-03, -1.7302e-02,  1.0364e-03,\n",
            "        -1.0223e-02, -9.6949e-03,  3.4534e-02,  6.1337e-03,  1.1582e-02,\n",
            "        -2.0529e-02, -2.1956e-02,  5.3109e-03,  3.4101e-02, -5.8079e-03,\n",
            "         2.9406e-02, -8.7954e-03, -5.2505e-03, -1.9088e-02,  3.0350e-02,\n",
            "         1.8445e-02, -2.1225e-02,  1.8432e-02,  1.3832e-02,  1.7848e-02,\n",
            "        -4.4762e-03,  3.5858e-02,  2.1762e-02,  1.0880e-02,  4.0255e-02,\n",
            "        -2.0049e-03, -3.0348e-03,  9.3293e-03, -1.6304e-02,  9.6253e-04,\n",
            "         1.8673e-02, -1.6567e-02,  1.4964e-02, -3.7206e-03, -7.6734e-03,\n",
            "        -7.9254e-06,  3.9732e-03, -9.5979e-03, -1.6833e-02,  5.8524e-05,\n",
            "        -6.4126e-03,  8.2977e-03,  4.8207e-03, -1.1467e-03,  4.8869e-03,\n",
            "         1.7349e-02,  3.9222e-03, -7.8080e-03,  1.6051e-02,  9.8802e-03,\n",
            "        -1.0144e-02,  2.0912e-02, -6.3203e-03, -2.3139e-02,  1.1646e-03,\n",
            "         2.2468e-02, -6.6953e-03,  1.8311e-02,  1.4623e-02, -1.1654e-02,\n",
            "        -1.4306e-02,  1.2974e-02, -9.6865e-03, -6.2351e-03,  1.3180e-02,\n",
            "         6.7543e-03,  4.6418e-02, -2.7962e-02, -1.5111e-02,  2.8716e-02,\n",
            "         9.1991e-03, -5.3710e-03, -6.0361e-03, -7.2140e-03, -9.2421e-03,\n",
            "         1.8536e-03, -3.1078e-03, -8.4004e-03, -1.6766e-02,  4.0936e-03,\n",
            "         6.2426e-03, -1.2470e-03, -1.2919e-02,  3.5819e-03,  1.1006e-02,\n",
            "        -1.3282e-02,  2.6395e-03,  8.9953e-03,  6.5421e-03, -1.2031e-02,\n",
            "         1.7149e-02,  1.7949e-02, -1.0581e-02, -2.6962e-02, -1.3564e-02,\n",
            "        -9.7173e-03, -2.1176e-03,  3.5370e-02,  1.8392e-02,  2.6676e-02,\n",
            "        -1.0594e-03, -3.3949e-03, -4.8838e-03,  1.3427e-02, -1.3948e-02,\n",
            "        -1.9559e-02, -2.3295e-02, -3.7834e-02, -1.4637e-02, -2.1323e-02,\n",
            "        -3.0952e-02, -3.0822e-02,  1.9438e-03,  2.8637e-03, -2.1198e-02,\n",
            "         1.0448e-02, -1.1316e-02, -4.2609e-03,  2.2647e-02, -1.2867e-02,\n",
            "        -1.1018e-02,  1.2336e-02, -2.0057e-02, -2.1837e-02, -6.8067e-03,\n",
            "        -1.0488e-02, -2.6298e-02, -9.9579e-03, -1.2966e-02, -2.4832e-03,\n",
            "        -7.0940e-03,  1.7997e-02, -6.4257e-03,  7.9069e-03, -1.2287e-02,\n",
            "         9.8176e-03,  2.6674e-03, -2.1524e-02,  2.7511e-03, -9.2075e-03,\n",
            "        -1.7541e-02, -1.7103e-03, -1.4588e-02,  4.4247e-03,  3.4405e-02,\n",
            "         1.2725e-02,  3.0885e-02,  1.3090e-03, -7.0084e-05, -2.3165e-03,\n",
            "        -3.7989e-03, -1.1148e-02,  1.7210e-02, -6.7575e-03, -1.3694e-04,\n",
            "        -8.9166e-03, -1.6281e-02, -4.4920e-03,  1.1332e-02, -1.5909e-03,\n",
            "        -8.8193e-03, -4.9399e-03,  4.5732e-03, -1.0949e-02,  1.2890e-02,\n",
            "        -6.6586e-03, -2.5605e-03,  2.7965e-03,  1.1225e-02, -2.2055e-02,\n",
            "        -3.9271e-03, -6.6467e-03, -1.8840e-02, -2.1687e-02, -7.4066e-04,\n",
            "        -2.7281e-02,  5.0448e-03, -2.0709e-02, -3.4103e-02, -2.2374e-02,\n",
            "        -1.6656e-02, -2.7916e-02, -9.8977e-03,  5.5252e-03,  1.6013e-02,\n",
            "        -1.4895e-02,  3.5091e-03,  9.0003e-03, -8.3982e-03, -3.7479e-02,\n",
            "         2.0727e-02, -5.8799e-03,  9.1768e-03, -2.0297e-02, -7.3148e-03,\n",
            "        -1.6966e-03, -1.4029e-03,  3.2229e-03,  2.9212e-02,  1.2487e-02,\n",
            "        -2.0100e-02,  2.1170e-02, -2.5300e-02,  3.1815e-02, -1.0645e-03,\n",
            "        -1.0449e-02, -2.3419e-02,  1.4564e-02,  2.1245e-02,  1.6530e-02,\n",
            "        -3.2436e-03, -2.0437e-02, -3.6982e-02, -8.7213e-03,  5.4575e-03,\n",
            "         1.1048e-03,  2.2012e-03,  2.9512e-03, -5.9939e-05,  5.5785e-04,\n",
            "        -3.6906e-03,  5.3763e-03, -2.4765e-02,  9.2729e-03,  9.6081e-03,\n",
            "         9.1647e-03,  9.0880e-03,  7.4842e-03, -1.1946e-02,  2.1395e-02,\n",
            "         2.7922e-02,  1.4692e-02, -2.4958e-03,  2.8887e-02,  1.3422e-02,\n",
            "         1.7173e-03,  2.5018e-03, -2.1253e-02, -8.2424e-04,  4.2183e-03,\n",
            "         8.5981e-03,  1.8735e-02,  8.5622e-03, -8.8255e-03,  1.7462e-02,\n",
            "        -1.3693e-02,  2.1955e-03,  1.0772e-02,  2.8693e-03,  3.1032e-02,\n",
            "         8.5460e-03, -1.4198e-02, -2.2472e-03,  1.8740e-02, -1.2905e-02,\n",
            "         4.0370e-02, -7.7538e-04,  1.8671e-03,  7.2793e-03, -2.6508e-02,\n",
            "        -1.7609e-02, -2.4142e-02,  2.9577e-03, -1.5917e-02,  1.6273e-03,\n",
            "         1.1132e-02,  1.4574e-02, -8.1919e-03, -7.6581e-03, -1.8452e-02,\n",
            "        -9.0419e-03,  4.0883e-03,  4.4482e-02, -2.3664e-02, -5.2547e-03,\n",
            "        -1.9529e-02,  3.2860e-03,  5.4667e-03, -4.9558e-03,  7.6805e-03,\n",
            "        -3.3026e-03, -2.6248e-03, -1.1094e-02,  2.3922e-02,  1.8079e-02,\n",
            "        -1.8135e-02,  5.2204e-03, -1.3559e-02,  1.9448e-02,  1.0981e-02,\n",
            "         2.6869e-02, -6.6801e-03, -8.9389e-04, -3.4924e-03, -1.9667e-02,\n",
            "        -1.8511e-02, -7.6262e-04, -1.6382e-02, -1.5862e-02, -1.3717e-02,\n",
            "         1.7528e-02, -1.1419e-03, -6.3346e-03, -1.1118e-02,  1.3159e-02,\n",
            "        -2.3464e-02,  2.7993e-04, -3.6273e-04,  2.3797e-02, -2.7353e-03,\n",
            "        -2.2223e-02,  1.3415e-02,  1.0443e-02, -2.3512e-02,  1.6832e-02,\n",
            "         4.3699e-03, -1.3243e-02, -2.8605e-03,  5.4212e-03,  1.9924e-03,\n",
            "        -6.8664e-04, -3.9092e-04,  1.7806e-02,  1.8391e-02,  2.8473e-02,\n",
            "        -3.3835e-02, -1.0778e-02, -1.2371e-02, -1.9110e-03, -1.6381e-03,\n",
            "         1.7288e-02, -3.9813e-03, -1.5167e-02, -1.0781e-02,  5.3808e-03,\n",
            "        -3.3947e-04,  3.3885e-04, -1.0162e-02, -4.0266e-03, -3.4751e-03,\n",
            "         4.2359e-03, -1.4677e-03,  1.3207e-02,  7.5580e-03,  1.9397e-04,\n",
            "         3.0048e-03,  8.6283e-03, -1.1193e-02,  3.8466e-02, -2.6220e-02,\n",
            "        -2.0251e-02, -6.3872e-03,  2.1906e-02, -7.3400e-03,  5.2753e-03,\n",
            "        -1.1709e-02,  8.4009e-03,  2.8530e-03, -4.7220e-03,  2.3118e-02,\n",
            "        -7.6039e-03,  2.8136e-03, -1.1701e-02, -4.4118e-03,  1.1846e-02,\n",
            "        -1.7632e-03, -1.2260e-02, -2.1210e-03,  1.2072e-02,  6.7523e-03,\n",
            "        -1.9128e-04, -2.5105e-02,  1.2693e-02,  1.6062e-02,  8.1264e-03,\n",
            "         1.3857e-03,  3.0087e-03, -1.4111e-02,  1.9784e-02, -9.2301e-04,\n",
            "        -1.8428e-02,  7.8059e-03,  1.5319e-02, -1.2768e-02, -9.0166e-03,\n",
            "         1.8031e-02,  2.4853e-02,  1.7788e-02,  8.8640e-03, -9.4422e-03,\n",
            "        -1.3652e-03,  1.2932e-02,  9.0133e-03,  1.6655e-02, -5.4321e-03,\n",
            "         2.7480e-02, -3.1781e-02, -1.3331e-02,  5.5792e-03, -1.3278e-02,\n",
            "        -1.9219e-02, -1.3307e-02,  4.2390e-03,  3.0246e-02, -8.1990e-03,\n",
            "         8.3008e-03,  1.8993e-02,  1.0643e-02,  3.1324e-02,  1.9283e-02,\n",
            "         3.3642e-03,  1.9669e-02,  2.2673e-03, -1.9630e-02,  2.0147e-02,\n",
            "        -1.1433e-02, -7.6073e-03,  1.5071e-02, -3.0395e-03, -9.3430e-03,\n",
            "        -4.1657e-03,  2.2972e-03, -5.0985e-03, -1.4499e-02, -2.7673e-02,\n",
            "        -3.8721e-02,  5.4249e-03,  1.3504e-02, -1.2811e-03,  3.7465e-02,\n",
            "         1.5154e-03,  2.4035e-02, -2.0557e-02,  9.8406e-03,  1.0352e-02,\n",
            "         3.8597e-02, -1.1905e-02, -2.1718e-02,  8.3778e-03,  1.4691e-02,\n",
            "         2.2631e-02, -3.7629e-03,  1.5570e-02, -9.3990e-03,  5.3536e-03,\n",
            "         1.9584e-02, -1.1156e-02,  1.5190e-02,  5.4622e-03,  2.2995e-02,\n",
            "         2.9260e-02, -1.5236e-03,  6.6009e-03, -3.1939e-02, -1.0486e-02,\n",
            "        -4.4617e-03,  3.1853e-02,  1.3736e-02,  1.3561e-02,  7.0907e-03,\n",
            "        -1.6753e-02, -2.5470e-02,  1.9752e-02,  2.6715e-02, -4.6859e-03,\n",
            "         1.7682e-02,  3.2496e-02,  1.4553e-02,  2.6101e-02,  1.1341e-02,\n",
            "        -2.2271e-03,  3.5237e-02, -1.1892e-02, -1.8683e-02, -5.5245e-03,\n",
            "        -7.0732e-03, -5.2670e-03,  7.5946e-03, -1.8465e-02, -1.6897e-02,\n",
            "         1.0127e-02,  1.3006e-02, -1.8251e-03,  6.6651e-04, -1.1207e-02,\n",
            "         1.3563e-02, -1.8153e-02, -2.6487e-02,  6.0652e-03,  3.9711e-02,\n",
            "        -1.4285e-02,  1.8001e-02, -1.4039e-02, -1.8762e-02, -1.1778e-02,\n",
            "        -1.6449e-02,  9.0423e-03, -7.2730e-03,  1.7517e-02, -7.3016e-04,\n",
            "         1.0212e-02,  2.3785e-02,  8.1286e-03,  8.0260e-03,  1.1922e-02,\n",
            "         6.2416e-03, -2.4625e-02,  2.9461e-02, -1.4183e-02, -1.8672e-02,\n",
            "        -1.4057e-02,  7.9872e-03, -2.1081e-02, -2.7560e-02, -3.5690e-03,\n",
            "         1.5993e-03,  9.4720e-03,  1.8272e-02, -2.3742e-03,  1.1843e-03,\n",
            "        -5.7722e-04,  8.7818e-03,  2.7804e-03,  6.7973e-04,  1.5877e-02,\n",
            "        -7.0359e-03,  2.5487e-03, -1.7925e-02,  8.4912e-03,  4.3375e-03,\n",
            "         2.4508e-02,  3.6686e-03,  1.0252e-02, -1.3396e-02,  4.5706e-04,\n",
            "         1.0313e-02,  1.5229e-02,  3.9907e-02, -8.0809e-03,  1.3760e-02,\n",
            "        -6.5863e-03,  6.6066e-03, -3.1480e-02,  2.4665e-02,  3.4374e-03,\n",
            "         2.0973e-02,  1.9384e-02, -2.0880e-02,  7.1465e-03,  1.0406e-02,\n",
            "         2.2273e-05, -1.9182e-02,  6.3135e-03, -1.6891e-04,  8.8664e-03,\n",
            "        -4.7666e-03, -1.4493e-02,  3.2176e-03,  7.3346e-03,  2.0694e-02,\n",
            "        -4.9972e-04,  1.8820e-02,  3.9147e-02, -2.7095e-02, -1.8293e-02,\n",
            "        -1.9868e-02, -9.4048e-03,  4.1552e-03,  5.3837e-03, -4.6663e-03,\n",
            "        -1.3019e-02, -2.4452e-02, -8.9231e-03, -1.4603e-02,  2.5529e-03,\n",
            "        -3.0766e-02,  1.1169e-02, -6.8113e-03, -7.5967e-03, -9.3191e-03,\n",
            "         1.4919e-03, -2.3428e-03,  4.4398e-04, -1.0810e-02,  8.8498e-03,\n",
            "        -2.1022e-02, -8.0380e-03, -1.0818e-02, -6.4815e-03, -2.0681e-03,\n",
            "         2.2326e-02, -1.9234e-02,  4.0844e-03,  7.7233e-04,  1.7226e-03,\n",
            "        -1.7454e-02, -1.3190e-02, -7.4112e-03, -1.7550e-03,  1.2926e-03,\n",
            "        -6.7029e-03, -7.0588e-03,  6.2745e-03, -1.8068e-02, -9.4855e-03,\n",
            "        -2.0856e-02,  8.9604e-03,  2.1294e-02,  1.7025e-02,  2.1015e-02,\n",
            "         8.8233e-03, -9.8277e-03, -2.2293e-02,  2.4295e-02, -1.1174e-02,\n",
            "        -7.5753e-03,  6.1182e-03, -2.0653e-02, -1.6264e-02,  2.6457e-02,\n",
            "        -1.4782e-02,  1.8654e-02,  2.5488e-02,  2.4106e-02,  4.7888e-03,\n",
            "         2.3329e-02,  3.5806e-04,  2.5154e-02,  1.7094e-02,  1.7803e-02,\n",
            "         2.4687e-02,  9.0085e-03,  2.3610e-03,  2.6088e-02, -1.4110e-02,\n",
            "        -5.4212e-04,  8.9498e-04,  2.1150e-02,  4.8484e-03, -3.0503e-02,\n",
            "        -7.5025e-03, -3.3718e-02, -2.8913e-02,  1.5691e-02,  6.2047e-03,\n",
            "        -1.0853e-02,  1.9524e-02, -1.6188e-02,  8.9890e-03,  9.1894e-03,\n",
            "        -2.8592e-03, -1.0911e-02,  1.0848e-02,  4.8784e-02, -1.9687e-03,\n",
            "         2.6843e-02, -4.8715e-03,  1.3489e-02, -1.4523e-02, -2.7585e-02,\n",
            "         6.1228e-03,  4.8171e-03,  2.1566e-03, -3.7561e-02,  3.0775e-02,\n",
            "         1.9977e-02,  1.8480e-02,  3.0368e-03,  9.3825e-03,  4.5243e-04,\n",
            "         6.1650e-02, -8.6416e-03, -2.6913e-02,  6.3527e-03,  7.7985e-03,\n",
            "         1.3180e-02, -1.6666e-03,  2.0865e-02,  9.9480e-03,  8.8136e-03,\n",
            "         1.4841e-02,  3.3211e-03,  3.6342e-03,  2.8740e-02, -2.2120e-02,\n",
            "        -7.1567e-03,  1.0352e-02,  1.6433e-02,  1.1683e-02, -5.8058e-03,\n",
            "        -6.9297e-04,  2.6578e-02,  8.7967e-03, -3.1689e-02,  1.8949e-02,\n",
            "        -8.5859e-03,  3.4228e-02, -1.5237e-02, -5.9709e-03,  1.1069e-03,\n",
            "        -1.8394e-02, -1.9246e-02, -3.6361e-02,  3.9839e-03,  4.1237e-02,\n",
            "         1.3816e-02, -7.3304e-03,  3.8832e-03,  2.4367e-03, -2.1625e-02,\n",
            "        -1.4523e-02, -1.6281e-04,  6.2566e-04, -1.6798e-02,  2.3083e-02,\n",
            "         9.7114e-03, -8.2207e-03,  1.1595e-03, -2.0983e-02, -6.6540e-03,\n",
            "        -1.4097e-02,  3.4067e-03, -7.7575e-03, -1.4738e-02, -2.1343e-02,\n",
            "         5.4123e-03,  3.9747e-03, -4.6185e-03, -1.5462e-02, -7.6229e-03,\n",
            "         1.2211e-02, -4.8453e-03, -8.8757e-03, -1.0275e-02,  7.3482e-03,\n",
            "        -6.0349e-03,  2.3658e-03,  2.1053e-02, -8.5688e-03, -1.1630e-02,\n",
            "        -2.7332e-02, -2.0648e-02,  4.4952e-03, -1.8649e-02, -1.1564e-02,\n",
            "         4.5905e-04, -6.1831e-03, -2.4435e-02, -7.1187e-03, -1.4394e-02,\n",
            "        -2.3544e-03,  2.1556e-02,  2.2924e-02, -1.3725e-02,  7.7785e-03,\n",
            "        -8.5513e-03,  2.4221e-02,  3.8192e-03,  7.0947e-04,  1.6114e-02,\n",
            "         2.5932e-02,  1.8108e-02,  2.9306e-02,  1.6773e-03, -3.0166e-03,\n",
            "         3.2015e-02, -1.4034e-02,  2.7365e-02, -1.8858e-02,  2.5832e-03,\n",
            "         1.3498e-02, -1.3502e-02, -1.4940e-02, -1.0904e-02,  1.8642e-02,\n",
            "         4.2593e-03, -1.6742e-02, -1.2638e-02, -4.5468e-02, -5.0823e-03,\n",
            "        -2.5093e-02,  6.7847e-03, -1.7868e-02, -7.8250e-04, -6.3448e-03],\n",
            "       requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace the classifier layer (fully connected layer)\n",
        "num_ftrs = model.fc.in_features\n",
        "print(f'Total Input Feature in Fully Connected Layer: {num_ftrs}')\n",
        "\n",
        "model.fc = nn.Linear(num_ftrs, num_classes)       #Replace output feature 1000 to 10, cause CIFAR10 has 10 classes\n",
        "print(model.fc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFgSXtj9i7m2",
        "outputId": "c23462bb-a604-4856-d62e-20c44d13b0ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Input Feature in Fully Connected Layer: 512\n",
            "Linear(in_features=512, out_features=10, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WG4oxTJdjITv",
        "outputId": "23dbd04b-aa75-41e7-dd0a-5594ba039a0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Print Model summary\n",
        "summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPkyyirJjNbA",
        "outputId": "cd79bd4f-c9a6-486e-8dd6-38b12e553cf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
            "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
            "             ReLU-14           [-1, 64, 56, 56]               0\n",
            "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
            "             ReLU-17           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
            "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
            "             ReLU-21          [-1, 128, 28, 28]               0\n",
            "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
            "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
            "             ReLU-26          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
            "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
            "             ReLU-30          [-1, 128, 28, 28]               0\n",
            "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
            "             ReLU-33          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
            "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
            "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
            "             ReLU-37          [-1, 256, 14, 14]               0\n",
            "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
            "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
            "             ReLU-42          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
            "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
            "             ReLU-46          [-1, 256, 14, 14]               0\n",
            "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
            "             ReLU-49          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
            "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
            "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-53            [-1, 512, 7, 7]               0\n",
            "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
            "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
            "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-58            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
            "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-62            [-1, 512, 7, 7]               0\n",
            "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-65            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
            "           Linear-68                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 11,181,642\n",
            "Trainable params: 11,181,642\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 62.79\n",
            "Params size (MB): 42.65\n",
            "Estimated Total Size (MB): 106.01\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Optimizer and Loss Function\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)     #Only train the classifier layer\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "dRNhsEOqjxJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training Function\n",
        "def train_model(model, train_loader, epochs):\n",
        "  model.train()   #Set the model to training mode\n",
        "  for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in train_loader:\n",
        "      data, target = data.to(device), target.to(device)     #Move the data and target to cuda enabled GPU\n",
        "      optimizer.zero_grad()     #Reset Gradients\n",
        "      output = model(data)\n",
        "      loss = criterion(output, target)\n",
        "      loss.backward()     #Backpropagation(Calculate derivative of loss with respect to parameters)\n",
        "      optimizer.step()    #Update weight\n",
        "      print(f'Loss of Item: {loss.item()}')     # retrieve the scalar value from a loss tensor\n",
        "      print(f'Batch Size of Tensor Data: {data.size(0)}')   #batch size of a tensor data\n",
        "      total_loss += loss.item() * data.size(0)      #Accumulates total loss (not per batch).\n",
        "      pred = output.argmax(dim=1)                   #Gets predicted class from Output(logits).\n",
        "      correct += pred.eq(target).sum().item()       #Counts correct predictions.\n",
        "    print(f'Total Loss: {total_loss}')\n",
        "    print(f'Correct Predictions: {correct}')\n",
        "    avg_loss = total_loss / len(train_loader.dataset)\n",
        "    accuracy = correct / len(train_loader.dataset)\n",
        "\n",
        "    print(f'Epoch: [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "DYJ7btH3j6Xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate the model\n",
        "train_model(model, train_loader, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "RFk20c04kE_1",
        "outputId": "02817c0e-412e-4ca3-c62c-45845888802e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-c737db40a8e2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train and evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-3707d1888d75>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m#Move the data and target to cuda enabled GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m#Reset Gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m#Backpropagation(Calculate derivative of loss with respect to parameters)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         return F.max_pool2d(\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation Function\n",
        "def evaluate_model(model, test_loader):\n",
        "  model.eval()      #Set Model to evaluation mode\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():           #used to disable gradient computation within a block of code\n",
        "    for data, target in test_loader:\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      output = model(data)\n",
        "      print(f'Output: {output}')\n",
        "      loss = criterion(output, target)\n",
        "      test_loss += loss.item() * data.size(0)\n",
        "      pred = output.argmax(dim=1)       #retrieve the indices of the maximum values along a specified dimension in a tensor.\n",
        "      print(f'Prediction: {pred}')\n",
        "      print(f'Target: {target}')\n",
        "      correct += pred.eq(target).sum().item()\n",
        "\n",
        "  avg_loss = test_loss / len(test_loader.dataset)\n",
        "  accuracy = correct / len(test_loader.dataset)\n",
        "\n",
        "  print(f'Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.4f}')\n",
        "\n",
        "\n",
        "#evaluate the model\n",
        "evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "id": "AnSoZRZ9oN2-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}